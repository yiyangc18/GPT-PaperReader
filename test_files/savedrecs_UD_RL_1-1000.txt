PT	AU	BA	CA	GP	RI	OI	BE	Z2	AU	AA	TI	X1	Y1	Z1	FT	PN	AE	Z3	SO	S1	SE	BS	VL	IS	SI	MA	BP	EP	AR	VN	VH	DI	D2	L1	L2	L3	EA	SU	DT	PD	PY	AB	X4	Y4	Z4	AK	CT	CY	SP	CL	TC	Z8	ZR	ZA	ZB	ZS	Z9	U1	U2	SN	EI	BN	G1	NR	CR	LA	AS	AC	CG	DG	C1	C3	EC	DE	DA	UT	PM	
C	Ma, Zibo; Liu, Xudong; Zhang, Liguo						Peng, C; Sun, J				Dissipation of Stop-and-Go Waves of Mixed Autonomous Vehicle Flow with Reinforcement Learning								2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)		Chinese Control Conference						6064	6069											Proceedings Paper	2021	2021	Traffic congestion is a common phenomenon in cities, and improving traffic efficiency has become an urgent problem to be solved. Many researchers have proposed feasible solutions from different perspectives, such as traffic flow, signal lights, etc. This paper proposes a method of dissipating stop-and-go wave based on reinforcement learning (RL). Specifically, we take the mixed autonomous vehicle flow as the research object, using RL to train the driving strategy of connected autonomous vehicle (CAV), and dissipating the stop-and-go waves in vehicle flow by adjusting the CAV's driving behavior. We propose the concept of "Equivalent Density Difference" as a model to describe the difference of traffic flow dynamics before and after a specific vehicle within a certain range, and use this index to design RL model. The proposed method combines the advantages of data-driven and model-driven, improving the training efficiency of RL. Experimental results show that this method can increase the system-level speed and improve the stability of the mixed autonomous vehicle flow.					40th Chinese Control Conference (CCC)40th Chinese Control Conference (CCC)	JUL 26-28, 2021JUL 26-28, 2021	TCCT; CAA; Syst Engn Soc China; Shanghai Univ; Chinese Acad Sci, Acad Math & Syst Sci; China Soc Ind & Appl Math; Asian Control Assoc; IEEE Control Syst Soc; Inst Control, Robot & Syst; Soc Instrument & Control Engineers; Shanghai Jiao Tong Univ; Shanghai Assoc AutomatTCCT; CAA; Syst Engn Soc China; Shanghai Univ; Chinese Acad Sci, Acad Math & Syst Sci; China Soc Ind & Appl Math; Asian Control Assoc; IEEE Control Syst Soc; Inst Control, Robot & Syst; Soc Instrument & Control Engineers; Shanghai Jiao Tong Univ; Shanghai Assoc Automat	Shanghai, PEOPLES R CHINAShanghai, PEOPLES R CHINA	0	0	0	0	0	0	0			2161-2927		978-988-15638-0-4									Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R ChinaMinist Educ, Engn Res Ctr Digital Community, Beijing 100124, Peoples R China	Minist Educ			2021-01-01	WOS:000931046706032		
C	Xia, Yuyang; Liu, Shuncheng; Chen, Xu; Xu, Zhi; Zheng, Kai; Su, Han			ACM							RISE: A Velocity Control Framework with Minimal Impacts based on Reinforcement Learning								PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022								2210	2219				10.1145/3511808.3557435							Proceedings Paper	2022	2022	Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade. However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes. In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle). In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles. To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle. Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles. To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph. Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.					31st ACM International Conference on Information and Knowledge Management (CIKM)31st ACM International Conference on Information and Knowledge Management (CIKM)	OCT 17-21, 2022OCT 17-21, 2022	Assoc Comp MachineryAssoc Comp Machinery	Atlanta, GAAtlanta, GA	0	0	0	0	0	0	0					978-1-4503-9236-5									Univ Elect Sci & Technol China, Chengdu, Peoples R China				2023-11-05	WOS:001074639602018		
C	Naruse, K; Kakazu, Y; Leu, MC				Sokabe, Masahiro/I-1565-2012		Fujimoto, H; DeVor, RE				Modular reinforcement learning for autonomous vehicle navigation in an unknown workspace								1998 JAPAN-U.S.A. SYMPOSIUM ON FLEXIBLE AUTOMATION - PROCEEDINGS, VOLS I AND II								577	583											Proceedings Paper	1998	1998	This paper presents the development of a planning system using modular reinforcement learning for autonomous vehicle navigation in an unknown workspace. The system consists of modules that learn subtasks independently: Each module carries out a subtask of the navigation such as obstacle avoidance or goal seeking. Computational simulations demonstrate that the proposed planning system solves the navigation problem efficiently.					International Conference on New Technical Innovation for the 21st-Century, at the 1998 Japan/US Symposium on Flexible AutomationInternational Conference on New Technical Innovation for the 21st-Century, at the 1998 Japan/US Symposium on Flexible Automation	JUL 12-15, 1998JUL 12-15, 1998	Inst Syst Control & Informat Engineers; Amer Soc Mech Engineers; Japan Soc Mech Engineers; Soc Instrument & Control Engineers Japan; Japan Soc Precis Engn; Robot Soc Japan; IEEE Robot & Automat Soc; Inst Ind Engineers; Shiga Prefecture; Shiga Ind Technol Assoc; Otsu City; Kusatsu City; Kansai United Assoc Ind; Shiga FA Consortium; Japan Automobile Manufactures Assoc Inc; Japan Iron & Steel Federat; Fdn Promot Automat TechnolInst Syst Control & Informat Engineers; Amer Soc Mech Engineers; Japan Soc Mech Engineers; Soc Instrument & Control Engineers Japan; Japan Soc Precis Engn; Robot Soc Japan; IEEE Robot & Automat Soc; Inst Ind Engineers; Shiga Prefecture; Shiga Ind Technol Assoc; Otsu City; Kusatsu City; Kansai United Assoc Ind; Shiga FA Consortium; Japan Automobile Manufactures Assoc Inc; Japan Iron & Steel Federat; Fdn Promot Automat Technol	OTSU, JAPANOTSU, JAPAN	0	0	0	0	0	0	0					*************									Hokkaido Univ, Sapporo, Hokkaido 0608628, Japan				1998-01-01	WOS:000078598200085		
J	Xu Zezhou; Qu Dayi; Hong Jiale; Song Xiaochen							徐泽洲; 曲大义; 洪家乐; 宋晓晨			Research on Decision-making Method for Autonomous Driving Behavior of Connected and Automated Vehicle			智能网联汽车自动驾驶行为决策方法研究				复杂系统与复杂性科学	Complex Systems and Complexity Science				18	3			88	94	1672-3813(2021)18:3<88:ZNWLQC>2.0.TX;2-2										Article	2021	2021	Aiming at the problem of direct conflict between autonomous vehicles and other humandriven vehicles at intersections,an autonomous vehicle behavior decision model is built,and deep reinforcement learning is used to train autonomous vehicles when passing road intersections,allowing autonomous vehicles to make autonomous decisions and achieve fast control of complex scenarios, and the comparison with the non-dominated sorting genetic algorithm-Ⅱverifies the stability of the autonomous vehicle.The simulation results show that the autonomous vehicle behavior decision-making method using the depth deterministic strategy gradient algorithm has better output speed to ensure the smooth changes of the throttle and brake values,and effectively solve the safety and comfort problems of autonomous vehicles.			针对在交叉口自动驾驶车辆与其他车辆直行冲突的问题,构建自动驾驶汽车行为决策模型,采用深度强化学习对自动驾驶汽车通过道路交叉口进行训练,让自动驾驶汽车自主决策学习,实现复杂场景的快速控制,并与非支配排序遗传算法对比验证自动驾驶汽车的稳定性。仿真结果表明采用深度确定性策略梯度算法的自动驾驶车辆行为决策方法能够更好地输出速度确保了油门及刹车值的平稳变化,有效解决了自动驾驶汽车的安全和舒适问题。						1	1	0	0	0	0	2			1672-3813											青岛市城市规划设计研究院;;青岛理工大学, ;;, 青岛;;青岛, 山东;;山东 266071;;266520, 中国青岛理工大学, 青岛, 山东 266520, 中国Institute of Urban Transportation;;Qingdao University of Technology, ;;, Qingdao;;Qingdao, ;; 266071;;266520Qingdao University of Technology, Qingdao, Shandong 266520, China	青岛市城市规划设计研究院;;青岛理工大学青岛理工大学Institute of Urban Transportation;;Qingdao University of TechnologyQingdao University of Technology			2021-09-10	CSCD:6994048		
J	Quang-Duy Tran; Bae, Sang-Hoon					tran quang, duy/0000-0002-3639-2179					An Efficiency Enhancing Methodology for Multiple Autonomous Vehicles in an Urban Network Adopting Deep Reinforcement Learning								APPLIED SCIENCES-BASEL				11	4					1514			10.3390/app11041514							Article	FEB 2021	2021	To reduce the impact of congestion, it is necessary to improve our overall understanding of the influence of the autonomous vehicle. Recently, deep reinforcement learning has become an effective means of solving complex control tasks. Accordingly, we show an advanced deep reinforcement learning that investigates how the leading autonomous vehicles affect the urban network under a mixed-traffic environment. We also suggest a set of hyperparameters for achieving better performance. Firstly, we feed a set of hyperparameters into our deep reinforcement learning agents. Secondly, we investigate the leading autonomous vehicle experiment in the urban network with different autonomous vehicle penetration rates. Thirdly, the advantage of leading autonomous vehicles is evaluated using entire manual vehicle and leading manual vehicle experiments. Finally, the proximal policy optimization with a clipped objective is compared to the proximal policy optimization with an adaptive Kullback-Leibler penalty to verify the superiority of the proposed hyperparameter. We demonstrate that full automation traffic increased the average speed 1.27 times greater compared with the entire manual vehicle experiment. Our proposed method becomes significantly more effective at a higher autonomous vehicle penetration rate. Furthermore, the leading autonomous vehicles could help to mitigate traffic congestion.									7	1	0	0	0	0	8				2076-3417										Pukyong Natl Univ, Smart Transportat Lab, Busan 48513, South Korea				2021-03-29	WOS:000632148800001		
C	Corso, Anthony; Du, Peter; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.			IEEE	Kochenderfer, Mykel/E-7069-2010	Kochenderfer, Mykel/0000-0002-7238-9663					Adaptive Stress Testing with Reward Augmentation for Autonomous Vehicle Validation								2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						163	168											Proceedings Paper	2019	2019	Determining possible failure scenarios is a critical step in the evaluation of autonomous vehicle systems. Real world vehicle testing is commonly employed for autonomous vehicle validation, but the costs and time requirements are high. Consequently, simulation driven methods such as Adaptive Stress Testing (AST) have been proposed to aid in validation. AST formulates the problem of finding the most likely failure scenarios as a Markov decision process, which can be solved using reinforcement learning. In practice, AST tends to find scenarios where failure is unavoidable and tends to repeatedly discover the same types of failures of a system. This work addresses these issues by encoding domain relevant information into the search procedure. With this modification, the AST method discovers a larger and more expressive subset of the failure space when compared to the original AST formulation. We show that our approach is able to identify useful failure scenarios of an autonomous vehicle policy.					IEEE Intelligent Transportation Systems Conference (IEEE-ITSC)IEEE Intelligent Transportation Systems Conference (IEEE-ITSC)	OCT 27-30, 2019OCT 27-30, 2019	IEEE; Intel; Griffith Univ; Auckland Tourism Events & Econ DevIEEE; Intel; Griffith Univ; Auckland Tourism Events & Econ Dev	Auckland, NEW ZEALANDAuckland, NEW ZEALAND	49	4	0	0	0	0	59			2153-0009		978-1-5386-7024-8									Stanford Univ, Aeronaut & Astronaut Dept, Stanford, CA 94305 USAUniv Illinois, Elect & Comp Engn, Urbana, IL USA				2019-01-01	WOS:000521238100026		
C	Arvind, C. S.; Senthilnath, J.			IEEE	Senthilnath, J./L-2949-2013	Senthilnath, J./0000-0002-1737-7985					Autonomous RL: Autonomous Vehicle Obstacle Avoidance in a Dynamic Environment using MLP-SARSA Reinforcement Learning								2019 IEEE 5TH INTERNATIONAL CONFERENCE ON MECHATRONICS SYSTEM AND ROBOTS (ICMSR 2019)								120	124				10.1109/icmsr.2019.8835462							Proceedings Paper	2019	2019	This paper presents a Multi-Layer Perceptron-State Action Reward State Action (MLP-SARSA) based reinforcement learning methodology for dynamic obstacle detection and avoidance for autonomous vehicle navigation. MLP-SARSA is an on-policy reinforcement learning approach, which gains information and rewards from the environment and helps the autonomous vehicle to avoid dynamic moving obstacles. MLP with SARSA provides a significant advantage over dynamic environment compared to other traditional reinforcement algorithms. In this study, a MLP-SARSA model is trained in a complex urban simulation environment with dynamic obstacles using the pygame library. Experimental results show that the trained MLP-SARSA can navigate the autonomous vehicle in a dynamic environment with more confidences than traditional Q-learning and SARSA reinforcement algorithms.					5th IEEE International Conference on Mechatronics System and Robots (ICMSR)5th IEEE International Conference on Mechatronics System and Robots (ICMSR)	MAY 03-05, 2019MAY 03-05, 2019	IEEEIEEE	Singapore, SINGAPORESingapore, SINGAPORE	11	1	0	0	0	0	13					978-1-7281-2223-6									Dr Ambedkar Inst Technol, Dept Comp Sci, Bengaluru, IndiaASTAR, Inst Infocomm Res, Singapore 138632, Singapore	Dr Ambedkar Inst Technol			2019-01-01	WOS:000852945400024		
B	Xinchen Zhang; Jun Zhang; Yuansheng Lin; Longyang Xie						Li Wenzheng; Yang Jixing				Research on Decision Model of Autonomous Vehicle Based on Deep Reinforcement Learning								Proceedings of 2021 IEEE 11th International Conference on Electronics Information and Emergency Communication (ICEIEC)								136	40				10.1109/ICEIEC51955.2021.9463817							Conference Paper	2021	2021	The Deep Q Network (DQN) model has been widely used in autonomous vehicle lane change decision in highway scenes, but the traditional DQN has the problems of overestimation and slow convergence speed. Aiming at these problems, an autonomous vehicle lane changing decision model based on the improved DQN is proposed. First, the obtained state values are input into two neural networks with the same structure and different parameter update frequencies to reduce the correlation between empirical samples, and then the hybrid strategy based on e-greedy and Boltzmann is used to make the vehicles explore the environment. Finally, the model is trained and tested in the experimental scene built by the NGSIM dataset. The experimental results show that the Double Deep Q Network (DDQN) model based on the hybrid strategy improves the success rate of the autonomous vehicle's lane-changing decision and the convergence speed of the network.					2021 IEEE 11th International Conference on Electronics Information and Emergency Communication (ICEIEC)2021 IEEE 11th International Conference on Electronics Information and Emergency Communication (ICEIEC)	18-20 June 202118-20 June 2021		Beijing, ChinaBeijing, China	0	0	0	0	0	0	0					978-1-6654-0385-6									Beijing Key Lab. of Inf. Service Eng., Beijing Union Univ., Beijing, ChinaColl. of Robot., Beijing Union Univ., Beijing, China				2021-09-10	INSPEC:20895420		
B	Xu, G.; Chen, B.; Li, G.; He, X.				He, Xiangkun/ABA-5268-2021	He, Xiangkun/0000-0001-9818-0879; Xu, Guangfei/0000-0002-2439-5321	Wei Xiang; Fengling Han; Tran Khoa Phan				Connected Autonomous Vehicle Platoon Control Through Multi-agent Deep Reinforcement Learning								Broadband Communications, Networks, and Systems: 12th EAI International Conference, BROADNETS 2021, Proceedings. Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering (413)								239	48				10.1007/978-3-030-93479-8_16							Conference Paper	2022	2022	The rise of the artificial intelligence (AI) brings golden opportunity to accelerate the development of the intelligent transportation system (ITS). The platoon control of connected autonomous vehicle (CAV) as the key technology exhibits superior for improving traffic system. However, there still exist some challenges in multi-objective platoon control and multi-agent interaction. Therefore, this paper proposed a connected autonomous vehicle latoon control approach with multi-agent deep reinforcement learning (MADRL). Finally, the results in stochastic mixed traffic flow based on SUMO (simulation of urban mobility) platform demonstrate that the proposed method is feasible, effective and advanced.					International Conference on Broadband Communications, Networks and SystemsInternational Conference on Broadband Communications, Networks and Systems	28-29 Oct. 202128-29 Oct. 2021		Virtual Conference, AustraliaVirtual Conference, Australia	0	0	0	0	0	0	0					978-3-030-93479-8									Shandong Univ. of Technol., Zibo, ChinaLiaocheng Acad. of Agric. Sci., Liaocheng, ChinaBentron Inf. Technol. Co. Ltd., Shenzhen, ChinaGuangxi Univ., Nanning, ChinaSch. of Mech. & Aerosp. Eng., Nanyang Technol. Univ., SingaporeRMIT Univ., Melbourne, VIC, AustraliaLa Trobe Univ., Melbourne, VIC, Australia				2022-11-01	INSPEC:22001259		
C	Kuhnert, KD; Krödel, M			IEEE							Evaluative feedback as the basis for behavior optimization in the area of autonomous vehicle steering								2005 IEEE Intelligent Transportation Systems Conference (ITSC)								671	675											Proceedings Paper	2005	2005	Steering an autonomous vehicle requires the permanent adaptation of behavior in relationship to the various situations the vehicle is in. This paper describes a research which implements such adaptation and optimization based on Reinforcement Learning (RL) which in detail purely learns from evaluative feedback in contrast to instructive feedback. In this way it self-explores and self-optimises actions for situations in a defined environment. The target of this research is to determine to what extent RL-based Systems serve as an enhancement or even an alternative to classical concepts of autonomous intelligent vehicles such as modelling or neural nets.					8th IEEE International Conference on Intelligent Transportation Systems (ITSC 2005)8th IEEE International Conference on Intelligent Transportation Systems (ITSC 2005)	SEP 13-16, 2005SEP 13-16, 2005	IEEEIEEE	Vienna, AUSTRIAVienna, AUSTRIA	0	0	0	0	0	0	0					0-7803-9215-9									Univ Siegen, Inst Real Time Syst, D-57068 Siegen, Germany				2005-01-01	WOS:000234256100116		
J	Rasheed, Iftikhar; Hu, Fei; Zhang, Lin					Rasheed, Iftikhar/0000-0003-1408-8528					Deep reinforcement learning approach for autonomous vehicle systems for maintaining security and safety using LSTM-GAN								VEHICULAR COMMUNICATIONS				26						100266			10.1016/j.vehcom.2020.100266							Article	DEC 2020	2020	The success of autonomous vehicles (AV hs) depends upon the effectiveness of sensors being used and the accuracy of communication links and technologies being employed. But these sensors and communication links have great security and safety concerns as they can be attacked by an adversary to take the control of an autonomous vehicle by influencing their data. Especially during the state estimation process for monitoring of autonomous vehicles' dynamics system, these concerns require immediate and effective solution. In this paper we present a new adversarial deep reinforcement learning algorithm (NDRL) that can be used to maximize the robustness of autonomous vehicle dynamics in the presence of these attacks. In this approach the adversary tries to insert defective data to the autonomous vehicle's sensor readings so that it can disrupt the safe and optimal distance between the autonomous vehicles traveling on the road. The attacker tries to make sure that there is no more safe and optimal distance between the autonomous vehicles, thus it may lead to the road accidents. Further attacker can also add fake data in such a way that it leads to reduced traffic flow on the road. On the other hand, autonomous vehicle will try to defend itself from these types of attacks by maintaining the safe and optimal distance i.e. by minimizing the deviation so that adversary does not succeed in its mission. This attacker-autonomous vehicle action reaction can be studied through the game theory formulation with incorporating the deep learning tools. Each autonomous vehicle will use Long-Short-Term-Memory (LSTM)-Generative Adversarial Network (GAN) models to find out the anticipated distance variation resulting from its actions and input this to the new deep reinforcement learning algorithm (NDRL) which attempts to reduce the variation in distance. Whereas attacker also chooses deep reinforcement learning algorithm (NDRL) and wants to maximize the distance variation between the autonomous vehicles. (c) 2020 Elsevier Inc. All rights reserved.									23	2	0	0	0	0	26			2214-2096											Univ Alabama, Elect & Comp Engn, Tuscaloosa, AL 35487 USA				2021-01-07	WOS:000599317700008		
J	Fu Yihao; Bao Hong; Liang Tianjiao; Fu Dongpu; Pan Feng							付一豪; 鲍泓; 梁天骄; 付东普; 潘峰			Research on decision algorithm for autonomous vehicle lane change based on vision DQN			基于视觉DQN的无人车换道决策算法研究				传感器与微系统	Transducer and Microsystem Technology				42	10			52	55	2096-2436(2023)42:10<52:JYSJDD>2.0.TX;2-H										Article	2023	2023	Unmanned driving technology is current research hotspot in the field of artificial intelligence(AI). Traditional deep Q network(DQN)has problems such as slow convergence speed in the vision-based lane change decision. Aiming at this problem,a vision-based DQN autonomous vehicle lane-changing decision model is proposed. Visual perception based on the attention mechanism is fused with DQN to focus the network on important image features,and introduces the Q-Masking mechanism to reduce the complexity of decision-making and speed up the convergence speed of DQN training. Finally,a DQN-based speed decision algorithm is proposed to form a complete lane changing decision model,which is trained and tested in a simulation environment. The experimental results show that the proposed model can realize a faster lane change decision-making strategy,and accelerate the convergence speed of DQN,at the same time.			无人驾驶技术是当前人工智能(AI)领域的研究热点。针对传统深度Q网络(DQN)在基于视觉的换道决策上存在收敛速度慢等问题,提出了基于视觉的DQN的无人车换道决策模型。将基于注意力机制的视觉感知与DQN融合,使网络聚焦在重要的图像特征上;并引入Q-Masking机制来降低决策复杂度,加快DQN的训练收敛速度。最后,提出基于DQN的速度决策算法来构成一个完整的换道决策模型,并在仿真环境中进行训练与测试。实验结果表明:所提模型可以实现更快的换道决策策略,同时加快了DQN的收敛速度。						0	0	0	0	0	0	0			2096-2436											北京联合大学;;北京联合大学机器人学院, 北京市信息服务工程重点实验室;;, ;;, 北京;;北京 100101;;100027, 中国首都经济贸易大学管理工程学院, 北京 100070, 中国Beijing Union University;;College of Robotics,Beijing Union University, Beijing Key Laboratory of Information Service Engineering;;, ;;, Beijing;;Beijing 100101;;100027School of Management and Engineering,Capital University of Economics and Business, Beijing 100070, China	北京联合大学;;北京联合大学机器人学院首都经济贸易大学管理工程学院Beijing Union University;;College of Robotics,Beijing Union UniversitySchool of Management and Engineering,Capital University of Economics and Business			2024-01-06	CSCD:7576453		
C	Mokhtari, Kasra; Wagner, Alan R.			IEEE							Don't Get into Trouble! Risk-aware Decision-Making for Autonomous Vehicles								2022 31ST IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2022)								1570	1577				10.1109/RO-MAN53752.2022.9900795							Proceedings Paper	2022	2022	Risk is traditionally described as the expected likelihood of an undesirable outcome, such as a collision for an autonomous vehicle. Accurately predicting risk or potentially risky situations is critical for the safe operation of an autonomous vehicle. This work combines use of a controller trained to navigate around individuals in a crowd and a risk-based decision-making framework for an autonomous vehicle that integrates high-level risk-based path planning with a reinforcement learning-based lowlevel control. We evaluated our method using a high-fidelity simulation environment. We show our method results in zero collisions with pedestrians and predicted the least risky path, time to travel, or day to travel in approximately 72% of traversals. This work can improve safety by allowing an autonomous vehicle to one day avoid and react to risky situations.					31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) - Social, Asocial, and Antisocial Robots31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) - Social, Asocial, and Antisocial Robots	AUG 29-SEP 02, 2022AUG 29-SEP 02, 2022	IEEE; LuxAI; Furhat Robot; Navel Robot; PAL Robot; Prensilia s r l; Naver Labs; Paladyn Journal Behav Robot; DIETI UNINA; PERSE; EurAi; IEEE Robot & Automat Soc; Robot Soc Japan; Korea Robot SocIEEE; LuxAI; Furhat Robot; Navel Robot; PAL Robot; Prensilia s r l; Naver Labs; Paladyn Journal Behav Robot; DIETI UNINA; PERSE; EurAi; IEEE Robot & Automat Soc; Robot Soc Japan; Korea Robot Soc	Napoli, ITALYNapoli, ITALY	0	0	0	0	0	0	0					978-1-7281-8859-1									Penn State Univ, Dept Mech Engn, State Coll, PA 16802 USAPenn State Univ, Dept Aerosp Engn, State Coll, PA 16802 USA				2022-12-07	WOS:000885903300222		
J	Bin Issa, Razin; Das, Modhumonty; Rahman, Md. Saferi; Barua, Monika; Rhaman, Md. Khalilur; Ripon, Kazi Shah Nawaz; Alam, Md. Golam Rabiul				Alam, Md Golam Rabiul/L-4373-2013; Ripon, Kazi Shah Nawaz/N-7873-2014	Alam, Md Golam Rabiul/0000-0002-9054-7557; Barua, Monika/0000-0002-0575-259X; Rahman, Md. Saferi/0000-0001-8073-3682; Das, Modhumonty/0000-0002-2265-7412; Ripon, Kazi Shah Nawaz/0000-0002-6551-9714; Issa, Razin Bin/0000-0003-2283-3292					Double Deep Q-Learning and Faster R-CNN-Based Autonomous Vehicle Navigation and Obstacle Avoidance in Dynamic Environment								SENSORS				21	4					1468			10.3390/s21041468							Article	FEB 2021	2021	Autonomous vehicle navigation in an unknown dynamic environment is crucial for both supervised- and Reinforcement Learning-based autonomous maneuvering. The cooperative fusion of these two learning approaches has the potential to be an effective mechanism to tackle indefinite environmental dynamics. Most of the state-of-the-art autonomous vehicle navigation systems are trained on a specific mapped model with familiar environmental dynamics. However, this research focuses on the cooperative fusion of supervised and Reinforcement Learning technologies for autonomous navigation of land vehicles in a dynamic and unknown environment. The Faster R-CNN, a supervised learning approach, identifies the ambient environmental obstacles for untroubled maneuver of the autonomous vehicle. Whereas, the training policies of Double Deep Q-Learning, a Reinforcement Learning approach, enable the autonomous agent to learn effective navigation decisions form the dynamic environment. The proposed model is primarily tested in a gaming environment similar to the real-world. It exhibits the overall efficiency and effectiveness in the maneuver of autonomous land vehicles.									8	0	0	0	0	0	8				1424-8220										BRAC Univ, Sch Data & Sci, Dept Comp Sci & Engn, 66 Mohakhali, Dhaka 1212, BangladeshOstfold Univ Coll, Fac Comp Sci, N-1783 Halden, Norway				2021-03-14	WOS:000624703800001	33672476	
C	Du, Guodong; Zou, Yuan; Zhang, Xudong; Dong, Guoshun; Yin, Xin										Heuristic Reinforcement Learning Based Overtaking Decision for an Autonomous Vehicle								IFAC PAPERSONLINE				54	10			59	66				10.1016/j.ifacol.2021.10.141					NOV 2021		Proceedings Paper; Early Access		2021	This paper proposes an intelligent overtaking decision based on the heuristic reinforcement learning method for an autonomous vehicle. The proposed overtaking control focuses on the safety and efficiency of the autonomous vehicle driving. Firstly, the overtaking problem is modeled and the adaptive safe driving area is constructed. Then, a heuristic reinforcement learning method called Heu-Dyna is developed to derive the optimal overtaking decision, which introduces the heuristic planning function. Besides, the generalized correlation coefficient is designed to evaluate the training perfection of the control strategy. The simulation results show that the performance of the proposed method on the rapidity and optimality is superior to the Q-learning method and the Dyna method. Furthermore, the adaptability of the proposed method is validated by applying different driving conditions. Copyright (C) 2021 The Authors.					6th IFAC Conference on Engine Powertrain Control, Simulation and Modeling (E-COSM)6th IFAC Conference on Engine Powertrain Control, Simulation and Modeling (E-COSM)	AUG 23-25, 2021AUG 23-25, 2021	Int Federat Automat Control, Tech Comm 7 1 Automot Control; Int Federat Automat Control, Tech Comm 1 3 Discrete Event & Hybrid Syst; Int Federat Automat Control, Tech Comm 2 2 Linear Control Syst; Int Federat Automat Control, Tech Comm 2 4 Optimal Control; Int Federat Automat Control, Tech Comm 4 5 Human Machine Syst; Soc Instrument & Control Engineers; Japan Assoc Automat Control; Sophia Univ; MathWorksInt Federat Automat Control, Tech Comm 7 1 Automot Control; Int Federat Automat Control, Tech Comm 1 3 Discrete Event & Hybrid Syst; Int Federat Automat Control, Tech Comm 2 2 Linear Control Syst; Int Federat Automat Control, Tech Comm 2 4 Optimal Control; Int Federat Automat Control, Tech Comm 4 5 Human Machine Syst; Soc Instrument & Control Engineers; Japan Assoc Automat Control; Sophia Univ; MathWorks	Tokyo, JAPANTokyo, JAPAN	4	0	0	0	0	0	4			2405-8963											Beijing Inst Technol, Sch Mech Engn, Beijing, Peoples R China				2021-11-25	WOS:000714394400010		
B	Hsiao-Ting Tseng; Chen-Chiung Hsieh; Wei-Ting Lin; Jyun-Ting Lin										Deep Reinforcement Learning for Collision Avoidance of Autonomous Vehicle								2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)								2 pp.	2 pp.				10.1109/ICCE-Taiwan49838.2020.9258199							Conference Paper	2020	2020	To save training efforts, reinforcement learning approach is applied to the autonomous vehicle for obstacle avoidance. Therefore, this study is aimed to let the autonomous vehicle to learn from mistakes and readdress its movement accuracy for collision avoidance in working environment. An enhanced learning method Q-learning is used to record and update the Q values for different movement through a table that the autonomous vehicle can use it to determine how and where to move. The Q table is learned through the deep learning neural network which may encounter innumerable situations from the environments and the different actions performed by the autonomous vehicle. In the experiments, the depth camera is adopted as the input device to be not affected by light intensity and road color. The Q table is ready to use after 9000 epochs or about 3.5 hours training. Let the autonomous vehicle run for 3 minutes at a time in three different environments with lights on and off 10 times each. The success rate of obstacle avoidance is as high as 95% which proves the feasibility of proposed approach.					2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)	28-30 Sept. 202028-30 Sept. 2020		Taoyuan, TaiwanTaoyuan, Taiwan	0	0	0	0	0	0	0					978-1-7281-7399-3									Dept. of Inf. Manage., Nat. Central Univ., Taoyuan, TaiwanDept. of Comput. Sci. & Eng., Tatung Univ., Taipei, Taiwan				2021-01-15	INSPEC:20198455		
C	Pan, Kai; Kong, Weibo; Guo, Xing			IEEE							A Route Planning for Autonomous Vehicle in 5G and Edge Computing Environment								2022 8TH INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING AND COMMUNICATIONS, BIGCOM								294	303				10.1109/BigCom57025.2022.00044							Proceedings Paper	2022	2022	Route planning have a huge impact on the safe driving and energy efficiency of autonomous vehicle, which is an inevitable part of Internet of Vehicle. In the 5G edge environment, autonomous vehicle needs to communicate with other vehicles and base stations, route planning needs to consider the communication overhead of autonomous vehicle in addition to the driving distance. However, the current route planning methods only consider the shortest path, which cannot comprehensively consider the distance overhead and time cost. To cope with these challenges, this paper proposes an algorithm based on reinforcement learning to solve autonomous vehicle route planning through 5G networks and edge computing(RLVRP), which can obtain the goal of minimizing the driving distance based on minimizing the task processing time and server response time. In addition, we update our route planning strategy according to the dynamic change of network resources. Extensive experimental results show that the proposed algorithm greatly reduces service delay compared with state-of-the-art baselines.					8th International Conference on Big Data Computing and Communications (BigCom)8th International Conference on Big Data Computing and Communications (BigCom)	AUG 06-07, 2022AUG 06-07, 2022		Xiamen, PEOPLES R CHINAXiamen, PEOPLES R CHINA	0	0	0	0	0	0	0					978-1-6654-7384-2									Anhui Univ, Dept Comp Sci & Technol, Hefei, Peoples R China				2023-06-03	WOS:000982340400036		
J	Szoke, Laszlo; Aradi, Szilard; Becsi, Tamas; Gaspar, Peter				Bécsi, Tamás/H-9818-2012; Gaspar, Peter/C-1721-2013; Aradi, Szilard/AAE-2623-2019; Gaspar, Peter/L-3805-2013	Bécsi, Tamás/0000-0002-1487-9672; Aradi, Szilard/0000-0001-6811-2584; Szoke, Laszlo/0000-0001-9926-4054; Gaspar, Peter/0000-0003-3388-1724					Skills to Drive: Successor Features for Autonomous Highway Pilot								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	10			18707	18718				10.1109/TITS.2022.3150493					FEB 2022		Article; Early Access		2022	Reinforcement learning applications are spreading among different domains, including autonomous vehicle control. The diverse situations that can happen during, for instance, at a highway commute are infinite, and with labeled data, the perfect coverage of all use-cases sounds ambitious. However, with the complex tasks and complicated scenarios faced during an autonomous vehicle system design, the credit assignment problem arises. How to construct appropriate objectives for the artificial intelligence to learn and the preferences between the different goals also matter of the designer's choice. This work attempts to tackle the problem by utilizing successor features and providing a possible decomposition of the reward functions, guiding the agent's actions. This method makes the training easier for the agent and enables immediate, profound performance on new combined tasks. Furthermore, with the optimal composition, the desired behavior can be fine-tuned, and as an auxiliary gain, the decomposition empowers different driving styles and makes driving preferences rapidly changeable. We introduce the adaptation of FastRL algorithm to autonomous vehicle domain, meanwhile developing a stabilizing way of using Successor Features, namely DoubleFastRL. We compare our solution for a highway driving scenario with basic agents such as Q-learning having multi-objective training.									2	0	0	0	0	0	2			1524-9050	1558-0016										Budapest Univ Technol & Econ, Dept Control Transportat & Vehicle Syst, H-1111 Budapest, HungaryRobert Bosch Kft, H-1103 Budapest, HungaryInst Comp Sci & Control, Syst & Control Lab, H-1111 Budapest, Hungary				2022-03-12	WOS:000761645200001		
C	Mo, Shuojie; Pei, Xiaofei; Chen, Zhenfu			IEEE							Decision-Making for Oncoming Traffic Overtaking Scenario using Double DQN								2019 3RD CONFERENCE ON VEHICLE CONTROL AND INTELLIGENCE (CVCI)								230	233				10.1109/cvci47823.2019.8951626							Proceedings Paper	2019	2019	Great progress has been made in the field of machine learning in recent years. And learning-based methods have been widely utilized for developing highly autonomous vehicle. To this end, we introduce a reinforcement learning based intelligent autonomous vehicle decision making method for oncoming overtaking scenario. The goal of reinforcement learning is to learn how to take optimal decision in corresponding observations through interactions with the environment using a reward function to estimate whether the decision is good or not. A Double Deep Q-learning (Double DQN) agent was used to learn policies (control strategies) for both longitudinal speed and lane change decision. Prioritized Experience Replay (PER) was used to accelerate convergence of the policies. A two-way 3-car scenario with oncoming traffic was established in SUMO (Simulation of Urban Mobility) to train and test the policies.					3rd Conference on Vehicle Control and Intelligence (CVCI)3rd Conference on Vehicle Control and Intelligence (CVCI)	SEP 21-22, 2019SEP 21-22, 2019	Hefei Univ Technol; Chinese Assoc Automat, Tech Comm Vehicle Control & Intelligence; IEEE Syst Man & Cybernet SocHefei Univ Technol; Chinese Assoc Automat, Tech Comm Vehicle Control & Intelligence; IEEE Syst Man & Cybernet Soc	Hefei, PEOPLES R CHINAHefei, PEOPLES R CHINA	10	0	0	0	0	0	10					978-1-7281-2684-5									Wuhan Univ Technol, Sch Automot Engn, Wuhan, Peoples R China				2020-05-19	WOS:000530747200042		
C	You, Changxi; Lu, Jianbo; Filev, Dimitar; Tsiotras, Panagiotis			IEEE	Lu, Jianbo/Z-1316-2018; You, Changxi/AAQ-8713-2020; You, Changxi/Y-6598-2018	Lu, Jianbo/0000-0001-9088-5663; You, Changxi/0000-0003-0359-7917; Filev, Dimitar/0000-0001-7127-6782					Highway Traffic Modeling and Decision Making for Autonomous Vehicle Using Reinforcement Learning								2018 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						1227	1232											Proceedings Paper	2018	2018	This paper studies the decision making problem of autonomous vehicles in traffic. We model the interaction between an autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an experienced driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. By designing the reward function of the MDP, the desired, driving behavior of the autonomous vehicle is obtained using reinforcement learning. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle.					IEEE Intelligent Vehicles Symposium (IV)IEEE Intelligent Vehicles Symposium (IV)	JUN 26-30, 2018JUN 26-30, 2018	IEEEIEEE	Changshu, PEOPLES R CHINAChangshu, PEOPLES R CHINA	28	4	0	0	0	0	33			1931-0587		978-1-5386-4452-2													2018-01-01	WOS:000719424500193		
J	Jacinto, Edwar; Martinez, Fernando; Martinez, Fredy										Navigation of Autonomous Vehicles using Reinforcement Learning with Generalized Advantage Estimation								INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS				14	1			954	959											Article	JAN 2023	2023	This study proposes a reinforcement learning ap-proach using Generalized Advantage Estimation (GAE) for autonomous vehicle navigation in complex environments. The method is based on the actor-critic framework, where the actor network predicts actions and the critic network estimates state values. GAE is used to compute the advantage of each action, which is then used to update the actor and critic networks. The approach was evaluated in a simulation of an autonomous vehicle navigating through challenging environments and it was found to effectively learn and improve navigation performance over time. The results suggest GAE as a promising direction for further research in autonomous vehicle navigation in complex environments.									1	0	0	0	0	0	1			2158-107X	2156-5570										Univ Dist Francisco Jose de Caldas, Bogota, Colombia				2023-03-15	WOS:000935597000001		
J	Lu, Chengjie; Shi, Yize; Zhang, Huihui; Zhang, Man; Wang, Tiexin; Yue, Tao; Ali, Shaukat				Zhang, Man/AGR-7496-2022; yue, tao/H-6783-2013	Zhang, Man/0000-0003-1204-9322; Yue, Tao/0000-0003-3262-5577; Lu, Chengjie/0000-0002-5818-7547; wang, tiexin/0000-0002-5432-3812; Ali, Shaukat/0000-0002-9979-3519					Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions								IEEE TRANSACTIONS ON SOFTWARE ENGINEERING				49	1			384	402				10.1109/TSE.2022.3150788							Article	JAN 1 2023	2023	Autonomous vehicles must operate safely in their dynamic and continuously-changing environment. However, the operating environment of an autonomous vehicle is complicated and full of various types of uncertainties. Additionally, the operating environment has many configurations, including static and dynamic obstacles with which an autonomous vehicle must avoid collisions. Though various approaches targeting environment configuration for autonomous vehicles have shown promising results, their effectiveness in dealing with a continuous-changing environment is limited. Thus, it is essential to learn realistic environment configurations of continuously-changing environment, under which an autonomous vehicle should be tested regarding its ability to avoid collisions. Featured with agents dynamically interacting with the environment, Reinforcement Learning (RL) has shown great potential in dealing with complicated problems requiring adapting to the environment. To this end, we present an RL-based environment configuration learning approach, i.e., DeepCollision, which intelligently learns environment configurations that lead an autonomous vehicle to crash. DeepCollision employs Deep Q-Learning as the RL solution, and selects collision probability as the safety measure, to construct the reward function. We trained four DeepCollision models and conducted an experiment to compare them with two baselines, i.e., random and greedy. Results show that DeepCollision demonstrated significantly better effectiveness in generating collisions compared with the baselines. We also provide recommendations on configuring DeepCollision with the most suitable time interval based on different road structures.									4	0	0	0	0	0	4			0098-5589	1939-3520										Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R ChinaQilu Univ Technol, Shandong Acad Sci, Sch Comp Sci & Technol, Jinan, Peoples R ChinaWeifang Univ, Weifang 261061, Peoples R ChinaKristiania Univ Coll, N-999026 Oslo, NorwayNanjing Univ Aeronaut & Astronaut, Nanjing 210016, Peoples R ChinaSimula Res Lab, N-999026 Oslo, Norway	Kristiania Univ CollSimula Res Lab			2023-07-20	WOS:001020827200020		
J	Zhiqian Qiao; Tyree, Z.; Mudalige, P.; Schneider, J.; Dolan, J.M.										Hierarchical reinforcement learning method for autonomous vehicle behavior planning [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	9 Nov. 2019	2019	In this work, we propose a hierarchical reinforcement learning (HRL) structure which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals. In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other similar tasks with the same sub-goals. The states are defined as processed observations which are transmitted from the perception system of the autonomous vehicle. A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure. Compared to traditional RL methods, our algorithm is more sample-efficient since its modular design allows reusing the policies of sub-goals across similar tasks. The results show that the proposed method converges to an optimal policy faster than traditional RL methods.									0	0	0	0	0	0	0														Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA				2020-05-01	INSPEC:19488344		
B	Sahal, M.; Hidayat, Z.; Saputra, F.D.; Rizqifadiilah, M.A.; Putra, R.A.										Obstacle Avoidance System on Autonomous Car Using D3QN								2023 14th International Conference on Information & Communication Technology and System (ICTS)								199	204				10.1109/ICTS58770.2023.10330873							Conference Paper	2023	2023	An autonomous vehicle's triumphant and safe navigation, which can circumvent obstacles, necessitates a skill set encompassing steering wheel control, sometimes called obstacle avoidance. One potential approach to address this issue is using a simulation framework wherein an automobile is subjected to various barriers. During this simulation, the sensory input of the car, as well as its corresponding actions, are recorded and analyzed. An alternative approach involves allowing the vehicle to autonomously acquire knowledge to optimize its performance towards the desired objective. The Dueling Deep Double QNetworks (D3QN) approach is a strategy that enables the model to autonomously learn and optimize its performance to attain the most favorable conclusion. The D3QN architecture is a computational framework incorporating Dueling and double-Q processes. The implementation of D3QN is anticipated to result in a reduction in the training time required for an autonomous vehicle. This study is expected to substitute for training an autonomous vehicle.					2023 14th International Conference on Information & Communication Technology and System (ICTS)2023 14th International Conference on Information & Communication Technology and System (ICTS)	20232023		Surabaya, IndonesiaSurabaya, Indonesia	0	0	0	0	0	0	0					979-8-3503-1216-4									Dept. of Electr. Eng., Inst. Teknologi Sepuluh Nopember, Surabaya, Indonesia				2024-01-06	INSPEC:24216214		
B	Cheng, Y.; Li, W.; Duan, F.; Li, S.E.										Discrete-time Finite Horizon Adaptive Dynamic Programming for Autonomous Vehicle Control								2022 IEEE International Conference on Unmanned Systems (ICUS)								424	9				10.1109/ICUS55513.2022.9987042							Conference Paper	2022	2022	Nowadays, the autonomous vehicle control has become more and more critical, especially for the trajectory tracking problem. The existing model predictive control method applied to autonomous vehicle can reduce the tracking error. However, it requires online calculation for the model recurrence in long horizon, which causes heavy computational burden. This paper presents a novel discrete-time model predictive control solving framework. A finite horizon adaptive dynamic programming method was proposed to estimate the control policy for trajectory tracking. Under the constraint of self consistent condition, the control policy is fitted through deep neural network as a parameterized function to obtain faster solution. The performance of the proposed method is tested and verified under double lane change conditions via CarSim. The experimental results show that the system can accurately track given reference trajectory with 5000Hz frequency, which is three times faster than that solved by least squares estimation method.					2022 IEEE International Conference on Unmanned Systems (ICUS)2022 IEEE International Conference on Unmanned Systems (ICUS)	28-30 Oct. 202228-30 Oct. 2022	IEEE; CICCIEEE; CICC	Guangzhou, ChinaGuangzhou, China	0	0	0	0	0	0	0					978-1-6654-8456-5									Coll. of Artificial Intelligence, Nankai Univ., Tianjin, ChinaSch. of Vehicle & Mobility, Tsinghua Univ., Beijing, China				2023-03-22	INSPEC:22473755		
C	Tseng, Hsiao-Ting; Hsieh, Chen-Chiung; Lin, Wei-Ting; Lin, Jyun-Ting			IEEE	Tseng, Hsiao-Ting/E-3346-2013	Tseng, Hsiao-Ting/0000-0002-8289-5236					Deep Reinforcement Learning for Collision Avoidance of Autonomous Vehicle								2020 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN (ICCE-TAIWAN)		IEEE International Conference on Consumer Electronics-Taiwan											10.1109/icce-taiwan49838.2020.9258199							Proceedings Paper	2020	2020	To save training efforts, reinforcement learning approach is applied to the autonomous vehicle for obstacle avoidance. Therefore, this study is aimed to let the autonomous vehicle to learn from mistakes and readdress its movement accuracy for collision avoidance in working environment. An enhanced learning method Q-learning is used to record and update the Q values for different movement through a table that the autonomous vehicle can use it to determine how and where to move. The Q table is learned through the deep learning neural network which may encounter innumerable situations from the environments and the different actions performed by the autonomous vehicle. In the experiments, the depth camera is adopted as the input device to be not affected by light intensity and road color. The Q table is ready to use after 9000 epochs or about 3.5 hours training. Let the autonomous vehicle run for 3 minutes at a time in three different environments with lights on and off 10 times each. The success rate of obstacle avoidance is as high as 95% which proves the feasibility of proposed approach.					7th IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)7th IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)	SEP 28-30, 2020SEP 28-30, 2020	Inst Elect & Elect Engineers; Taiwan Consumer Elect Soc; IEEE Consumer Technol Soc; Natl Cent Univ; Minist Sci & Technol; Bur Foreign Trade, Minist Econ AffairsInst Elect & Elect Engineers; Taiwan Consumer Elect Soc; IEEE Consumer Technol Soc; Natl Cent Univ; Minist Sci & Technol; Bur Foreign Trade, Minist Econ Affairs	Taoyuan, TAIWANTaoyuan, TAIWAN	1	0	0	0	0	0	1			2381-5779		978-1-7281-7399-3									Natl Cent Univ, Dept Informat Management, Taoyuan, TaiwanTatung Univ, Dept Comp Sci & Engn, Taipei, Taiwan				2020-01-01	WOS:000648532300183		
J	Amini, Alexander; Gilitschenski, Igor; Phillips, Jacob; Moseyko, Julia; Banerjee, Rohan; Karaman, Sertac; Rus, Daniela					Rus, Daniela/0000-0001-5473-3566; Gilitschenski, Igor/0000-0001-6426-365X; Phillips, Jacob/0000-0002-3276-7914; Moseyko, Julia/0000-0002-4977-4918; /0000-0002-9673-1267					Learning Robust Control Policies for End-to-End Autonomous Driving From Data-Driven Simulation								IEEE ROBOTICS AND AUTOMATION LETTERS				5	2			1143	1150				10.1109/LRA.2020.2966414							Article	APR 2020	2020	In this work, we present a data-driven simulation and training engine capable of learning end-to-end autonomous vehicle control policies using only sparse rewards. By leveraging real, human-collected trajectories through an environment, we render novel training data that allows virtual agents to drive along a continuum of new local trajectories consistent with the road appearance and semantics, each with a different view of the scene. We demonstrate the ability of policies learned within our simulator to generalize to and navigate in previously unseen real-world roads, without access to any human control labels during training. Our results validate the learned policy onboard a full-scale autonomous vehicle, including in previously un-encountered scenarios, such as new roads and novel, complex, near-crash situations. Our methods are scalable, leverage reinforcement learning, and apply broadly to situations requiring effective perception and robust operation in the physical world.									81	2	0	0	1	0	89			2377-3766											MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02142 USAMIT, Lab Informat & Decis Syst, Cambridge, MA 02139 USA				2020-04-01	WOS:000526691200001		
J	Huang, Kaisi; Wen, Mingyun; Park, Jisun; Sung, Yunsick; Park, Jong Hyuk; Cho, Kyungeun				Park, Jong Hyuk/AHD-6698-2022						Enhanced Image Preprocessing Method for an Autonomous Vehicle Agent System								COMPUTER SCIENCE AND INFORMATION SYSTEMS				18	2			461	479				10.2298/CSIS200212005H							Article	APR 2021	2021	Excessive training time is a major issue face when training autonomous vehicle agents with neural networks by using images as input. This paper proposes a deep time-economical Q network (DQN) input image preprocessing method to train an autonomous vehicle agent in a virtual environment. The environmental information is extracted from the virtual environment. A top-view image of the entire environment is then redrawn according to the environmental information. During training of the DQN model, the top-view image is cropped to place the vehicle agent at the center of the cropped image. The current frame top-view image is combined with the images from the previous two training iterations. The DQN model use this combined image as input. The experimental results indicate higher performance and shorter training time for the DQN model trained with the preprocessed images compared with that trained without preprocessing.									1	0	0	0	0	0	1			1820-0214											Dongguk Univ, Dept Multimedia Engn, 30,Pildongro 1 Gil, Seoul 04620, South KoreaSeoul Natl Univ Sci & Technol SeoulTech, Dept Comp Sci & Engn, 232 Gongneung Ro, Seoul 01811, South Korea				2021-04-26	WOS:000637996700006		
J	Likas, A; Blekas, K										A reinforcement learning approach based on the fuzzy min-max neural network								NEURAL PROCESSING LETTERS				4	3			167	172				10.1007/BF00426025							Article	DEC 1996	1996	The fuzzy min-max neural network constitutes a neural architecture that is based on hyperbox fuzzy sets and can be incrementally trained by appropriately adjusting the number of hyperboxes and their corresponding volumes. Two versions have been proposed: for supervised and unsupervised learning. In this paper a modified approach is presented that is appropriate for reinforcement learning problems with discrete action space and is applied to the difficult task of autonomous vehicle navigation when no a priori knowledge of the enivronment is available. Experimental results indicate that the proposed reinforcement learning network exhibits superior learning behavior compared to conventional reinforcement schemes.									9	0	0	0	0	0	9			1370-4621											UNIV IOANNINA,DEPT COMP SCI,GR-45110 IOANNINA,GREECENATL TECH UNIV ATHENS,DEPT ELECT & COMP ENGN,DIV COMP SCI,GR-15773 ZOGRAFOS,ATHENS,GREECE				1996-12-01	WOS:A1996WN61100006		
J	Li, Nan; Oyler, Dave W.; Zhang, Mengxuan; Yildiz, Yildiray; Kolmanovsky, Ilya; Girard, Anouck R.				Yildiz, Yildiray/AAK-8370-2021; Li, Nan/Q-5511-2019	Yildiz, Yildiray/0000-0001-6270-5354; Li, Nan/0000-0001-7928-8796; Kolmanovsky, Ilya/0000-0002-7225-4160					Game Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems								IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY				26	5			1782	1797				10.1109/TCST.2017.2723574							Article	SEP 2018	2018	Autonomous driving has been the subject of increased interest in recent years both in industry and in academia. Serious efforts are being pursued to address legal, technical, and logistical problems and make autonomous cars a viable option for everyday transportation. One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience. Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where interactions among multiple drivers and vehicles occur simultaneously. Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help to decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests. In this paper, we present a game theoretic traffic model that can be used to: 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system. We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.									143	6	0	0	3	0	163			1063-6536	1558-0865										Univ Michigan, Dept Aerosp Engn, Ann Arbor, MI 48109 USABilkent Univ, Dept Mech Engn, TR-06800 Ankara, Turkey				2018-09-01	WOS:000441448600020		
B	Liu, Z.; Gao, H.; Ma, H.; Cai, S.; Hu, Y.; Qu, T.; Chen, H.; Gong, X.										Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation								2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)								1609	14				10.1109/IROS55552.2023.10341750							Conference Paper	2023	2023	Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.					2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	20232023		Detroit, MI, USADetroit, MI, USA	0	0	0	0	0	0	0					978-1-6654-9190-7									Sch. of Artificial Intelligence China & Eng. Res. Center of Knowledge-Driven Human-Machine Intelligence, Jilin Univ., Jilin, ChinaColl. of Commun. Eng., Jilin Univ., Changchun, ChinaState Key Lab. of Automotive Simulation & Control, Jilin Univ., Changchun, ChinaSch. of Electron. & Inf. Eng., Tongji Univ., Shanghai, China				2024-01-11	INSPEC:24257443		
S	Zhang, Xinchen; Zhang, Jun; Lin, Yuansheng; Xie, Longyang						Wenzheng, L; Jixing, Y				Research on Decision Model of Autonomous Vehicle Based on Deep Reinforcement Learning								PROCEEDINGS OF 2021 IEEE 11TH INTERNATIONAL CONFERENCE ON ELECTRONICS INFORMATION AND EMERGENCY COMMUNICATION (ICEIEC 2021)		IEEE International Conference on Electronics Information and Emergency Communication						131	135				10.1109/ICEIEC51955.2021.9463817							Article; Proceedings Paper	2021	2021	The Deep Q Network (DQN) model has been widely used in autonomous vehicle lane change decision in highway scenes, but the traditional DQN has the problems of overestimation and slow convergence speed. Aiming at these problems, an autonomous vehicle lane changing decision model based on the improved DQN is proposed. First, the obtained state values are input into two neural networks with the same structure and different parameter update frequencies to reduce the correlation between empirical samples, and then the hybrid strategy based on s-greedy and Boltzmann is used to make the vehicles explore the environment. Finally, the model is trained and tested in the experimental scene built by the NGSIM dataset. The experimental results show that the Double Deep Q Network (DDQN) model based on the hybrid strategy improves the success rate of the autonomous vehicle's lane-changing decision and the convergence speed of the network.					11th IEEE International Conference on Electronics Information and Emergency Communication (ICEIEC)11th IEEE International Conference on Electronics Information and Emergency Communication (ICEIEC)	JUN 18-20, 2021JUN 18-20, 2021	IEEE; IEEE Beijing SectIEEE; IEEE Beijing Sect	Beijing, PEOPLES R CHINABeijing, PEOPLES R CHINA	0	0	0	0	0	0	0			2377-8431		978-0-7381-1135-3									Beijing Union Univ, Beijing Key Lab Informat Serv Engn, Beijing, Peoples R ChinaBeijing Union Univ, Coll Robot, Beijing, Peoples R China				2021-11-21	WOS:000717504300029		
J	Kontoravdis, D.; Likas, A.; Stafylopatis, A.										Collision-free movement of an autonomous vehicle using reinforcement learning								ECAI 92. 10th European Conference on Artificial Intelligence Proceedings								666	70											Conference Paper	1992	1992	Explores the potential use of reinforcement learning in control applications. Reinforcement learning systems are of particular interest because they require as training feedback only a scalar signal provided to the entire neural network and they admit a simple on-line implementation. The authors demonstrate the ability of a reinforcement learning adaptive controller to drive an autonomous vehicle through simulated paths comprising left and right turns. The neural network is responsible for providing the proper control commands so that the vehicle stays on the road and avoids collision.					ECAI 92. 10th European Conference on Artificial IntelligenceECAI 92. 10th European Conference on Artificial Intelligence	3-7 Aug. 19923-7 Aug. 1992		Vienna, AustriaVienna, Austria	1	0	0	0	0	0	1														Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece				1992-01-01	INSPEC:4493695		
B	Jie Xu; Xiaofei Pei; Kexuan Lv										Decision-Making for Complex Scenario using Safe Reinforcement Learning								Proceedings of the 2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	6				10.1109/CVCI51460.2020.9338584							Conference Paper	2020	2020	In recent years, machine learning is widely used in many fields. Compared with the rule-based method, machine learning plays a more excellent role in the decision-making of the autonomous vehicle. Some complex situations are often met in our daily life. To this end, Safe reinforcement learning(RL) is introduced to ensure that safer actions are selected. Constant Turn Rate and Acceleration(CTRA) model is first used to predict the future trajectories of surrounding vehicles. Then Double Deep Q-Learning(DDQN) method is used to make decisions and ensure the autonomous vehicle can move at the desired speed as much as possible. In order to achieve a safer decision-making, some safety rules are introduced. Finally, the algorithm is demonstrated in Simulation of Urban Mobility(SUMO) and has been proved to have an outstanding performance on such a complex scenario.					2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)	18-20 Dec. 202018-20 Dec. 2020	IEEE Syst. Man & Cybern. Soc.IEEE Syst. Man & Cybern. Soc.	Hangzhou, ChinaHangzhou, China	3	0	0	0	0	0	3					978-1-7281-8497-5									Sch. of Autonomous Eng., Wuhan Univ. of Techonology, Wuhan, China				2021-04-03	INSPEC:20424333		
J	Hyun Jae Cho; Behl, M.										Towards Automated Safety Coverage and Testing for Autonomous Vehicles with Reinforcement Learning [arXiv]								arXiv								16 pp.	16 pp.											Journal Paper	22 May 2020	2020	The kind of closed-loop verification likely to be required for autonomous vehicle (AV) safety testing is beyond the reach of traditional test methodologies and discrete verification. Validation puts the autonomous vehicle system to the test in scenarios or situations that the system would likely encounter in everyday driving after its release. These scenarios can either be controlled directly in a physical (closed-course proving ground) or virtual (simulation of predefined scenarios) environment, or they can arise spontaneously during operation in the real world (open-road testing or simulation of randomly generated scenarios). In AV testing, simulation serves primarily two purposes: to assist the development of a robust autonomous vehicle and to test and validate the AV before release. A challenge arises from the sheer number of scenario variations that can be constructed from each of the above sources due to the high number of variables involved (most of which are continuous). Even with continuous variables discretized, the possible number of combinations becomes practically infeasible to test. To overcome this challenge we propose using reinforcement learning (RL) to generate failure examples and unexpected traffic situations for the AV software implementation. Although reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving.									0	0	0	0	0	0	0																		2020-08-17	INSPEC:19784678		
J	You, Changxi; Lu, Jianbo; Filev, Dimitar; Tsiotras, Panagiotis				You, Changxi/Y-6598-2018; You, Changxi/AAQ-8713-2020	You, Changxi/0000-0003-0359-7917; 					Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning								ROBOTICS AND AUTONOMOUS SYSTEMS				114				1	18				10.1016/j.robot.2019.01.003							Article	APR 2019	2019	Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion. They represent the main trend in future intelligent transportation systems. This paper concentrates on the planning problem of autonomous vehicles in traffic. We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MOP) and consider the driving style of an expert driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques. Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning. The unknown reward function of the expert driver is approximated using a deep neural-network (DNN). We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques. (C) 2019 Elsevier B.V. All rights reserved.									127	11	1	0	2	0	150			0921-8890	1872-793X										Georgia Inst Technol, Sch Aerosp Engn, Atlanta, GA 30332 USAFord Motor Co, Res & Adv Engn, Dearborn, MI 48121 USAGeorgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA				2019-03-28	WOS:000461268200001		
B	Xia, Y.; Liu, S.; Hu, R.; Yu, Q.; Feng, X.; Zheng, K.; Su, H.						Wang, X.; Sapino, M.L.; Han, W.-S.; El Abbadi, A.; Dobbie, G.; Feng, Z.; Shao, Y.; Yin, H.				SMART: A Decision-Making Framework with Multi-modality Fusion for Autonomous Driving Based on Reinforcement Learning								Database Systems for Advanced Applications: 28th International Conference, DASFAA 2023, Proceedings. Lecture Notes in Computer Science (13946)								447	62				10.1007/978-3-031-30678-5_33							Conference Paper	2023	2023	Decision-making in autonomous driving is an emerging technology that has rapid progress over the last decade. In single-lane scenarios, autonomous vehicles should simultaneously optimize their velocity decisions and steering angle decisions to achieve safety, efficiency, comfort, small impacts on rear vehicles, and small offsets to the lane center line. Previous studies, however, have typically optimized these two decisions separately, ignoring the potential relationship between them. In this work, we propose a decision-making framework, named SMART (deci S ion- M aking fr A mework based on R einforcemen T learning), to optimize the velocity and steering angle of the autonomous vehicle in parallel. In order for the autonomous vehicle to effectively perceive the curvature of the lane and interactions with other vehicles, we adopt a graph attention mechanism to extract and fuse the features from different modalities (i.e., sensor-collected vehicle states and camera-collected lane information). Then a hybrid reward function takes into account aspects of safety, efficiency, comfort, impact, and lane centering to instruct the autonomous vehicle to make optimal decisions. Furthermore, our framework enables the autonomous vehicle to adaptively choose the duration of an action, which helps the autonomous vehicle pursue higher reward values. Extensive experiments evidence that SMART significantly outperforms the existing methods in multiple metrics.					International Conference on Database Systems for Advanced ApplicationsInternational Conference on Database Systems for Advanced Applications	17-20 April 202317-20 April 2023		Tianjin, ChinaTianjin, China	0	0	0	0	0	0	0					978-3-031-30678-5									Sch. of Comput. Sci. & Eng., Univ. of Electron. Sci. & Technol. of China, Chengdu, ChinaYangtze Delta Region Inst. (Quzhou), Univ. of Electron. Sci. & Technol. of China, Chengdu, ChinaShenzhen Inst. for Adv. Study, Univ. of Electron. Sci. & Technol. of China, Chengdu, ChinaTianjin Univ., Tianjin, ChinaUniv. of Torino, Turin, ItalyPOSTECH, Pohang, South KoreaUniv. of California Santa Barbara, Santa Barbara, CA, USAUniv. of Auckland, Auckland, New ZealandBeijing Univ. of Posts & Telecommun., Beijing, ChinaUniv. of Queensland, Brisbane, QLD, Australia				2023-08-03	INSPEC:23473422		
J	Cabezas-Olivenza, Mireya; Zulueta, Ekaitz; Sanchez-Chica, Ander; Fernandez-Gamiz, Unai; Teso-Fz-Betono, Adrian				; Zulueta, Ekaitz/B-8028-2009	Cabezas Olivenza, Mireya/0000-0001-5025-1711; Sanchez-Chica, Ander/0000-0002-0271-6825; Zulueta, Ekaitz/0000-0001-6062-9343					Stability Analysis for Autonomous Vehicle Navigation Trained over Deep Deterministic Policy Gradient								MATHEMATICS				11	1					132			10.3390/math11010132							Article	JAN 2023	2023	The Deep Deterministic Policy Gradient (DDPG) algorithm is a reinforcement learning algorithm that combines Q-learning with a policy. Nevertheless, this algorithm generates failures that are not well understood. Rather than looking for those errors, this study presents a way to evaluate the suitability of the results obtained. Using the purpose of autonomous vehicle navigation, the DDPG algorithm is applied, obtaining an agent capable of generating trajectories. This agent is evaluated in terms of stability through the Lyapunov function, verifying if the proposed navigation objectives are achieved. The reward function of the DDPG is used because it is unknown if the neural networks of the actor and the critic are correctly trained. Two agents are obtained, and a comparison is performed between them in terms of stability, demonstrating that the Lyapunov function can be used as an evaluation method for agents obtained by the DDPG algorithm. Verifying the stability at a fixed future horizon, it is possible to determine whether the obtained agent is valid and can be used as a vehicle controller, so a task-satisfaction assessment can be performed. Furthermore, the proposed analysis is an indication of which parts of the navigation area are insufficient in training terms.									0	0	0	0	0	0	0				2227-7390										Univ Basque Country UPV EHU, Syst Engn & Automat Control Dept, Nieves Cano 12, Vitoria 01006, SpainUniv Basque Country UPV EHU, Dept Nucl & Fluid Mech, Nieves Cano 12, Vitoria 01006, Spain				2023-01-25	WOS:000909352100001		
J	Chalaki, B.; Beaver, L.; Remer, B.; Jang, K.; Vinitsky, E.; Bayen, A.M.; Malikopoulos, A.A.										Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	12 March 2019	2019	In this paper, we demonstrate the first successful zero-shot transfer of an autonomous driving policy directly from simulator to a scaled autonomous vehicle under stochastic disturbances. Using adversarial multi-agent reinforcement learning, in which an adversary perturbed both the inputs and outputs of the autonomous vehicle during training, we train an autonomous vehicle to manage a roundabout merge in the presence of an adversary in simulation. At test time in hardware, the adversarial policy successfully reproduces the simulated ramp metering behavior and outperforms both a human driving baseline and adversary-free trained policies. Finally, we demonstrate that the addition of adversarial training considerably improves the stability and robustness of policies being transferred to the real world. Supplementary information and videos can be found at: (https://sites.google.com/view/ud-ids-lab/arlv).									0	0	0	0	0	0	0														Dept. of Mech. Eng., Univ. of Delaware, Newark, DE, USADept. of Electr. Eng. & Comput. Sci., Univ. of California, Berkeley, Berkeley, CA, USADept. of Mech. Eng., Univ. of California, Berkeley, Berkeley, CA, USA				2019-08-02	INSPEC:18802987		
B	Emamifar, M.; Ghoreishi, S.F.										Uncertainty-Aware Reinforcement Learning for Safe Control of Autonomous Vehicles in Signalized Intersections								2023 IEEE Conference on Artificial Intelligence (CAI)								81	2				10.1109/CAI54212.2023.00042							Conference Paper	2023	2023	This paper proposes a reinforcement learning approach for the control of autonomous vehicles at signalized intersections. The proposed method is a modified version of the Q-learning approach that takes into account the risky scenarios that might arise in the control of an autonomous vehicle due to the inherent uncertainties in the system. The proposed algorithm enables robust and risk-aware decision-making in uncertain and sensitive environments. The proposed algorithm is evaluated in a simulated autonomous vehicle scenario, where it outperforms the standard Q-learning in terms of safety.					2023 IEEE Conference on Artificial Intelligence (CAI)2023 IEEE Conference on Artificial Intelligence (CAI)	5-6 June 20235-6 June 2023		Santa Clara, CA, USASanta Clara, CA, USA	0	0	0	0	0	0	0					979-8-3503-3984-0									Coll. of Eng., Northeastern Univ., Shenyang, ChinaColl. of Eng. & Khoury Coll. of Comput. Sci., Northeastern Univ., Shenyang, China				2023-08-17	INSPEC:23517284		
R	De Abreu, Ricardo										Anti-lock braking systems								Figshare													https://doi.org/10.25403/UPresearchdata.20363601.v1							Data set	2022-09-01	2022	The data provided summarises the model used to describe the dynamics of the anti-lock braking system. This includes the specifics of the model-free control method. The results of the systems are provided as well. Copyright: CC BY 4.0									0	0	0	0	0	0	0																		2022-09-29	DRCI:DATA2022148024897469		
C	Qiao, Zhiqian; Tyree, Zachariah; Mudalige, Priyantha; Schneider, Jeff; Dolan, John M.			IEEE							Hierarchical Reinforcement Learning Method for Autonomous Vehicle Behavior Planning								2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						6084	6089				10.1109/IROS45743.2020.9341496							Proceedings Paper	2020	2020	Behavioral decision making is an important aspect of autonomous vehicles (AV). In this work, we propose a behavior planning structure based on hierarchical reinforcement learning (HRL) which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals. In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other tasks with the same sub-goals. A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure. Compared to traditional RL methods, our algorithm is more sample-efficient, since its modular design allows reusing the policies of sub-goals across similar tasks for various transportation scenarios. The results show that the proposed method converges to an optimal policy faster than traditional RL methods.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	OCT 24-JAN 24, 2020-2021OCT 24-JAN 24, 2020-2021	IEEE; RSJIEEE; RSJ	ELECTR NETWORKELECTR NETWORK	15	1	0	0	0	0	16			2153-0858		978-1-7281-6212-6									Carnegie Mellon Univ, Elect & Comp Engn, Pittsburgh, PA 15213 USAGen Motors, Res & Dev, Detroit, MI USACarnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA				2021-12-03	WOS:000714033803107		
J	Crewe, Jacob; Humnabadkar, Aditya; Liu, Yonghuai; Ahmed, Amr; Behera, Ardhendu				Liu, Yonghuai/ABF-3794-2020; Behera, Ardhendu/O-2213-2015	Liu, Yonghuai/0000-0002-3774-2134; Crewe, Jacob/0009-0007-6029-6164; Behera, Ardhendu/0000-0003-0276-9000					SLAV-Sim: A Framework for Self-Learning Autonomous Vehicle Simulation								SENSORS				23	20					8649			10.3390/s23208649							Article	OCT 2023	2023	With the advent of autonomous vehicles, sensors and algorithm testing have become crucial parts of the autonomous vehicle development cycle. Having access to real-world sensors and vehicles is a dream for researchers and small-scale original equipment manufacturers (OEMs) due to the software and hardware development life-cycle duration and high costs. Therefore, simulator-based virtual testing has gained traction over the years as the preferred testing method due to its low cost, efficiency, and effectiveness in executing a wide range of testing scenarios. Companies like ANSYS and NVIDIA have come up with robust simulators, and open-source simulators such as CARLA have also populated the market. However, there is a lack of lightweight and simple simulators catering to specific test cases. In this paper, we introduce the SLAV-Sim, a lightweight simulator that specifically trains the behaviour of a self-learning autonomous vehicle. This simulator has been created using the Unity engine and provides an end-to-end virtual testing framework for different reinforcement learning (RL) algorithms in a variety of scenarios using camera sensors and raycasts.									0	0	0	0	0	0	0				1424-8220										Edge Hill Univ, Dept Comp Sci, Ormskirk L39 4QP, England				2023-11-19	WOS:001095439700001	37896742	
B	Liu, S.; Chang, P.; Chen, H.; Chakraborty, N.; Driggs-Campbell, K.										Learning to Navigate Intersections with Unsupervised Driver Trait Inference								2022 International Conference on Robotics and Automation (ICRA)								3576	82				10.1109/ICRA46639.2022.9811635							Conference Paper	2022	2022	Navigation through uncontrolled intersections is one of the key challenges for autonomous vehicles. Identifying the subtle differences in hidden traits of other drivers can bring significant benefits when navigating in such environments. We propose an unsupervised method for inferring driver traits such as driving styles from observed vehicle trajectories. We use a variational autoencoder with recurrent neural networks to learn a latent representation of traits without any ground truth trait labels. Then, we use this trait representation to learn a policy for an autonomous vehicle to navigate through a T-intersection with deep reinforcement learning. Our pipeline enables the autonomous vehicle to adjust its actions when dealing with drivers of different traits to ensure safety and efficiency. Our method demonstrates promising performance and outperforms state-of-the-art baselines in the T-intersection scenario.					2022 IEEE International Conference on Robotics and Automation (ICRA)2022 IEEE International Conference on Robotics and Automation (ICRA)	23-27 May 202223-27 May 2022		Philadelphia, PA, USAPhiladelphia, PA, USA	1	0	0	0	0	0	1					978-1-7281-9681-7									Dept. of Electr. & Comput. Eng., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA				2022-10-20	INSPEC:22115868		
C	Ni, Jiayang; Ma, Rubing; Zhong, Hua; Wang, Bo				zhong, hua/JRW-4786-2023		Li, Z; Sun, J				Autonomous Vehicles Roundup Strategy by Reinforcement Learning with Prediction Trajectory								2022 41ST CHINESE CONTROL CONFERENCE (CCC)		Chinese Control Conference						3370	3375											Proceedings Paper	2022	2022	Autonomous vehicles are increasingly applied on many situations, but their autonomous decision-making ability needs to be improved. Multi-Agent Deep Deterministic Policy Gradient(MADDPG) adopts the method of centralized evaluation and decentralized execution, so that the autonomous vehicle can obtain the whole-field status information and make decisions through the companion information. In the process of autonomous vehicle training, we introduce artificial potential field, action guidance and other methods to alleviate the problem of sparse rewards. At the same time, we add a repulsion function to consider the relationship between team vehicles. Extended Kalman Filter(EKF) is also applied to predict the autonomous vehicle trajectory, changing the training network state input information. At the same time, secondary correction of the predicted autonomous vehicle trajectory is made to change the prediction range with the training time, and improve the training convergence speed while the speed of opposite agents increases. Simulation experiments show that the convergence speed and win rate ofMADDPG algorithm based on trajectory prediction and artificial potential field is significantly improved, and it also has strong adaptability to various task scenarios.					41st Chinese Control Conference (CCC)41st Chinese Control Conference (CCC)	JUL 25-27, 2022JUL 25-27, 2022	Chinese Assoc Automat, Tech Comm Control Theory; Syst Engn Soc China; Univ Sci & Technol China; Chinese Acad Sci, Acad Math & Syst Sci; China Soc Ind & Appl Math; Asian Control Assoc; IEEE Control Syst Soc; Inst Control, Robot & Syst; Soc Instrument & Control Engineers; Hefei Univ Technol; Anhui Univ; Univ Sci & Technol China, Inst Adv Technol; Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence; Anhui Prov Assoc Automat; Anhui Robot AssocChinese Assoc Automat, Tech Comm Control Theory; Syst Engn Soc China; Univ Sci & Technol China; Chinese Acad Sci, Acad Math & Syst Sci; China Soc Ind & Appl Math; Asian Control Assoc; IEEE Control Syst Soc; Inst Control, Robot & Syst; Soc Instrument & Control Engineers; Hefei Univ Technol; Anhui Univ; Univ Sci & Technol China, Inst Adv Technol; Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence; Anhui Prov Assoc Automat; Anhui Robot Assoc	Hefei, PEOPLES R CHINAHefei, PEOPLES R CHINA	0	0	0	0	0	0	0			2161-2927		978-988-75815-3-6									Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R ChinaBeijing Aerosp Control Ctr, Beijing 100094, Peoples R China	Beijing Aerosp Control Ctr			2023-04-16	WOS:000932071603083		
C	Cabaneros, Alex; Angulo, Cecilio				Angulo, Cecilio/A-7953-2010	Angulo, Cecilio/0000-0001-9589-8199	SabaterMir, J; Torra, V; Aguilo, I; GonzalezHidalgo, M				A Comparison of Autonomous Vehicle Navigation Simulators Under Regulatory and Reinforcement Learning Constraints								ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT		Frontiers in Artificial Intelligence and Applications		319				115	124				10.3233/FAIA190114							Proceedings Paper	2019	2019	The transition from conventional vehicles to autonomous vehicles is regulated thorough ADAS (Advanced Driver Assistance Systems) functionalities. The combination of different ADAS functions allows vehicles navigate on a highway autonomously, but at the same time, following the traffic rules and regulations requirements, and also guaranteeing safety on the road. The practical objective in this article is to implement a Reinforcement Learning method whose actions are based in these regulated functions for autonomous vehicles navigation. With this aim, a study of the state-of-the-art of autonomous vehicles simulators has been completed. Hence, the algorithm will be tested using a five-lane highway simulator, previously selected. Results and performance of the model through experimentation will be presented and evaluated using the simulator for different network architectures.					22nd International Conference of the Catalan-Association-for-Artificial-Intelligence (CCIA)22nd International Conference of the Catalan-Association-for-Artificial-Intelligence (CCIA)	OCT 23-25, 2019OCT 23-25, 2019	Catalan Assoc Artificial Intelligence; Uni Illes Balears; Inst Investigacio Intelligencia Artificial; Hamilton Inst; Soft Comp Processament Agregacio; Conselo Super Investigaciones Cientificas; Conselleria Medi Ambient, Agricultura Pesca, Direccio Gen Espais Naturals Biodiversitat; Generalitat Catalunya, Dept Politiques Digitals Adm PublicaCatalan Assoc Artificial Intelligence; Uni Illes Balears; Inst Investigacio Intelligencia Artificial; Hamilton Inst; Soft Comp Processament Agregacio; Conselo Super Investigaciones Cientificas; Conselleria Medi Ambient, Agricultura Pesca, Direccio Gen Espais Naturals Biodiversitat; Generalitat Catalunya, Dept Politiques Digitals Adm Publica	Mallorca, SPAINMallorca, SPAIN	0	0	0	0	0	0	0			0922-6389	1879-8314	978-1-64368-015-6; 978-1-64368-014-9									Univ Politecn Cataluna, IDEAI UPC, Jordi Girona 1-3, Barcelona 08034, Spain				2020-09-29	WOS:000569638200015		
C	Emamifar, Mehrnoosh; Ghoreishi, Seyede Fatemeh			IEEE							Uncertainty-Aware Reinforcement Learning for Safe Control of Autonomous Vehicles in Signalized Intersections								2023 IEEE CONFERENCE ON ARTIFICIAL INTELLIGENCE, CAI								81	82				10.1109/CAI54212.2023.00042							Proceedings Paper	2023	2023	This paper proposes a reinforcement learning approach for the control of autonomous vehicles at signalized intersections. The proposed method is a modified version of the Q-learning approach that takes into account the risky scenarios that might arise in the control of an autonomous vehicle due to the inherent uncertainties in the system. The proposed algorithm enables robust and risk-aware decision-making in uncertain and sensitive environments. The proposed algorithm is evaluated in a simulated autonomous vehicle scenario, where it outperforms the standard Q-learning in terms of safety.					IEEE Conference on Artificial Intelligence (IEEE CAI)IEEE Conference on Artificial Intelligence (IEEE CAI)	JUN 05-06, 2023JUN 05-06, 2023	IEEE; IEEE Comp Soc; IEEE Signal Proc Soc; IEEE Syst, Man, & Cybernet SocIEEE; IEEE Comp Soc; IEEE Signal Proc Soc; IEEE Syst, Man, & Cybernet Soc	Santa Clara, CASanta Clara, CA	0	0	0	0	0	0	0					979-8-3503-3984-0									Northeastern Univ, Coll Engn, Boston, MA 02115 USANortheastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA				2023-09-06	WOS:001046447800032		
J	He, Xiangkun; Yang, Haohan; Hu, Zhongxu; Lv, Chen				Lv, Chen/N-7055-2018; Hu, Zhongxu/AAH-3982-2020; He, Xiangkun/ABA-5268-2021; Yang, Haohan/AAW-1224-2021	Lv, Chen/0000-0001-6897-4512; Hu, Zhongxu/0000-0001-8236-7903; He, Xiangkun/0000-0001-9818-0879; Yang, Haohan/0000-0002-1545-2793					Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach								IEEE TRANSACTIONS ON INTELLIGENT VEHICLES				8	1			184	193				10.1109/TIV.2022.3165178							Article	JAN 2023	2023	Reinforcementlearning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties. Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.									32	0	0	0	0	0	32			2379-8858	2379-8904										Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore 639798, Singapore				2023-05-23	WOS:000966725200001		
B	Sinha, A.; White, D.; Cao, Y.										Deep Reinforcement Learning-based Optimal Time-constrained Intercept Guidance								AIAA SCITECH 2024 Forum								2206	2206				10.2514/6.2024-2206							Conference Paper	2024	2024	In this work, we design the guidance strategy for an autonomous vehicle to rendezvous with a stationary target at a time specified a priori. We first design the suitable guidance law for the autonomous vehicle to steer it on the desired trajectory towards the target. In particular, our aim is to drive the necessary error variable to zero in an optimal fashion by minimizing a meaningful cost function. In essence, the proposed method provides an optimal transient performance for the autonomous vehicle to rendezvous with the stationary target precisely at the desired time. As the optimal transient behavior depends on the design parameters, especially the controller gain, we leverage the proximal policy optimization in the reinforcement learning framework to obtain the required design parameter. We show through simulations that the proposed technique is energy-efficient when compared to other finite/fixed-time convergent guidance laws.					AIAA SCITECH 2024 ForumAIAA SCITECH 2024 Forum	20242024	Jacobs; Caltech; Ball; Ennova Technologies; FAMU-FSU College of Engineering; Los Alamos National Laboratory; TXT PACE; Bastion TechnologiesJacobs; Caltech; Ball; Ennova Technologies; FAMU-FSU College of Engineering; Los Alamos National Laboratory; TXT PACE; Bastion Technologies	Orlando, FL, USAOrlando, FL, USA	0	0	0	0	0	0	0					978-1-62410-711-5									Univ. of Texas at San Antonio, San Antonio, TX, USA				2024-02-23	INSPEC:24573803		
C	Tiong, Teckchai; Saad, Ismail; Teo, Kenneth Tze Kin; bin Lago, Herwansyah					TIONG, TECK CHAI/0000-0001-5199-7232	Paul, R				Autonomous Vehicle Driving Path Control with Deep Reinforcement Learning								2023 IEEE 13TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE, CCWC								84	92				10.1109/CCWC57344.2023.10099122							Proceedings Paper	2023	2023	Autonomous vehicle (AV) uses the artificial intelligence (AI) technologies to control the vehicle without human intervention. The implementation of AV has the advantages over the human- driven vehicle such as reducing the road traffic deaths that caused by human errors, increasing the traffic efficiency and minimizing the carbon emission to save the environment. The main objective of this paper is to develop an AV that keeps a safe distance while following the lead car and remains at the centerline of road. The proposed Deep Reinforcement Learning (DRL) algorithm for the autonomous driving simulation is Deep Deterministic Policy Gradient (DDPG). In this paper, the DDPG model for the path following control, reward function, actor network and critic network are created. The DDPG agent has been trained until 1650-episode rewards have been received. After the training, the proposed DDPG agent has been simulated to verify the performance. Then, the values of the two hyperparameters, which are mini-batch size and actor learning rate, are tuned to obtain the shortest training time.					IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)	MAR 08-11, 2023MAR 08-11, 2023	IEEE; SMART; IEEE Reg 1; IEEE USA; Inst Engn & Management; Univ Engn & ManagementIEEE; SMART; IEEE Reg 1; IEEE USA; Inst Engn & Management; Univ Engn & Management	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1					979-8-3503-3286-5									Curtin Univ Malaysia, Elect & Comp Engn, Miri Sarawak, Malaysia				2023-06-16	WOS:000995182600014		
C	Parras, Juan; Zazo, Santiago			IEEE							ROBUST DEEP REINFORCEMENT LEARNING FOR UNDERWATER NAVIGATION WITH UNKNOWN DISTURBANCES								2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021)								3440	3444				10.1109/ICASSP39728.2021.9414937							Proceedings Paper	2021	2021	We study an underwater navigation problem, where an Underwater Autonomous Vehicle must reach a target position in the presence of a disturbance that may be unknown. In order to deal with this problem, we make use of Deep Reinforcement Learning tools, and more concretely, we make use of robust control ideas, which allow training an agent in the presence of uncertainty. We propose a robust Proximal Policy Optimization agent and train it using simulations of an underwater medium: this agent shows an excellent performance when facing unknown disturbances, being able to approach the performance of the optimal agent which had an exact knowledge of the underwater disturbance.					IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	JUN 06-11, 2021JUN 06-11, 2021	IEEE; Inst Elect & Elect Engineers, Signal Proc SocIEEE; Inst Elect & Elect Engineers, Signal Proc Soc	ELECTR NETWORKELECTR NETWORK	4	0	0	0	0	0	4					978-1-7281-7605-5									Univ Politecn Madrid, Informat Proc & Telecommun Ctr, Madrid, Spain				2021-11-24	WOS:000704288403139		
B	Bautista-Montesano, R.; Galluzzi, R.; Di, X.; Bustamante-Bello, R.										Reinforcement Learning-Based Navigation Approach for a Downscaled Autonomous Vehicle in Simplified Urban Scenarios								2023 International Symposium on Electromobility (ISEM)								1	7				10.1109/ISEM59023.2023.10334911							Conference Paper	2023	2023	This paper presents the implementation of a reinforcement learning based navigation architecture for autonomous vehicles in urban scenarios. These types of scenarios represent a challenging task due to the presence of dynamic and static road elements. This work validates the use and feasibility of high-level reinforcement learning controllers in the autonomous vehicle software pipeline. Tests are performed using a 1:10 downscaled autonomous prototype on a track with one main and two secondary roads. The platform is equipped with a LIDAR, inertial measurement units, a stereo camera and motor drives for steering and propulsion. Experiments yield favorable outcomes in terms of collision avoidance, lane keeping and navigational comfort.					2023 International Symposium on Electromobility (ISEM)2023 International Symposium on Electromobility (ISEM)	20232023		Monterrey, MexicoMonterrey, Mexico	0	0	0	0	0	0	0					979-8-3503-4007-5									Sch. of Eng. & Sci., Tecnologico de Monterrey, Mexico CityColumbia Univ., New York, NY, USA				2024-01-06	INSPEC:24217785		
J	Pan, Huihui; Zhang, Chi; Sun, Weichao					Pan, Huihui/0000-0002-8931-1774					Fault-Tolerant Multiplayer Tracking Control for Autonomous Vehicle via Model-Free Adaptive Dynamic Programming								IEEE TRANSACTIONS ON RELIABILITY				72	4			1395	1406				10.1109/TR.2022.3208467					SEP 2022		Article; Early Access		2023	This article investigates the completely unknown autonomous vehicle tracking issues with actuator faults through model-free adaptive dynamic programming (MFADP) approaches. Because partial parameters are measured difficultly or inaccurately, the model-based control theories are imperfect for the vehicles. Therefore, the proposed multiplayer optimal control method in this work, which is not necessary to know the prior system knowledge, achieves the purpose of unknown vehicle tracking control via a novel MFADP theory. Besides, the control strategies are robust, which contain adaptive regulators to eliminate the disturbance of the vehicle systems caused by actuator faults, modeling errors, and curvature interference. To reduce the computational burden of control, a single neural network (NN) architecture is constructed with minimal computational cost and fast response speed. In addition, the convergence analysis of the NN structure, the stability and robustness analysis of identification, and the control schemes in this work are supplied. Finally, two driving scenario simulations are shown to prove the effectiveness of the established controller.									5	0	0	0	0	0	5			0018-9529	1558-1721										Harbin Inst Technol, Res Inst Intelligent Control & Syst, Harbin 150001, Peoples R ChinaHarbin Inst Technol, Robot Innovat Ctr Co Ltd, Harbin, Peoples R China				2022-10-17	WOS:000865091400001		
J	Isele, D.; Nakhaei, A.; Fujimura, K.										Safe Reinforcement Learning on Autonomous Vehicles [arXiv]								arXiv								6 pp.	6 pp.											Journal Paper	27 Sept. 2019	2019	There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications. Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems. We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle. [IROS 2018].									0	0	0	0	0	0	0														Honda Res. Inst., USA				2020-06-01	INSPEC:19560521		
B	Xia, Y.; Liu, S.; Chen, X.; Xu, Z.; Zheng, K.; Su, H.										RISE: A Velocity Control Framework with Minimal Impacts based on Reinforcement Learning								CIKM '22: Proceedings of the 31st ACM International Conference on Information & Knowledge Management								2210	9				10.1145/3511808.3557435							Conference Paper	2022	2022	Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade. However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes. In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle). In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles. To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle. Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles. To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph. Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.					CIKM '22: The 31st ACM International Conference on Information and Knowledge ManagementCIKM '22: The 31st ACM International Conference on Information and Knowledge Management	17-21 Oct. 202217-21 Oct. 2022	SIGWEB; SIGIRSIGWEB; SIGIR	Atlanta, GA, USAAtlanta, GA, USA	0	0	0	0	0	0	0					978-1-4503-9236-5									Univ. of Electron. Sci. & Technol. of China, Chengdu, China				2023-03-23	INSPEC:22747994		
C	Leung, Karen; Veer, Sushant; Schmerling, Edward; Pavone, Marco			IEEE		Pavone, Marco/0000-0002-0206-4337					Learning Autonomous Vehicle Safety Concepts from Demonstrations								2023 AMERICAN CONTROL CONFERENCE, ACC		Proceedings of the American Control Conference						3193	3200											Proceedings Paper	2023	2023	Evaluating the safety of an autonomous vehicle (AV) depends on the behavior of surrounding agents which can be heavily influenced by factors such as environmental context and informally-defined driving etiquette. A key challenge is in determining a minimum set of assumptions on what constitutes reasonable foreseeable behaviors of other road users for the development of AV safety models and techniques. In this paper, we propose a data-driven AV safety design methodology that first learns "reasonable" behavioral assumptions from data, and then synthesizes an AV safety concept using these learned behavioral assumptions. We borrow techniques from control theory, namely high-order control barrier functions and Hamilton-Jacobi reachability, to provide inductive bias to aid interpretability, verifiability, and tractability of our approach. In our experiments, we learn an AV safety concept using demonstrations collected from a highway traffic-weaving scenario, compare our learned concept to existing baselines, and showcase its efficacy in evaluating real-world driving logs.					American Control Conference (ACC)American Control Conference (ACC)	MAY 31-JUN 02, 2023MAY 31-JUN 02, 2023	Mitsubishi Elect Res Lab; Boeing; MathWorks; Quanser; ASML; Halliburton; Lockheed Martin; dSPACE; General Motors Co; Soc Ind & Appl Math; Springer; Collimator; JuliaHub; Unitree RobotMitsubishi Elect Res Lab; Boeing; MathWorks; Quanser; ASML; Halliburton; Lockheed Martin; dSPACE; General Motors Co; Soc Ind & Appl Math; Springer; Collimator; JuliaHub; Unitree Robot	San Diego, CASan Diego, CA	0	0	0	0	0	0	0			0743-1619	2378-5861	979-8-3503-2806-6									NVIDIA, Santa Clara, CA 95051 USAUniv Washington, Seattle, WA USAStanford Univ, Stanford, CA USA				2023-09-07	WOS:001027160302130		
B	Tiong, T.; Saad, I.; Teo, K.T.K.; Lago, H.B.						Paul, R.				Autonomous vehicle driving path control with deep reinforcement learning								2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)								0084	92				10.1109/CCWC57344.2023.10099122							Conference Paper	2023	2023	Autonomous vehicle (AV) uses the artificial intelligence (AI) technologies to control the vehicle without human intervention. The implementation of AV has the advantages over the human-driven vehicle such as reducing the road traffic deaths that caused by human errors, increasing the traffic efficiency and minimizing the carbon emission to save the environment. The main objective of this paper is to develop an AV that keeps a safe distance while following the lead car and remains at the centerline of road. The proposed Deep Reinforcement Learning (DRL) algorithm for the autonomous driving simulation is Deep Deterministic Policy Gradient (DDPG). In this paper, the DDPG model for the path following control, reward function, actor network and critic network are created. The DDPG agent has been trained until 1650-episode rewards have been received. After the training, the proposed DDPG agent has been simulated to verify the performance. Then, the values of the two hyperparameters, which are mini-batch size and actor learning rate, are tuned to obtain the shortest training time.					2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)	8-11 March 20238-11 March 2023	IEM; SMART; IEEE; UEMIEM; SMART; IEEE; UEM	Las Vegas, NV, USALas Vegas, NV, USA	0	0	0	0	0	0	0					979-8-3503-3286-5									Curtin Univ. Malaysia Miri Sarawak, Miri, MalaysiaElectr. & Electron. Eng., Univ. Malaysia Sabah, Kota Kinabalu, Malaysia				2023-05-05	INSPEC:22961852		
B	Wang, Y.; Dai, X.; Wang, K.; Ali, H.; Zhu, F.										Embed Trajectory Imitation in Reinforcement Learning: A Hybrid Method for Autonomous Vehicle Planning								2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)								1	6				10.1109/DTPI59677.2023.10365415							Conference Paper	2023	2023	Learning-based autonomous vehicle trajectory planning methods have shown excellent performance in a variety of complex traffic scenarios. However, the existing imitation learning (IL) and reinforcement learning (RL) algorithms still have their limitations, such as poor safety and generalizability for IL, and low data efficiency for RL. To leverage their respective advantages and mitigate the limitations, this paper proposes a novel hybrid RL algorithm for autonomous vehicle planning, where IL is embedded in it to guide its exploration with expert knowledge. Different from existing approaches, we use multi-step trajectory prediction instead of behavior cloning as the IL method integrated with online RL. Through such design, we make a further step in the research about how expert demonstration can be helpful to RL. Moreover, we conduct parallel training and testing of the algorithm based on real-world driving data. Experimental results demonstrate that our proposed approach outperforms standalone IL and RL methods, and performs better than RL methods enhanced by behavior cloning.					2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)	20232023	IEEE; CAAIEEE; CAA	Orlando, FL, USAOrlando, FL, USA	0	0	0	0	0	0	0					979-8-3503-1847-0									State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, ChinaSch. of Artificial Intelligence, Univ. of Chinese Acad. of Sci., Beijing, ChinaSch. of Artificial Intelligence, Anhui Univ., Hefei, China				2024-01-18	INSPEC:24317240		
C	Cui, Leilei; Ozbay, Kaan; Jiang, Zhong-Ping			IEEE							Combined Longitudinal and Lateral Control of Autonomous Vehicles based on Reinforcement Learning								2021 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						1929	1934				10.23919/ACC50511.2021.9483388							Proceedings Paper	2021	2021	In this paper, in order for the autonomous vehicle to keep a desired distance from the preceding vehicle and stay in the lane, a data-driven optimal control approach is proposed. Firstly, the dynamics of the autonomous vehicle is derived. In order to overcome the cutting-edge limitation, a virtual preceding vehicle is defined which is perpendicular to the preceding vehicle. The tracking error is defined as the deviation between the look ahead point of the autonomous vehicle and the virtual preceding vehicle. Then, the error system is derived. Secondly, based on the error system, in order to minimize the cost determined by the tracking error and the energy consumption, the Hamilton-Jacobi-Bellman (HJB) equation is established. A model-based policy iteration technique is proposed to solve the HJB equation. Thirdly, a two-phase data-driven policy iteration algorithm is proposed and implemented by using adaptive dynamic programming (ADP). The efficacy of the proposed data-driven optimal control approach is validated by computer simulations.					American Control Conference (ACC)American Control Conference (ACC)	MAY 25-28, 2021MAY 25-28, 2021	Amer Automat Control Council; Mitsubishi Elect Res Lab; Halliburton; MathWorks; Wiley; GE Global Res; Soc Ind Appl Math; dSPACE; Tangibles That Teach; Elsevier; GMAmer Automat Control Council; Mitsubishi Elect Res Lab; Halliburton; MathWorks; Wiley; GE Global Res; Soc Ind Appl Math; dSPACE; Tangibles That Teach; Elsevier; GM	ELECTR NETWORKELECTR NETWORK	6	2	0	0	0	0	6			0743-1619	2378-5861	978-1-6654-4197-1									NYU, Tandon Sch Engn, Control & Networks Lab, Dept Elect & Comp Engn, Brooklyn, NY 11201 USANYU, C2SMART Ctr, Tandon Sch Engn, Brooklyn, NY 11201 USA				2021-10-29	WOS:000702263302003		
B	Mokhtari, K.; Wagner, A.R.										Don't Get into Trouble! Risk-aware Decision-Making for Autonomous Vehicles								2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)								1570	7				10.1109/RO-MAN53752.2022.9900795							Conference Paper	2022	2022	Risk is traditionally described as the expected likelihood of an undesirable outcome, such as a collision for an autonomous vehicle. Accurately predicting risk or potentially risky situations is critical for the safe operation of an autonomous vehicle. This work combines use of a controller trained to navigate around individuals in a crowd and a risk-based decision-making framework for an autonomous vehicle that integrates high-level risk-based path planning with a reinforcement learning-based low-level control. We evaluated our method using a high-fidelity simulation environment. We show our method results in zero collisions with pedestrians and predicted the least risky path, time to travel, or day to travel in approximately 72% of traversals. This work can improve safety by allowing an autonomous vehicle to one day avoid and react to risky situations.					2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)	29 Aug.-2 Sept. 202229 Aug.-2 Sept. 2022	LuxAI; Furhat Robotics; navel robotics; PAL ROBOTICS; PRENSILIA s.r.l.; NAVER LABS Europe; PALADYN, Journal of Behavioral Robtics; DIETA.UNIA; PERSE; EurAiLuxAI; Furhat Robotics; navel robotics; PAL ROBOTICS; PRENSILIA s.r.l.; NAVER LABS Europe; PALADYN, Journal of Behavioral Robtics; DIETA.UNIA; PERSE; EurAi	Naples, ItalyNaples, Italy	0	0	0	0	0	0	0					978-1-7281-8859-1									Dept. of Mech. Eng., Pennsylvania State Univ., State College, PA, USA				2022-11-01	INSPEC:22095969		
J	Kolat, Mate; Becsi, Tamas				Bécsi, Tamás/H-9818-2012	Bécsi, Tamás/0000-0002-1487-9672					Multi-Agent Reinforcement Learning for Highway Platooning								ELECTRONICS				12	24					4963			10.3390/electronics12244963							Article	DEC 2023	2023	The advent of autonomous vehicles has opened new horizons for transportation efficiency and safety. Platooning, a strategy where vehicles travel closely together in a synchronized manner, holds promise for reducing traffic congestion, lowering fuel consumption, and enhancing overall road safety. This article explores the application of Multi-Agent Reinforcement Learning (MARL) combined with Proximal Policy Optimization (PPO) to optimize autonomous vehicle platooning. We delve into the world of MARL, which empowers vehicles to communicate and collaborate, enabling real-time decision making in complex traffic scenarios. PPO, a cutting-edge reinforcement learning algorithm, ensures stable and efficient training for platooning agents. The synergy between MARL and PPO enables the development of intelligent platooning strategies that adapt dynamically to changing traffic conditions, minimize inter-vehicle gaps, and maximize road capacity. In addition to these insights, this article introduces a cooperative approach to Multi-Agent Reinforcement Learning (MARL), leveraging Proximal Policy Optimization (PPO) to further optimize autonomous vehicle platooning. This cooperative framework enhances the adaptability and efficiency of platooning strategies, marking a significant advancement in the pursuit of intelligent and responsive autonomous vehicle systems.									0	0	0	0	0	0	0				2079-9292										Budapest Univ Technol & Econ, Dept Control Transportat & Vehicle Syst, H-1111 Budapest, Hungary				2024-01-11	WOS:001130679200001		
J	Quek, Yang Thee; Koh, Li Ling; Koh, Ngiap Tiam; Tso, Wai Ann; Woo, Wai Lok				Woo, Wai Lok/T-3662-2018	Woo, Wai Lok/0000-0002-8698-7605; Quek, Yang Thee/0000-0003-1981-1607					Deep Q-network implementation for simulated autonomous vehicle control								IET INTELLIGENT TRANSPORT SYSTEMS				15	7			875	885				10.1049/itr2.12067					MAY 2021		Article; Early Access		2021	Deep reinforcement learning is poised to be a revolutionised step towards newer possibilities in solving navigation and autonomous vehicle control tasks. Deep Q-network (DQN) is one of the more popular methods of deep reinforcement learning that allows the agent that controls the vehicle to learn through its mistakes based on its actions and interactions with the environment. This paper presents the implementation of DQN to an autonomous self-driving vehicle control in two different simulated environments; first environment is in Python which is a simple 2D environment and then advanced to Unity software separately which is a 3D environment. Based on the scores and pixel inputs, the agent in the vehicle learns and adapts to its surrounding. It develops the best solution strategy to direct itself in the environment where its task is to manoeuvre the vehicle from point to point on a simulated highway scenario. The implemented DQN technique approximates the action value function with convolutional neural network. This evaluates the Q-function for the Q-learning architecture and updates the action value function. This paper shows that DQN is an effective learning method for the agent of an autonomous vehicle. In both simulated environments, the autonomous vehicle gradually learnt the manoeuvre operations and progressively gained the ability to successfully navigate itself and avoid obstacles without prior information of the surrounding.									3	0	0	0	0	0	3			1751-956X	1751-9578										Republ Polytech, Sch Engn, 9 Woodlands Ave 9, Singapore 738964, SingaporeNewcastle Univ Int Singapore, Sch Elect & Elect Engn, 172A Ang Mo Kio Ave 8 05-01, Singapore 567739, SingaporeNorthumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne, Tyne & Wear, England	Republ PolytechNewcastle Univ Int Singapore			2021-05-27	WOS:000647335000001		
J	Duy Quang Tran; Bae, Sang-Hoon					tran quang, duy/0000-0002-3639-2179					Proximal Policy Optimization Through a Deep Reinforcement Learning Framework for Multiple Autonomous Vehicles at a Non-Signalized Intersection								APPLIED SCIENCES-BASEL				10	16					5722			10.3390/app10165722							Article	AUG 2020	2020	Advanced deep reinforcement learning shows promise as an approach to addressing continuous control tasks, especially in mixed-autonomy traffic. In this study, we present a deep reinforcement-learning-based model that considers the effectiveness of leading autonomous vehicles in mixed-autonomy traffic at a non-signalized intersection. This model integrates the Flow framework, the simulation of urban mobility simulator, and a reinforcement learning library. We also propose a set of proximal policy optimization hyperparameters to obtain reliable simulation performance. First, the leading autonomous vehicles at the non-signalized intersection are considered with varying autonomous vehicle penetration rates that range from 10% to 100% in 10% increments. Second, the proximal policy optimization hyperparameters are input into the multiple perceptron algorithm for the leading autonomous vehicle experiment. Finally, the superiority of the proposed model is evaluated using all human-driven vehicle and leading human-driven vehicle experiments. We demonstrate that full-autonomy traffic can improve the average speed and delay time by 1.38 times and 2.55 times, respectively, compared with all human-driven vehicle experiments. Our proposed method generates more positive effects when the autonomous vehicle penetration rate increases. Additionally, the leading autonomous vehicle experiment can be used to dissipate the stop-and-go waves at a non-signalized intersection.									16	1	0	0	0	0	17				2076-3417										Pukyong Natl Univ, Smart Transportat Lab, Busan 48513, South Korea				2020-09-21	WOS:000567258000001		
J	Pei Xiaofei; Mo Shuojie; Chen Zhenfu; Yang Bo							裴晓飞; 莫烁杰; 陈祯福; 杨波			Lane Changing of Autonomous Vehicle Based on TD3Algorithm in Human-machine Hybrid Driving Environment			基于TD3算法的人机混驾交通环境自动驾驶汽车换道研究				中国公路学报	China Journal of Highway and Transport				34	11			246	254	1001-7372(2021)34:11<246:JYTSFD>2.0.TX;2-3										Article	2021	2021	Improving the human acceptance of autonomous vehicles in the future is important,and deep reinforcement learning is a key technology for their acceptance.To solve the lane-changing decision problem in a human-machine hybrid driving traffic flow,this study used the deep reinforcement-learning algorithm Twin Delayed Deep Deterministic Policy Gradient (TD3) to realize the free-lane-changing behavior of an autonomous vehicle.First,the theoretical framework of reinforcement learning based on the Markov decision process was introduced.Then,according to the driving data from actual conditions in the NGSIM dataset,a six-lane moderate traffic-congestion simulation scene was established using the autonomous driving simulator NGSIM-ENV.Other non-autonomous vehicles were controlled according to the recorded data in NGSIM.For lane-changing decision making in a continuous action space,the TD3 algorithm was used to develop a lane-changing model to control the driving behavior of the autonomous vehicle.In the proposed lane-changing model,the state space that contained the selfvehicle and environment information and the action space,which included the vehicle acceleration and heading angle,were established.Simultaneously,a reward function in the reinforcement learning was designed to consider factors such as safety,driving efficiency,and comfort.Finally,in the NGSIM-ENV simulation platform,the lane-changing behavior of the autonomous vehicles based on the TD3 algorithm was compared with that in the driving data of human drivers.The average driving velocity was found to increase by 4.8%,and the driving safety and comfort were improved.The simulation results verify the effectiveness of the lane-changing model after the model training is completed.Furthermore,safe,comfortable,and reasonable lane-changing behavior in a complex traffic environment can be realized.			提高人类驾驶人的接受度是自动驾驶汽车未来的重要方向,而深度强化学习是其发展的一项关键技术。为了解决人机混驾混合交通流下的换道决策问题,利用深度强化学习算法TD3(Twin Delayed Deep Deterministic Policy Gradient)实现自动驾驶汽车的自主换道行为。首先介绍基于马尔科夫决策过程的强化学习的理论框架,其次基于来自真实工况的NGSIM数据集中的驾驶数据,通过自动驾驶模拟器NGSIM-ENV搭建单向6车道、交通拥挤程度适中的仿真场景,非自动驾驶车辆按照数据集中驾驶人行车数据行驶。针对连续动作空间下的自动驾驶换道决策,采用改进的深度强化学习算法TD3构建换道模型控制自动驾驶汽车的换道驾驶行为。在所提出的TD3换道模型中,构建决策所需周围环境及自车信息的状态空间、包含受控汽车加速度和航向角的动作空间,同时综合考虑安全性、行车效率和舒适性等因素设计强化学习的奖励函数。最终在NGSIMENV仿真平台上,将基于TD3算法控制的自动驾驶汽车换道行为与人类驾驶人行车数据进行比较。研究结果表明:基于TD3算法控制的车辆其平均行驶速度比人类驾驶人的平均行车速度高4.8%,在安全性以及舒适性上也有一定的提升;试验结果验证了训练完成后TD3换道模型的有效性,其能够在复杂交通环境下自主实现安全、舒适、流畅的换道行为。						3	7	0	0	0	0	10			1001-7372											武汉理工大学, 现代汽车零部件技术湖北省重点实验室, 武汉, 湖北 430070, 中国武汉理工大学, 汽车零部件技术湖北省协同创新中心, 武汉, 湖北 430070, 中国Wuhan University of Technology, Hubei Key Laboratory of Advanced Technology of Automotive Components, Wuhan, Hubei 430070, ChinaWuhan University of Technology, Hubei Collaborative Innovation Center of Automotive Components Technology, Wuhan, Hubei 430070, China	武汉理工大学武汉理工大学Wuhan University of TechnologyWuhan University of Technology			2022-05-27	CSCD:7160688		
C	Lee, Jaeyoung; Balakrishnan, Aravind; Gaurav, Ashish; Czarnecki, Krzysztof; Sedwards, Sean					Gaurav, Ashish/0000-0003-3071-9385	Parker, D; Wolf, V				WISEMOVE: A Framework to Investigate Safe Deep Reinforcement Learning for Autonomous Driving								QUANTITATIVE EVALUATION OF SYSTEMS (QEST 2019)		Lecture Notes in Computer Science		11785				350	354				10.1007/978-3-030-30281-8_20							Proceedings Paper	2019	2019	WISEMOVE is a platform to investigate safe deep reinforcement learning (DRL) in the context of motion planning for autonomous driving. It adopts a modular architecture that mirrors our autonomous vehicle software stack and can interleave learned and programmed components. Our initial investigation focuses on a state-of-the-art DRL approach from the literature, to quantify its safety and scalability in simulation, and thus evaluate its potential use on our vehicle.					16th International Conference on Quantitative Evaluation of Systems (QEST)16th International Conference on Quantitative Evaluation of Systems (QEST)	SEP 10-12, 2019SEP 10-12, 2019	SpringerSpringer	Glasgow, SCOTLANDGlasgow, SCOTLAND	6	0	0	0	0	0	6			0302-9743	1611-3349	978-3-030-30281-8; 978-3-030-30280-1									Univ Waterloo, Waterloo, ON, Canada				2019-01-01	WOS:000679281300020		
R	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano										Testing autonomous vehicle lane keeping assist system in BeamNG simulator								Zenodo													https://doi.org/10.5281/ZENODO.8252677							Data set	2023-09-26	2023	In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator. Road topologies were generated by our tool RIGAA (available at:Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: Creative Commons Attribution 4.0 International Open Access									0	0	0	0	0	0	0																		2023-12-30	DRCI:DATA2023206027914757		
C	Jiang, Zhenyu; Wang, Zhongli; Cui, Xin; Zheng, Chaochao						Liu, XJ; Nie, Z; Yu, J; Xie, F; Song, R				Intelligent Safety Decision-Making for Autonomous Vehicle in Highway Environment								INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2021, PT IV		Lecture Notes in Artificial Intelligence		13016				702	713				10.1007/978-3-030-89092-6_64							Proceedings Paper	2021	2021	Safe driving policies is the key technology to realize the adaptive cruise control of autonomous vehicle in highway environment. In this paper, the reinforcement learning method is applied to autonomous driving's decision-making. To solve the problem that present reinforcement learning methods are difficult to deal with the randomness and uncertainty in driving environment, a model-free method for analyzing the Lyapunov stability and H infinity performance is applied to Actor-Critic algorithm to improve the stability and robustness of reinforcement learning. The safety of taking an action is judged by setting a safety threshold, thus improving the safety of behavioral decisions. Our method also designs a set of reward functions to better meet the safety and efficiency of driving decisions in the highway environment. The results show that the method can provide safe driving strategies for driverless vehicles in both normal road conditions and environments with unexpected situations, enabling the vehicles to drive safely.					14th International Conference on Intelligent Robotics and Applications (ICIRA)14th International Conference on Intelligent Robotics and Applications (ICIRA)	OCT 22-25, 2021OCT 22-25, 2021	Tsinghua Univ; Beihang Univ; Shandong Univ; YEDA; Yantai Univ; IFToMM China; SpringerTsinghua Univ; Beihang Univ; Shandong Univ; YEDA; Yantai Univ; IFToMM China; Springer	Yantai, PEOPLES R CHINAYantai, PEOPLES R CHINA	0	0	0	0	0	0	0			0302-9743	1611-3349	978-3-030-89092-6; 978-3-030-89091-9									Beijing Jiaotong Univ, Sch Elect Informat Engn, Beijing, Peoples R ChinaBeijing Engn Res Ctr EMC&GNSS Technol Rail Transp, Beijing 100044, Peoples R ChinaChina Railway Electrificat Bur Grp Co Ltd, Beijing 100036, Peoples R China	Beijing Engn Res Ctr EMC&GNSS Technol Rail TranspChina Railway Electrificat Bur Grp Co Ltd			2021-12-03	WOS:000722273400064		
C	Arvind, C. S.; Senthilnath, J.				Senthilnath, J./L-2949-2013	Senthilnath, J./0000-0002-1737-7985	Das, KN; Bansal, JC; Deep, K; Nagar, AK; Pathipooranam, P; Naidu, RC				Autonomous Vehicle for Obstacle Detection and Avoidance Using Reinforcement Learning								SOFT COMPUTING FOR PROBLEM SOLVING, SOCPROS 2018, VOL 1		Advances in Intelligent Systems and Computing		1048				55	66				10.1007/978-981-15-0035-0_5							Proceedings Paper	2020	2020	Obstacle detection and avoidance during navigation of an autonomous vehicle is one of the challenging problems. Different sensors like RGB camera, Radar, and Lidar are presently used to analyze the environment around the vehicle for obstacle detection. Analyzing the environment using supervised learning techniques has proven to be an expensive process due to the training of different obstacle for different scenarios. In order to overcome such difficulty, in this paper Reinforcement Learning (RL) techniques are used to understand the uncertain environment based on sensor information to make the decision. Policy free, model-free Q-learning based RL algorithm with the multilayer perceptron neural network (MLP-NN) is applied and trained to predict optimal vehicle future action based on the current state of the vehicle. Further, the proposed Q-Learning with MLP-NN based approach is compared with the state of the art, namely, Q-learning. A simulated urban area obstacles scenario is considered with the different number of ultrasonic radar sensors in detecting obstacles. The experimental result shows that Q-learning with MLP-NN along with the ultrasonic sensors is proven to be more accurate than conventional Q-learning technique with the ultrasonic sensors. Hence it is demonstrated that combining Qlearning with MLP-NN will improve in predicting obstacles for autonomous vehicle navigation.					8th International Conference on Soft Computing for Problem Solving (SocProS)8th International Conference on Soft Computing for Problem Solving (SocProS)	DEC 17-19, 2018DEC 17-19, 2018		Vellore Inst Technol, Vellore, INDIAVellore Inst Technol, Vellore, INDIA	8	0	0	0	0	0	8			2194-5357	2194-5365	978-981-15-0035-0; 978-981-15-0034-3									Dr Ambedkar Inst Technol, Dept Comp Sci, Bengaluru, IndiaNanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore	Dr Ambedkar Inst Technol			2021-04-17	WOS:000631845000005		
B	Vartika, V.; Singh, S.; Das, S.; Mishra, S.K.; Sahu, S.S.				Mishra, Sudhansu Kumar/O-9008-2017	Mishra, Sudhansu Kumar/0000-0003-1733-8270; Sahu, Sitanshu Sekhar/0000-0003-1236-2255	Reddy, M.J.B.; Mohanta, D.K.; Kumar, D.; Ghosh, D.				A Review on Intelligent PID Controllers in Autonomous Vehicle								Advances in Smart Grid Automation and Industry 4.0. Select Proceedings of ICETSGAI4.0. Lecture Notes in Electrical Engineering (LNEE 693)								391	9				10.1007/978-981-15-7675-1_39							Conference Paper	2021	2021	In recent times, autonomous vehicle is gaining popularity in several of applications ranging from conventional transport system to nuclear reactor. The importance of autonomous vehicle increases manifold for those applications where it is not possible or very much risky for human to reach out. Many Artificial Intelligence techniques have been successfully applied in this field. In this paper, an adequate number of recent machine learning-based techniques to design intelligent PID controllers in autonomous vehicles have been reviewed and suitably compared with some other competitive approaches. Particularly, the parameters tuning ofKp,Ki,Kd as well as the rise time characteristics by two methods, namely Ziegler-Nichols and genetic algorithm (GA), have been discussed. Also, we have compared the properties of rise time and percentage overshoot by conventional PID controllers and fuzzy-based PID controllers.					Advances in Smart Grid Automation and Industry 4.0. ICETSGAI4.0Advances in Smart Grid Automation and Industry 4.0. ICETSGAI4.0	5-7 Dec. 20195-7 Dec. 2019		Ranchi, IndiaRanchi, India	2	0	0	0	0	0	2					978-981-15-7674-4									Control Syst., Electr. & Electron., BIT Mesra, Ranchi, IndiaNat. Inst. of Technol. Tiruchirappalli, Tiruchirappalli, IndiaDept. of Electr. & Electron. Eng., Birla Inst. of Technol., Mesra, Ranchi, India				2021-09-23	INSPEC:20939224		
S	Kuhnert, KD; Krödel, M						Perner, P; Imilya, A				Autonomous vehicle steering based on evaluative feedback by reinforcement learning								MACHINE LEARNING AND DATA MINING IN PATTERN RECOGNITION, PROCEEDINDS		LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		3587				405	414											Article; Proceedings Paper	2005	2005	Steering an autonomous vehicle requires the permanent adaptation of behavior in relation to the various situations the vehicle is in. This paper describes a research which implements such adaptation and optimization based on Reinforcement Learning (RL) which in detail purely learns from evaluative feedback in contrast to instructive feedback. Convergence of the learning process has been achieved at various experimental results revealing the impact of the different RL parameters. While using RL for autonomous steering is in itself already a novelty, additional attention has been given to new proposals for post-processing and interpreting the experimental data.					4th International Conference on Machine Learning and Data Minining in Pattern Recognition4th International Conference on Machine Learning and Data Minining in Pattern Recognition	JUL 09-11, 2005JUL 09-11, 2005		Leipzig, GERMANYLeipzig, GERMANY	0	0	0	0	0	0	0			0302-9743		3-540-26923-1									Univ Siegen, Inst Real Time Learningsyst, D-57068 Siegen, Germany				2005-01-01	WOS:000230895100040		
J	Cheng, Yanqiu; Chen, Chenxi; Hu, Xianbiao; Chen, Kuanmin; Tang, Qing; Song, Yang				Chen, Chenxi/JXN-5581-2024	Chen, Chenxi/0000-0002-7094-3296					Enhancing Mixed Traffic Flow Safety via Connected and Autonomous Vehicle Trajectory Planning with a Reinforcement Learning Approach								JOURNAL OF ADVANCED TRANSPORTATION				2021						6117890			10.1155/2021/6117890							Article	JUN 14 2021	2021	The longitudinal trajectory planning of connected and autonomous vehicle (CAV) has been widely studied in the literature to reduce travel time or fuel consumptions. The safety impact of CAV trajectory planning to the mixed traffic flow with both CAV and human-driven vehicle (HDV), however, is not well understood yet. This study presents a reinforcement learning modeling approach, named Monte Carlo tree search-based autonomous vehicle safety algorithm, or MCTS-AVS, to optimize the safety of mixed traffic flow, on a one-lane roadway with signalized intersection control. Crash potential index (CPI) is defined to quantitively measure the safety performance of the mixed traffic flow. The CAV trajectory planning problem is firstly formulated as an optimization model; then, the solution procedure based on reinforcement learning is proposed. The tree-expansion determination module and rollout termination module are developed to identify and reduce the unnecessary tree expansion, so as to train the model more efficiently towards the desired direction. The case study results showed that the proposed algorithm was able to reduce the CPI by 76.56%, when compared with a benchmark model without any intelligence, and 12.08%, when compared with another benchmark model that the team developed earlier. These results demonstrated the satisfactory performance of the proposed algorithm in enhancing the safety of the mixed traffic flow.									7	0	0	0	0	0	7			0197-6729	2042-3195										Changan Univ, Coll Transportat Engn, Dept Traff Engn, Xian 710064, Shaanxi, Peoples R ChinaMissouri Univ Sci & Technol, Dept Civil Architectural & Environm Engn, Rolla, MO 65409 USA				2021-07-16	WOS:000670436300004		
J	Jebamikyous, Hrag-Harout; Kashef, Rasha					kashef, Rasha/0000-0002-3430-1536					Autonomous Vehicles Perception (AVP) Using Deep Learning: Modeling, Assessment, and Challenges								IEEE ACCESS				10				10523	10535				10.1109/ACCESS.2022.3144407							Article	2022	2022	Perception is the fundamental task of any autonomous driving system, which gathers all the necessary information about the surrounding environment of the moving vehicle. The decision-making system takes the perception data as input and makes the optimum decision given that scenario, which maximizes the safety of the passengers. This paper surveyed recent literature on autonomous vehicle perception (AVP) by focusing on two primary tasks: Semantic Segmentation and Object Detection. Both tasks play an important role as a vital component of the vehicle's navigation system. A comprehensive overview of deep learning for perception and its decision-making process based on images and LiDAR point clouds is discussed. We discussed the sensors, benchmark datasets, and simulation tools widely used in semantic segmentation and object detection tasks, especially for autonomous driving. This paper acts as a road map for current and future research in AVP, focusing on models, assessment, and challenges in the field.									18	0	0	0	0	0	20			2169-3536											Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada				2022-02-06	WOS:000748262900001		
C	Mostafizi, Alireza; Siam, Mohammad Rayeedul Kalam; Wang, Haizhong				Siam, Mohammad Rayeedul Kalam/T-4839-2019; zhang, yan/JGL-8022-2023; Yang, Yifan/JTV-1487-2023	Siam, Mohammad Rayeedul Kalam/0000-0003-2042-5412; 	Wang, YH; McNerney, MT				Autonomous Vehicle Routing Optimization in a Competitive Environment: A Reinforcement Learning Application								INTERNATIONAL CONFERENCE ON TRANSPORTATION AND DEVELOPMENT 2018: CONNECTED AND AUTONOMOUS VEHICLES AND TRANSPORTATION SAFETY								109	118											Proceedings Paper	2018	2018	This paper presents a multiagent approach to identify the shortest path for the intelligent agents (i.e., autonomous vehicles) traveling through the transportation network using a Q-learning algorithm. In addition, this paper discusses how a machine achieves an optimal solution in the case that there are a large number of intelligent agents trying to minimize their travel times simultaneously. The Q-learning algorithm with a reverse order Q matrix is updated to reach the optimal path within a grid network, during which different coefficients have been evaluated and analyzed based on their impact on the algorithm's performance. The reinforcement learning algorithm is then applied to the multiagent system with different market penetration of autonomous vehicle agents. The results revealed that as the percentage of intelligent agents increases, it is more difficult to converge to an optimal solution. A physical interpretation is that when all of the agents are trying to maximize their utility, and if the intelligent agents have conflicted interests with each other, the system does not converge to a global optimum. We found that the final converged total travel time reaches its minimum when 75% of the agents are intelligent autonomous vehicles. The critical threshold lies between 50% and 75%, below which the system performance improves marginally over time.					American-Society-of-Civil-Engineers (ASCE) International Conference on Transportation and Development (ICTD)American-Society-of-Civil-Engineers (ASCE) International Conference on Transportation and Development (ICTD)	JUL 15-18, 2018JUL 15-18, 2018	Amer Soc Civil Engineers; Transportat & Dev Inst; Amer Soc Civil Engineers, Transportat & Dev InstAmer Soc Civil Engineers; Transportat & Dev Inst; Amer Soc Civil Engineers, Transportat & Dev Inst	Pittsburgh, PAPittsburgh, PA	3	0	0	0	0	0	3					978-0-7844-8153-0									Oregon State Univ, Dept Civil & Construct Engn, Corvallis, OR 97331 USA				2018-01-01	WOS:000541122600011		
C	Naruse, K; Leu, MC				Sokabe, Masahiro/I-1565-2012		Kakazu, Y; Wada, M; Sato, T				Autonomous vehicle navigation by reinforcement learning with problem structure identification								INTELLIGENT AUTONOMOUS SYSTEMS: IAS-5								226	233											Proceedings Paper	1998	1998	This paper presents a mechanism of an efficient reinforcement learning algorithm for autonomous vehicle navigation. The efficiency is achieved by identifying the structure of a given problem, and it is represented as a set of behaviors, which are efficient sequences of actions to solve the problem. Computational simulations are carried out and demonstrate the proposed mechanism successfully.					5th Conference on Intelligent Autonomous Systems5th Conference on Intelligent Autonomous Systems	19981998		SAPPORO, JAPANSAPPORO, JAPAN	0	0	0	0	0	0	0					90-5199-398-6									Hokkaido Univ, Sapporo, Hokkaido 060, Japan				1998-01-01	WOS:000075894500030		
J	Ayimba, Constantine; Cislaghi, Valerio; Quadri, Christian; Casari, Paolo; Mancuso, Vincenzo					Ayimba, Constantine/0000-0003-4032-9735; Casari, Paolo/0000-0002-6401-1660; Mancuso, Vincenzo/0000-0002-4661-381X					Copy-CAV: V2X-enabled wireless towing for emergency transport								COMPUTER COMMUNICATIONS				205				87	96				10.1016/j.comcom.2023.04.009					APR 2023		Article; Early Access		2023	As smart connected vehicles become increasingly common and pave the way for the autonomous vehicles of the future, their ability to provide enhanced safety and assistance services has improved. One such service is the emergency transport of drivers in medical distress: as a positive solution of the distress is typically more likely after timely response, an autonomous vehicle could cut on emergency response times, and thus play a key role in saving the life of its driver.In this paper, we show how such an autonomous emergency transport service can be run from a wireless cellular network, and discuss the importance of having a human in the loop in order to expedite driving. We present a Monte-Carlo-based driver assessment system that the network can use when selecting the most suitable candidate to wirelessly tow an autonomous vehicle with an incapacitated driver. We show that this mechanism results in a selection policy that ensures better cohesion between the vehicles, thereby significantly improving service reliability by reducing the chances of disruptions by intervening traffic.									2	0	0	0	0	0	2			0140-3664	1873-703X										IMDEA Networks Inst, Ave Mar Mediterraneo 22, Madrid 28918, SpainUniv Milan, Comp Sci Dept, Via Celoria,18, I-20133 Milan, ItalyUniv Trento, Via Sommar,9, I-38123 Trento, Italy				2023-06-05	WOS:000989234700001		
C	Anh Huynh; Ba-Tung Nguyen; Hoai-Thu Nguyen; Sang Vu; Hien Nguyen						Ali, R; Kaindl, H; Maciaszek, L				A Method of Deep Reinforcement Learning for Simulation of Autonomous Vehicle Control								ENASE: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON EVALUATION OF NOVEL APPROACHES TO SOFTWARE ENGINEERING								372	379				10.5220/0010478903720379							Proceedings Paper	2021	2021	Nowadays autonomous driving is expected to revolutionize the transportation sector. Carmakers, researchers, and administrators have been working on this field for years and significant progress has been made. However, the doubts and challenges to overcome are still huge, regarding not only complex technologies but also human awareness, culture, current traffic infrastructure. In terms of technical perspective, the accurate detection of obstacles, avoiding adjacent obstacles, and automatic navigation through the environment are some of the difficult problems. In this paper, an approach for solving those problems is proposed by using of Policy Gradient to control a simulated car via reinforcement learning. The proposed method is worked effectively to train an agent to control the simulated car in Unity ML-agents Highway, which is a simulating environment. This environment is chosen from some criteria of an environment simulating autonomous vehicle. The testing of the proposed method got positive results. Beside the average speed was w ell, the agent successfully learned the turning operation, progressively gaining the ability to navigate larger sections of the simulated raceway without crashing.					16th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)16th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)	APR 26-27, 2021APR 26-27, 2021	INSTICCINSTICC	ELECTR NETWORKELECTR NETWORK	3	0	0	0	0	0	3					978-989-758-508-1									Univ Informat Technol, Fac Software Engn, Ho Chi Minh City, VietnamUniv Informat Technol, Fac Informat Syst, Ho Chi Minh City, VietnamUniv Informat Technol, Fac Comp Sci, Ho Chi Minh City, VietnamVietnam Natl Univ, Ho Chi Minh City, Vietnam	Univ Informat TechnolUniv Informat TechnolUniv Informat Technol			2022-04-28	WOS:000783843700038		
C	Zarrouki, Baha; Kloes, Verena; Heppner, Nikolas; Schwan, Simon; Ritschel, Robert; Vosswinkel, Rick			IEEE							Weights-varying MPC for Autonomous Vehicle Guidance: a Deep Reinforcement Learning Approach								2021 EUROPEAN CONTROL CONFERENCE (ECC)								119	125											Proceedings Paper	2021	2021	Model Predictive Control (MPC) can achieve excellent results for complex control tasks like path-following of autonomous vehicles. However, its performance depends on the right choice of a cost function for its internal optimization problem. Optimizing the cost function to different objectives is challenging and time-consuming. In this paper, we propose to automatically learn context-dependent optimal weights for the cost function with Deep Reinforcement Learning and to adapt the weights online. We show that our approach outperforms the results of a human expert.					European Control Conference (ECC)European Control Conference (ECC)	JUN 29-JUL 02, 2021JUN 29-JUL 02, 2021		ELECTR NETWORKELECTR NETWORK	4	0	0	0	0	0	4					978-94-6384-236-5									Tech Univ Berlin, Inst Software Engn & Theoret Comp Sci, Berlin, GermanyIAV GmbH, Intelligent Driving Funkt, Berlin, Germany				2022-04-07	WOS:000768455200018		
B	Priisalu, M.; Paduraru, C.; Smichisescu, C.						Gade, R.; Felsberg, M.; Kamarainen, J.-K..				Varied Realistic Autonomous Vehicle Collision Scenario Generation								Image Analysis: 23rd Scandinavian Conference, SCIA 2023, Proceedings. Lecture Notes in Computer Science (13886)								354	72				10.1007/978-3-031-31438-4_24							Conference Paper	2023	2023	Recently there has been an increase in the number of available autonomous vehicle (AV) models. To evaluate and compare the safety of the various models the AVs need to be tested in several diverse safety-critical scenarios. We propose the Adversarial Test Case Generator (ATCG) that differently from previous test case generators allows for the generation of realistic collision scenarios with varied AV and pedestrian behaviour models, on varied scenes and with varied traffic density. Given a top-view image and the semantic segmentation of a traffic scene, the ATCG learns to place multiple AVs and goal-reaching pedestrians in the scene such that collisions occur. Pedestrians in previous multi-agent traffic scenario generation works are confined to unrealistic behaviours such as seeking collisions with the AV or ignoring the AV. Although such scenarios with multiple suicidal pedestrians are collision prone it is unlikely in reality that all pedestrians act abnormally. In realistic collision scenarios the generated pedestrians' behaviours must resemble real pedestrians. The ATCG is a team of Reinforcement Learning (RL) agents and can be easily extended with additional RL agents to produce more complex scenes allowing for advanced AVs to be tested.					Scandinavian Conference on Image AnalysisScandinavian Conference on Image Analysis	18-21 April 202318-21 April 2023		Sirkka, FinlandSirkka, Finland	0	0	0	0	0	0	0					978-3-031-31438-4									Lund Univ., Lund, SwedenUniv. of Bucharest, Bucharest, RomaniaGoogle Res., Zurich, SwitzerlandAalborg Univ., Aalborg, DenmarkLinkoping Univ., Linkoping, SwedenTampere Univ., Tampere, Finland				2023-08-10	INSPEC:23496258		
C	Krödel, M; Kuhnert, KD			ieee							Optimising situation-based behaviour of autonomous vehicles								2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM								380	385											Proceedings Paper	2004	2004	Reinforcement Learning (RL) is a method which provides true learning capabilities regarding situation-based actions. RL-systems explore and self-optimise actions for situations in a defined environment. This paper describes the research of a driver (assistance) system based on pure Reinforcement Learning in the framework of an autonomous vehicle. The target of this research is to determine to what extend RL-based Systems serve as an enhancement or even an alternative to classical concepts of autonomous intelligent vehicles such as modelling or neural nets.					IEEE Intelligent Vehicles SymposiumIEEE Intelligent Vehicles Symposium	JUN 14-17, 2004JUN 14-17, 2004	IEEEIEEE	Parma, ITALYParma, ITALY	1	0	0	0	0	0	1					0-7803-8310-9									Univ Siegen, Inst Real Time Syst, D-57068 Siegen, Germany				2004-01-01	WOS:000222973200068		
B	Shuojie Mo; Xiaofei Pei; Zhenfu Chen										Decision-Making for Oncoming Traffic Overtaking Scenario using Double DQN								2019 3rd Conference on Vehicle Control and Intelligence (CVCI)								4 pp.	4 pp.				10.1109/CVCI47823.2019.8951626							Conference Paper	2019	2019	Great progress has been made in the field of machine learning in recent years. And learning-based methods have been widely utilized for developing highly autonomous vehicle. To this end, we introduce a reinforcement learning based intelligent autonomous vehicle decision making method for oncoming overtaking scenario. The goal of reinforcement learning is to learn how to take optimal decision in corresponding observations through interactions with the environment using a reward function to estimate whether the decision is good or not. A Double Deep Q-learning (Double DQN) agent was used to learn policies (control strategies) for both longitudinal speed and lane change decision. Prioritized Experience Replay (PER) was used to accelerate convergence of the policies. A two-way 3-car scenario with oncoming traffic was established in SUMO (Simulation of Urban Mobility) to train and test the policies.					2019 3rd Conference on Vehicle Control and Intelligence (CVCI)2019 3rd Conference on Vehicle Control and Intelligence (CVCI)	21-22 Sept. 201921-22 Sept. 2019		Hefei, ChinaHefei, China	0	0	0	0	0	0	0					978-1-7281-2684-5									Sch. of Automotive Eng., Wuhan Univ. of Technol., Wuhan, China				2020-02-06	INSPEC:19262686		
B	Liu, S.; Xia, Y.; Chen, X.; Xie, J.; Su, H.; Zheng, K.										Impact-aware Maneuver Decision with Enhanced Perception for Autonomous Vehicle								2023 IEEE 39th International Conference on Data Engineering (ICDE)								3255	68				10.1109/ICDE55515.2023.00250							Conference Paper	2023	2023	Autonomous driving is an emerging technology that has developed rapidly over the last decade. There have been numerous interdisciplinary challenges imposed on the current transportation system by autonomous vehicles. In this paper, we conduct an algorithmic study on the autonomous vehicle decision-making process, which is a fundamental problem in the vehicle automation field and the root cause of most traffic congestion. We propose a perception-and-decision framework, called HEAD, which consists of an enHanced pErception module and a mAneuver Decision module. HEAD aims to enable the autonomous vehicle to perform safe, efficient, and comfortable maneuvers with minimal impact on other vehicles. In the enhanced perception module, a graph-based state prediction model with a strategy of phantom vehicle construction is proposed to predict the one-step future states for multiple surrounding vehicles in parallel, which deals with sensor limitations such as limited detection range and poor detection accuracy under occlusions. Then in the maneuver decision module, a deep reinforcement learning-based model is designed to learn a policy for the autonomous vehicle to perform maneuvers in continuous action space w.r.t. a parameterized action Markov decision process. A hybrid reward function takes into account aspects of safety, efficiency, comfort, and impact to guide the autonomous vehicle to make optimal maneuver decisions. Extensive experiments offer evidence that HEAD can advance the state of the art in terms of both macroscopic and microscopic effectiveness.					2023 IEEE 39th International Conference on Data Engineering (ICDE)2023 IEEE 39th International Conference on Data Engineering (ICDE)	3-7 April 20233-7 April 2023		Anaheim, CA, USAAnaheim, CA, USA	0	0	0	0	0	0	0					979-8-3503-2227-9									Sch. of Comput. Sci. & Eng., Univ. of Electron. Sci. & Technol. of China, Chengdu, ChinaHuawei Cloud Database Innovation Lab, China				2023-08-10	INSPEC:23485447		
J	Rasouli, A.; Goebel, R.; Taylor, M.E.; Kotseruba, I.; Alizadeh, S.; Yang, T.; Alban, M.; Shkurti, F.; Zhuang, Y.; Scibior, A.; Rezaee, K.; Garg, A.; Meger, D.; Luo, J.; Paull, L.; Zhang, W.; Wang, X.; Chen, X.										NeurIPS 2022 Competition: Driving SMARTS [arXiv]								arXiv								10 pp.	10 pp.											Journal Paper	14 Nov. 2022	2022	Driving SMARTS is a regular competition designed to tackle problems caused by the distribution shift in dynamic interaction contexts that are prevalent in real-world autonomous driving (AD). The proposed competition supports methodologically diverse solutions, such as reinforcement learning (RL) and offline learning methods, trained on a combination of naturalistic AD data and open-source simulation platform SMARTS. The two-track structure allows focusing on different aspects of the distribution shift. Track 1 is open to any method and will give ML researchers with different backgrounds an opportunity to solve a real-world autonomous driving challenge. Track 2 is designed for strictly offline learning methods. Therefore, direct comparisons can be made between different methods with the aim to identify new promising research directions. The proposed setup consists of 1) realistic traffic generated using real-world data and micro simulators to ensure fidelity of the scenarios, 2) framework accommodating diverse methods for solving the problem, and 3) baseline method. As such it provides a unique opportunity for the principled investigation into various aspects of autonomous vehicle deployment.									0	0	0	0	0	0	0																		2023-05-12	INSPEC:23020996		
C	Kim, Yujin; Pae, Dong-Sung; Jang, Sun-Ho; Kang, Seong-Woo; Lim, Myo-Taeg			IEEE	Pae, Dong-Sung/AAD-2470-2022						Reinforcement Learning for Autonomous Vehicle using MPC in Highway Situation								2022 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC)													10.1109/ICEIC54506.2022.9748810							Proceedings Paper	2022	2022	Path planning for Autonomous Vehicle(AV) is a challenging problem, as the vehicle is required to obey the traffic rules while avoiding the collision with the other vehicles. Model Predictive Control(MPC) is one of the popular approach for proposing a feasible and stable path by reflecting vehicle dynamics in solving objective function and constraining the expected future control input. However, one of the drawbacks with this approach is that the demanded computational power increases proportionally to the number of considered future inputs. This paper presents a path planning algorithm using Reinforcement Learning(RL). RL is similar to MPC in finding the optimal solution that maximizes the reward function which can be seen as intrinsic objective function. In that respect, adequate employment of MPC path in training resulted in improved efficiency and performance. Through the simulations, proposed method showed 98% of similarity with path of MPC and reduced computation time by 91.13% on average, thus it is qualified for real-time path planning.					International Conference on Electronics, Information, and Communication (ICEIC)International Conference on Electronics, Information, and Communication (ICEIC)	FEB 06-09, 2022FEB 06-09, 2022		Jeju, SOUTH KOREAJeju, SOUTH KOREA	0	0	0	0	0	0	0					978-1-6654-0934-6									Korea Univ, Dept Elect Engn, Seoul, South KoreaSangmyung Univ, Dept Software, Cheonan, South KoreaSangmyung Univ, Dept Human Intelligence & Robot Engn, Cheonan, South Korea				2023-03-15	WOS:000942023400138		
J	Peixoto, Maria J. P.; Azim, Akramul				Peixoto, Joelma/JYQ-5998-2024	Pereira Peixoto, Maria Joelma/0000-0001-9049-2579					Improving environmental awareness for autonomous vehicles								APPLIED INTELLIGENCE				53	2			1842	1854				10.1007/s10489-022-03468-6					MAY 2022		Article; Early Access		2023	Autonomous vehicles (AVs) have multiple tasks with different priorities and safety levels where classic supervised learning techniques are no longer applicable. Thus, reinforcement learning (RL) algorithms become increasingly appropriate for this domain as the RL algorithms can act on complex problems and adapt their responses in the face of unforeseen situations and environments. The RL agent aims to perform the action that guarantees the optimal reward with the best score. The problem with this approach is if the agent finds a possible optimal action with a reasonable premium and gets stuck in this mediocre strategy, which at the same time is neither the best nor the worst solution. Therefore, the agent avoids performing a more extensive exploration to find new paths and learn alternatives to generate a higher reward. To alleviate this problem, we research the behavior of two types of noise in AVs training. We analyze the results and point out the noise method that most stimulates exploration. A vast exploration of the environment is highly relevant to AVs because they know more about the environment and learn alternative ways of acting in the face of uncertainties. With that, AVs can expect more reliable actions in front of sudden changes in the environment. According to our experiments' results in a simulator, we can see that noise allows the autonomous vehicle to improve its exploration and increase the reward.									2	1	0	0	0	0	3			0924-669X	1573-7497										Ontario Tech Univ, Dept Elect Comp & Software Engn, Oshawa, ON, Canada	Ontario Tech Univ			2022-05-12	WOS:000790100900004		
J	Liu, Z.; Gao, H.; Ma, H.; Cai, S.; Hu, Y.; Qu, T.; Chen, H.; Gong, X.										Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation [arXiv]								Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation [arXiv]																				Preprint	2023	2023	Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.									0	0	0	0	0	0	0																		2023-10-22	INSPEC:23819415		
C	Soleimanitaleb, Zahra; Keyvanrad, Mohammad Ali; Jafari, Ali			IEEE							Object Tracking Methods:A Review								2019 9TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE 2019)								282	288				10.1109/iccke48569.2019.8964761							Proceedings Paper	2019	2019	Object tracking is one of the most important tasks in computer vision that has many practical applications such as traffic monitoring, robotics, autonomous vehicle tracking, and so on. Different researches have been done in recent years, but because of different challenges such as occlusion, illumination variations, fast motion, etc. researches in this area continues. In this paper, various methods of tracking objects are examined and a comprehensive classification is presented that classified tracking methods into four main categories of feature-based, segmentation-based, estimation-based, and learning-based methods that each of which has its own sub-categories. The main focus of this paper is on learning-based methods, which are classified into three categories of generative methods, discriminative methods, and reinforcement learning. One of the sub-categories of the discriminative model is deep learning. Because of high-performance, deep learning has recently been very much considered.yyyyyy					9th International Conference on Computer and Knowledge Engineering (ICCKE)9th International Conference on Computer and Knowledge Engineering (ICCKE)	OCT 24-25, 2019OCT 24-25, 2019	Ferdowsi Univ Mashhad, Dept Comp EngnFerdowsi Univ Mashhad, Dept Comp Engn	Ferdowsi Univ Mashhad, Mashhad, IRANFerdowsi Univ Mashhad, Mashhad, IRAN	24	0	0	0	1	0	25					978-1-7281-5075-8									Malek Ashtar Univ Technol, Shahin Shahr, Iran				2019-01-01	WOS:000540216700045		
J	Zhang, Kun; Su, Rong; Zhang, Huaguang; Tian, Yunlin				Su, Rong/A-5036-2011	Su, Rong/0000-0003-3448-0586; Zhang, K./0000-0003-4527-6847					Adaptive Resilient Event-Triggered Control Design of Autonomous Vehicles With an Iterative Single Critic Learning Framework								IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS				32	12			5502	5511				10.1109/TNNLS.2021.3053269							Article	DEC 2021	2021	This article investigates the adaptive resilient event-triggered control for rear-wheel-drive autonomous (RWDA) vehicles based on an iterative single critic learning framework, which can effectively balance the frequency/changes in adjusting the vehicle's control during the running process. According to the kinematic equation of RWDA vehicles and the desired trajectory, the tracking error system during the autonomous driving process is first built, where the denial-of-service (DoS) attacking signals are injected into the networked communication and transmission. Combining the event-triggered sampling mechanism and iterative single critic learning framework, a new event-triggered condition is developed for the adaptive resilient control algorithm, and the novel utility function design is considered for driving the autonomous vehicle, where the control input can be guaranteed into an applicable saturated bound. Finally, we apply the new adaptive resilient control scheme to a case of driving the RWDA vehicles, and the simulation results illustrate the effectiveness and practicality successfully.									53	0	0	0	0	0	53			2162-237X	2162-2388										Chinese Acad Sci, Acad Math & Syst Sci, Inst Syst Sci, Key Lab Syst & Control, Beijing 100049, Peoples R ChinaNanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, SingaporeNortheastern Univ, State Key Lab Synthet Automat Proc Ind, Shenyang 110819, Peoples R ChinaNortheastern Univ, Sch Informat Sci & Engn, Shenyang 110819, Peoples R ChinaUniv Wollongong, Fac Business, Wollongong, NSW 2522, Australia				2021-12-11	WOS:000724480600028	33534717	
B	Deb Raha, A.; Shirajum Munir, M.; Adhikary, A.; Qiao, Y.; Park, S.-B.; Seon Hong, C.										An Artificial Intelligent-Driven Semantic Communication Framework for Connected Autonomous Vehicular Network								2023 International Conference on Information Networking (ICOIN)								352	7				10.1109/ICOIN56518.2023.10049005							Conference Paper	2023	2023	Semantic communication will considerably enhance transmission efficiency by exploring and only transmitting semantic information. However, most of the previous work in this field is limited to particular applications such as text, audio, or images and does not consider task-oriented communications, where the effectiveness of the transmitted information must be taken into account for completing a specific task. This paper focuses on developing a semantic communication framework for a high altitude platform (HAP)-supported fully connected autonomous vehicle network. A system model is proposed in which the traffic infrastructure (TI) transmits its semantic information to the macro base station (MBS) whenever it observes a connected and autonomous vehicle (CAV). The semantic information has been extracted using a convolutional autoencoder (CAE) as the encoder of CAE gives a smaller representation of the input data. Then, after receiving the semantic concept, the MBS decides on an appropriate action for the CAVs. A proximal policy optimization (PPO) algorithm in the MBS for interpreting and making a decision for the semantic concepts. Simulation results show that the proposed method can reduce up to 63.26% of the communication cost.					2023 International Conference on Information Networking (ICOIN)2023 International Conference on Information Networking (ICOIN)	11-14 Jan. 202311-14 Jan. 2023	KICSKICS	Bangkok, ThailandBangkok, Thailand	0	0	0	0	0	0	0					978-1-6654-6268-6									Dept. of Comput. Sci. & Eng., Kyung Hee Univ., Yongin, South KoreaDept. of Artificial Intelligence, Kyung Hee Univ., Yongin, South Korea				2023-03-17	INSPEC:22683017		
J	Cai, Yingfeng; Zhou, Rong; Wang, Hai; Sun, Xiaoqiang; Chen, Long; Li, Yicheng; Liu, Qingchao; He, Youguo				Liu, Qingchao/AAE-6849-2021; Yan, Jing/JFA-6705-2023	Wang, Hai/0000-0002-9136-8091; Cai, Yingfeng/0000-0002-0633-9887					Rule-constrained reinforcement learning control for autonomous vehicle left turn at unsignalized intersection								IET INTELLIGENT TRANSPORT SYSTEMS				17	11			2143	2153				10.1049/itr2.12336					FEB 2023		Article; Early Access		2023	Controlling an autonomous vehicle's unprotected left turn at an intersection is a challenging task. Traditional rule-based autonomous driving decision and control algorithms struggle to construct accurate and trustworthy mathematical models for such circumstances, owing to their considerable uncertainty and unpredictability. To overcome this problem, a rule-constrained reinforcement learning (RCRL) control method is proposed in this work for autonomous driving. To train a reinforcement learning controller with rule constraints, outcomes of the path planning module are used as a goal condition in the reinforcement learning framework. Since they include vehicle dynamics, the proposed approach is safer and more reliable compared to end-to-end learning, thereby ensuring that the generated trajectories are locally optimal while adjusting to unpredictable situations. In the experiments, a highly randomized two-way four-lane intersection is established based on the CARLA simulator to verify the effectiveness of the proposed RCRL control method. Accordingly, the results show that the proposed method can provide real-time safe planning and ensure high passing efficiency for autonomous vehicles in the unprotected left turn task.									0	0	0	0	0	0	0			1751-956X	1751-9578										Jiangsu Univ, Automot Engn Res Inst, Dept Intelligent Transportat, Zhenjiang, Peoples R ChinaTraff Engn Jiangsu Univ, Sch Automot, Dept Vehicle Engn, Zhenjiang, Peoples R ChinaZhenjiang City Jiangsu Univ Engn Technol Res Inst, Dept Vehicle Engn, Zhenjiang, Peoples R China	Traff Engn Jiangsu UnivZhenjiang City Jiangsu Univ Engn Technol Res Inst			2023-03-01	WOS:000926931300001		
B	Capasso, A.P.; Maramotti, P.; Dell'Eva, A.; Broggi, A.										End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning								2021 IEEE Intelligent Vehicles Symposium (IV)								443	50				10.1109/IV48863.2021.9575135							Conference Paper	2021	2021	Navigating through intersections is one of the main challenging tasks for an autonomous vehicle. However, for the majority of intersections regulated by traffic lights, the problem could be solved by a simple rule-based method in which the autonomous vehicle behavior is closely related to the traffic light states. In this work, we focus on the implementation of a system able to navigate through intersections where only traffic signs are provided. We propose a multi-agent system using a continuous, model-free Deep Reinforcement Learning algorithm used to train a neural network for predicting both the acceleration and the steering angle at each time step. We demonstrate that agents learn both the basic rules needed to handle intersections by understanding the priorities of other learners inside the environment, and to drive safely along their paths. Moreover, a comparison between our system and a rule-based method proves that our model achieves better results especially with dense traffic conditions. Finally, we test our system on real world scenarios using real recorded traffic data, proving that our module is able to generalize both to unseen environments and to different traffic conditions.					2021 IEEE Intelligent Vehicles Symposium (IV)2021 IEEE Intelligent Vehicles Symposium (IV)	11-15 July 202111-15 July 2021	Toyota; TierIV; Denso; AISIN Corp.; J-Quad; sensetime; IDTechEx; Qualcomm; Toshiba; SonyToyota; TierIV; Denso; AISIN Corp.; J-Quad; sensetime; IDTechEx; Qualcomm; Toshiba; Sony	Nagoya, JapanNagoya, Japan	12	0	0	0	0	0	12					978-1-7281-5394-0									VisLab, Univ. of Parma, Parma, ItalyVisLab, Univ. of Bologna, Bologna, ItalyVisLab srl, Ambarella Inc. Co., Parma, Italy				2022-02-25	INSPEC:21412987		
B	Zhang, C.; Pan, H.; Sun, W.						Xingjian Jing; Hu Ding; Jiqiang Wang				Dual-Loop Adaptive Dynamic Programming for Autonomous Vehicle Trajectory Following Control Against Actuator Faults								Advances in Applied Nonlinear Dynamics, Vibration and Control -2021: The proceedings of 2021 International Conference on Applied Nonlinear Dynamics, Vibration and Control (ICANDVC2021). Lecture Notes in Electrical Engineering (799)								199	211				10.1007/978-981-16-5912-6_15							Conference Paper	2022	2022	This article presents a novel control strategy based on dual-loop adaptive dynamic programming (ADP) to optimize the tracking performance and ensure the security of autonomous vehicles when actuator faults occur. The proposed dual-loop ADP of controller, composed of two online policy iteration algorithms and an adaptive observer for fault-tolerant control (FTC), guarantees the online tracking accuracy of underactuated systems with uncertain faults, which is difficult to control by single ADP. In addition to actuator faults, the proposed controller can also address the external disturbances such as icy roads and uncertainty of wheel cornering stiffness, which means the controller possesses better robustness. The autonomous vehicle is simulated in multiple driving scenarios with different types of faults, demonstrating the effectiveness of the control strategy.					International Conference on Applied Nonlinear Dynamics, Vibration and ControlInternational Conference on Applied Nonlinear Dynamics, Vibration and Control	23-25 Aug. 202123-25 Aug. 2021		Hong Kong, ChinaHong Kong, China	0	0	0	0	0	0	0					978-981-16-5912-6									Res. Inst. of Intelligent Control & Syst., Harbin Inst. of Technol., Harbin, ChinaDept. of Comput., The Hong Kong Polytech. Univ., Hong Kong, ChinaSch. of Comput. Eng., Shanghai Univ., Shanghai, ChinaNanjing Univ. of Aeronautics & Astronautics, Nanjing, China				2023-08-25	INSPEC:21793738		
C	Wang, Yongshuai; Zheng, Chen; Sun, Mingwei; Chen, Zengqiang; Sun, Qinglin						Sun, M; Chen, Z				Reinforcement-learning-aided adaptive control for autonomous driving with combined lateral and longitudinal dynamics								2023 IEEE 12TH DATA DRIVEN CONTROL AND LEARNING SYSTEMS CONFERENCE, DDCLS		Data Driven Control and Learning Systems						840	845				10.1109/DDCLS58216.2023.10166569							Proceedings Paper	2023	2023	This paper presents a deep reinforcement learning-aided controller for a 3-DOF autonomous vehicle with combined lateral and longitudinal dynamics. In this scheme, the active disturbance rejection control (ADRC) gives full play to its advantages of being model-free and being able to estimate and compensate for internal uncertainties and external disturbances in real-time, and deep deterministic policy gradient (DDPG) fully considers safety, comfort, economy, and combines driving demand with state, action, reward to achieve real-time adaptive adjustment of control parameters. Thus, the adaptive controller can better deal with uncertainties from modeling, parameters, and driving environment, and self-learning and adaptation ability is obtained simultaneously. Moreover, simulation results illustrate that the adaptive controller performs satisfactorily for different driving operations and environments due to the online tuning and optimization of control parameters.					IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)	MAY 12-14, 2023MAY 12-14, 2023	IEEE; Hunan Univ Sci & Technol; Chinese Assoc Automat, Tech Comm Data Driven Control, Learning & Optimizat; Qingdao Univ; IEEE Beijing Sect; Beijing Informat Sci & Technol UnivIEEE; Hunan Univ Sci & Technol; Chinese Assoc Automat, Tech Comm Data Driven Control, Learning & Optimizat; Qingdao Univ; IEEE Beijing Sect; Beijing Informat Sci & Technol Univ	Xiangtan, PEOPLES R CHINAXiangtan, PEOPLES R CHINA	0	0	0	0	0	0	0			2767-9853		979-8-3503-2105-0									Nankai Univ, Coll Artificial Intelligence, Tianjin 300350, Peoples R ChinaBeijing Inst Astronaut Syst Engn, Beijing 100076, Peoples R China	Beijing Inst Astronaut Syst Engn			2023-09-06	WOS:001047443700143		
C	Zhang, Songan; Peng, Huei; Nageshrao, Subramanya; Tseng, H. Eric			IEEE COMP SOC	Zhang, Songan/AAE-1463-2019						Generating Socially Acceptable Perturbations for Efficient Evaluation of Autonomous Vehicles								2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW 2020)		IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops						1341	1347				10.1109/CVPRW50498.2020.00173							Proceedings Paper	2020	2020	Deep reinforcement learning methods have been considered and implemented for autonomous vehicle's decision-making in recent years. A key issue is that deep neural networks can be fragile to adversarial attacks through unseen inputs, and thus the reinforcement learning policy, that uses deep neural network would be also fragile to malicious attacks or benign but out of distribution perturbations. In this paper, we address the latter issue: we focus on generating socially acceptable perturbations (SAP), so that the autonomous vehicle (AV agent under evaluation), instead of the challenging vehicle (challenger), is primarily responsible for the crash. In our process, one challenger is added to the environment and trained by deep reinforcement learning to generate the desired perturbation. The reward is designed so that the challenger aims to fail the AV agent in a socially acceptable way. After training the challenger, the AV agent policy is evaluated in both the original naturalistic environment and the environment with one challenger. The results show that the AV agent policy which is safe in the naturalistic environment has many crashes in the perturbed environment.					IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)	JUN 14-19, 2020JUN 14-19, 2020	IEEE; CVF; IEEE Comp SocIEEE; CVF; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	7	0	0	0	0	0	7			2160-7508		978-1-7281-9360-1									Univ Michigan, Mech Engn, Ann Arbor, MI 48109 USAFord Motor Co, Dearborn, MI 48121 USA				2020-01-01	WOS:000788279001051		
C	Raha, Avi Deb; Munir, Md Shirajum; Adhikary, Apurba; Qiao, Yu; Park, Seong-Bae; Hong, Choong Seon			IEEE	Raha, Avi Deb/GQI-1241-2022; Munir, Md. Shirajum/AAB-4647-2020	Raha, Avi Deb/0000-0003-0240-1214; Munir, Md. Shirajum/0000-0002-7255-1085; Adhikary, Apurba/0000-0003-3970-1878					An Artificial Intelligent-Driven Semantic Communication Framework for Connected Autonomous Vehicular Network								2023 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING, ICOIN		International Conference on Information Networking						352	357				10.1109/ICOIN56518.2023.10049005							Proceedings Paper	2023	2023	Semantic communication will considerably enhance transmission efficiency by exploring and only transmitting semantic information. However, most of the previous work in this field is limited to particular applications such as text, audio, or images and does not consider task-oriented communications, where the effectiveness of the transmitted information must be taken into account for completing a specific task. This paper focuses on developing a semantic communication framework for a high altitude platform (HAP)-supported fully connected autonomous vehicle network. A system model is proposed in which the traffic infrastructure (TI) transmits its semantic information to the macro base station (MBS) whenever it observes a connected and autonomous vehicle (CAV). The semantic information has been extracted using a convolutional autoencoder (CAE) as the encoder of CAE gives a smaller representation of the input data. Then, after receiving the semantic concept, the MBS decides on an appropriate action for the CAVs. A proximal policy optimization (PPO) algorithm in the MBS for interpreting and making a decision for the semantic concepts. Simulation results show that the proposed method can reduce up to 63.26% of the communication cost.					37th International Conference on Information Networking (ICOIN)37th International Conference on Information Networking (ICOIN)	JAN 11-14, 2023JAN 11-14, 2023	IEEE; IEEE Comp Soc; IEICE Commun Soc; Korean Inst Informat Scientists & Engineers; Korean Inst Commun & Informat SciIEEE; IEEE Comp Soc; IEICE Commun Soc; Korean Inst Informat Scientists & Engineers; Korean Inst Commun & Informat Sci	Bangkok, THAILANDBangkok, THAILAND	0	0	0	0	0	0	0			1976-7684		978-1-6654-6268-6									Kyung Hee Univ, Dept Comp Sci & Engn, Yongin 17104, South KoreaKyung Hee Univ, Dept Artificial Intelligence, Yongin 17104, South Korea				2023-07-02	WOS:000981938900062		
J	Kundu, M.; Kumar, D.J.N.				Dirisala, J Nagendra Kumar/AAS-5521-2021	Dirisala, J Nagendra Kumar/0000-0001-8527-9976; kundu, mayuri/0000-0002-2669-9564					Route Optimization of Unmanned Aerial Vehicle by using Reinforcement Learning								Journal of Physics: Conference Series				1921				012076 (6 pp.)	012076 (6 pp.)				10.1088/1742-6596/1921/1/012076							Conference Paper; Journal Paper	2021	2021	The study proposes the machine learning based algorithm for autonomous vehicle. The dynamic characteristic of unmanned aerial vehicle and real time disturbances such as wind current, obstacles are considered. The novelty of the work lies in the introduction of reinforcement learning for achieving optimized path which can be followed by the unmanned aerial vehicles to complete the tour from initial to destination point. The feasible optimal route may be found by incorporating the L algorithm with a reasonable optimality and computational cost when map and current field data are given.					First International Conference on Advances in Smart Sensor, Signal Processing and Communication Technology (ICASSCT 2021)First International Conference on Advances in Smart Sensor, Signal Processing and Communication Technology (ICASSCT 2021)	19-20 March 202119-20 March 2021		Goa, IndiaGoa, India	0	0	0	0	0	0	0			1742-6596											Dept. of Comput. Sci. & Eng., Vishnu Inst. of Technol., Bhimavaram, IndiaDept. of Inf. & Technol., Vishnu Inst. of Technol., Bhimavaram, India				2021-09-03	INSPEC:20871047		
C	Pal, Mayank K.; Bhati, Rupali; Sharma, Anil; Kaul, Sanjit K.; Anand, Saket; Sujit, P. B.			IEEE	PB, Sujit/ABY-5084-2022	PB, Sujit/0000-0002-7297-1493; Kaul, Sanjit Krishnan/0000-0001-5867-8584					A Reinforcement Learning Approach to Jointly Adapt Vehicular Communications and Planning for Optimized Driving								2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						3287	3293											Proceedings Paper	2018	2018	Our premise is that autonomous vehicles must optimize communications and motion planning jointly. Specifically, a vehicle must adapt its motion plan staying cognizant of communications rate related constraints and adapt the use of communications while being cognizant of motion planning related restrictions that may be imposed by the on-road environment. To this end, we formulate a reinforcement learning problem wherein an autonomous vehicle jointly chooses (a) a motion planning action that executes on-road and (hi a communications action of querying sensed information from the infrastructure. The goal is to optimize the driving utility of the autonomous vehicle. We apply the Q-learning algorithm to make the vehicle learn the optimal policy, which makes the optimal choice of planning and communications actions at any given time. We demonstrate the ability of the optimal policy to smartly adapt communications and planning actions, while achieving large driving utilities, using simulations.					21st IEEE International Conference on Intelligent Transportation Systems (ITSC)21st IEEE International Conference on Intelligent Transportation Systems (ITSC)	NOV 04-07, 2018NOV 04-07, 2018	IEEE; IEEE Intelligent Transportat Syst SocIEEE; IEEE Intelligent Transportat Syst Soc	Maui, HIMaui, HI	3	0	0	0	0	0	3			2153-0009		978-1-7281-0323-5									IIIT Delhi, Delhi, India				2019-02-19	WOS:000457881303042		
B	Saqib, N.; Yousuf, M.M.					Yousuf, Mir Mohammad/0009-0001-7453-3358					Design and Implementation of Shortest Path Line Follower Autonomous Rover Using Decision Making Algorithms								2021 Asian Conference on Innovation in Technology (ASIANCON)								6 pp.	6 pp.				10.1109/ASIANCON51346.2021.9544672							Conference Paper	2021	2021	For an autonomous rover, navigating through an unknown environment achieving the target location is the key feature of the rover. Also, path finding is the main problem in the navigation of autonomous vehicles. This is generally done via a dynamic control paradigm, which requires local translation from defined states to actions. Reinforcement learning can be used to develop a control strategy that has learning capabilities in an uncertain environment. The result analysis shows that an autonomous vehicle that is trained by reinforcement can roam freely in a useful or "reasonably" manner, and therefore its training process is increased significantly.					2021 Asian Conference on Innovation in Technology (ASIANCON)2021 Asian Conference on Innovation in Technology (ASIANCON)	27-29 Aug. 202127-29 Aug. 2021		Pune, IndiaPune, India	0	0	0	0	0	0	0					978-1-7281-8402-9									Dept. of Comput. Sci. & Eng., Lovely Professional Univ., Phagwara, India				2021-12-10	INSPEC:21202814		
B	Gotad, Siddhesh Vivek										Application of Neural Networks for Design and Development of Low-cost Autonomous Vehicles																												Dissertation/Thesis	Jan 01 2019	2019										0	0	0	0	0	0	0					9781658410892									North Carolina State University, North Carolina, United States	North Carolina State University				PQDT:67851311		
C	Chen, Jun; Meng, Xiangyu; Li, Zhaojian			IEEE	Chen, Jun/H-4506-2011	Chen, Jun/0000-0002-0934-8519					Reinforcement Learning-based Event-Triggered Model Predictive Control for Autonomous Vehicle Path Following								2022 AMERICAN CONTROL CONFERENCE (ACC)								3342	3347											Proceedings Paper	2022	2022	Event-triggered model predictive control (MPC) has been proposed in literature to alleviate the high computational requirement of MPC. Compared to conventional time-triggered MPC, event-triggered MPC solves the optimal control problem only when an event is triggered. Several event-trigger policies have been studied in literature, typically requiring a prior knowledge of the MPC closed-loop system behavior. This paper addresses such limitation by investigating the use of model-free reinforcement learning (RL) to trigger MPC. Specifically, the optimal event-trigger policy is learnt by an RL agent through interactions with the MPC closed-loop system, whose dynamical behavior is assumed to be unknown to the RL agent. A reward function is defined to balance the closed-loop control performance and event frequency. As an illustrative example, the autonomous vehicle path following problem is used to demonstrate the applicability of using RL to learn and execute trigger policy for event-triggered MPC.					American Control Conference (ACC)American Control Conference (ACC)	JUN 08-10, 2022JUN 08-10, 2022	Amer Automat Control CouncilAmer Automat Control Council	Atlanta, GAAtlanta, GA	0	0	0	0	0	0	0					978-1-6654-5196-3									Oakland Univ, Dept Elect & Comp Engn, Rochester, MI 48309 USALouisiana State Univ, Div Elect & Comp Engn, Baton Rouge, LA 70803 USAMichigan State Univ, Dept Mech Engn, E Lansing, MI 48824 USA				2022-11-09	WOS:000865458703024		
J	Garcia Cuenca, Laura; Puertas, Enrique; Fernandez Andres, Javier; Aliane, Nourdine				Aliane, Nourdine/P-4414-2016; Puertas, Enrique/L-5656-2014; shen, jickie/GSN-7389-2022; CUENCA, LAURA GARCIA/AAA-8121-2019	Aliane, Nourdine/0000-0001-7739-881X; Puertas, Enrique/0000-0002-5115-1226; CUENCA, LAURA GARCIA/0000-0003-4687-9180; Fernandez, Javier/0000-0001-8230-7011					Autonomous Driving in Roundabout Maneuvers Using Reinforcement Learning with Q-Learning								ELECTRONICS				8	12					1536			10.3390/electronics8121536							Article	DEC 2019	2019	Navigating roundabouts is a complex driving scenario for both manual and autonomous vehicles. This paper proposes an approach based on the use of the Q-learning algorithm to train an autonomous vehicle agent to learn how to appropriately navigate roundabouts. The proposed learning algorithm is implemented using the CARLA simulation environment. Several simulations are performed to train the algorithm in two scenarios: navigating a roundabout with and without surrounding traffic. The results illustrate that the Q-learning-algorithm-based vehicle agent is able to learn smooth and efficient driving to perform maneuvers within roundabouts.									30	0	0	0	0	0	31				2079-9292										Univ Europea Madrid, Sch Architecture Engn & Design, Tajo S-N, Madrid 28670, Spain				2020-01-24	WOS:000506679900003		
C	Balas, Cristian; Karlsen, Robert; Muench, Paul; Mikulski, Dariusz; Mohammed, Utayba; Al-Holou, Nizar						Shoemaker, CM; Nguyen, HG; Muench, PL				Collective trust estimation in multi-agent systems								UNMANNED SYSTEMS TECHNOLOGY XXI		Proceedings of SPIE		11021						UNSP 110210O			10.1117/12.2518751							Proceedings Paper	2019	2019	In previous work, a multi-layered neural network trust model, dubbed NeuroTrust, was introduced. This trust model was also implemented in an autonomous vehicles convoy simulation, in which speed and gap distance depended on trust. It has been shown that, in time, through on-line reinforcement learning, this trust model produces better results for significant performance metrics in the respective autonomous vehicle convoy when compared to a baseline trust algorithm. In this paper, the NeuroTrust model is expanded to leverage the experience of multiple decision-making agents. A trust aggregation method is proposed for NeuroTrust and is simulated for multiple autonomous vehicle convoy scenarios. It is shown that the NeuroTrust model tends to optimize faster by leveraging each agent's experience.					Conference on Unmanned Systems Technology XXIConference on Unmanned Systems Technology XXI	APR 16-18, 2019APR 16-18, 2019	SPIESPIE	Baltimore, MDBaltimore, MD	0	0	0	0	0	0	0			0277-786X	1996-756X	978-1-5106-2708-6									US Army, CCDC Ground Vehicle Syst Ctr, Ground Vehicle Robot, Warren, MI 48091 USAUniv Detroit Mercy, Dept Elect & Comp Engn, Detroit, MI 48221 USA	US Army			2019-10-22	WOS:000489759600018		
B	Harsha gullapalli, V.S.; M, P.										Machine Learning for Vehicle Behavioural Control								2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)								1453	8				10.1109/ICACCS54159.2022.9785076							Conference Paper	2022	2022	With the advancement of Artificial Intelligence, a revolution in the automotive field has taken place in last few decades. With the development of Advanced Driver Assistance Systems (ADAS), various sensors were available, which paved theway for fully- autonomous vehicles. The first autonomous car has appeared in 1980's, since then a lot of research has taken place in order to develop an autonomous vehicle, which can drive the passengers all by itself without any human-driver intervention. This project work is aimed at developing a simulation model of an Autonomous Vehicle. The so developed Autonomous Vehicle is able to navigate through a pre-defined path avoiding obstacles, walkers, and other vehicles autonomously without any human control. For attaining this driving autonomy, a Reinforcement Learning Algorithm was developed and trained using Deep Q- Learning technique and in-loop-simulations on CARLA-simulator. Upon training the agent for 20,000 episodes of simulation, it is overt from simulation results that the learning accuracy of the agent aggrandized, the average reward nabbed by the agent amplified, the agent's training loss subsided, which reinforce that the agent started to acquire more positive average-reward than the negative average-reward. At the 17,150th, episode of training, the agent concomitantly set forth peak learning accuracy, diminutive training loss and best average-reward. This tacit validate that the agent is adept in maneuvering without colliding with other vehicles, walkers, and with no lane invasions, autonomously from its experience earned from episodic in-loop-simulations.					2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)	25-26 March 202225-26 March 2022		Coimbatore, IndiaCoimbatore, India	0	0	0	0	0	0	0					978-1-6654-0816-5									Amrita Vishwa Vidyapeetham, Amritapuri, IndiaDept of Mech. Eng., Amrita Sch. of Eng., Coimbatore, India				2023-03-17	INSPEC:22480956		
C	Isele, David; Nakhaei, Alireza; Fujimura, Kikuo	Kosecka, J					Maciejewski, AA; Okamura, A; Bicchi, A; Stachniss, C; Song, DZ; Lee, DH; Chaumette, F; Ding, H; Li, JS; Wen, J; Roberts, J; Masamune, K; Chong, NY; Amato, N; Tsagwarakis, N; Rocco, P; Asfour, T; Chung, WK; Yasuyoshi, Y; Sun, Y; Maciekeski, T; Althoefer, K; AndradeCetto, J; Chung, WK; Demircan, E; Dias, J; Fraisse, P; Gross, R; Harada, H; Hasegawa, Y; Hayashibe, M; Kiguchi, K; Kim, K; Kroeger, T; Li, Y; Ma, S; Mochiyama, H; Monje, CA; Rekleitis, I; Roberts, R; Stulp, F; Tsai, CHD; Zollo, L				Safe Reinforcement Learning on Autonomous Vehicles								2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						6162	6167											Proceedings Paper	2018	2018	There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications. Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems. We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle.					25th IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)25th IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	OCT 01-05, 2018OCT 01-05, 2018	IEEE Robot & Automat Soc; IEEE Ind Elect Soc; Robot Soc Japan; Soc Instrument & Control Engineers; New Technol Fdn; IEEE; Adept MobileRobots; Willow Garage; Aldebaran Robot; Natl Instruments; Reflexxes GmbH; Schunk Intec S L U; Univ Carlos III Madrid; BOSCH; JD COM; Pal Robot; KUKA; Santander; Squirrel AI Learning; Baidu; Generat Robots; KINOVA Robot; Ouster; Univ Pablo Olavide Sevilla; Rapyuta Robot; SICK; TOYOTA; UP; Amazon; ARGO; Built Robot; Disney Res; Easy Mile; Hitachi; Robot; Khalifa Univ; Magazino; MathWorks; New Dexterity; Schunk; nuTonomy; PILZ; Prophesee; Rootnik; Saga Robot; Shadow; Soft Bank Robot; Anyverse; GalTech; Generat Robot; IEEE CAA Journal Automatica Sinica; Sci Robot, AAAS; TERASIEEE Robot & Automat Soc; IEEE Ind Elect Soc; Robot Soc Japan; Soc Instrument & Control Engineers; New Technol Fdn; IEEE; Adept MobileRobots; Willow Garage; Aldebaran Robot; Natl Instruments; Reflexxes GmbH; Schunk Intec S L U; Univ Carlos III Madrid; BOSCH; JD COM; Pal Robot; KUKA; Santander; Squirrel AI Learning; Baidu; Generat Robots; KINOVA Robot; Ouster; Univ Pablo Olavide Sevilla; Rapyuta Robot; SICK; TOYOTA; UP; Amazon; ARGO; Built Robot; Disney Res; Easy Mile; Hitachi; Robot; Khalifa Univ; Magazino; MathWorks; New Dexterity; Schunk; nuTonomy; PILZ; Prophesee; Rootnik; Saga Robot; Shadow; Soft Bank Robot; Anyverse; GalTech; Generat Robot; IEEE CAA Journal Automatica Sinica; Sci Robot, AAAS; TERAS	Madrid, SPAINMadrid, SPAIN	30	1	0	0	0	0	35			2153-0858		978-1-5386-8094-0									Honda Res Inst USA, Mountain View, CA 94043 USA				2019-03-07	WOS:000458872705097		
R	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano										Testing autonomous vehicle lane keeping assist system in BeamNG simulator								Zenodo													https://doi.org/10.5281/ZENODO.8252678							Data set	2023-09-26	2023	In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator. Road topologies were generated by our tool RIGAA (available at:Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: Creative Commons Attribution 4.0 International Open Access									0	0	0	0	0	0	0																		2023-12-30	DRCI:DATA2023206027914758		
J	KIM, Hyun; Seonhyung, YOO，; Jinwoo, LEE，; Beomyeol, BAEK，; Junghee, SHIN，										Real-time Dynamic Route Generation Algorithm for Demand-responsive Driverless Transit Operation (DRDTO) Applied to Corridors to Consider U-Turns			유턴을 고려한 교통축 적용 수요대응 자율주행 대중교통(DRDTO)의 실시간 동적 경로 생성 알고리즘					Korean Society of Transportation	대한교통학회지			40	2			260	276											research-article	2022	2022	Recently, autonomous vehicle is receiving much attention in various sectors including transportation and public transportation. This study presents an adaptive routing algorithm for the real-time demand response service of autonomous transit vehicles. The experimental setting includes the real-time demand occurring randomly over time within the study area. Each demand (service call) request the transit service from designated origin to destination. The routing algorithm is designed to make U-turn and skip stops to improve service and reduce the wait time of users. This study adapts Reinforcement Learning, one of the machine learning techniques, namely reinforcement learning, which can precede a complex calculation process in an offline process. Simulation experiments conducted on a testbed of the Chungju campus of the Korea National University of Transportation. The simulation results show that the proposed routing algorithm can improve the adaptive transit service over the fixed operation in selected performance indicators.				최근 자율주행 차량이 대중교통분야로 진출하여 관련 연구들이 활발히 진행되고 있다. 본 연구에서는 시공간적으로 다양하게 발생하는 실시간 호출수요에 대응하여 자율주행 차량에 동적경로를 제시하여 효율적인 대중교통 서비스를 제공할 수 있는 알고리즘을 개발했다. 실시간 호출수요는 다수의 출발지와 다수의 목적지로 구성되어 복잡하고 다양한 경우의 수를 고려하여 최적 경로를 제시할 수 있는 방안이 필수적이다. 복잡한 계산과정은 오프라인 과정에서 선행할 수 있는 방법으로 머신러닝의 강화학습 기술을 사용하여 제시하였다. 개발된 자율주행 대중교통(DTO)를 위한 실시간 호출 수요대응 동적경로 알고리즘은 테스트베드를 선정하여 시뮬레이션 실험이 이루어 졌다. 시뮬레이션 결과는 개발된 실시간 호출 수요대응 동적경로 생성 알고리즘을 고정노선의 운행결과 대비하여 효율적인 운행결과가 나타났다. 이는 실시간 호출수요량이 적을수록 효과가 크게 나타나지만 호출 수요량이 증가하면서 그 효과 폭이 점차적으로 감소했다.					0	0	0	0	0	0	0			1229-1366															2022-05-25	KJD:ART002834915		
J	Stafylopatis, A; Blekas, K										Autonomous vehicle navigation using evolutionary reinforcement learning								EUROPEAN JOURNAL OF OPERATIONAL RESEARCH				108	2			306	318				10.1016/S0377-2217(97)00372-X							Article; Proceedings Paper	JUL 16 1998	1998	Reinforcement learning schemes perform direct on-line search in control space. This makes them appropriate for modifying control rules to obtain improvements in the performance of a system. The effectiveness of a reinforcement learning strategy is studied here through the training of a learning classifier system (LCS) that controls the movement of an autonomous vehicle in simulated paths including left and right turns. The LCS comprises a set of condition-action rules (classifiers) that compete to control the system and evolve by means of a genetic algorithm (GA). Evolution and operation of classifiers depend upon an appropriate credit assignment mechanism based on reinforcement learning. Different design options and the role of various parameters have been investigated experimentally. The performance of vehicle movement under the proposed evolutionary approach is superior compared with that of other (neural) approaches based on reinforcement learning that have been applied previously to the same benchmark problem. (C) 1998 Elsevier Science B.V.					Workshop on Biologically Inspired Autonomous SystemsWorkshop on Biologically Inspired Autonomous Systems	MAR, 1996MAR, 1996	Natl Sci Fdn; Duke Univ, Off Vice Provost Int Acad ProgramsNatl Sci Fdn; Duke Univ, Off Vice Provost Int Acad Programs	DUKE UNIV, DURHAM, NORTH CAROLINADUKE UNIV, DURHAM, NORTH CAROLINA	28	1	0	0	1	0	32			0377-2217											Natl Tech Univ Athens, Dept Elect & Comp Engn, GR-15773 Athens, Greece				1998-07-16	WOS:000074232900006		
C	Lee, Minkyung; Hong, Choong Seon			IEEE	Hong, Choong Seon/ABF-5527-2020	Hong, Choong Seon/0000-0003-3484-7333					Service Chaining Offloading Decision in the EdgeAI: A Deep Reinforcement Learning Approach								APNOMS 2020: 2020 21ST ASIA-PACIFIC NETWORK OPERATIONS AND MANAGEMENT SYMPOSIUM (APNOMS)		Asia-Pacific Network Operations and Management Symposium-APNOMS						393	396				10.23919/apnoms50412.2020.9237048							Proceedings Paper	2020	2020	Many mission critical devices are increasing with upcoming 56 network to fulfill a low latency for a real time network service on smart factory, autonomous vehicle, etc. Distributed cloud computing system also has a key role to execute the various mobile devices, because, an edge computing is the nearest from the mobile devices to provide low latency and computation energy consumption. In this paper, we consider the autonomous vehicles with video live streaming services. Especially, the vehicles require a low transmission delay as within 10 ms. To reduce a latency with low energy consumption, we propose a service chaining offloading decision with a deep reinforcement learning. We split tasks of the vehicle per service function blocks which have their own role. So it can do partial offloading and user association in a On-Device Edge of the vehicle and in the SBS at the same time. We can get results that service chaining offloading decision gives more optimal energy consumption with low-latency to autonomous vehicle users.					21st Asia-Pacific Network Operations and Management Symposium (APNOMS)21st Asia-Pacific Network Operations and Management Symposium (APNOMS)	SEP 22-25, 2020SEP 22-25, 2020	KICS, KNOM; IEICE ICM; IEEE; IEEE Commun SocKICS, KNOM; IEICE ICM; IEEE; IEEE Commun Soc	Daegu, SOUTH KOREADaegu, SOUTH KOREA	0	0	0	0	0	0	0			2576-8565	2576-8557	978-89-950043-8-8									Kyung Hee Univ, Dept Comp Sci & Engn, Yongin, South Korea				2021-04-23	WOS:000630323900081		
C	Kolomanski, Michal; Sakhai, Mustafa; Nowak, Jakub; Wielgosz, Maciej				Sakhai, Mustafa/JZD-4188-2024; Wielgosz, Maciej/J-5132-2018	Wielgosz, Maciej/0000-0002-4401-2957	Arai, K				Towards End-to-End Chase in Urban Autonomous Driving Using Reinforcement Learning								INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 3		Lecture Notes in Networks and Systems		544				408	426				10.1007/978-3-031-16075-2_29							Proceedings Paper	2023	2023	This paper addresses the challenging task of developing an autonomous chase protocol. First, training of an autonomous vehicle capable of driving autonomously from point A to B was developed to proceed with a chase protocol as a second step. A dedicated driving setup, based on a discrete action space and a single RGB camera, was developed through a series of experiments. A dedicated curriculum learning agenda allowed to train the model capable of performing all fundamental road maneuvers. Several reward functions were proposed, which enabled effective training of the agent. In the subsequent experiments, we selected the reward function and model that produced the most significant outcome, guaranteeing that the chasing car was within 25 m of a runaway car for 63% of the episode duration. To the best of our knowledge, this work is the first one that addressed the task of the chase in urban driving using the Reinforcement Learning approach.					Intelligent Systems Conference (IntelliSys)Intelligent Systems Conference (IntelliSys)	SEP 01-02, 2022SEP 01-02, 2022		Amsterdam, NETHERLANDSAmsterdam, NETHERLANDS	0	0	0	0	0	0	0			2367-3370	2367-3389	978-3-031-16075-2; 978-3-031-16074-5									AGH Univ Sci & Technol, Fac Comp Sci Elect & Telecommun, Al Adama Mickiewicza 30, PL-30059 Krakow, PolandACC Cyfronet AGH, Nawojki 11, PL-30950 Krakow, Poland				2023-07-09	WOS:000889408400029		
C	El Hamdani, Sara; Loudari, Salaheddine; Novotny, Stanislav; Bouchner, Petr; Benamar, Nabil			IEEE	El hamdani, Sara/AAP-5914-2020; Bouchner, Petr/ABF-2354-2021; Benamar, Nabil/D-8832-2019	El hamdani, Sara/0000-0002-5358-835X; Bouchner, Petr/0000-0002-8308-1656; Benamar, Nabil/0000-0002-1804-6977					A Markov Decision Process Model for a Reinforcement Learning-based Autonomous Pedestrian Crossing Protocol								2021 3RD IEEE MIDDLE EAST AND NORTH AFRICA COMMUNICATIONS CONFERENCE (MENACOMM)								147	151				10.1109/MENACOMM50742.2021.9678310							Proceedings Paper	2021	2021	Autonomous Traffic Management (ATM) systems empowered with Machine Learning (ML) technics are a promising solution for eliminating traffic light and decreasing traffic congestion in the future. However, few efforts have focused on integrating pedestrians in ATM, namely the static programming-based cooperative protocol called Autonomous Pedestrian Crossing (APC). In this paper, we model a Markov Decision Process ( MDP) to enable a Deep Reinforcement Learning (DRL)-based version of APC protocol that is able to dynamically achieve the same objectives (i.e. decreasing traffic delay at the crossing area). Using concrete state space, action set and reward functions, our model forces the Autonomous Vehicle (AV) to "think" and behave according to APC architecture. Compared to the traditional programming APC system, our approach permits the AV to learn from its previous experiences in non-signalized crossing and optimize the distance and the velocity parameters accordingly.					3rd IEEE Middle East and North Africa Communications Conference (MENACOMM)3rd IEEE Middle East and North Africa Communications Conference (MENACOMM)	DEC 03-05, 2021DEC 03-05, 2021	IEEE; IEEE Commun Soc, Morocco Chapter; IEEE ComSoc Chapters Consortium; IEEE Commun Soc, Jordan Chapter; IEEE Commun Soc, Lebanon Chapter; IEEE Commun Soc, Bahrain Chapter; IEEE Commun Soc, Saudi Arabia Chapter; IEEE Commun Soc, Tunisia ChapterIEEE; IEEE Commun Soc, Morocco Chapter; IEEE ComSoc Chapters Consortium; IEEE Commun Soc, Jordan Chapter; IEEE Commun Soc, Lebanon Chapter; IEEE Commun Soc, Bahrain Chapter; IEEE Commun Soc, Saudi Arabia Chapter; IEEE Commun Soc, Tunisia Chapter	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1					978-1-6654-3443-0									Tech Univ Prague, Fac Transportat Sci Czech, Prague, Czech RepublicUniv Ibn Tofail Kenitra, Kenitra, MoroccoMoulay Ismail Univ Meknes, Meknes, Morocco				2022-08-31	WOS:000841562200027		
J	Kontoravdis, D.; Likas, A.; Blekas, K.; Stafylopatis, A.										A fuzzy neural network approach to autonomous vehicle navigation								Proceedings EURISCON '94. European Robotics and Intelligent Systems Conference								243	52 vol.1											Conference Paper	1994	1994	The paper presents an application of the GARIC architecture to autonomous vehicle navigation. This method constitutes a fuzzy neural approach that maps fuzzy control rules into the architecture of a neural network and uses a reinforcement learning scheme employing an adaptive heuristic critic to adjust the parameters of the fuzzy variables. Since the original GARIC formulation assumes continuous valued control variables, the authors have developed a modified scheme that can be applied to problems assuming discrete control variables (as is the case with their motion control problem). Experimental tests on difficult grounds comprising left and right turns justify the effectiveness of the proposed method, since the vehicle learns to navigate almost perfectly in only a few steps and exhibits very steep learning curves compared to the pure reinforcement case.					European Conference on Robotics and Intelligent SystemsEuropean Conference on Robotics and Intelligent Systems	22-25 Aug. 199422-25 Aug. 1994		Malaga, SpainMalaga, Spain	1	0	0	0	0	0	1														Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece				1994-01-01	INSPEC:5247894		
C	Zhang, Yuxiang; Gao, Bingzhao; Zhou, Jinghua; Guo, Lulu; Chen, Hong			IEEE	Chen, Hong/A-2851-2012; Zhang, Yuxiang/GSJ-2574-2022	Chen, Hong/0000-0002-1724-8649; Zhang, Yuxiang/0000-0001-9047-9585					Velocity control in a right-turn across traffic scenario for autonomous vehicles using kernel-based reinforcement learning								2017 CHINESE AUTOMATION CONGRESS (CAC)		Chinese Automation Congress						6211	6216											Proceedings Paper	2017	2017	Recently, advanced control methods like machine leaning are increasingly applied to autonomous vehicle. This paper focuses on velocity control in a right-turn traffic scenario. A Markov Decision Processes(MDPs) is modeled and the actor-critic reinforcement learning architecture is employed. Then the kernel-based least squares policy iteration algorithm(KLSPI) is applied. Simulation results show that the proposed method can perform different policy in different cases, which preliminarily verify the rationality.					Chinese Automation Congress (CAC)Chinese Automation Congress (CAC)	OCT 20-22, 2017OCT 20-22, 2017	IEEE; CAA; IEEE Syst Man & Cybernet SocIEEE; CAA; IEEE Syst Man & Cybernet Soc	Jinan, PEOPLES R CHINAJinan, PEOPLES R CHINA	0	0	0	0	0	0	0			2688-092X	2688-0938	978-1-5386-3524-7									Jilin Univ, State Key Lab Automot Simulat & Control, Changchun 130025, Peoples R ChinaJilin Normal Univ, Coll Comp, Siping 136000, Peoples R ChinaJilin Univ, Dept Control Sci & Engn, Changchun 130025, Peoples R China				2018-04-27	WOS:000427816106053		
J	Havenstrom, Simen Theie; Rasheed, Adil; San, Omer				San, Omer/AAA-9282-2020	San, Omer/0000-0002-2241-4648; Rasheed, Adil/0000-0003-2690-983X					Deep Reinforcement Learning Controller for 3D Path Following and Collision Avoidance by Autonomous Underwater Vehicles								FRONTIERS IN ROBOTICS AND AI				7						566037			10.3389/frobt.2020.566037							Article	JAN 25 2021	2021	Control theory provides engineers with a multitude of tools to design controllers that manipulate the closed-loop behavior and stability of dynamical systems. These methods rely heavily on insights into the mathematical model governing the physical system. However, in complex systems, such as autonomous underwater vehicles performing the dual objective of path following and collision avoidance, decision making becomes nontrivial. We propose a solution using state-of-the-art Deep Reinforcement Learning (DRL) techniques to develop autonomous agents capable of achieving this hybrid objective without having a priori knowledge about the goal or the environment. Our results demonstrate the viability of DRL in path following and avoiding collisions towards achieving human-level decision making in autonomous vehicle systems within extreme obstacle configurations.									19	0	0	0	0	0	21			2296-9144											Norwegian Univ Sci & Technol, Dept Engn Cybernet, Trondheim, NorwaySINTEF Digital, Math & Cybernet, Trondheim, NorwayOklahoma State Univ, Sch Mech & Aerosp Engn, Stillwater, OK 74078 USA				2021-01-25	WOS:000615722500001	33585570	
C	Fleicher, Michael; Musicant, Oren; Azaria, Amos						Reformat, M; Zhang, D; Bourbakis, N				Using Physiological Metrics to Improve Reinforcement Learning for Autonomous Vehicles								2022 IEEE 34TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI		Proceedings-International Conference on Tools With Artificial Intelligence						1223	1230				10.1109/ICTAI56018.2022.00186							Proceedings Paper	2022	2022	Thanks to recent technological advances Autonomous Vehicles (AVs) are becoming available at some locations. Safety impacts of these devices have, however, been difficult to assess. In this paper we utilize physiological metrics to improve the performance of a reinforcement learning agent attempting to drive an autonomous vehicle in simulation. We measure the performance of our reinforcement learner in several aspects, including the amount of stress imposed on potential passengers, the number of training episodes required, and a score measuring the vehicle's speed as well as the distance successfully traveled by the vehicle, without traveling off-track or hitting a different vehicle. To that end, we compose a human model, which is based on a dataset of physiological metrics of passengers in an autonomous vehicle. We embed this model in a reinforcement learning agent by providing negative reward to the agent for actions that cause the human model an increase in heart rate. We show that such a "passenger-aware" reinforcement learner agent does not only reduce the stress imposed on hypothetical passengers, but, quite surprisingly, also drives safer and its learning process is more effective than an agent that does not obtain rewards from a human model.					34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)	OCT 31-NOV 02, 2022OCT 31-NOV 02, 2022	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			1082-3409		979-8-3503-9744-4									Ariel Univ, Comp Sci Dept, Ariel, IsraelAriel Univ, Ind Engn Dept, Ariel, Israel				2023-06-18	WOS:000992544400178		
J	Ashwin, S.H.; Naveen Raj, R.					Naveen Raj, Rashmi/0000-0002-7683-4955					Deep reinforcement learning for autonomous vehicles: lane keep and overtaking scenarios with collision avoidance								International Journal of Information Technology								3541	53				10.1007/s41870-023-01412-6							Journal Paper	2023	2023	Numerous accidents and fatalities occur every year across the world as a result of the reckless driving of drivers and the ever-increasing number of vehicles on the road. Due to these factors, autonomous cars have attracted enormous attention as a potentially game-changing technology to address a number of persistent problems in the transportation industry. Autonomous vehicles need to be modeled as intelligent agents with the capacity to observe, and perceive the complex and dynamic environment on the road, and decide an action with the highest priority to the lives of people in every scenarios. The proposed deep deterministic policy gradient-based sequential decision algorithm models the autonomous vehicle as a learning agent and trains it to drive on a lane, overtake a static and a moving vehicle, and avoid collisions with obstacles on the front and right side. The proposed work is simulated using a TORC simulator and has shown the expected performance under the above-said scenarios.									1	0	0	0	0	0	1			2511-2112											Dept. of Inf. & Commun. Technol., Manipal Acad. of Higher Educ. Manipal Inst. of Technol., Manipal, India				2023-10-22	INSPEC:23830045		
C	Liu, Shuijing; Chang, Peixin; Chen, Haonan; Chakraborty, Neeloy; Driggs-Campbell, Katherine			IEEE	Liu, Shuijing/JRX-8725-2023; Chakraborty, Neeloy/JVN-1903-2024	Chakraborty, Neeloy/0000-0001-7132-6671					Learning to Navigate Intersections with Unsupervised Driver Trait Inference								2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022)								3576	3582				10.1109/ICRA46639.2022.9811635							Proceedings Paper	2022	2022	Navigation through uncontrolled intersections is one of the key challenges for autonomous vehicles. Identifying the subtle differences in hidden traits of other drivers can bring significant benefits when navigating in such environments. We propose an unsupervised method for inferring driver traits such as driving styles from observed vehicle trajectories. We use a variational autoencoder with recurrent neural networks to learn a latent representation of traits without any ground truth trait labels. Then, we use this trait representation to learn a policy for an autonomous vehicle to navigate through a T-intersection with deep reinforcement learning. Our pipeline enables the autonomous vehicle to adjust its actions when dealing with drivers of different traits to ensure safety and efficiency. Our method demonstrates promising performance and outperforms state-of-the-art baselines in the T-intersection scenario.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 23-27, 2022MAY 23-27, 2022	IEEE; IEEE Robot & Automat SocIEEE; IEEE Robot & Automat Soc	Philadelphia, PAPhiladelphia, PA	0	0	0	0	0	0	0					978-1-7281-9681-7									Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA				2023-04-26	WOS:000941265702041		
C	Liu, Yuqi; Zhang, Qichao; Zhao, Dongbin			IEEE	Liu, Yuqi/HMV-1429-2023						A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios								2021 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2021)													10.1109/SSCI50451.2021.9660172							Proceedings Paper	2021	2021	In recent years, control under urban intersection scenarios has become an emerging research topic. In such scenarios, the autonomous vehicle confronts complicated situations since it must deal with the interaction with social vehicles timely while obeying the traffic rules. Generally, the autonomous vehicle is supposed to avoid collisions while pursuing better efficiency. The existing work fails to provide a framework that emphasizes the integrity of the scenarios while deploying and testing reinforcement learning(RL) methods. Specifically, we propose a benchmark for training and testing RL-based autonomous driving agents in complex intersection scenarios, which is called RL-CIS. Then, a set of baselines consisting various algorithms are deployed. The test benchmark and baselines provide a fair and comprehensive training and testing platform for the study of RL for autonomous driving in the intersection scenario, advancing RL-based methods for autonomous driving control. The code of our proposed framework can be found at https://github.com/liuyuqi123/ComplexUrbanScenarios.					IEEE Symposium Series on Computational Intelligence (IEEE SSCI)IEEE Symposium Series on Computational Intelligence (IEEE SSCI)	DEC 05-07, 2021DEC 05-07, 2021	IEEE; IEEE Computat Intelligence SocIEEE; IEEE Computat Intelligence Soc	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0					978-1-7281-9048-8									Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing, Peoples R ChinaUniv Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China				2022-07-29	WOS:000824464300348		
J	Yildiz, A.; Yel, E.; Corso, A.L.; Wray, K.H.; Witwicki, S.J.; Kochenderfer, M.J.										Experience Filter: Using Past Experiences on Unseen Tasks or Environments [arXiv]								arXiv																				Journal Paper	29 May 2023	2023	One of the bottlenecks of training autonomous vehicle (AV) agents is the variability of training environments. Since learning optimal policies for unseen environments is often very costly and requires substantial data collection, it becomes computationally intractable to train the agent on every possible environment or task the AV may encounter. This paper introduces a zero-shot filtering approach to interpolate learned policies of past experiences to generalize to unseen ones. We use an experience kernel to correlate environments. These correlations are then exploited to produce policies for new tasks or environments from learned policies. We demonstrate our methods on an autonomous vehicle driving through T-intersections with different characteristics, where its behavior is modeled as a partially observable Markov decision process (POMDP). We first construct compact representations of learned policies for POMDPs with unknown transition functions given a dataset of sequential actions and observations. Then, we filter parameterized policies of previously visited environments to generate policies to new, unseen environments. We demonstrate our approaches on both an actual AV and a high-fidelity simulator. Results indicate that our experience filter offers a fast, low-effort, and near-optimal solution to create policies for tasks or environments never seen before. Furthermore, the generated new policies outperform the policy learned using the entire data collected from past environments, suggesting that the correlation among different environments can be exploited and irrelevant ones can be filtered out.									0	0	0	0	0	0	0																		2023-07-14	INSPEC:23357213		
J	Kuutti, Sampo; Bowden, Richard; Fallah, Saber				Bowden, Richard/AAF-8283-2019	Bowden, Richard/0000-0003-3285-8020; Kuutti, Sampo/0000-0002-7020-4370; Fallah, Saber/0000-0002-1298-1040					Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages								SENSORS				21	6					2032			10.3390/s21062032							Article	MAR 2021	2021	The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control. However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles. In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent. By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy. This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle. We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance. Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone.									5	0	0	0	0	0	5				1424-8220										Univ Surrey, Connected & Autonomous Vehicles Lab, Guildford GU2 7XH, Surrey, EnglandUniv Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England				2021-03-01	WOS:000652713600001	33805601	
J	Du, Guodong; Zou, Yuan; Zhang, Xudong; Li, Zirui; Liu, Qi				li, zr/JTT-8305-2023	Liu, Qi/0000-0001-5172-0989; Du, Guodong/0000-0001-7011-268X					Hierarchical Motion Planning and Tracking for Autonomous Vehicles Using Global Heuristic Based Potential Field and Reinforcement Learning Based Predictive Control								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	8			8304	8323				10.1109/TITS.2023.3266195					APR 2023		Article; Early Access		2023	The autonomous vehicle is widely applied in various ground operations, in which motion planning and tracking control are becoming the key technologies to achieve autonomous driving. In order to further improve the performance of motion planning and tracking control, an efficient hierarchical framework containing motion planning and tracking control for the autonomous vehicles is constructed in this paper. Firstly, the problems of planning and control are modeled and formulated for the autonomous vehicle. Then, the logical structure of the hierarchical framework is described in detail, which contains several algorithmic improvements and logical associations. The global heuristic planning based artificial potential field method is developed to generate the real-time optimal motion sequence, and the prioritized Q-learning based forward predictive control method is proposed to further optimize the effectiveness of tracking control. The hierarchical framework is evaluated and validated by the numerical simulation, virtual driving environment simulation and real-world scenario. The results show that both the motion planning layer and the tracking control layer of the hierarchical framework perform better than other previous methods. Finally, the adaptability of the proposed framework is verified by applying another driving scenario. Furthermore, the hierarchical framework also has the ability for the real-time application.									1	0	0	0	0	0	1			1524-9050	1558-0016										Swiss Fed Inst Technol, Inst Dynam Syst & Control, CH-8092 Zurich, SwitzerlandNatl Engn Lab Elect Vehicles, Sch Mech Engn, Beijing 100081, Peoples R ChinaBeijing Inst Technol, Collaborat Innovat Ctr Elect Vehicles Beijing, Beijing 100081, Peoples R ChinaDelft Univ Technol, Dept Transport & Planning, NL-2628 CD Delft, NetherlandsBeijing Inst Technol, Sch Mech Engn, Beijing 100081, Peoples R China	Natl Engn Lab Elect Vehicles			2023-05-22	WOS:000976742800001		
C	Masmoudi, Mehdi; Ghazzai, Hakim; Frikha, Mounir; Massoud, Yehia			IEEE	Ghazzai, Hakim/K-2518-2019; Frikha, Mounir/HPC-2178-2023	Ghazzai, Hakim/0000-0002-8636-4264; 					Autonomous Car-Following Approach Based on Real-time Video Frames Processing								2019 IEEE INTERNATIONAL CONFERENCE OF VEHICULAR ELECTRONICS AND SAFETY (ICVES 19)																				Proceedings Paper	2019	2019	Car-following theory has received considerable attention from the transportation fields in the last decade. Autonomous vehicles are designed to provide convenient and safe driving by avoiding accidents caused by driver errors. However, it is always important to enhance the recognition of driver's driving-style in our roads. With car following models, automated vehicles can imitate the human behavior driving and assure a high safety level on road. All the built-in technologies must be integrated and complemented to achieve these goals. Automated object detection and the following process are one of the main research tasks that must be undertaken for this purpose. In this paper, we design a car following framework for autonomous vehicles using the reinforcement learning technique. The objective is to follow one leader car based on image-frames. To this end, we propose to employ the YOLOv3 object detector to detect vehicles the Q-learning technique to train the follower vehicle to autonomously navigate and follow the leader. Simulation results show the convergence of the model and investigate the behavior of the image-based autonomous vehicle in following its leader.					IEEE International Conference on Vehicular Electronics and Safety (ICVES)IEEE International Conference on Vehicular Electronics and Safety (ICVES)	SEP 04-06, 2019SEP 04-06, 2019	IEEEIEEE	Cairo, EGYPTCairo, EGYPT	6	0	0	0	0	0	6					978-1-7281-3473-4									Univ Carthage, Higher Sch Commun Tunis, Tunis, TunisiaStevens Inst Technol, Sch Syst & Enterprises, Hoboken, NJ 07030 USA				2019-01-01	WOS:000535695600004		
R	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano										Testing autonomous vehicle lane keeping assist system in BeamNG simulator								Figshare													https://doi.org/10.6084/m9.figshare.23968398.v1							Data set	2023-09-11	2023	In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator. Road topologies were generated by our tool RIGAA (available at: Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: CC BY 4.0									0	0	0	0	0	0	0																		2023-10-18	DRCI:DATA2023170027773837		
C	Nageshrao, Subramanya; Tseng, H. Eric; Filev, Dimitar			IEEE							Autonomous Highway Driving using Deep Reinforcement Learning								2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)		IEEE International Conference on Systems Man and Cybernetics Conference Proceedings						2326	2331											Proceedings Paper	2019	2019	The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. Due to this, formulating a rule based decision maker for selecting driving maneuvers may not be ideal. Similarly, it may not be efficient to solve optimal control problem in real-time for a predefined cost function. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with the simulated traffic. Here the decision maker is a deep neural network that provides an action choice for a given system state. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density.					IEEE International Conference on Systems, Man and Cybernetics (SMC)IEEE International Conference on Systems, Man and Cybernetics (SMC)	OCT 06-09, 2019OCT 06-09, 2019	IEEEIEEE	Bari, ITALYBari, ITALY	55	1	0	0	0	0	60			1062-922X		978-1-7281-4569-3									Ford Greenfield Labs, 3251 Hillview Ave, Palo Alto, CA 94304 USAFord Res & Innovat Ctr, 2101 Village Rd, Dearborn, MI 48124 USA	Ford Greenfield Labs			2020-04-21	WOS:000521353902057		
C	Moradipari, Ahmadreza; Bae, Sangjae; Alizadeh, Mahnoosh; Pari, Ehsan Moradi; Isele, David			IEEE							Predicting Parameters for Modeling Traffic Participants								2022 IEEE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						703	708				10.1109/ITSC55140.2022.9922467							Proceedings Paper	2022	2022	Accurately modeling the behavior of traffic participants is essential for safely and efficiently navigating an autonomous vehicle through heavy traffic. We propose a method, based on the intelligent driver model, that allows us to accurately model individual driver behaviors from only a small number of frames using easily observable features. On average, this method makes prediction errors that have less than 1 meter difference from an oracle with full-information when analyzed over a 10-second horizon of highway driving. We then validate the efficiency of our method through extensive analysis against a competitive data-driven method such as Reinforcement Learning that may be of independent interest.					IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)	OCT 08-12, 2022OCT 08-12, 2022	IEEEIEEE	Macau, PEOPLES R CHINAMacau, PEOPLES R CHINA	0	0	0	0	0	0	1			2153-0009		978-1-6654-6880-0									Honda Res Inst USA Inc, San Jose, CA 95134 USAUniv Calif Santa Barbara, Santa Barbara, CA 93106 USA				2023-04-26	WOS:000934720600109		
C	Bin Issa, Razin; Rahman, Md. Saferi; Das, Modhumonty; Barua, Monika; Alam, Md. Golam Rabiul			IEEE	Alam, Md Golam Rabiul/L-4373-2013	Alam, Md Golam Rabiul/0000-0002-9054-7557; Das, Modhumonty/0000-0002-2265-7412; Issa, Razin Bin/0000-0003-2283-3292					Reinforcement Learning based Autonomous Vehicle for Exploration and Exploitation of Undiscovered Track								2020 34TH INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2020)								276	281				10.1109/icoin48656.2020.9016539							Proceedings Paper	2020	2020	This research focuses on autonomous traversal of land vehicles through exploring undiscovered tracks and overcoming environmental barriers. Most of the existing systems can only operate and traverse in a distinctive mapped model especially in a known area. However, the proposed system which is trained by Deep Reinforcement learning can learn by itself to operate autonomously in extreme conditions. The dynamic double deep Q-learning (DDQN) model enables the proposed system not be confined to only in known environments. The ambient environmental obstacles are identified through Faster R-CNN for smooth movement of the autonomous vehicle. The exploration and exploitation strategies of DDQN enables the autonomous agent to learn proper decisions for various dynamic environments and tracks. The proposed model is tested in a gaming environment. It shows the overall effectiveness in traversing of autonomous land vehicles. The goal is to integrate Deep Reinforcement learning and Faster R-CNN to make the system effective to traverse through undiscovered paths by detecting obstacles.					34th International Conference on Information Networking (ICOIN)34th International Conference on Information Networking (ICOIN)	JAN 07-10, 2020JAN 07-10, 2020	IEEE; IEEE Comp Soc; IEICE Commun Soc; Korean Inst Informat Scientists & Engineers; Korean Inst Commun & Informat SciIEEE; IEEE Comp Soc; IEICE Commun Soc; Korean Inst Informat Scientists & Engineers; Korean Inst Commun & Informat Sci	Barcelona, SPAINBarcelona, SPAIN	3	0	0	0	0	0	3					978-1-7281-4198-5									BRAC Univ, Dept Comp Sci & Engn, Dhaka, Bangladesh				2020-08-07	WOS:000552238400059		
B	Lopez Pulgarin, E.J.; Irmak, T.; Paul, J.V.; Meekul, A.; Herrmann, G.; Leonards, U.					Leonards, Ute/0000-0001-6143-7466; Lopez Pulgarin, Erwin Jose/0000-0001-9927-6688	Giuliani, M.; Assaf, T.; Giannaccini, M.E.				Comparing Model-based and Data-driven Controllers for an Autonomous Vehicle Task								Towards Autonomous Robotic Systems. 19th Annual Conference, TAROS 2018 Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10965)								170	82				10.1007/978-3-319-96728-8_15							Conference Paper	2018	2018	The advent of autonomous vehicles comes with many questions from an ethical and technological point of view. The need for high performing controllers, which show transparency and predictability is crucial to generate trust in such systems. Popular data-driven, black box-like approaches such as deep learning and reinforcement learning are used more and more in robotics due to their ability to process large amounts of information, with outstanding performance, but raising concerns about their transparency and predictability. Model-based control approaches are still a reliable and predictable alternative, used extensively in industry but with restrictions of their own. Which of these approaches is preferable is difficult to assess as they are rarely directly compared with each other for the same task, especially for autonomous vehicles. Here we compare two popular approaches for control synthesis, model-based control i.e. Model Predictive Controller (MPC), and data-driven control i.e. Reinforcement Learning (RL) for a lane keeping task with speed limit for an autonomous vehicle; controllers were to take control after a human driver had departed lanes or gone above the speed limit. We report the differences between both control approaches from analysis, architecture, synthesis, tuning and deployment and compare performance, taking overall benefits and difficulties of each control approach into account.					Towards Autonomous Robotic Systems. 19th Annual Conference, TAROS 2018Towards Autonomous Robotic Systems. 19th Annual Conference, TAROS 2018	25-27 July 201825-27 July 2018		Bristol, UKBristol, UK	2	0	0	0	0	0	2					978-3-319-96727-1									Mech. Eng., Univ. of Bristol, Bristol, UKExp. Psychol., Univ. of Bristol, Bristol, UKUniv. of the West of England, Bristol, UKUniv. of Bath, Bath, UKUniv. of Bristol, Bristol, UK				2018-08-23	INSPEC:17964215		
J	Hong Yun Chen; Yan Qiang Li; Zi Hui Zhang; Yong Wang										Test Method for Decision Planning of Autonomous Vehicles Based on DQN Algorithm								E3S Web of Conferences				253				03022 (5 pp.)	03022 (5 pp.)				10.1051/e3sconf/202125303022							Conference Paper; Journal Paper	2021	2021	In February 2020, Beijing, China andCalifornia, USA respectively released road test reports of 2019 for autonomous vehicles. Beijing and California respectively represent the highest level of testing and application of autonomous vehicles in the two countries. This article will compare the test items, evaluation criteria and technical defects of each autonomous vehicle company in the road test reports of China and the United States, also analyze the existing problems, and propose an idea for the construction of a comprehensive test site for autonomous vehicles. This article aims to solve the prominently exposed problems in decision-making and planning in autonomous vehicles with DQN algorithm-base vehicle fleet, and to look forward to the future development trend of autonomous driving testing.					2021 International Conference on Environmental and Engineering Management (EEM 2021)2021 International Conference on Environmental and Engineering Management (EEM 2021)	23-25 April 202123-25 April 2021		Changsha, ChinaChangsha, China	0	0	0	0	0	0	0			2267-1242											Shandong Provincial Key Lab. of Automotive Electron. Technol., Qilu Univ. of Technol., Jinan, China				2021-01-01	INSPEC:21019051		
J	Nemeth, Balazs; Gaspar, Peter				Gaspar, Peter/C-1721-2013; Gaspar, Peter/L-3805-2013	Nemeth, Balazs/0000-0003-0211-3204; Gaspar, Peter/0000-0003-3388-1724					The Design of Performance Guaranteed Autonomous Vehicle Control for Optimal Motion in Unsignalized Intersections								APPLIED SCIENCES-BASEL				11	8					3464			10.3390/app11083464							Article	APR 2021	2021	The design of the motion of autonomous vehicles in non-signalized intersections with the consideration of multiple criteria and safety constraints is a challenging problem with several tasks. In this paper, a learning-based control solution with guarantees for collision avoidance is proposed. The design problem is formed in a novel way through the division of the control problem, which leads to reduced complexity for achieving real-time computation. First, an environment model for the intersection was created based on a constrained quadratic optimization, with which guarantees on collision avoidance can be provided. A robust cruise controller for the autonomous vehicle was also designed. Second, the environment model was used in the training process, which was based on a reinforcement learning method. The goal of the training was to improve the economy of autonomous vehicles, while guaranteeing collision avoidance. The effectiveness of the method is presented through simulation examples in non-signalized intersection scenarios with varying numbers of vehicles.									5	0	0	0	0	0	5				2076-3417										Eotvos Lorand Res Network ELKH, Inst Comp Sci & Control SZTAKI, 13-17 Kende Utca, H-1111 Budapest, HungaryBudapest Univ Technol & Econ, Dept Control Transportat & Vehicle Syst, 3-7 Muegyet Rakpart, H-1111 Budapest, Hungary				2021-05-24	WOS:000643984400001		
C	Hu, Yeping; Nakhaei, Alireza; Tomizuka, Masayoshi; Fujimura, Kikuo			IEEE	Hu, Yeping/ACF-9092-2022						Interaction-aware Decision Making with Adaptive Strategies under Merging Scenarios								2019 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						151	158				10.1109/iros40897.2019.8968478							Proceedings Paper	2019	2019	In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants. Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness. Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise. Many traditional methods have been proposed to solve decision making problems under merging scenarios. However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios. In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios. A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically. A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency. An exemplar merging scenario was used to implement and examine the proposed method.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	NOV 04-08, 2019NOV 04-08, 2019	IEEE; RSJIEEE; RSJ	Macau, PEOPLES R CHINAMacau, PEOPLES R CHINA	24	0	0	0	0	0	28			2153-0858		978-1-7281-4004-9									Univ Calif Berkeley, Dept Mech Engn, Berkeley, CA 94720 USAHonda Res Inst, Mountain View, CA 94043 USA				2020-08-13	WOS:000544658400017		
C	Trasnea, Bogdan; Marina, Liviu A.; Vasilcoi, Andrei; Pozna, Claudiu R.; Grigorescu, Sorin M.			IEEE		Trasnea, Bogdan/0000-0001-6169-1181					GridSim: A Vehicle Kinematics Engine for Deep Neuroevolutionary Control in Autonomous Driving								2019 THIRD IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC 2019)								443	444				10.1109/IRC.2019.00091							Proceedings Paper	2019	2019	Current state of the art solutions in the control of an autonomous vehicle mainly use supervised end-to-end learning, or decoupled perception, planning and action pipelines. Another possible solution is deep reinforcement learning, but such a method requires that the agent interacts with its surroundings in a simulated environment. In this paper we introduce GridSim, which is an autonomous driving simulator engine running a car-like robot architecture to generate occupancy grids from simulated sensors. We use GridSim to study the performance of two deep learning approaches, deep reinforcement learning and driving behavioral learning through genetic algorithms. The deep network encodes the desired behavior in a two elements fitness function describing a maximum travel distance and a maximum forward speed, bounded to a specific interval. The algorithms are evaluated on simulated highways, curved roads and inner-city scenarios, all including different driving limitations.					3rd IEEE International Conference on Robotic Computing (IRC)3rd IEEE International Conference on Robotic Computing (IRC)	FEB 25-27, 2019FEB 25-27, 2019	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	Naples, ITALYNaples, ITALY	5	0	0	0	0	0	5					978-1-5386-9245-5									Transilvania Univ Brasov, Elektrobit Automot Romania, Brasov 500036, Romania				2019-05-27	WOS:000465234300082		
C	Hester, Todd; Stone, Peter						Behnke, S; Veloso, M; Visser, A; Xiong, R				The Open-Source TEXPLORE Code Release for Reinforcement Learning on Robots								ROBOCUP 2013: ROBOT WORLD CUP XVII		Lecture Notes in Artificial Intelligence		8371				536	543											Proceedings Paper	2014	2014	The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations on-line. RL is a paradigm for learning sequential decision making tasks, usually formulated as a Markov Decision Process (MDP). For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. In addition, the algorithm must learn efficiently in the face of noise, sensor/actuator delays, and continuous state features. In this paper, we present the texplore ROS code release, which contains texplore, the first algorithm to address all of these challenges together. We demonstrate texplore learning to control the velocity of an autonomous vehicle in real-time. texplore has been released as an open-source ROS repository, enabling learning on a variety of robot tasks.					17th International Symposium on Robot World Cup (RoboCup)17th International Symposium on Robot World Cup (RoboCup)	JUN 24-JUL 01, 2013JUN 24-JUL 01, 2013		Eindhoven Univ Technol, Eindhoven, NETHERLANDSEindhoven Univ Technol, Eindhoven, NETHERLANDS	0	0	0	0	0	0	0			0302-9743	1611-3349	978-3-662-44468-9; 978-3-662-44467-2									Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA				2015-02-26	WOS:000349024200047		
J	Xiaobai Ma; Driggs-Campbell, K.; Kochenderfer, M.J.										Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning [arXiv]								arXiv								7 pp.	7 pp.											Journal Paper	8 March 2019	2019	To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods. [Intelligent Vehicles Symposium (IV), 2018 IEEE].									0	0	0	0	0	0	0														Aeronaut. & Astronaut. Dept., Stanford Univ., Stanford, CA, USA				2019-03-08	INSPEC:18787512		
J	Wang, Hongbo; Hu, Chenglei; Zhou, Juntao; Feng, Lizhao; Ye, Bin; Lu, Yongjie				Feng, Lizhao/JHT-7417-2023	LU, YONGJIE/0000-0002-0179-3984; Hu, Chenglei/0000-0002-5688-5982					Path tracking control of an autonomous vehicle with model-free adaptive dynamic programming and RBF neural network disturbance compensation								PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING				236	5			825	841	09544070211033835			10.1177/09544070211033835					JUL 2021		Article; Early Access		2022	The performance of the model-based controller is always affected by the uncertainty and nonlinearity of the model parameters in the vehicle path tracking process. To address this issue, a novel path tracking controller based on model-free adaptive dynamic programming (ADP) is proposed for autonomous vehicles in this paper. To be specific, the proposed controller obtains information from the online state and front-wheel angle input data which are repeatedly used to calculate the controller gain iteratively. So, this controller features not requiring accurate knowledge of vehicle model parameters for controller development. Meanwhile, the path tracking performance of the autonomous vehicle will be inevitably disturbed by unknown nonlinear external disturbance. To approximate this disturbance, the learning characteristics of Radial Basis Function Neural Network (RBFNN) are applied to generate compensation for the front-wheel angle. Afterward, the weight updating law of RBFNN is derived by Lyapunov function to ensure the stability and convergence of the whole system. Finally, Hardware in the loop (HIL) test results demonstrate that the proposed ADP-RBF controller can improve the comprehensive performance of the vehicle path tracking control system and achieve the balance between path tracking accuracy and minimum sideslip angle.									4	2	0	0	0	0	6			0954-4070	2041-2991										Hefei Univ Technol, Sch Automot & Transportat Engn, Hefei, Peoples R ChinaAnhui Intelligent Vehicle Engn Lab, Hefei, Peoples R ChinaShijiazhuang Tiedao Univ, State Key Lab Mech Behav & Syst Safety Traff Engn, Room 719,Jiaotong Bldg, Shijiazhuang 050043, Hebei, Peoples R China	Anhui Intelligent Vehicle Engn Lab			2021-08-15	WOS:000682125400001		
J	Kuutti, S.; Bowden, R.; Fallah, S.										Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages [arXiv]								arXiv								18 pp.	18 pp.											Journal Paper	17 March 2021	2021	The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control. However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles. In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent. By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy. This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle. We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance. Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone. [Sensors 2021, 21, 2032 doi:10.3390/s21062032].									0	0	0	0	0	0	0														Connected & Autonomous Vehicles Lab., Univ. of Surrey, Guildford, UKCentre for Vision Speech & Signal Process., Univ. of Surrey, Guildford, UK				2021-05-22	INSPEC:20572774		
J	Sur, Chiranjib				Sur, Chiranjib/Z-4268-2019	Sur, Chiranjib/0000-0002-1563-9304					UCRLF: unified constrained reinforcement learning framework for phase-aware architectures for autonomous vehicle signaling and trajectory optimization								EVOLUTIONARY INTELLIGENCE				12	4			689	712				10.1007/s12065-019-00278-7							Article	DEC 2019	2019	Signaling and trajectory optimization work as contention and researchers have debated on what should be the best for the vehicle, but it seems that both components are complement to each other and there can be combined situations with bounds where maximum optimization can be achieved. This paper introduces a novel approach called Phase-Aware Deep Learning and Constrained Reinforcement Learning for optimization and constant improvement of signal and trajectory for autonomous vehicle operation modules for an intersection. It deals with all the components required for the signaling system to operate, communicate and also navigate the vehicle with proper trajectory so that it faces less waiting time and the overall system operates with minimum waiting time and comparable throughput rate. We have done analysis on the operating time and the vehicle movement as these are vital for pollution and energy consumption. Our methodologies are not only efficient in time and computation but also have incorporated highly optimized data representation to reduce the overhead of maintaining and accessing the data. This ensures very efficient time complexity and theoretical computation time and better lower bounds. Constrained Reinforcement Learning concept is the main contribution of this work and it helped in decreasing 84% of the waiting time for the vehicles.									6	0	0	0	0	0	6			1864-5909	1864-5917										Univ Florida, Comp & Informat Sci & Engn Dept, Gainesville, FL 32611 USA				2019-11-19	WOS:000494688300012		
B	Ghnaya, I.; Mosbah, M.; Aniss, H.; Ahmed, T.						Akkaya, K.; Festor, O.; Fung, C.; Rahman, M.A.; Granville, L.Z.; dos Santos, C.R.P.				Multi-Agent Advantage Actor-Critic Learning For Message Content Selection in Cooperative Perception Networks								NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium								1	9				10.1109/NOMS56928.2023.10154436							Conference Paper	2023	2023	Recent advancements in autonomous vehicle perception haveexposed limitations of onboard sensors such as radar, lidar, and cameras, which road obstacles and adverse weather conditions can impede. Connected and Autonomous Vehicles (CAVs) are leveraging wireless communications to share perception information through a process called Cooperative Perception (CP), aiming to provide a more comprehensive understanding of their environment. However, this can result in excessive redundant and useless information in the network, as the same road objects may be detected and exchanged simultaneously by multiple CAVs. This not only consumes more network resources but also may overload the communication channel, reducing the delivery of perception information to CAVs and ultimately decreasing the overall CP awareness in the network. This paper introduces MCORM, a multi-agent learning method based on the advantage actor-critic algorithm to maximize object usefulness and reduce redundancy in the network. Our evaluations demonstrate that through this method, CAVs learn optimal CP message content selection policies that maximize usefulness. Further more, our proposal proves to be more effective in mitigating object redundancy and improving network reliability in comparison to existing approaches.					NOMS 2023-2023 IEEE/IFIP Network Operations and Management SymposiumNOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium	8-12 May 20238-12 May 2023		Miami, FL, USAMiami, FL, USA	0	0	0	0	0	0	0					978-1-6654-7716-1									Univ. Bordeaux, Talence, FranceCOSYS-ERENA Lab, Gustave Eiffel Univ., Bordeaux, FranceFlorida Int. Univ., Miami, USALorraine Univ., FranceConcordia Univ., Montreal, CanadaUFRGS, Porto Alegre, Brazil				2023-07-07	INSPEC:23322021		
B	Zeng, X.; Yu, Q.; Liu, S.; Xia, Y.; Su, H.; Zheng, K.										Target-Oriented Maneuver Decision for Autonomous Vehicle: A Rule-Aided Reinforcement Learning Framework								CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management								3124	33				10.1145/3583780.3615072							Conference Paper	2023	2023	Autonomous driving systems (ADSs) have the potential to revolutionize transportation by improving traffic safety and efficiency. As the core component of ADSs, maneuver decision aims to make tactical decisions to accomplish road following, obstacle avoidance, and efficient driving. In this work, we consider a typical but rarely studied task, called Target-Lane-Entering (TLE), where an autonomous vehicle should enter a target lane before reaching an intersection to ensure a smooth transition to another road. For navigation-assisted autonomous driving, a maneuver decision module chooses the optimal timing to enter the target lane in each road section, thus avoiding rerouting and reducing travel time. To achieve the TLE task, we propose a ruLe-aided reINforcement lEarning framework, called LINE, which combines the advantages of RL-based policy and rule-based strategy, allowing the autonomous vehicle to make target-oriented maneuver decisions. Specifically, an RL-based policy with a hybrid reward function is able to make safe, efficient, and comfortable decisions while considering the factors of target lanes. Then a strategy of rule revision aims to help the policy learn from intervention and block the risk of missing target lanes. Extensive experiments based on the SUMO simulator confirm the effectiveness of our framework. The results show that LINE achieves state-of-the-art driving performance with over 95% task success rate.					CIKM '23: The 32nd ACM International Conference on Information and Knowledge ManagementCIKM '23: The 32nd ACM International Conference on Information and Knowledge Management	20232023	SIGWEB; SIGIRSIGWEB; SIGIR	Birmingham, UKBirmingham, UK	0	0	0	0	0	0	0					979-8-4007-0124-5									Univ. of Electron. Sci. & Technol. of China, Chengdu, ChinaUniv. of Electron. Sci. & Technol. of China, Quzhou, China				2023-11-23	INSPEC:24056602		
B	Saxena, D.M.; Sangjae Bae; Nakhaei, A.; Fujimura, K.; Likhachev, M.										Driving in Dense Traffic with Model-Free Reinforcement Learning								2020 IEEE International Conference on Robotics and Automation (ICRA)								5385	92				10.1109/ICRA40945.2020.9197132							Conference Paper	2020	2020	Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.					2020 IEEE International Conference on Robotics and Automation (ICRA)2020 IEEE International Conference on Robotics and Automation (ICRA)	31 May-31 Aug. 202031 May-31 Aug. 2020		Paris, FranceParis, France	1	0	0	0	0	0	1					978-1-7281-7395-5									Carnegie Mellon Univ., Pittsburgh, PA, USAUniv. of California, Berkeley, Berkeley, CA, USAHonda Res. Inst. USA, San Jose, CA, USA				2020-10-23	INSPEC:19986863		
J	Mokhtari, K.; Wagner, A.R.										Pedestrian Collision Avoidance for Autonomous Vehicles at Unsignalized Intersection Using Deep Q-Network [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	30 April 2021	2021	Prior research has extensively explored Autonomous Vehicle (AV) navigation in the presence of other vehicles, however, navigation among pedestrians, who are the most vulnerable element in urban environments, has been less examined. This paper explores AV navigation in crowded, unsignalized intersections. We compare the performance of different deep reinforcement learning methods trained on our reward function and state representation. The performance of these methods and a standard rule-based approach were evaluated in two ways, first at the unsignalized intersection on which the methods were trained, and secondly at an unknown unsignalized intersection with a different topology. For both scenarios, the rule-based method achieves less than 40\% collision-free episodes, whereas our methods result in a performance of approximately 100\%. Of the three methods used, DDQN/PER outperforms the other two methods while it also shows the smallest average intersection crossing time, the greatest average speed, and the greatest distance from the closest pedestrian.									0	0	0	0	0	0	0														Dept. of Mech. Eng., Pennsylvania State Univ., State College, PA, USADept. of Aerosp. Eng., Pennsylvania State Univ., State College, PA, USA				2021-09-23	INSPEC:20937805		
C	Saxena, Dhruv Mauria; Bae, Sangjae; Nakhaei, Alireza; Fujimura, Kikuo; Likhachev, Maxim			IEEE							Driving in Dense Traffic with Model-Free Reinforcement Learning								2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						5385	5392				10.1109/icra40945.2020.9197132							Proceedings Paper	2020	2020	Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 31-JUN 15, 2020MAY 31-JUN 15, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	40	5	0	0	0	0	50			1050-4729	2577-087X	978-1-7281-7395-5									Carnegie Mellon Univ, Pittsburgh, PA 15213 USAUniv Calif Berkeley, Berkeley, CA 94720 USAHonda Res Inst USA Inc, San Jose, CA USA				2020-01-01	WOS:000712319503106		
J	Wright, M.A.; Horowitz, R.										Attentional policies for cross-context multi-agent reinforcement learning [arXiv]								arXiv								11 pp.	11 pp.											Journal Paper	31 May 2019	2019	Many potential applications of reinforcement learning in the real world involve interacting with other agents whose numbers vary over time. We propose new neural policy architectures for these multi-agent problems. In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we follow the spirit of recent work on the power of relational inductive biases in deep networks by learning multi-agent relationships at the policy level via an attentional architecture. In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents' state information when selecting their next action. The structure of our architectures allow them to be applied on environments with varying numbers of agents. We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and significantly outperforming it when scaling to large numbers of agents.									0	0	0	0	0	0	0														Univ. of California, Berkeley, Berkeley, CA, USA				2019-10-14	INSPEC:18999704		
J	Wang, Yongshuai; Chen, Zengqiang; Sun, Mingwei; Sun, Qinglin					Chen, Zengqiang/0000-0002-1415-4073					Enhancing active disturbance rejection design via deep reinforcement learning and its application to autonomous vehicle								EXPERT SYSTEMS WITH APPLICATIONS				239						122433			10.1016/j.eswa.2023.122433					NOV 2023		Article; Early Access		2024	Taking the velocity regulation of autonomous driving as an example, this paper developed an enhancing active disturbance rejection design employing the deep reinforcement learning-deep deterministic policy gradient (DDPG). In this scheme, the active disturbance rejection control (ADRC) is adopted to online estimate and compensate disturbances and uncertainties, and feasible regions of control parameters are obtained through the Lyapunov method. Then DDPG is combined with ADRC to online adaptively tune control parameters in response to the changing environments, where safety, comfort, and energy-saving are considered in the reward design, and mapping relation between the defined action and state is constructed for the maximal reward. Besides, numerical simulations demonstrate the better performance and stronger robustness of the enhancing design when facing uncertainties, sensor noise, and mechanical faults, and comfort and energy consumption have also been improved to some extent compared with the general ADRC and model predictive control (MPC).									0	0	0	0	0	0	0			0957-4174	1873-6793										Nankai Univ, Coll Artificial Intelligence, Tianjin 300350, Peoples R ChinaKey Lab Intelligent Robot Tianjin, Tianjin 300350, Peoples R China	Key Lab Intelligent Robot Tianjin			2023-12-17	WOS:001112021300001		
B	Kadam, R.; Vidhani, V.; Valecha, B.; Bane, A.; Giri, N.						Joshi, A.; Khosravy, M.; Gupta, N.				Autonomous Vehicle Simulation Using Deep Reinforcement Learning								Machine Learning for Predictive Analysis. Proceedings of ICTIS 2020. Lecture Notes in Networks and Systems (LNNS 141)								541	50				10.1007/978-981-15-7106-0_53							Conference Paper	2021	2021	The reinforcement learning algorithms have been proven to be extremely accurate in performing a variety of tasks. These algorithms have outperformed humans in traditional games. This paper proposes a reinforcement learning based approach to autonomous driving. The autonomous vehicles must be able to deal with all external situations to ensure safety and to avoid undesired circumstances such as collisions. Thus, we propose the use of deep deterministic policy gradient (DDPG) algorithm which is able to work in a complex and continuous domain. To avoid physical damage and reduce costs, we choose to use a simulator to test the proposed approach. The CARLA simulator would be used as the environment. To fit the DDPG algorithm to the CARLA environment, our network architecture consists of critic and actor networks. The performance would be evaluated based on rewards generated by the agent while driving in the simulated environment.					Machine Learning for Predictive Analysis. ICTIS 2020Machine Learning for Predictive Analysis. ICTIS 2020	15-16 May 202015-16 May 2020		Ahmedabad, IndiaAhmedabad, India	1	0	0	0	0	0	1					978-981-15-7105-3									Vivekanand Educ. Soc.s Inst. of Technol., Mumbai, IndiaGlobal Knowledge Res. Found., Gujarat, IndiaUniv. of Osaka, Suita, JapanSch. of Eng. & Comput. Sci., Oakland Univ., Rochester Hills, MI, USA				2020-12-11	INSPEC:20126789		
J	Yu, Jiaxing; Arab, Aliasghar; Yi, Jingang; Pei, Xiaofei; Guo, Xuexun				Arab, Aliasghar/HNT-0662-2023	Arab, Aliasghar/0000-0002-7480-5229					Hierarchical framework integrating rapidly-exploring random tree with deep reinforcement learning for autonomous vehicle								APPLIED INTELLIGENCE				53	13			16473	16486				10.1007/s10489-022-04358-7					DEC 2022		Article; Early Access		2023	This paper proposes a systematic driving framework where the decision making module of reinforcement learning (RL) is integrated with rapidly-exploring random tree (RRT) as motion planning. RL is used to generate local goals and semantic speed commands to control the longitudinal speed of a vehicle while rewards are designed for the driving safety and the traffic efficiency. Guaranteeing the driving comfort, RRT returns a feasible path to be followed by the vehicle with the speed commands. The scene decomposition approach is implemented to scale the deep neural network (DNN) to environments with multiple traffic participants and double deep Q-networks (DDQN) with prioritized experience replay (PER) is utilized to accelerate the training process. To handle the disturbance of the perception of the agent, we use an ensemble of neural networks to evaluate the uncertainty of decisions. It has shown that the proposed framework can tackle unexpected actions of traffic participants at an intersection yielding safe, comfort and efficient driving behaviors. Also, the ensemble of DDQN with PER is proved to be superior over standard DDQN in terms of learning efficiency and disturbance vulnerability.									7	0	0	0	0	0	7			0924-669X	1573-7497										Wuhan Univ Technol, Hubei Key Lab Adv Technol Automot Components, Luogui Rd, Wuhan 430070, Hubei, Peoples R ChinaRutgers State Univ, Dept Mech & Aerosp Engn, 98 Brett Rd, Piscataway, NJ 08854 USA				2022-12-21	WOS:000895047200001		
B	Cheng, Y.; Wang, H.; Chu, H.; Yu, L.; Gao, B.; Chen, H.										Lane-keeping Control of Autonomous Vehicles Using Reinforcement Learning and Predictive Safety Filter *								2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	6				10.1109/CVCI59596.2023.10397167							Conference Paper	2023	2023	The urgent need for both safety and high-performance in the motion planning and decision-making system of the autonomous vehicle presents a significant challenge for learning-based technologies. In this paper, we introduces a lane-keeping control structure that combines reinforcement learning and a predictive safety filter to ensure the vehicle's safety motion. A vehicle agent under the racetrack environment is trained by the soft actor critic algorithm, to achieve high-performance continuous lane-keeping motion. Additionally, we establish the safety task for lane-keeping, which results in a learning-based safety filter, to avoid violation of driving on the outer lane. Through simulations, we demonstrate the effectiveness of our method in ensuring the safe and trustworthy control of autonomous vehicle lane-keeping.					2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)	20232023		Changsha, ChinaChangsha, China	0	0	0	0	0	0	0					979-8-3503-4048-8									Coll. of Electron. & Inf. Eng., Tongji Univ., Shanghai, ChinaSch. of Automative Studies, Tongji Univ., Shanghai, ChinaTongji Univ., Shanghai, China				2024-02-15	INSPEC:24508357		
J	Zheng, Haotian; Chen, Chaoyi; Li, Shuai; Zheng, Sifa; Li, Shengbo Eben; Xu, Qing; Wang, Jianqiang				Xu, Qing/JWP-4901-2024; Li, Shengbo/J-7662-2013	Li, Shengbo/0000-0003-4923-3633; Zheng, Haotian/0000-0002-3805-5547; Li, Shuai/0009-0001-0219-5206; Chen, Chaoyi/0000-0002-2158-7969; Zheng, Sifa/0000-0001-5160-1365					Learning-Based Safe Control for Robot and Autonomous Vehicle Using Efficient Safety Certificate								IEEE OPEN JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS				4				419	430				10.1109/OJITS.2023.3280573							Article	2023	2023	Energy-function-based safety certificates can provide demonstrable safety for complex automatic control systems used in safety control tasks. However, recent studies on learning-based energy function synthesis have only focused on feasibility, which can lead to over-conservatism and reduce controller efficiency. In this study, we propose using magnitude regularization techniques to enhance the efficiency of safe controllers by reducing conservativeness within the energy function while maintaining promising demonstrable safety guarantees. Specifically, we measure conservativeness by the magnitude of the energy function and reduce it by adding a magnitude regularization term to the integrated loss. We present the SafeMR algorithm, which is synthesized using reinforcement learning (RL) to unify the learning process of the safety controller and the energy function. To verify the effectiveness of the algorithm, we conducted two sets of experiments, one in a robot-based environment and the other in an autonomous vehicle environment. The experimental results demonstrate that the proposed approach reduces the conservativeness of the energy function and outperforms the baseline in terms of controller efficiency for the robot, while ensuring safety.									3	1	0	0	0	0	4				2687-7813										Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R China				2023-07-22	WOS:001012644000001		
C	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano			IEEE							RIGAA at the SBFT 2023 Tool Competition - Cyber-Physical Systems Track								2023 IEEE/ACM INTERNATIONAL WORKSHOP ON SEARCH-BASED AND FUZZ TESTING, SBFT								49	50				10.1109/SBFT59156.2023.00011							Proceedings Paper	2023	2023	Testing and verification of autonomous systems is critically important. In the context of SBFT 2023 CPS testing tool competition, we present our tool RIGAA for generating virtual roads to test an autonomous vehicle lane keeping assist system. RIGAA combines reinforcement learning as well as evolutionary search to generate test scenarios. It has achieved the second highest final score among 5 other submitted tools.					16th IEEE/ACM International Workshop on Search-Based and Fuzz Testing (SBFT)16th IEEE/ACM International Workshop on Search-Based and Fuzz Testing (SBFT)	MAY 14, 2023MAY 14, 2023	IEEE; Assoc Comp Machinery; IEEE Comp SocIEEE; Assoc Comp Machinery; IEEE Comp Soc	Melbourne, AUSTRALIAMelbourne, AUSTRALIA	0	0	0	0	0	0	0					979-8-3503-0182-3									Polytech Montreal, Montreal, PQ, Canada				2023-09-01	WOS:001042317000011		
J	Saxena, D.M.; Sangjae Bae; Nakhaei, A.; Fujimura, K.; Likhachev, M.										Driving in Dense Traffic with Model-Free Reinforcement Learning [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	14 Sept. 2019	2019	Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation.									0	0	0	0	0	0	0														Carnegie Mellon Univ., Pittsburgh, PA, USAHonda Res. Inst. USAUniv. of California, Berkeley, Berkeley, CA, USA				2020-08-28	INSPEC:19820545		
C	Giannini, Francesco; Fortino, Giancarlo; Franze, Giuseppe; Pupo, Francesco			IEEE	Fortino, Giancarlo/J-2950-2017						A Deep Q Learning-Model Predictive Control Approach to vehicle routing and control with platoon constraints								2022 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING (CASE)		IEEE International Conference on Automation Science and Engineering						563	568				10.1109/CASE49997.2022.9926699							Proceedings Paper	2022	2022	In this paper we deal with the control of platoon of autonomous vehicles driving in Urban Road Networks (URNs). We exploit the idea of using Deep Reinforcement Learning (DRL) as the path planner of the proposed architecture. The advantage of this solution is the capability to deal with the actual traffic congestion while driving the autonomous vehicle to its destination. In particular, the high-level routing decisions are translated into manipulable set-points for Receding Horizon controllers by making more computational affordable and efficient the control action on the vehicle dynamics. Feasibility and asymptotic closed-loop stability are formally proved. Some simulations on a platoon, consisting of three agents described by double-integrator models, are provided to show the effectiveness of the overall architecture.					IEEE 18th International Conference on Automation Science and Engineering (IEEE CASE)IEEE 18th International Conference on Automation Science and Engineering (IEEE CASE)	AUG 20-24, 2022AUG 20-24, 2022	IEEEIEEE	Mexico City, MEXICOMexico City, MEXICO	1	0	0	0	0	0	1			2161-8070		978-1-6654-9042-9									Univ Calabria, DIMEG, Via Pietro Bucci,Cubo 44-C, I-87036 Arcavacata Di Rende, CS, ItalyUniv Calabria, DIMES, Via Pietro Bucci,Cubo 41-C, I-87036 Arcavacata Di Rende, CS, Italy				2023-03-04	WOS:000927622400061		
J	Khalid, Muhammad; Wang, Liang; Wang, Kezhi; Aslam, Nauman; Pan, Cunhua; Cao, Yue				Pan, Cunhua/AAM-5342-2020; Wang, Kezhi/AAM-5657-2020; Cao, Yue/T-5536-2019; 任, 红/IUM-5764-2023	Pan, Cunhua/0000-0001-5286-7958; Wang, Kezhi/0000-0001-8602-0800; Cao, Yue/0000-0002-5425-5848; 任, 红/0000-0002-3477-1087; Cao, Yue/0000-0002-2098-7637; Khalid, Muhammad/0000-0002-2674-2489					Deep reinforcement learning-based long-range autonomous valet parking for smart cities								SUSTAINABLE CITIES AND SOCIETY				89						104311			10.1016/j.scs.2022.104311					DEC 2022		Article; Early Access		2023	In this paper, to reduce the congestion rate at the city center and increase the traveling quality of experience (QoE) of each user, the framework of long-range autonomous valet parking is presented. Here, an Autonomous Vehicle (AV) is deployed to pick up, and drop off users at their required spots, and then drive to the car park around well-organized places of city autonomously. In this framework, we aim to minimize the overall distance of AV, while guarantee all users are served with great QoE, i.e., picking up, and dropping off users at their required spots through optimizing the path planning of the AV and number of serving time slots. To this end, we first present a learning-based algorithm, which is named as Double-Layer Ant Colony Optimization (DLACO) algorithm to solve the above problem in an iterative way. Then, to make the fast decision, while considers the dynamic environment (i.e., the AV may pick up and drop off users from different locations), we further present a deep reinforcement learning-based algorithm, i.e., Deep Q-learning Network (DQN) to solve this problem. Experimental results show that the DL-ACO and DQN-based algorithms both achieve the considerable performance.									3	0	0	0	0	0	3			2210-6707	2210-6715										Univ Hull, Sch Comp Sci, Kingston Upon Hull HU67RX, EnglandCranfield Univ, Sch Aerosp, Transport & Mfg, Milton Keynes MK430AL, Buckinghamshire, EnglandBrunel Univ London, Dept Comp Sci, Uxbridge UB83PH, Middlesex, EnglandNorthumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE18ST, Northumberland, EnglandSoutheast Univ, Natl Mobile Commun Res Lab, Nanjing, Peoples R ChinaWuhan Univ, Sch Cyber Sci & Engn, Wuhan 430072, Peoples R China				2023-01-12	WOS:000905154400001		
B	Chi Kit Ngai; Yung, N.H.C.						Mellouk, A.				DAQL-enabled autonomous vehicle navigation in dynamically changing environment								Advances in Reinforcement Learning								385	410											Book Chapter	2011	2011	In this chapter we have presented a multiple goal reinforcement learning framework and illustrated on a two-goal problem in autonomous vehicle navigation. In general, DAQL can be applied in any goals that environmental response is available, whereas QL would suffice if environmental response is not available or can be ignored. A proportional goal fusion function was used to maintain balance between the two goals in this case. Extensive simulations have been carried out to evaluate its performance under different obstacle behaivors and sensing accuracy. The results showed that the proposed method is characterized by its ability to (1) deal with single obstacles at any speed and from any directions; (2) deal with two obstacles approaching from different directions; (3) cope with large sensor noise; (4) navigate in high obstacle density and high relative velocity environment. Detailed comparison of the proposed method with the R&G method reveals that improvements by the proposed method in path time and the number of collision-free episodes are substantial.									0	0	0	0	0	0	0					978-953-307-369-9									Univ. of Hong Kong, Hong Kong, ChinaNetwork & Telecom Dept., Univ. Paris-Est Creteil, Paris, France				2011-01-01	INSPEC:12737614		
J	Nie, Xiaotong; Liang, Yupeng; Ohkura, Kazuhiro										Autonomous highway driving using reinforcement learning with safety check system based on time-to-collision								ARTIFICIAL LIFE AND ROBOTICS				28	1			158	165				10.1007/s10015-022-00846-8					DEC 2022		Article; Early Access		2023	Decision making is an essential component of autonomous vehicle technology and received significant attention from academic and industry organizations. One of the promising approaches in designing a decision-making method is Reinforcement Learning (RL). To apply an RL algorithm to an autonomous driving problem, a feature representation of the state must first be chosen. The most commonly used representation is the spatial-temporal state feature. However, if the number or order of the surrounding vehicle changes, the feature representation will be affected. In this paper, we utilize time-to-collision (TTC) as the feature representation and propose a TTC-based safety check system. The action output by the RL controller would be replaced with a safer action chosen by the safety check system when an agent detects a potential collision, i.e., the TTC is below the time threshold. A ramp merging task is used to illustrate the effect. Simulation results show that the proposed method can effectively improve the arrival rate and reduce the collision rate, even in the case of dense traffic situations. Furthermore, we also conducted experiments to examine the performance of the safety check system with different time thresholds.									1	0	0	0	0	0	1			1433-5298	1614-7456										Hiroshima Univ, Grad Sch Adv Sci & Engn, Hiroshima, Japan				2023-01-20	WOS:000903996600001		
J	Lin, Bing; Lin, Kai; Lin, Changhang; Lu, Yu; Huang, Ziqing; Chen, Xinwei					Lin, Changhang/0000-0002-2477-0988					Computation offloading strategy based on deep reinforcement learning for connected and autonomous vehicle in vehicular edge computing								JOURNAL OF CLOUD COMPUTING-ADVANCES SYSTEMS AND APPLICATIONS				10	1					33			10.1186/s13677-021-00246-6							Article	JUN 8 2021	2021	Connected and Automated Vehicle (CAV) is a transformative technology that has great potential to improve urban traffic and driving safety. Electric Vehicle (EV) is becoming the key subject of next-generation CAVs by virtue of its advantages in energy saving. Due to the limited endurance and computing capacity of EVs, it is challenging to meet the surging demand for computing-intensive and delay-sensitive in-vehicle intelligent applications. Therefore, computation offloading has been employed to extend a single vehicle's computing capacity. Although various offloading strategies have been proposed to achieve good computing performace in the Vehicular Edge Computing (VEC) environment, it remains challenging to jointly optimize the offloading failure rate and the total energy consumption of the offloading process. To address this challenge, in this paper, we establish a computation offloading model based on Markov Decision Process (MDP), taking into consideration task dependencies, vehicle mobility, and different computing resources for task offloading. We then design a computation offloading strategy based on deep reinforcement learning, and leverage the Deep Q-Network based on Simulated Annealing (SA-DQN) algorithm to optimize the joint objectives. Experimental results show that the proposed strategy effectively reduces the offloading failure rate and the total energy consumption for application offloading.									12	2	0	0	0	0	14				2192-113X										Fujian Normal Univ, Coll Phys & Energy, Fujian Prov Collaborat Innovat Ctr Adv High Field, Fujian Prov Key Lab Quantum Manipulat & New Energ, Fuzhou 350117, Peoples R ChinaFujian Prov Collaborat Innovat Ctr Optoelect Semi, Xiamen 361005, Peoples R ChinaPutian Univ, Engn Res Ctr Big Data Applicat Private Hlth Med, Putian 351100, Peoples R ChinaFujian Polytech Normal Univ, Sch Big Data & Artificial Intelligence, Fuzhou 350300, Peoples R ChinaFujian Normal Univ, Concord Univ Coll, Fuzhou 350117, Peoples R ChinaMinjiang Univ, Fujian Prov Univ, Engn Res Ctr Big Data Applicat Private Hlth Med, Fuzhou 351008, Peoples R China	Fujian Prov Collaborat Innovat Ctr Optoelect Semi			2021-06-14	WOS:000659184600001		
B	Qi, H.; Hou, E.; Liu, G.; Ye, P.										Cognitive Reinforcement Learning For Autonomous Driving								2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)								1	5				10.1109/DTPI59677.2023.10365456							Conference Paper	2023	2023	Most existing reinforcement learning methods are combined with deep neural networks in autonomous driving. There is a well-known trouble named 'black-box' of deep neural networks, which lacks of interpretability, occurring in the decision-making process. That will affect the safety of autonomous vehicle. In this work, we propose a cognitive reinforcement learning framework. This framework illustrates a cognitive model that transfers the state space to cognitive signals. Subsequencely, based on these signals, the model simulates the human decision-making process. As a result, the outcome of decision provides reward to the reinforcement learning. The computational experiments conducted in CARLA demonstrate that our framework performs equally well as the conventional reinforcement learning methods and provides interpretability under the same circumstance in the reinforcement learning.					2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)	20232023	IEEE; CAAIEEE; CAA	Orlando, FL, USAOrlando, FL, USA	0	0	0	0	0	0	0					979-8-3503-1847-0									Sch. of Rail Transp., Shandong Jiaotong Univ., Jinan, ChinaState Key Lab. of Multimodal Artificial Intelligence Syst., Inst. of Autom., Beijing, ChinaSch. of Artificial Intelligence, Univ. of Chinese Acad. of Sci., Beijing, China				2024-01-18	INSPEC:24317196		
J	Lee, Dongsu; Kwon, Minhae										Combating Stop-and-Go Wave Problem at a Ring Road Using Deep Reinforcement Learning Based Autonomous Vehicles			심층강화학습기반 자율주행차량을 이용한 원형도로의 Stop-and-Go Wave 현상 해결 전략 연구					The Journal of Korean Institute of Communications and Information Sciences	한국통신학회논문지			46	10			1667	1682				10.7840/kics.2021.46.10.1667							research-article	2021	2021	With the rapid development of artificial intelligence, autonomous driving has recently attracted considerable attention. This paper aims to use an autonomous vehicle to improve road flow by solving the stop-and-go-wave problem on a ring road. We design a special model of Markov decision process model to solve stop-and-go-wave and use three deep reinforcement learning algorithms to train autonomous vehicles: Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3). We then compare their driving patterns and performances. We confirmed that an autonomous vehicle on the ring road could control the flow of multiple non-autonomous vehicles with an extensive simulation study, thus successfully solving the stop-and-go wave problem.				인공지능 기술의 발전과 함께 자율주행기술의 발전 또한 가속화되고 있다. 본 연구에서는 심층 강화학습 알고리즘을 기반으로 한 자율주행차량을 이용하여 원형 도로에서 빈번하게 발생하는 Stop-and-go wave 현상을 해결하여 도로 흐름을 개선하고자 한다. 이를 위해 원형 도로에 적합한 마르코프 의사결정과정 모델을 제안한다. 자율주행차의 가속도 제어 정책 학습을 위해 Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG), Twin Delayed DDPG (TD3) 알고리즘을 사용하고, 각 알고리즘별 학습 성능 및 주행 패턴의 차이를 확인한다. 시뮬레이션을 통해 자율주행차량을 도로에 배치함으로써 원형 도로의 Stop-and-go wave 현상을 해결할수 있음을 확인하였다.					0	0	0	0	0	0	0			1226-4717															2022-04-25	KJD:ART002766327		
J	Urrea, Claudio; Garrido, Felipe; Kern, John					Urrea, Claudio/0000-0001-7197-8928					Design and Implementation of Intelligent Agent Training Systems for Virtual Vehicles								SENSORS				21	2					492			10.3390/s21020492							Article	JAN 2021	2021	This paper presents the results of the design, simulation, and implementation of a virtual vehicle. Such a process employs the Unity videogame platform and its Machine Learning-Agents library. The virtual vehicle is implemented in Unity considering mechanisms that represent accurately the dynamics of a real automobile, such as motor torque curve, suspension system, differential, and anti-roll bar, among others. Intelligent agents are designed and implemented to drive the virtual automobile, and they are trained using imitation or reinforcement. In the former method, learning by imitation, a human expert interacts with an intelligent agent through a control interface that simulates a real vehicle; in this way, the human expert receives motion signals and has stereoscopic vision, among other capabilities. In learning by reinforcement, a reward function that stimulates the intelligent agent to exert a soft control over the virtual automobile is designed. In the training stage, the intelligent agents are introduced into a scenario that simulates a four-lane highway. In the test stage, instead, they are located in unknown roads created based on random spline curves. Finally, graphs of the telemetric variables are presented, which are obtained from the automobile dynamics when the vehicle is controlled by the intelligent agents and their human counterpart, both in the training and the test track.									4	0	0	0	0	0	5				1424-8220										Univ Santiago Chile, Dept Elect Engn, Av Ecuador 3519, Santiago 9170124, Chile				2021-02-09	WOS:000611718700001	33445582	
B	Brunoud, A.; Lombard, A.; Abbas-Turki, A.; Gaud, N.; Kang-Hyun, J.										Hybrid deep reinforcement learning model for safe and efficient autonomous vehicles at pedestrian crossings								2023 International Workshop on Intelligent Systems (IWIS)								1	6				10.1109/IWIS58789.2023.10284662							Conference Paper	2023	2023	The autonomous vehicle is a particularly promising area of application for machine learning algorithms. Autonomous driving is challenging due to the complexity of the road network, the hardly predictable human behaviors, either pedestrians or drivers and the imperative need to ensure the safety of all road users. Deep Reinforcement Learning algorithms have garnered significant interest as a viable option for controlling autonomous vehicles. They allow defining a control policy by solely describing the observation space, the action space, and a reward function. Yet, the most common solutions face difficulties when the action space is hybrid (simultaneously continuous and discrete), and the reward is sparse, which is the case when a selection between choices must be made (like yielding or not), and the trajectory of the vehicle must be adapted according to this decision. This paper addresses these challenges by focusing on controlling an autonomous vehicle at pedestrian crossings. The vehicle adapts its speed (continuous decision) and the displayed signal (discrete decision) according to the pedestrian's state. The unique characteristic of the speed profile is to facilitate safe pedestrian crossings without requiring the vehicle to come to a complete stop. This raises the consistency problem between the speed profile and the signals issued by the vehicle. This paper introduces a new hybrid DRL model capable of making both decisions. It discusses the training method, the construction and necessary constraints on the reward, as well as the success of generalization to different speed limitations.					2023 International Workshop on Intelligent Systems (IWIS)2023 International Workshop on Intelligent Systems (IWIS)	20232023		Ulsan, South KoreaUlsan, South Korea	0	0	0	0	0	0	0					979-8-3503-0504-3									Univ. Bourgogne Franche-Comte, Belfort, FranceDep. of Electr., Electron., & Comput. Eng., Univ. of Ulsan, Ulsan, South Korea				2023-11-11	INSPEC:23948176		
J	Chen, Yuanhang; Wu, Shaofang										Framework of Active Obstacle Avoidance for Autonomous Vehicle Based on Hybrid Soft Actor-Critic Algorithm								JOURNAL OF TRANSPORTATION ENGINEERING PART A-SYSTEMS				149	4					04023002			10.1061/JTEPBS.0000772							Article	APR 1 2023	2023	In this paper, a framework of active obstacle avoidance for autonomous vehicles based on the hybrid soft actor-critic (SAC) algorithm is proposed. In the stage of local path planning, a comprehensive cost function considering collision risks, deviation from the global route, road lines crossing, and driving comfortability is developed to provide a local optimal path avoiding both static and dynamic obstacles considering multiple predicting timesteps. Then, a path tracking controller on the foundation of a hybrid SAC algorithm is designed to mitigate the problem of high sample complexity caused by random initialization of parameters in conventional reinforcement learning approaches. Model predictive control (MPC) plays a guiding role by applying its control action to combine with the action of SAC online to obtain a more effective state and reward information for training. The mechanism of the combination of MPC with SAC to balance the exploration and reliability is explained in detail. In order to improve the convergence rate and learning efficiency, a dual actor network structure for two different control actions is adopted. With considerations of various relevant factors influencing the control effect, the reward for the hybrid SAC algorithm is designed carefully. Finally, the results of simulation experiments illustrate that the proposed approach performs effectively with the assurance of safety and driving comfortability. In summary, the hybrid SAC algorithm with dual actor networks performs better than other algorithms for comparison in all test scenarios in this paper.									0	0	0	0	0	0	0			2473-2907	2473-2893										Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangdong Prov Key Lab Intelligent Transport Syst, Guangzhou 510006, Peoples R China				2023-03-27	WOS:000935169000001		
C	Fiedler, Julius; Gerwien, Maximilian; Knoll, Carsten			IEEE		Fiedler, Julius/0009-0009-0163-9600					A Hybrid Tactical Decision-Making Approach in Automated Driving Combining Knowledge-Based Systems and Reinforcement Learning								2022 IEEE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						3478	3483				10.1109/ITSC55140.2022.9922505							Proceedings Paper	2022	2022	Decision-making in automated driving is influenced both by objective traffic rules and subjective perceptions and goals of the driver. Thus, a suitable representation of the environment of the autonomous vehicle is required to model complex traffic situations and extract key features. To achieve this objective, this work uses an ontology-based situation interpretation (OBSI) to model traffic situations. The resulting semantic state representation is used to train models of vehiclecontrolling agents using reinforcement learning. Based on our simulations, it can be shown that the semantic preprocessing of traffic situations significantly improves the agent's performance regarding safety and driving style.					IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)	OCT 08-12, 2022OCT 08-12, 2022	IEEEIEEE	Macau, PEOPLES R CHINAMacau, PEOPLES R CHINA	0	0	0	0	0	0	0			2153-0009		978-1-6654-6880-0									Tech Univ Dresden, Inst Control Theory, Dresden, GermanyIAV GmbH, Dept Intelligent Driving Funct, Dresden, Germany				2023-04-26	WOS:000934720603079		
B	Rezgui, J.; Bisaillon, C.; O'leary, L.O.										A platform for sharing artificial intelligence algorithms in autonomous driving: an overview of enhanced LAOP								2020 International Symposium on Networks, Computers and Communications (ISNCC)								6 pp.	6 pp.				10.1109/ISNCC49221.2020.9297192							Conference Paper	2020	2020	To solve the Autonomous Vehicle (AV) problem, an artificial learning model that translates sensor data into controls must be built. This way, the vehicle can react to its changing environment. To the best of our knowledge, there are no platforms where researchers can both develop new machine learning models, train them and compare them directly to others. We believe that such a platform can greatly impact the field of artificial intelligence and AV. In this paper, we propose an enhanced version of the Learning Algorithm Optimization Platform LAOP, called LAOP 2.0. It helps researchers in the field of artificial intelligence develop their models by allowing them to easily test and compare them. We also introduce the Learning Algorithm Sharing Platform (LASP), which makes it easy to share deep learning algorithms. As a demonstration of the versatility of our platform, we compared two learning algorithms. The first one, Fully Connected Neural Network (FUCONN), uses reinforcement learning to train itself. The second one, Mimicking HUman Behaviour (MHUB), uses supervised learning to adjust its weights and learns from human input. We demonstrate through extensive simulations that FUCONN outperforms MHUB after being trained.					2020 International Symposium on Networks, Computers and Communications (ISNCC)2020 International Symposium on Networks, Computers and Communications (ISNCC)	20-22 Oct. 202020-22 Oct. 2020		Montreal, QC, CanadaMontreal, QC, Canada	0	0	0	0	0	0	0					978-1-7281-5628-6									Lab. Rech. Inf. Maisonneuve (LRIMa), Montreal, QC, Canada				2021-02-23	INSPEC:20303771		
J	Paravarzar, S.; Mohammad, B.										Motion Prediction on Self-driving Cars: A Review [arXiv]								arXiv								9 pp.	9 pp.											Journal Paper	6 Nov. 2020	2020	The autonomous vehicle motion prediction literature is reviewed. Motion prediction is the most challenging task in autonomous vehicles and self-drive cars. These challenges have been discussed. Later on, the state-of-the art has reviewed based on the most recent literature and the current challenges are discussed. The state-of-the-art consists of classical and physical methods, deep learning networks, and reinforcement learning. prons and cons of the methods and gap of the research presented in this review. Finally, the literature surrounding object tracking and motion will be presented. As a result, deep reinforcement learning is the best candidate to tackle self-driving cars.									0	0	0	0	0	0	0														Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada				2021-03-10	INSPEC:20226560		
J	Lowe, Evan; Guvenc, Levent					Guvenc, Levent/0000-0001-8823-1820					Autonomous Vehicle Emergency Obstacle Avoidance Maneuver Framework at Highway Speeds								ELECTRONICS				12	23					4765			10.3390/electronics12234765							Article	DEC 2023	2023	An autonomous vehicle (AV) uses high-level decision making and lower-level actuator controls, such as throttle (acceleration), braking (deceleration), and steering (change in lateral direction) to navigate through various types of road networks. Path planning and path following for highway driving are currently available in series-produced highly automated vehicles. In addition to these, emergency collision avoidance decision making and maneuvering are another key and essential feature that is needed in a series production AV at highway driving speeds. For reliability, low cost, and fast computation, such an emergency obstacle avoidance maneuvering system should use well-established conventional methods as opposed to data-driven neural networks or reinforcement learning methods, which are currently not suitable for use in highway AV driving. This paper presents a novel Emergency Obstacle Avoidance Maneuver (EOAM) methodology for AVs traveling at higher speeds and lower road surface friction, involving time-critical maneuver determination and control. The proposed EOAM framework offers usage of the AV's sensing, perception, control, and actuation system abilities as one cohesive system to avoid an on-road obstacle, based first on performance feasibility and second on passenger comfort, and it is designed to be well integrated within an AV's high-level control and decision-making system. To demonstrate the efficacy of the proposed method, co-simulation including the AV's EOAM logic in Simulink and a vehicle model in CarSim is conducted with speeds ranging from 55 to 165 km/h and on road surfaces with friction ranging from 1.0 to 0.1. The results are analyzed and interpreted in the context of an entire AV system, with implications for future work.									0	0	0	0	0	0	0				2079-9292										Ohio State Univ, Automated Driving Lab, Columbus, OH 43210 USA				2023-12-19	WOS:001116236100001		
B	Naruse, K.; Kakazu, Y.						Asama, H.; Fukuda, T.; Arai, T.; Endo, I.				Rule generation and generalization by inductive decision tree and reinforcement learning								Distributed Autonomous Robotic Systems								91	8											Conference Paper	1994	1994	In this paper, we attempt to construct a planning mechanism composed of distributed agents for autonomous vehicle navigation in an unknown workspace. Each agent decides a direction that the vehicle should move without any communication to the other agents, by only observing the workspace and the other agents. The agent includes the reinforcement learning mechanism for generating rules for the navigation. However, the rules depend on a state observation method. For generalizing the rules, an inductive decision tree is introduced to the agent. In a new workspace, the agent plans a path efficiently by learning a specific rule to the new workspace, and using the generalized rules. Some computational simulations have been carried out for verifying the proposed agent.					Proceedings of 1994 2nd International Symposium on D Distributed Autonomous Robotic Systems (DARS'94)Proceedings of 1994 2nd International Symposium on D Distributed Autonomous Robotic Systems (DARS'94)	14-15 July 199414-15 July 1994		Saitama, JapanSaitama, Japan	0	0	0	0	0	0	0					4-431-70147-8									Dept. of Precision Eng., Hokkaido Univ., Sapporo, Japan				1994-01-01	INSPEC:5397845		
B	Yuqi Liu; Qichao Zhang; Dongbin Zhao										A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios								2021 IEEE Symposium Series on Computational Intelligence (SSCI)								8 pp.	8 pp.				10.1109/SSCI50451.2021.9660172							Conference Paper	2021	2021	In recent years, control under urban intersection scenarios has become an emerging research topic. In such scenarios, the autonomous vehicle confronts complicated situations since it must deal with the interaction with social vehicles timely while obeying the traffic rules. Generally, the autonomous vehicle is supposed to avoid collisions while pursuing better efficiency. The existing work fails to provide a framework that emphasizes the integrity of the scenarios while deploying and testing reinforcement learning(RL) methods. Specifically, we propose a benchmark for training and testing RL-based autonomous driving agents in complex intersection scenarios, which is called RL-CIS. Then, a set of baselines consisting various algorithms are deployed. The test benchmark and baselines provide a fair and comprehensive training and testing platform for the study of RL for autonomous driving in the intersection scenario, advancing RL-based methods for autonomous driving control. The code of our proposed framework can be found at https://github.com/liuyufi123/ComplexUrbanScenarios.					2021 IEEE Symposium Series on Computational Intelligence (SSCI)2021 IEEE Symposium Series on Computational Intelligence (SSCI)	5-7 Dec. 20215-7 Dec. 2021	IEEEIEEE	Orlando, FL, USAOrlando, FL, USA	0	0	0	0	0	0	0					978-1-7281-9048-8									State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, ChinaSch. of Artificial Intelligence, Univ. of Chinese Acad. of Sci., Beijing, China				2022-03-25	INSPEC:21524198		
C	Kang, Liuwang; Shen, Haiying			IEEE	shen, haiying/AAA-4995-2021						A Data-Driven Optimal Control Decision-Making System for Multiple Autonomous Vehicles								2021 ACM/IEEE 6TH SYMPOSIUM ON EDGE COMPUTING (SEC 2021)								192	201				10.1145/3453142.3493686							Proceedings Paper	2021	2021	With the fast development and rising popularity of autonomous vehicle (AV) technology, multiple AVs may soon be driving on the same road simultaneously. Such multi-AV coexistence driving situations will lead to new and persistent challenges. Therefore, improvements on making control decisions for multiple AVs becomes necessary for continued driving safety. In this paper, we propose a multi-AV decision making system (MADM), which considers multi-AV coexistence driving situations during the decision-making process. In MADM, we first build a policy formation method to generate policies that learn the driving behaviors of an expert based on the expert's driving trajectory data. We then develop a multi-AV decision-making method, which adjusts the formed policies through multi-agent reinforcement learning. The adjusted policies make control decisions for multiple AVs with safety guarantee. We used a real-world traffic dataset to evaluate the decision making performance of MADM in comparison with several state-of-the-art methods. Experimental results show that MADM reduces emergency rate by as high as 51% when compared with existing methods.					6th ACM/IEEE Symposium on Edge Computing (SEC)6th ACM/IEEE Symposium on Edge Computing (SEC)	DEC 14-17, 2021DEC 14-17, 2021	IEEE; IEEE Comp Soc; Assoc Comp MachineryIEEE; IEEE Comp Soc; Assoc Comp Machinery	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0					978-1-4503-8390-5									Univ Virginia, Charlottesville, VA 22904 USA				2022-06-24	WOS:000800208500015		
J	Yuan, Yali; Tasik, Robert; Adhatarao, Sripriya Srikant; Yuan, Yachao; Liu, Zheli; Fu, Xiaoming				Yuan, Yachao/ABE-5182-2020; Yuan, Yachao/IVH-8025-2023; Yuan, Yali/ABC-2029-2020; LIU, zhe/HGD-6875-2022; Fu, Xiaoming/AAD-2828-2022; Fu, Xiaoming/B-7208-2016	Yuan, Yachao/0000-0001-7498-002X; Fu, Xiaoming/0000-0002-8012-4753; Fu, Xiaoming/0000-0002-8012-4753					RACE: Reinforced Cooperative Autonomous Vehicle Collision Avoidance								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				69	9			9279	9291				10.1109/TVT.2020.2974133							Article	SEP 2020	2020	With the rapid development of autonomous driving, collision avoidance has attracted attention from both academia and industry. Many collision avoidance strategies have emerged in recent years, but the dynamic and complex nature of driving environment poses a challenge to develop robust collision avoidance algorithms. Therefore, in this paper, we propose a decentralized framework named RACE: Reinforced Cooperative Autonomous Vehicle Collision AvoidancE. Leveraging a hierarchical architecture we develop an algorithm named Co-DDPG to efficiently train autonomous vehicles. Through a security abiding channel, the autonomous vehicles distribute their driving policies. We use the relative distances obtained by the opponent sensors to build the VANET instead of locations, which ensures the vehicle's location privacy. With a leader-follower architecture and parameter distribution, RACE accelerates the learning of optimal policies and efficiently utilizes the remaining resources. We implement the RACE framework in the widely used TORCS simulator and conduct various experiments to measure the performance of RACE. Evaluations show that RACE quickly learns optimal driving policies and effectively avoids collisions. Moreover, RACE also scales smoothly with varying number of participating vehicles. We further compared RACE with existing autonomous driving systems and show that RACE outperforms them by experiencing 65% less collisions in the training process and exhibits improved performance under varying vehicle density.									20	0	0	0	0	0	21			0018-9545	1939-9359										Univ Goettingen, Inst Comp Sci, Comp Networking Grp, D-37077 Gottingen, GermanyUniv Goettingen, Inst Informat Syst, Smart Mobil Res Grp, D-37075 Gottingen, GermanyNankai Univ, Coll Comp & Control Engn, Tianjin 300071, Peoples R China				2020-10-30	WOS:000577995300004		
J	Wu, Jingda; Huang, Zhiyu; Lv, Chen				Huang, Zhiyu/AGW-8179-2022; Wu, Jingda/GPX-6158-2022; Lv, Chen/N-7055-2018	Wu, Jingda/0000-0002-7336-4492; Lv, Chen/0000-0001-6897-4512; Huang, Zhiyu/0000-0003-1592-7215					Uncertainty-Aware Model-Based Reinforcement Learning: Methodology and Application in Autonomous Driving								IEEE TRANSACTIONS ON INTELLIGENT VEHICLES				8	1			194	203				10.1109/TIV.2022.3185159							Article	JAN 2023	2023	To further improve learning efficiency and performance of reinforcement learning (RL), a novel uncertainty-aware model-based RL method is proposed and validated in autonomous driving scenarios in this paper. First, an action-conditioned ensemble model with the capability of uncertainty assessment is established as the environment model. Then, a novel uncertainty-aware model-based RL method is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL's learning efficiency and performance. The proposed method is then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios. Validation results suggest that the proposed method outperforms the model-free RL approach with respect to learning efficiency, and model-based approach with respect to both efficiency and performance, demonstrating its feasibility and effectiveness.									18	0	0	0	0	0	19			2379-8858	2379-8904										Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore 639798, Singapore				2024-01-01	WOS:000966953700001		
C	Kasparaviciute, Gabriele; Nielsen, Stig Anton; Boruah, Dhruv; Nordin, Peter; Dancu, Alexandru					, Gabriele/0009-0007-9860-0299	Abramowicz, W; Paschke, A				Plastic Grabber: Underwater Autonomous Vehicle Simulation for Plastic Objects Retrieval Using Genetic Programming								BUSINESS INFORMATION SYSTEMS WORKSHOPS (BIS 2018)		Lecture Notes in Business Information Processing		339				527	533				10.1007/978-3-030-04849-5_46							Proceedings Paper	2019	2019	We propose a path planning solution using genetic programming for an autonomous underwater vehicle. Developed in ROS Simulator that is able to roam in an environment, identify a plastic object, such as bottles, grab it and retrieve it to the home base. This involves the use of a multi-objective fitness function as well as reinforcement learning, both required for the genetic programming to assess the model's behaviour. The fitness function includes not only the objective of grabbing the object but also the efficient use of stored energy. Sensors used by the robot include a depth image camera, claw and range sensors that are all simulated in ROS.					21st International Conference on Business Information Systems (BIS)21st International Conference on Business Information Systems (BIS)	JUL 18-20, 2018JUL 18-20, 2018	Fraunhofer Inst Open Commun Syst; Poznan Univ Econ & Business, Dept Informat SystFraunhofer Inst Open Commun Syst; Poznan Univ Econ & Business, Dept Informat Syst	Berlin, GERMANYBerlin, GERMANY	1	0	0	0	0	0	1			1865-1348	1865-1356	978-3-030-04849-5; 978-3-030-04848-8									Chalmers Univ Technol, Gothenburg, SwedenIT Univ Copenhagen, Copenhagen, DenmarkThethamesproject Org, London, EnglandMIT, Media Lab, Cambridge, MA 02139 USA	Thethamesproject Org			2019-01-01	WOS:000792555500046		
C	Mahmud, Shohaib; Shen, Haiying; Foutz, Ying Natasha Zhang; Anton, Joshua			IEEE	shen, haiying/AAA-4995-2021						Reinforcement Learning Based Route And Stop Planning For Autonomous Vehicle Shuttle Service								2022 IEEE 19TH INTERNATIONAL CONFERENCE ON MOBILE AD HOC AND SMART SYSTEMS (MASS 2022)								668	674				10.1109/MASS56207.2022.00098							Proceedings Paper	2022	2022	AV shuttles can be a complement to the existing bus services to fill the gap between the mobility demand of a city population and the supply of the existing bus services. In contrast to traditional approaches, this problem requires the consideration of new factors due to new objectives of the AV shuttle service such as mobility service for elderly and physically disabled persons, first/last mile transits, lessening of parking woes of the commuters, small local shop connectivity, and etc. In this work, we propose methods for route and stop planning for AV shuttle service in an urban city. We utilize a large scale human mobility dataset collected from users' cellphones' GPS sensors in Richmond city (as a city example) to identify the different potential transportation demands of AV shuttles. We propose an optimization problem-based solution and a deep reinforcement learning (RL) based solution. We conduct comprehensive evaluation based on our human mobility dataset along with real-world road network information. Our evaluation shows the proposed RL based system outperforms the comparison methods in terms of addressing the transportation needs as mentioned above by a significant margin while also minimizing the travel time on roads.					19th IEEE International Conference on Mobile Ad Hoc and Smart Systems (MASS)19th IEEE International Conference on Mobile Ad Hoc and Smart Systems (MASS)	OCT 20-22, 2022OCT 20-22, 2022	IEEE; IEEE Comp Soc TCDPIEEE; IEEE Comp Soc TCDP	Denver, CODenver, CO	0	0	0	0	0	0	0					978-1-6654-7180-0									Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USAUniv Virginia, McIntire Sch Commerce, Charlottesville, VA 22903 USAXMODE Social, Reston, VA 20190 USA	XMODE Social			2023-02-25	WOS:000925366900089		
J	Yeping Hu; Nakhaei, A.; Tomizuka, M.; Fujimura, K.										Interaction-aware decision making with adaptive strategies under merging scenarios [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	12 April 2019	2019	In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants. Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness. Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise. Many traditional methods have been proposed to solve decision making problems under merging scenarios. However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios. In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios. A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically. A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency. An exemplar merging scenario was used to implement and examine the proposed method.									0	0	0	0	0	0	0														Dept. of Mech. Eng., Univ. of California, Berkeley, Berkeley, CA, USAHonda Res. Inst., Mountain View, CA, USA				2019-08-26	INSPEC:18877402		
J	Liu, Yonggang; Liu, Gang; Wu, Yitao; He, Wen; Zhang, Yuanjian; Chen, Zheng				Zhang, Yuanjian/HKN-4832-2023; Chen, Zheng/AAO-6454-2020	Zhang, Yuanjian/0000-0001-5563-8480; Chen, Zheng/0000-0002-1634-7231; Liu, Yonggang/0000-0001-9768-328X					Reinforcement-Learning-Based Decision and Control for Autonomous Vehicle at Two-Way Single-Lane Unsignalized Intersection								ELECTRONICS				11	8					1203			10.3390/electronics11081203							Article	APR 2022	2022	Intersections have attracted wide attention owing to their complexity and high rate of traffic accidents. In the process of developing L3-and-above autonomous-driving techniques, it is necessary to solve problems in autonomous driving decisions and control at intersections. In this article, a decision-and-control method based on reinforcement learning and speed prediction is proposed to manage the conjunction of straight and turning vehicles at two-way single-lane unsignalized intersections. The key position of collision avoidance in the process of confluence is determined by establishing a road-geometry model, and on this basis, the expected speed of the straight vehicle that ensures passing safety is calculated. Then, a reinforcement-learning algorithm is employed to solve the decision-control problem of the straight vehicle, and the expected speed is optimized to direct the agent to learn and converge to the planned decision. Simulations were conducted to verify the performance of the proposed method, and the results show that the proposed method can generate proper decisions for the straight vehicle to pass the intersection while guaranteeing preferable safety and traffic efficiency.									5	0	0	0	0	0	5				2079-9292										Jilin Univ, State Key Lab Automot Simulat & Control, Changchun 130025, Peoples R ChinaChongqing Univ, Coll Mech & Vehicle Engn, Chongqing 400044, Peoples R ChinaChangan Automobile Intelligent Res Inst, Chongqing 710199, Peoples R ChinaLoughborough Univ, Dept Aeronaut & Automot Engn, Loughborough LE11 3TU, Leics, EnglandKunming Univ Sci & Technol, Fac Transportat Engn, Kunming 650500, Yunnan, Peoples R China	Changan Automobile Intelligent Res Inst			2022-05-04	WOS:000787043700001		
B	Bharilya, V.; Kumar, N.										A Survey of the State-of-the-Art Reinforcement Learning-Based Techniques for Autonomous Vehicle Trajectory Prediction								2023 International Conference on Electrical, Electronics, Communication and Computers (ELEXCOM)								1	6				10.1109/ELEXCOM58812.2023.10370504							Conference Paper	2023	2023	Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future trajectories of nearby traffic participants, similar to the predictive driving abilities of human drivers. Reinforcement Learning (RL) has emerged as a promising approach for learning complex decision-making policies in dynamic environments. This survey explores the application of RL approaches in trajectory prediction, focusing on inverse reinforcement learning, deep reinforcement learning, and imitation learning. It provides an in-depth analysis of the underlying principles, algorithms, and architectures employed in these methods, highlighting their respective strengths and limitations. Moreover, the survey addresses the current challenges in the field and presents potential future research directions, offering valuable insights to readers.					2023 International Conference on Electrical, Electronics, Communication and Computers (ELEXCOM)2023 International Conference on Electrical, Electronics, Communication and Computers (ELEXCOM)	20232023	Indian Institute of Technology Roorkee; IEEE Roorkee; ROHDE & SCHWARZ; iNSERB DIA; KEYSIGHT TECHNOLOGIES; IIITB COMET FOUNDATION; VERDET SCIENTIFIC; ANRITSU Advancing beyond; AGMATEL PARTNERING INNOVATION; MSPLIndian Institute of Technology Roorkee; IEEE Roorkee; ROHDE & SCHWARZ; iNSERB DIA; KEYSIGHT TECHNOLOGIES; IIITB COMET FOUNDATION; VERDET SCIENTIFIC; ANRITSU Advancing beyond; AGMATEL PARTNERING INNOVATION; MSPL	Roorkee, IndiaRoorkee, India	0	0	0	0	0	0	0					979-8-3503-0511-1									Dept. of Comp. Sci. & Engg., Indian Inst. of Technol. Roorkee, Haridwar, India				2024-01-25	INSPEC:24349776		
C	Muktadir, Golam Md; Whitehead, Jim			IEEE							Adversarial jaywalker modeling for simulation-based testing of Autonomous Vehicle Systems								2022 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						1697	1702				10.1109/IV51971.2022.9827422							Proceedings Paper	2022	2022	We present an approach for creating adversarial jaywalkers, autonomous pedestrian models which intentionally act to create unsafe situations involving other vehicles. An adversarial jaywalker employs a hybrid state-model with social forces and state transition rules. The parameters (for social forces and state transitions) of this model are tuned via reinforcement learning to create risky situations faster with synthetic yet plausible behavior. The resulting jaywalkers are capable of realistic behavior while still engaging in sufficiently risky actions to be useful for testing. These adversarial pedestrian models are useful in a wide range of scenario-based tests for autonomous vehicles.					33rd IEEE Intelligent Vehicles Symposium (IEEE IV)33rd IEEE Intelligent Vehicles Symposium (IEEE IV)	JUN 05-09, 2022JUN 05-09, 2022	IEEE; Bosch; IEEE Intelligent Transport Syst Soc; RWTH Aachen Univ, IKA; Mercedes BenzIEEE; Bosch; IEEE Intelligent Transport Syst Soc; RWTH Aachen Univ, IKA; Mercedes Benz	Aachen, GERMANYAachen, GERMANY	0	0	0	0	0	0	0			1931-0587		978-1-6654-8821-1									Univ Calif Santa Cruz, Comp Sci, Santa Cruz, CA 95064 USAUniv Calif Santa Cruz, Computat Media, Santa Cruz, CA USA				2022-10-13	WOS:000854106700239		
J	Cao, Zhong; Li, Xiang; Jiang, Kun; Zhou, Weitao; Liu, Xiaoyu; Deng, Nanshan; Yang, Diange					Zhou, Weitao/0000-0002-1266-3843; Jiang, Kun/0000-0003-4995-7244; Cao, Zhong/0000-0002-2243-5705					Autonomous Driving Policy Continual Learning With One-Shot Disengagement Case								IEEE TRANSACTIONS ON INTELLIGENT VEHICLES				8	2			1380	1391				10.1109/TIV.2022.3184729							Article	FEB 2023	2023	Disengagement cases during naturalistic driving are rare or even one-shot, but valuable for autonomous driving. The autonomous vehicles are necessary to continually learn from these disengagement cases, to improve the policy for better performance when next time meeting these cases. Manually adjusting the policy or adding the rules to fix these disengagement cases may cause engineering burden and may contradict other driving functions. To this end, this work proposes a continually learning agent which can automatically get improved once encountering a disengagement case. The main idea is to establish a disengagement-imagination environment, and then train the policy using imagination data for performance improvement, named disengagement-case imagination augmented continual learning (DICL). In the imagination environment, the surrounding objects are designed to first follow the recorded trajectory, and then switch to the interactive models for the policy training. The switch point is carefully designed to make the imagination contain the disengagement reasons but avoid overfitting the collected driving case. This method is evaluated by the real autonomous driving disengagement data, collected from an open-road-testing autonomous vehicle. The results show that the DICL agent can automatically learn to handle the emerging disengagement case and similar cases. This work provides a possible way to make the AV agents automatically get improvement during road testing.									1	0	0	0	0	0	1			2379-8858	2379-8904										Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R ChinaQiyuan Lab, Beijing 100095, Peoples R China	Qiyuan Lab			2023-08-26	WOS:000965696800001		
J	Israelsen, B.W.; Ahmed, N.R.; Frew, E.; Lawrence, D.; Argrow, B.										Machine Self-confidence in Autonomous Systems via Meta-analysis of Decision Processes [arXiv]								arXiv								11 pp.	11 pp.											Journal Paper	15 Oct. 2018	2018	Algorithmic assurances from advanced autonomous systems assist human users in understanding, trusting, and using such systems appropriately. Designing these systems with the capacity of assessing their own capabilities is one approach to creating an algorithmic assurance. The idea of `machine self-confidence' is introduced for autonomous systems. Using a factorization based framework for self-confidence assessment, one component of self-confidence, called `solver-quality', is discussed in the context of Markov decision processes for autonomous systems. Markov decision processes underlie much of the theory of reinforcement learning, and are commonly used for planning and decision making under uncertainty in robotics and autonomous systems. A `solver quality' metric is formally defined in the context of decision making algorithms based on Markov decision processes. A method for assessing solver quality is then derived, drawing inspiration from empirical hardness models. Finally, numerical experiments for an unmanned autonomous vehicle navigation problem under different solver, parameter and environment conditions indicate that the self-confidence metric exhibits the desired properties. Discussion of results, and avenues for future investigation are included.									0	0	0	0	0	0	0														Univ. of Colorado Boulder, Boulder, CO, USA				2019-01-04	INSPEC:18275420		
C	Choi, Yunho; Lee, Kyungjae; Oh, Songhwai			IEEE		Lee, Kyungjae/0000-0003-0147-2715	Howard, A; Althoefer, K; Arai, F; Arrichiello, F; Caputo, B; Castellanos, J; Hauser, K; Isler, V; Kim, J; Liu, H; Oh, P; Santos, V; Scaramuzza, D; Ude, A; Voyles, R; Yamane, K; Okamura, A				Distributional Deep Reinforcement Learning with a Mixture of Gaussians								2019 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						9791	9797				10.1109/icra.2019.8793505							Proceedings Paper	2019	2019	In this paper, we propose a novel distributional reinforcement learning (RL) method which models the distribution of the sum of rewards using a mixture density network. Recently, it has been shown that modeling the randomness of the return distribution leads to better performance in Atari games and control tasks. Despite the success of the prior work, it has limitations which come from the use of a discrete distribution. First, it needs a projection step and softmax parametrization for the distribution, since it minimizes the KL divergence loss. Secondly, its performance depends on discretization hyperparameters such as the number of atoms and bounds of the support which require domain knowledge. We mitigate these problems with the proposed parameterization, a mixture of Gaussians. Furthermore, we propose a new distance metric called the Jensen-Tsallis distance, which allows the computation of the distance between two mixtures of Gaussians in a closed form. We have conducted various experiments to validate the proposed method, including Atari games and autonomous vehicle driving.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 20-24, 2019MAY 20-24, 2019	Bosch; DJI; Kinova; Mercedes Benz; Samsung; Argo AI; Clearpath Robot; Element AI; Fetch Robot; Huawei; iRobot; KUKA; Quanser; SICK; Toyota Res Inst; Uber; Waymo; Zhejiang Lab; Amazon; Applanix; Cloudminds; Honda Res Inst; MathWorks; OusterBosch; DJI; Kinova; Mercedes Benz; Samsung; Argo AI; Clearpath Robot; Element AI; Fetch Robot; Huawei; iRobot; KUKA; Quanser; SICK; Toyota Res Inst; Uber; Waymo; Zhejiang Lab; Amazon; Applanix; Cloudminds; Honda Res Inst; MathWorks; Ouster	Montreal, CANADAMontreal, CANADA	10	0	0	0	0	0	10			1050-4729	2577-087X	978-1-5386-6026-3									Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South KoreaSeoul Natl Univ, ASRI, Seoul 08826, South Korea				2019-12-06	WOS:000494942307029		
B	Mekala, M.S.; Zhang, H.; Park, J.H.; Jung, H.-Y.										Quantum-based Offloading Strategy for Intelligent Vehicle Network								2023 IEEE 20th Consumer Communications & Networking Conference (CCNC)								987	8				10.1109/CCNC51644.2023.10059954							Conference Paper	2023	2023	Role of RSUs become essential in enhancing the service reliability rate of the end vehicle to strengthen autonomous vehicle technology. Computation-intensive services are delay-sensitive, and most existingmethodsattempted comprehensively to meet the application deadline but have not reached the expectations due to classical computing. In this regard, we design a novel offloading decision-making method based on quantum theory through reinforcement learning. Grover's algorithm is employed to select a feasible device based on cost and energy usage probability ratio. Theoretical and mathematical validations and simulation outcomes confine the impact of novel decision-making methods on the statistical constraints of the heterogeneous framework.					2023 IEEE 20th Consumer Communications & Networking Conference (CCNC)2023 IEEE 20th Consumer Communications & Networking Conference (CCNC)	8-11 Jan. 20238-11 Jan. 2023		Las Vegas, NV, USALas Vegas, NV, USA	1	0	0	0	0	0	1					978-1-6654-9734-3									Inf. Commun. Eng. & RLRC for Autonomous Vehicle arts & Mater. Innovation, Yeugnam Univ., South KoreaInst. of Artificial Intelligence & Robotics, Xi'an Jiaotong Univ., Xi'an, ChinaDept. of Electr. Eng., Yeungnam Univ., Gyeongsan, South KoreaInf. Commun. Eng., Yeugnam Univ., Gyeongsan, South Korea				2023-04-06	INSPEC:22812475		
B	Yuqi Zheng; Ruidong Yan; Bin Jia; Rui Jiang						Rong Song				Feedback Forecasting based Deep Deterministic Policy Gradient Algorithm for Car-Following of Autonomous Vehicle								2021 IEEE International Conference on Unmanned Systems (ICUS)								396	401				10.1109/ICUS52573.2021.9641292							Conference Paper	2021	2021	Deep reinforcement learning has been applied to the car-following control of autonomous driving in recent years. However, the safety of car-following through reinforcement learning remains a problem. To address this problem, a feedback forecasting based deep reinforcement learning algorithm is proposed. A differential equation model is used to predict the collision of adjacent vehicles. The output action of reinforcement learning is supervised by the feedback single to achieve a safe distance. As a result, the performance of autonomous driving safety is improved while others are slightly affected. The simulation results, based on NGSIM data, show that the predict collision avoidance algorithm is feasible and effective in the car-following control of autonomous driving.					2021 IEEE International Conference on Unmanned Systems (ICUS)2021 IEEE International Conference on Unmanned Systems (ICUS)	15-17 Oct. 202115-17 Oct. 2021	IEEE; CICCIEEE; CICC	Beijing, ChinaBeijing, China	1	0	0	0	0	0	1					978-1-6654-3885-8									Sch. of Traffic & Transp., Beijing Jiaotong Univ., Beijing, China				2022-11-01	INSPEC:21695004		
B	El Hamdani, S.; Loudari, S.; Novotny, S.; Bouchner, P.; Benamar, N.										A Markov Decision Process Model for a Reinforcement Learning-based Autonomous Pedestrian Crossing Protocol								2021 3rd IEEE Middle East and North Africa COMMunications Conference (MENACOMM)								147	51				10.1109/MENACOMM50742.2021.9678310							Conference Paper	2021	2021	Autonomous Traffic Management (ATM) systems empowered with Machine Learning (ML) technics are a promising solution for eliminating traffic light and decreasing traffic congestion in the future. However, few efforts have focused on integrating pedestrians in ATM, namely the static programming-based cooperative protocol called Autonomous Pedestrian Crossing (APC). In this paper, we model a Markov Decision Process (MDP) to enable a Deep Reinforcement Learning (DRL)-based version of APC protocol that is able to dynamically achieve the same objectives (i.e. decreasing traffic delay at the crossing area). Using concrete state space, action set and reward functions, our model forces the Autonomous Vehicle (AV) to "think" and behave according to APC architecture. Compared to the traditional programming APC system, our approach permits the AV to learn from its previous experiences in non-signalized crossing and optimize the distance and the velocity parameters accordingly.					2021 3rd IEEE Middle East and North Africa COMMunications Conference (MENACOMM)2021 3rd IEEE Middle East and North Africa COMMunications Conference (MENACOMM)	3-5 Dec. 20213-5 Dec. 2021		Agadir, MoroccoAgadir, Morocco	0	0	0	0	0	0	0					978-1-6654-3443-0									Fac. of Transp. Sci. Czech, Tech. Univ. in Prague, Prague, Czech RepublicUniv. Ibn Tofail in Kenitra, Kenitra, MoroccoMoulay Ismail Univ. of Meknes, Meknes, Morocco				2022-04-15	INSPEC:21574280		
B	Wang, Y.; Zheng, C.; Sun, M.; Chen, Z.; Sun, Q.										Reinforcement-learning-aided adaptive control for autonomous driving with combined lateral and longitudinal dynamics								2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)								840	5				10.1109/DDCLS58216.2023.10166569							Conference Paper	2023	2023	This paper presents a deep reinforcement learning-aided controller for a 3-DOF autonomous vehicle with combined lateral and longitudinal dynamics. In this scheme, the active disturbance rejection control (ADRC) gives full play to its advantages of being model-free and being able to estimate and compensate for internal uncertainties and external disturbances in real-time, and deep deterministic policy gradient (DDPG) fully considers safety, comfort, economy, and combines driving demand with state, action, reward to achieve real-time adaptive adjustment of control parameters. Thus, the adaptive controller can better deal with uncertainties from modeling, parameters, and driving environment, and self-learning and adaptation ability is obtained simultaneously. Moreover, simulation results illustrate that the adaptive controller performs satisfactorily for different driving operations and environments due to the online tuning and optimization of control parameters.					2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS)	12-14 May 202312-14 May 2023	IEEE; Beijing Information Science & Technology UniversityIEEE; Beijing Information Science & Technology University	Xiangtan, ChinaXiangtan, China	0	0	0	0	0	0	0					979-8-3503-2105-0									Coll. of Artificial Intelligence, Nankai Univ., Tianjin, ChinaBeijing Inst. of Astronaut. Syst. Eng., Beijing, China				2023-07-20	INSPEC:23389794		
C	Bai, Haoyu; Hsu, David; Lee, Wee Sun			IEEE							Planning How to Learn								2013 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						2853	2859											Proceedings Paper	2013	2013	When a robot uses an imperfect system model to plan its actions, a key challenge is the exploration-exploitation trade-off between two sometimes conflicting objectives: (i) learning and improving the model, and (ii) immediate progress towards the goal, according to the current model. To address model uncertainty systematically, we propose to use Bayesian reinforcement learning and cast it as a partially observable Markov decision process (POMDP). We present a simple algorithm for offline POMDP planning in the continuous state space. Offline planning produces a POMDP policy, which can be executed efficiently online as a finite-state controller. This approach seamlessly integrates planning and learning: it incorporates learning objectives in the computed plan, which then enables the robot to learn nearly optimally online and reach the goal. We evaluated the approach in simulations on two distinct tasks, acrobot swing-up and autonomous vehicle navigation amidst pedestrians, and obtained interesting preliminary results.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 06-10, 2013MAY 06-10, 2013	IEEEIEEE	Karlsruhe, GERMANYKarlsruhe, GERMANY	9	0	0	0	0	0	12			1050-4729	2577-087X	978-1-4673-5641-1; 978-1-4673-5643-5									Natl Univ Singapore, Dept Comp Sci, Singapore 117417, Singapore				2013-01-01	WOS:000337617302130		
B	Geng, J.; Li, W.; Duan, F.										Adaptive Dynamic Programming-Based Active Disturbance Rejection Control for Autonomous Vehicle Tracking								2023 IEEE International Conference on Unmanned Systems (ICUS)								1334	9				10.1109/ICUS58632.2023.10318320							Conference Paper	2023	2023	Nowadays, autonomous driving technology is more closely connected with human society, but the complex urban road environment puts higher requirements on the autonomous vehicle's robustness and fast response ability. To solve the problems mentioned above, this paper proposes a trajectory tracking control method with anti-interference and high real-time performance. The active disturbance rejection control approach is utilized to reject external disturbance and internal model uncertainties with the model predictive control controller. To speed up the computation, parameterized adaptive dynamic programming is used to estimate the optimal control sequence. A double-lane change demonstration is used to test and verify the proposed method. The experimental results show that our method improves the anti-interference ability of the system while speeding up the control speed. The proposed method can maintain the lateral error within 0.09m even with imposing 0.05rad disturbance to the input, and the single-step control time is half the traditional MPC.					2023 IEEE International Conference on Unmanned Systems (ICUS)2023 IEEE International Conference on Unmanned Systems (ICUS)	20232023		Hefei, ChinaHefei, China	0	0	0	0	0	0	0					979-8-3503-1630-8									Coll. of Artificial Intelligence, Nankai Univ., Tianjin, China				2023-12-14	INSPEC:24168808		
J	Merton, H.; Delamore, T.; Stol, K.; Williams, H.										Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle [arXiv]								Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle [arXiv]																				Preprint	2024	2024	With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack. This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following. Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation. Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed. Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following. Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.									0	0	0	0	0	0	0																		2024-01-25	INSPEC:24354780		
C	Kim, Taewan; Kim, H. Jin			IEEE							Path Tracking Control and Identification of Tire Parameters using On-line Model-based Reinforcement Learning								2016 16TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS)		International Conference on Control Automation and Systems						215	219											Proceedings Paper	2016	2016	Path tracking control for autonomous vehicle using model predictive control (MPC) algorithm maintains maneuverability by calculating a sequence of control input which minimizes a tracking error. The weakness of this method is that the performance of MPC may decrease significantly when the priori prediction model is not accurate. Therefore, it is important to keep the vehicle stable when MPC having model error. This paper uses an on-line model-based reinforcement learning (RL) to decrease the path error by learning unknown parameters and updating a prediction model. To validate, two kinds of path tracking simulation are conducted: one is the comparison the performance between on-line model-based RL and MPC with model error. The other one is about the test when the model used in MPC and the true dynamics, which actually received input, have different tire model. The model-based RL method succeeds to learn unknown tire parameters and maintains their maneuverability in both simulations.					16th International Conference on Control, Automation and Systems (ICCAS)16th International Conference on Control, Automation and Systems (ICCAS)	OCT 16-19, 2016OCT 16-19, 2016	Korea Tourism Org; Yujin Robot Co Ltd; Korean Federat Sci & Technol Soc; Chonnam Natl Univ, Robot Res Initiat; Gyeongju Convent & Visitors Bur; POSCO; RS Automat; LS Ind Syst Co LtdKorea Tourism Org; Yujin Robot Co Ltd; Korean Federat Sci & Technol Soc; Chonnam Natl Univ, Robot Res Initiat; Gyeongju Convent & Visitors Bur; POSCO; RS Automat; LS Ind Syst Co Ltd	Gyeongju, SOUTH KOREAGyeongju, SOUTH KOREA	6	2	0	0	0	0	8			2093-7121		978-89-93215-12-0									Seoul Natl Univ, Dept Mech & Aerosp Engn, Seoul, South Korea				2016-01-01	WOS:000401800200034		
J	Gao, Hongbo; Shi, Guanya; Xie, Guotao; Cheng, Bo										Car-following method based on inverse reinforcement learning for autonomous vehicle decision-making								INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS				15	6					1729881418817162			10.1177/1729881418817162							Article	DEC 6 2018	2018	There are still some problems need to be solved though there are a lot of achievements in the fields of automatic driving. One of those problems is the difficulty of designing a car-following decision-making system for complex traffic conditions. In recent years, reinforcement learning shows the potential in solving sequential decision optimization problems. In this article, we establish the reward function R of each driver data based on the inverse reinforcement learning algorithm, and r visualization is carried out, and then driving characteristics and following strategies are analyzed. At last, we show the efficiency of the proposed method by simulation in a highway environment.									17	2	0	0	2	0	20			1729-8814											Tsinghua Univ, State Key Lab Automot Safety & Energy, Beijing, Peoples R ChinaTsinghua Univ, Ctr Intelligent Connected Vehicles & Transportat, Beijing, Peoples R ChinaCALTECH, Dept Elect Engn, Pasadena, CA 91125 USAHunan Univ, Dept Automot Engn, Changsha, Hunan, Peoples R China				2018-12-28	WOS:000452884100001		
J	Gulino, C.; Fu, J.; Luo, W.; Tucker, G.; Bronstein, E.; Lu, Y.; Harb, J.; Pan, X.; Wang, Y.; Chen, X.; Co-Reyes, J.D.; Agarwal, R.; Roelofs, R.; Lu, Y.; Montali, N.; Mougin, P.; Yang, Z.; White, B.; Faust, A.; Mcallister, R.; Anguelov, D.; Sapp, B.										Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research [arXiv]								Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research [arXiv]																				Preprint	2023	2023	Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simulation requires accurate modeling of nuanced and complex multi-agent interactive behaviors. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing. Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios. It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imitation and reinforcement learning algorithms with ablation studies on different design decisions, where we highlight the effectiveness of routes as guidance for planning agents and the ability of RL to overfit against simulated agents.									0	0	0	0	0	0	0																		2023-11-02	INSPEC:23900806		
J	Kaloudi, Nektaria; Li, Jingyue					Li, Jingyue/0000-0002-7958-391X					AST-SafeSec: Adaptive Stress Testing for Safety and Security Co-Analysis of Cyber-Physical Systems								IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY				18				5567	5579				10.1109/TIFS.2023.3309160							Article	2023	2023	Cyber-physical systems are becoming more intelligent with the adoption of heterogeneous sensor networks and machine learning capabilities that deal with an increasing amount of input data. While this complexity aims to solve problems in various domains, it adds new challenges for the system assurance. One issue is the rise in the number of abnormal behaviors that affect system performance due to possible sensor faults and attacks. The combination of safety risks, which are usually caused by random sensor faults and security risks that can happen during any random system state, makes the full coverage testing of the cyber-physical system challenging. Existing techniques are inadequate to deal with complex safety and security co-risks against cyber-physical systems. In this paper, we propose AST-SafeSec, an analysis methodology for both safety and security aspects that utilizes reinforcement learning to identify the most likely adversarial paths at various normal or failure states of a cyber-physical system that can influence system behavior through its sensor data. The methodology is evaluated using an autonomous vehicle scenario by incorporating a security attack into the stochastic sensor elements of a vehicle. Evaluation results show that the methodology analyzes the interaction of malicious attacks with random faults and identifies the incident caused by the interactions and the most likely path that leads to the incident.									0	0	0	0	0	0	0			1556-6013	1556-6021										Norwegian Univ Sci & Technol, Dept Comp Sci, N-7491 Trondheim, NorwaySINTEF Digital, N-7034 Trondheim, Norway				2023-10-26	WOS:001081655300002		
B	Chandramohan, A.; Poel, M.; Meijerink, B.; Heijenk, G.										Machine learning for cooperative driving in a multi-lane highway environment								2019 Wireless Days (WD)								4 pp.	4 pp.				10.1109/WD.2019.8734192							Conference Paper	2019	2019	Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them. In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment. A driving algorithm is designed using deep Q learning, a type of reinforcement learning. In order to train and test driving algorithms, we deploy a simulated traffic system, using SUMO (Simulation of Urban Mobility). The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles. Furthermore, the impact of limited communication range and random packet loss is investigated. Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high. We propose directions for additional research to improve the performance of the algorithm.					2019 Wireless Days (WD)2019 Wireless Days (WD)	24-26 April 201924-26 April 2019		Manchester, UKManchester, UK	0	0	0	0	0	0	0					978-1-7281-0117-0									Dept. of Comput. Sci., Univ. of Twente, Enschede, Netherlands				2019-12-06	INSPEC:18742305		
J	Cai, Peide; Mei, Xiaodong; Tai, Lei; Sun, Yuxiang; Liu, Ming				Sun, Yuxiang/HPH-6656-2023; Liu, Ming/AAC-9891-2020	Sun, Yuxiang/0000-0002-7704-0559; Liu, Ming/0000-0002-4500-238X; CAI, Peide/0000-0002-9759-2991; Mei, Xiaodong/0000-0002-4687-6345					High-Speed Autonomous Drifting With Deep Reinforcement Learning								IEEE ROBOTICS AND AUTOMATION LETTERS				5	2			1247	1254				10.1109/LRA.2020.2967299							Article	APR 2020	2020	Drifting is a complicated task for autonomous vehicle control. Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely. We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic. The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed. After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map. The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.									49	2	0	0	0	0	55			2377-3766											Hong Kong Univ Sci & Technol, Hong Kong, Peoples R ChinaAlibaba Grp, AI Lab, Hangzhou 311000, Peoples R China				2020-04-01	WOS:000526693100002		
J	Gao, Weinan; Odekunle, Adedapo; Chen, Yunfeng; Jiang, Zhong-Ping				Jiang, Zhong-Ping/AAH-4981-2019; Gao, Weinan/ABA-9794-2021	Jiang, Zhong-Ping/0000-0002-4868-9359; Gao, Weinan/0000-0001-7921-018X; Chen, Yunfeng/0000-0002-0108-8484					Predictive cruise control of connected and autonomous vehicles via reinforcement learning								IET CONTROL THEORY AND APPLICATIONS				13	17			2849	2855				10.1049/iet-cta.2018.6031							Article	NOV 26 2019	2019	Predictive cruise control concerns designing controllers for autonomous vehicles using the broadcasted information from the traffic lights such that the idle time around the intersection can be reduced. This study proposes a novel adaptive optimal control approach based on reinforcement learning to solve the predictive cruise control problem of a platoon of connected and autonomous vehicles. First, the reference velocity is determined for each autonomous vehicle in the platoon. Second, a data-driven adaptive optimal control algorithm is developed to estimate the gains of the desired distributed optimal controllers without the exact knowledge of system dynamics. The obtained controller is able to regulate the headway, velocity, and acceleration of each vehicle in a suboptimal sense. The goal of trip time reduction is achieved without compromising vehicle safety and passenger comfort. Numerical simulations are presented to validate the efficacy of the proposed methodology.									11	1	0	0	1	0	11			1751-8644	1751-8652										Georgia Southern Univ, Allen E Paulson Coll Engn & Comp, Dept Elect & Comp Engn, Statesboro, GA 30460 USAPurdue Univ, Purdue Polytech Inst, Dept Construct Management Technol, W Lafayette, IN 47907 USANYU, Tandon Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA				2019-12-13	WOS:000499980600015		
B	Luyao Chen; Shaorong Xie; Tao Pang; Hang Yu; Xiangfeng Luo; Zhenyu Zhang						Reformat, M.; Zhang, D.; Bourbakis, N.				Learning from Suboptimal Demonstration via Trajectory-Ranked Adversarial Imitation								2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)								486	93				10.1109/ICTAI56018.2022.00078							Conference Paper	2022	2022	Robots trained by Imitation Learning(IL) are used in many tasks(e.g., autonomous vehicle manipulation). Generative Adversarial Imitation Learning (GAIL) assumes that the demonstration set used for training is of high quality. However, such demonstrations are difficult and expensive to obtain. GAIL-related methods fail to learn effective strategies if non-high quality demonstrations are used because the performance of agents trained by this method is limited by the demonstrator's operations. Our idea is to enable the agent to learn strategy with better performance than the demonstrator from a suboptimal demonstration set, which contains non-high quality demonstrations that are easier to obtain. Inspired by this, we propose the Trajectory-Ranked Adversarial Imitation Learning (TRAIL) method. First, for demonstration set processing, we introduce a ranking process and define the concept of Performance Relative Advantage of suboptimal demonstrations to specify the ranking order. Second, for model training, we reconstruct the objective function of GAIL and use an experience replay buffer, enabling the agent to learn implicit features and ranking information from the ranked suboptimal demonstration set and possess the ability to outperform the demonstrator. Experiments show that in Mujoco's tasks, our method can learn from a suboptimal demonstration set and can achieve better performance than baseline methods.					2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)	31 Oct.-2 Nov. 202231 Oct.-2 Nov. 2022		Macao, ChinaMacao, China	0	0	0	0	0	0	0					979-8-3503-9744-4									Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, ChinaThe 32nd Res. Inst. of China Electron. Technol. Group Corp.				2023-05-05	INSPEC:22962713		
C	Ma, Xiaobai; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.			IEEE							Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning								2018 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						1665	1671											Proceedings Paper	2018	2018	To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.					IEEE Intelligent Vehicles Symposium (IV)IEEE Intelligent Vehicles Symposium (IV)	JUN 26-30, 2018JUN 26-30, 2018	IEEEIEEE	Changshu, PEOPLES R CHINAChangshu, PEOPLES R CHINA	23	1	0	0	0	0	29			1931-0587		978-1-5386-4452-2									Stanford Univ, Aeronaut & Astronaut Dept, Stanford, CA 94305 USA				2018-01-01	WOS:000719424500260		
J	Gao, Hongbo; Shi, Guanya; Wang, Kelong; Xie, Guotao; Liu, Yuchao				Liu, Yuchao/U-2072-2019						Research on decision-making of autonomous vehicle following based on reinforcement learning method								INDUSTRIAL ROBOT-THE INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH AND APPLICATION				46	3	SI		444	452				10.1108/IR-07-2018-0154							Article	MAY 20 2019	2019	Purpose Over the past decades, there has been significant research effort dedicated to the development of autonomous vehicles. The decision-making system, which is responsible for driving safety, is one of the most important technologies for autonomous vehicles. The purpose of this study is the use of an intensive learning method combined with car-following data by a driving simulator to obtain an explanatory learning following algorithm and establish an anthropomorphic car-following model. Design/methodology/approach This paper proposed car-following method based on reinforcement learning for autonomous vehicles decision-making. An approximator is used to approximate the value function by determining state space, action space and state transition relationship. A gradient descent method is used to solve the parameter. Findings The effect of car-following on certain driving styles is initially achieved through the simulation of step conditions. The effect of car-following initially proves that the reinforcement learning system is more adaptive to car following and that it has certain explanatory and stability based on the explicit calculation of R. Originality/value The simulation results show that the car-following method based on reinforcement learning for autonomous vehicle decision-making realizes reliable car-following decision-making and has the advantages of simple sample, small amount of data, simple algorithm and good robustness.									14	1	0	0	0	0	15			0143-991X	1758-5791										Tsinghua Univ, State Key Lab Automot Safety & Energy, Beijing, Peoples R ChinaCALTECH, Pasadena, CA 91125 USABeijing Green Steam Technol Co Ltd, Beijing, Peoples R ChinaHunan Univ, Changsha, Hunan, Peoples R ChinaTsinghua Univ, Beijing, Peoples R China	Beijing Green Steam Technol Co Ltd			2019-10-18	WOS:000489012400017		
C	Afifi, Haitham; Ramaswamy, Arunselvan; Karl, Holger			IEEE		Ramaswamy, Arunselvan/0000-0001-7547-8111					Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks								IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2021)		IEEE International Conference on Communications											10.1109/ICC42927.2021.9500318							Proceedings Paper	2021	2021	In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs). In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength). Second, quality of service (QoS) which is used to measure the network's performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-Q-Networks (DQN). Additionally, we compare the performance of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.					IEEE International Conference on Communications (ICC)IEEE International Conference on Communications (ICC)	JUN 14-23, 2021JUN 14-23, 2021	IEEE; Telus; Huawei; Ciena; Nokia; Samsung; Qualcomm; Cisco; Google CloudIEEE; Telus; Huawei; Ciena; Nokia; Samsung; Qualcomm; Cisco; Google Cloud	ELECTR NETWORKELECTR NETWORK	3	0	0	0	0	0	3			1550-3607		978-1-7281-7122-7									Paderborn Univ, Comp Networks Grp, Paderborn, GermanyPaderborn Univ, Heinz Nixdorf Inst, D-33098 Paderborn, GermanyPaderborn Univ, Dept Comp Sci, D-33098 Paderborn, Germany				2021-12-12	WOS:000719386000074		
J	Valiente, R.; Razzaghpour, M.; Toghi, B.; Shah, G.; Fallah, Y.P.										Prediction-aware and Reinforcement Learning based Altruistic Cooperative Driving [arXiv]								arXiv																				Journal Paper	18 Nov. 2022	2022	Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.									0	0	0	0	0	0	0																		2023-04-28	INSPEC:22942751		
J	Li, Zequn; Lokhandwala, Mustafa; Al-Abbasi, Abubakr O.; Aggarwal, Vaneet; Cai, Hua				Li, Zequn/N-5117-2019						Integrating reinforcement-learning-based vehicle dispatch algorithm into agent-based modeling of autonomous taxis								TRANSPORTATION													10.1007/s11116-023-10433-w					NOV 2023		Article; Early Access		2023	While increasing number of studies are using agent-based models to study the potential environmental, economic, and social impacts of shared autonomous vehicles, the idle vehicle dispatching in these models is often simplified to heuristic rules. Because the system performance of an autonomous taxi fleet can be significantly affected by the vehicle dispatching algorithm, refining the vehicle dispatching can help us better evaluate the potential benefits and impacts of shared autonomous vehicle systems. The recent development of reinforcement-learning-based vehicle dispatching algorithms provides opportunities to improve autonomous vehicle system modeling with efficient and scalable vehicle dispatching. This study integrates a reinforcement learning algorithm into to an agent-based simulation model of a ride hailing system. Using an autonomous taxi fleet in New York City as a case study, we compared the system performance of using the proposed Deep Q-Network (DQN) method for dispatching decision with common rule-based and heuristic dispatch algorithms from relevant literatures. The results show that (1) DQN dispatches vehicles more conservatively (with less dispatching activities and distances) but achieved similar (slightly lower) rider service level with proactive dispatch methods; and (2) DQN outperformed all other dispatch methods evaluated in this study with significantly higher dispatch efficiency, as measured by the ratio of the number of extra riders served due to dispatching to the extra fleet dispatch distance.									0	0	0	0	0	0	0			0049-4488	1572-9435										Purdue Univ, Sch Ind Engn, 315 N Grant St, W Lafayette, IN 47907 USAPurdue Univ, Environm & Ecol Engn, W Lafayette, IN 47907 USA				2023-12-06	WOS:001108159600001		
C	D'Orazio, T; Cicirelli, G; Distante, C				D'Orazio, Tiziana/H-5032-2019; Cicirelli, Grazia/B-7699-2015; Distante, Cosimo/M-7996-2013	D'Orazio, Tiziana/0000-0003-1473-7110; Distante, Cosimo/0000-0002-1073-2390; Cicirelli, Grazia/0000-0003-1562-0467	Casasent, DP				A vision-based approach for learning an elementary navigation behavior								INTELLIGENT ROBOTS AND COMPUTER VISION XVII: ALGORITHMS, TECHNIQUES, AND ACTIVE VISION		PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		3522				320	326				10.1117/12.325778							Proceedings Paper	1998	1998	Developing elementary behavior is the starting point for the realization of complex systems. We present a learning algorithm that realizes a simple goal-reaching behavior for an autonomous vehicle when no a-priori knowledge of the environment is provided. Information coming from a visual sensor is used to detect a general state of the system. To each state an optimal action is associated using a Q-learning algorithm. As sets of states and actions are limited, a few training trials are sufficient in simulation to learn the optimal policy, During test trials (both in simulated and real environment) fuzzy sets with membership functions are introduced to compute the state of the system and the proper action at the extent of tackling errors in state estimation due to noise in vision measures. Experimental results both in simulated and real environment are shown.					Conference on Intelligent Robots and Computer Vision XVII - Algorithms, Techniques, and Active VisionConference on Intelligent Robots and Computer Vision XVII - Algorithms, Techniques, and Active Vision	NOV 02-03, 1998NOV 02-03, 1998	SPIE Int Soc Opt EngnSPIE Int Soc Opt Engn	BOSTON, MABOSTON, MA	0	0	0	0	0	0	0			0277-786X		0-8194-2983-X									CNR, Ist Elaboraz Segnali & Immagini, I-70126 Bari, Italy				1998-01-01	WOS:000077140600033		
B	Gu, J.; Li, J.; Tei, K.						Xu, B.				A Reinforcement Learning Approach for Adaptive Covariance Tuning in the Kalman Filter								2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)								1569	74				10.1109/IMCEC55388.2022.10020019							Conference Paper	2022	2022	State estimation and localization for the autonomous vehicle are essential for accurate navigation and safe maneuvers. The commonly used method is Kalman filtering, but its performance is affected by the noise covariance. An inappropriate set value will decrease the estimation accuracy and even makes the filter diverge. The noise covariance estimation problem has long been considered a tough issue because there is too much uncertainty in where the noise comes from and therefore unable to model it systematically. In recent years, Deep Reinforcement Learning (DRL) has made astonishing progress and is an excellent choice for tackling the problem that cannot be solved by conventional techniques, such as parameter estimation. By finely abstracting the problem as an MDP, we can use the DRL methods to solve it without too many prior assumptions. We propose an adaptive covariance tuning method applied to the Error State Extend Kalman Filter by taking advantage of DRL, called Reinforcement Learning Aided Covariance Tuning. The preliminary experiment result indicates that our method achieves a 14.73% estimation accuracy improvement on average compared with the vanilla fixed-covariance method and bound the estimation error within 0.4 m.					2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)	16-18 Dec. 202216-18 Dec. 2022		Chongqing, ChinaChongqing, China	0	0	0	0	0	0	0					978-1-6654-7968-4									Waseda Univ., Tokyo, Japan				2023-03-17	INSPEC:22546584		
C	Gao, Weinan; Jiang, Zhong-Ping; Ozbay, Kaan			IEEE	Jiang, Zhong-Ping/AAH-4981-2019; Gao, Weinan/ABA-9794-2021	Jiang, Zhong-Ping/0000-0002-4868-9359; Gao, Weinan/0000-0001-7921-018X					Adaptive Optimal Control of Connected Vehicles								2015 10TH INTERNATIONAL WORKSHOP ON ROBOT MOTION AND CONTROL (ROMOCO)								288	293											Proceedings Paper	2015	2015	In this paper, a data-driven non-model-based approach is proposed for the adaptive optimal control of connected vehicles, comprised of n human-driven vehicles only transmitting motional data and an autonomous vehicle in the tail receiving the broadcasted data from preceding vehicles by wireless vehicle-to-vehicle (V2V) communication devices. An optimal control problem is formulated to minimize the errors of distance and velocity and to optimize the fuel usage. By employing adaptive dynamic programming (ADP) technique, optimal controllers are obtained by online approximation for the connected vehicles without knowing the system dynamics. The effectiveness of the proposed approach is demonstrated via online learning control of the connected vehicles in two scenarios.					2015 10th International Workshop on Robot Motion and Control (RoMoCo)2015 10th International Workshop on Robot Motion and Control (RoMoCo)	JUL 06-08, 2015JUL 06-08, 2015	Chair Control & Syst Engn; IEEE Robot & Automat Soc; IEEE; CSS; Int Federat Automat Control; POLSPARChair Control & Syst Engn; IEEE Robot & Automat Soc; IEEE; CSS; Int Federat Automat Control; POLSPAR	Poznan Univ Technol, Poznan, POLANDPoznan Univ Technol, Poznan, POLAND	8	1	0	0	0	0	9					978-1-4799-7043-8									NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Control & Networks Lab, Brooklyn, NY 11201 USANYU, Dept Civil & Urban Engn, Brooklyn, NY 11201 USA				2016-09-08	WOS:000380392700045		
C	Wu, Jia; Li, ZiYan					Li, Ziyan/0000-0002-7100-539X	Alamaniotis, M; Pan, S				Hierarchical Joint Control for Urban Mixed-Autonomy Traffic Optimization								2020 IEEE 32ND INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE (ICTAI)		Proceedings-International Conference on Tools With Artificial Intelligence						700	705				10.1109/ICTAI50040.2020.00112							Proceedings Paper	2020	2020	With the fast development of autonomous vehicle technologies, the vehicle fleet will be made up of a mixture of human-driven vehicles and autonomous vehicles (AVs) in the coming 20-30 years. To efficiently utilize abundant data to deal with the mixed-autonomy traffic control problem, this paper formulates and approaches the problem using a deep reinforcement learning framework (DRL). DRL is a promising data-driven approach for traffic signal control and AVs control in a large-scale grid. However, traffic control based DRL is quite challenging since the complexity of control and the large search space of the policy. To deal with these issues, we propose a hierarchical joint control framework based on prior knowledge. Specifically, traffic signals at intersections and AVs are controlled by their local controllers, set according to well-adjusted policies; while the coordination of the traffic signals at intersections and the coordination among AVs are determined by two master controllers, respectively. Thus, the control of the whole grid is handled by two master controllers. In this way, the dimension of the action space is greatly decreased and the control operates much smoother. We verify our method by implementing a series of experiments in SUMO. The numerical experiments demonstrate the potential of the mixed-autonomy traffic control, compared with traditional traffic signal systems without AVs. We also demonstrate that our method is easy to train and operates robustly.					32nd IEEE International Conference on Tools with Artificial Intelligence (ICTAI)32nd IEEE International Conference on Tools with Artificial Intelligence (ICTAI)	NOV 09-11, 2020NOV 09-11, 2020	IEEE; IEEE Comp Soc; Biol & Artificial Intelligence FdnIEEE; IEEE Comp Soc; Biol & Artificial Intelligence Fdn	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			1082-3409		978-1-7281-9228-4									Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu, Peoples R ChinaFudan Univ, Acad Engn & Technol, Shanghai, Peoples R China				2021-06-16	WOS:000649734800102		
J	Cao, Y.; Ivanovic, B.; Xiao, C.; Pavone, M.										Reinforcement Learning with Human Feedback for Realistic Traffic Simulation [arXiv]								Reinforcement Learning with Human Feedback for Realistic Traffic Simulation [arXiv]																				Preprint	2023	2023	In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.									0	0	0	0	0	0	0																		2023-09-21	INSPEC:23672155		
C	Kuutti, Sampo; Fallah, Saber; Bowden, Richard			IEEE	Bowden, Richard/AAF-8283-2019	Bowden, Richard/0000-0003-3285-8020; Kuutti, Sampo/0000-0002-7020-4370					Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies								2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						108	114				10.1109/icra40945.2020.9197351							Proceedings Paper	2020	2020	Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 31-JUN 15, 2020MAY 31-JUN 15, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	16	1	0	0	0	0	18			1050-4729	2577-087X	978-1-7281-7395-5									Univ Surrey, Connected & Autonomous Vehicles Lab, Guildford GU2 7XH, Surrey, EnglandUniv Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England				2021-12-10	WOS:000712319500010		
B	Ma, N.; Zhang, Y.; Cai, W.; Qi, H.; Zhang, T.; Fu, C.										Reinforcement Learning-Based On-Ramp Merging Decision-Making for Autonomous Vehicles								2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	8				10.1109/CVCI59596.2023.10397432							Conference Paper	2023	2023	On-ramp merging is a complex and high-accidents scenario. The complexity of on-ramp merging scenario is mainly reflected in the aspects of varied merging environment and uncertain driving behavior of the surrounding vehicles, so it is a challenging scenario for autonomous vehicles. In this paper, a deep Q-learning network (DQN)-based merging decision-making method is proposed for autonomous vehicles. First, the models of vehicle dynamics and on-ramp merging scenarios are built, and the merging motivation is quantitative for designing the reinforcement learning. Second, the actions of the autonomous vehicle are defined, and a value-based DQN network for the merging decision-making is designed by simultaneously considering the driving safety, efficiency and comfort. Finally, in order to evaluate the performance of DQN-based merging decision-making method, a dynamic programming (DP)-based merging decision-making method is selected as the benchmarked method. The validation results demonstrate the autonomous vehicle by the proposed DQN-based merging decision-making method possesses good performance in safety, efficiency and comfort at on-ramp merging scenarios.					2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)	20232023		Changsha, ChinaChangsha, China	0	0	0	0	0	0	0					979-8-3503-4048-8									Sch. of Comput. Sci., Northwestern Polytech. Univ., Xi'an, ChinaR&D Dept., Beijing ConST Instrum. Technol. Inc, Beijing, China				2024-02-17	INSPEC:24508249		
J	Emuna, Ran; Duffney, Rotem; Borowsky, Avinoam; Biess, Armin					Biess, Armin/0000-0002-0087-3675					Example-guided learning of stochastic human driving policies using deep reinforcement learning								NEURAL COMPUTING & APPLICATIONS				35	23	SI		16791	16804				10.1007/s00521-022-07947-2					DEC 2022		Article; Early Access		2023	Deep reinforcement learning has been successfully applied to the generation of goal-directed behavior in artificial agents. However, existing algorithms are often not designed to reproduce human-like behavior, which may be desired in many environments, such as human-robot collaborations, social robotics and autonomous vehicles. Here we introduce a model-free and easy-to-implement deep reinforcement learning approach to mimic the stochastic behavior of a human expert by learning distributions of task variables from examples. As tractable use-cases, we study static and dynamic obstacle avoidance tasks for an autonomous vehicle on a highway road in simulation (Unity). Our control algorithm receives a feedback signal from two sources: a deterministic (handcrafted) part encoding basic task goals and a stochastic (data-driven) part that incorporates human expert knowledge. Gaussian processes are used to model human state distributions and to assess the similarity between machine and human behavior. Using this generic approach, we demonstrate that the learning agent acquires human-like driving skills and can generalize to new roads and obstacle distributions unseen during training.									0	0	0	0	0	0	0			0941-0643	1433-3058										Ben Gurion Univ Negev, Dept Ind Engn & Management, IL-8410501 Beer Sheva, Israel				2023-01-21	WOS:000903406800006		
J	Jeong-Hoon Lee; Se-Young Oh; Doo-Hyun Choi										Lateral control of an autonomous vehicle using reinforcement learning								Journal of the Institute of Electronics Engineers of Korea C				35-C	11			76	88											Journal Paper	Nov. 1998	1998	While most of the research on reinforcement learning assumed a discrete control space, many of the real world control problems need to have continuous output. This can be achieved by using continuous mapping functions for the value and action functions of the reinforcement learning architecture. Two questions arise here: 1) what sort of function representation to use? and 2) how to determine the amount of noise for search in action space? An ubiquitous neural network is used here to learn the value and policy functions. Next, the reinforcement predictor that is intended to predict the next reinforcement is introduced which also determines the amount of noise to add to the controller output. The proposed reinforcement learning architecture is found to have a sound online learning control performance especially at high-speed road following of high curvature road. Both computer simulation and actual experiments on a test vehicle have been performed and their efficiency and effectiveness has been verified.									0	0	0	0	0	0	0			1226-5853											Dept. of Electron. & Electr. Eng., Pohang Univ., Pohang, South Korea				1998-11-01	INSPEC:6191714		
B	Ziyan Chen										Analysis and Self-driving Algorithm Decision Mode Design								2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA).								93	7				10.1109/ICPECA53709.2022.9719038							Conference Paper	2022	2022	The countries around the world are closely monitoring the legislative progress of road traffic safety and autonomous vehicles. This paper aims to focus on the innovation of autonomous vehicle algorithm and technical progress, analyzing the ethical dilemmas and finding out the most proper data counting mode. The ethical choice of autonomous driving systems will be hugely controversial around the world for a long period. The behavior of intelligent robots in moral dilemmas can be simulated using a fairly simple value-based calculating mode, which is attributed by participants to the makers of intelligent operating systems. Using the fully deterministic model, all possible paths are searched and the best path is determined using the digital computation function in this paper. The algorithm design mode of this paper is that the matrix algorithm should be adopted as the core algorithm mode, regression algorithm, reinforcement algorithm as the auxiliary algorithm modes.					2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)	21-23 Jan. 202221-23 Jan. 2022		Shenyang, ChinaShenyang, China	1	0	0	0	0	0	1					978-1-6654-4276-3									Law Sch., Fudan Univ., Shanghai, China				2022-11-01	INSPEC:21743523		
C	Oyler, Dave W.; Yildiz, Yildiray; Girard, Anouck R.; Li, Nan I.; Kolmanovsky, Ilya V.			IEEE	Li, Nan/Q-5511-2019; Yildiz, Yildiray/AAK-8370-2021	Yildiz, Yildiray/0000-0001-6270-5354; Kolmanovsky, Ilya/0000-0002-7225-4160					A Game Theoretical Model of Traffic with Multiple Interacting Drivers for Use in Autonomous Vehicle Development								2016 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						1705	1710											Proceedings Paper	2016	2016	This paper describes a game theoretical model of traffic where multiple drivers interact with each other. The model is developed using hierarchical reasoning, a game theoretical model of human behavior, and reinforcement learning. It is assumed that the drivers can observe only a partial state of the traffic they are in and therefore although the environment satisfies the Markov property, it appears as non-Markovian to the drivers. Hence, each driver implicitly has to find a policy, i.e. a mapping from observations to actions, for a Partially Observable Markov Decision Process. In this paper, a computationally tractable solution to this problem is provided by employing hierarchical reasoning together with a suitable reinforcement learning algorithm. Simulation results are reported, which demonstrate that the resulting driver models provide reasonable behavior for the given traffic scenarios.					American Control Conference (ACC)American Control Conference (ACC)	JUL 06-08, 2016JUL 06-08, 2016	Amer Automat Control CouncilAmer Automat Control Council	Boston, MABoston, MA	34	4	0	0	0	0	43			0743-1619	2378-5861	978-1-4673-8682-1									Univ Michigan, Dept Aerosp Engn, 1320 Beal Ave, Ann Arbor, MI 48109 USABilkent Univ, Dept Mech Engn, TR-06800 Ankara, Turkey				2017-03-22	WOS:000388376101122		
J	Liu, Teng; Huang, Bing; Deng, Zejian; Wang, Hong; Tang, Xiaolin; Wang, Xiao; Cao, Dongpu					, xiaolin/0000-0001-5709-291X					Heuristics-oriented overtaking decision making for autonomous vehicles using reinforcement learning								IET ELECTRICAL SYSTEMS IN TRANSPORTATION				10	4	SI		417	424				10.1049/iet-est.2020.0044							Article; Proceedings Paper	DEC 2020	2020	This study presents a three-lane highway overtaking strategy for an automated vehicle, which is based on a heuristic planning reinforcement learning algorithm. The proposed decision-making controller focuses on keeping the autonomous vehicle operating safely and efficiently. First, the modelling of the overtaking driving scenario is introduced and the reference approaches named intelligent driver model and minimise overall braking induced by lane changes are formulated. Second, the Dyna-H algorithm, which combines the modified Q-learning algorithm with a heuristic planning policy, is utilised for highway overtaking decision-making. Three different heuristic strategies are formulated to improve learning efficiency and compare performance. This algorithm is applied to determine the lane change and speed selection for an ego vehicle in the environment with uncertainties. Finally, the performance of Dyna-H is estimated in the autonomous overtaking scenario by comparing it with the reference and traditional learning methods. Furthermore, the Dyna-H-enabled decision-making strategies are validated and analysed in an open-sourcing driving dataset. Results prove that the proposed decision-making strategy could produce superior performance in convergence rate and control.					16th IEEE Vehicle Power and Propulsion Conference (VPPC)16th IEEE Vehicle Power and Propulsion Conference (VPPC)	OCT 14-17, 2019OCT 14-17, 2019	IEEE; Bach Khoa; CTI; Univ Tokyo; IEEE VTSIEEE; Bach Khoa; CTI; Univ Tokyo; IEEE VTS	Hanoi, VIETNAMHanoi, VIETNAM	9	1	0	0	0	0	10			2042-9738	2042-9746										Chongqing Univ, Dept Automot Engn, Chongqing 400044, Peoples R ChinaUniv Waterloo, Mech & Mechatron Engn Dept, Waterloo, ON N2L 3G1, CanadaTsinghua Univ, Sch Vehicle & Mobil, Beijing, Peoples R ChinaQingdao Acad Intelligent Ind, Qingdao 266109, Shandong, Peoples R China	Qingdao Acad Intelligent Ind			2021-01-28	WOS:000607118700012		
C	Testi, Enrico; Favarelli, Elia; Giorgetti, Andrea			IEEE	Favarelli, Elia/HGC-3294-2022; Testi, Enrico/AAR-5579-2021	Testi, Enrico/0000-0003-2238-9160; Giorgetti, Andrea/0000-0001-6341-3927					Reinforcement Learning for Connected Autonomous Vehicle Localization via UAVs								PROCEEDINGS OF 2020 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR AGRICULTURE AND FORESTRY (METROAGRIFOR)								13	17				10.1109/metroagrifor50201.2020.9277630							Proceedings Paper	2020	2020	In precision farming, a very promising scenario is represented by a connected and autonomous vehicle (CAV) moving in a cultivated field and collecting high-resolution videos and hyperspectral images, requiring both localization and broadband communication. An effective approach to provide both localization and wideband communication exploits unmanned aerial vehicles (UAVs) that may act as relays to ensure seamless connectivity with a base station (BS). In this paper, we propose a reinforcement learning (RL)-based algorithm to find the best spatial configuration of a swarm of UAVs to localize a CAV in an unknown environment and assist the communication with a BS. The UAVs cooperate to estimate the position of the CAV exploiting only the received signal strength (RSS). A reward function, based on the distance between the UAVs and the CAV, and the estimated geometric diluition of precision (GDOP), is designed. Numerical results show how the proposed multi-agent Q-learning allows the UAVs to reach low root mean square error (RMSE) in the target localization, even without previous knowledge about the environment.					3rd IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)3rd IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)	NOV 04-06, 2020NOV 04-06, 2020	IEEE; Univ Trento; Minist Politiche Agricole Alimentari ForestaliIEEE; Univ Trento; Minist Politiche Agricole Alimentari Forestali	ELECTR NETWORKELECTR NETWORK	6	0	0	0	1	0	6					978-1-7281-8783-9									Univ Bologna, Dept Elect Elect & Informat Engn Guglielmo Marcon, CNIT, Via Univ 50, I-47522 Cesena, Italy				2021-06-04	WOS:000646354300003		
J	Chen, Xue-Mei; Xu, Shu-Yuan; Wang, Zi-Jia; Zheng, Xue-Long; Han, Xin-Tong; Liu, En-Hao					Xu, Shuyuan/0000-0003-0646-3105					A Decision-Making Model for Autonomous Vehicles at Intersections Based on Hierarchical Reinforcement Learning								UNMANNED SYSTEMS													10.1142/S2301385024500122					JAN 2023		Article; Early Access		2023	By aiming at addressing the left-turning problem of an autonomous vehicle considering the oncoming vehicles at an urban unsignallized intersection, a hierarchical reinforcement learning is proposed and a two-layer model is established to study behaviors of left-turning driving. Compared with the conventional decision-making models with a fixed path, the proposed multi-paths decision-making algorithm with horizontal and vertical strategies can improve the efficiency of autonomous vehicles crossing intersections while ensuring safety.									1	0	0	0	0	0	1			2301-3850	2301-3869										Beijing Inst Technol, Sch Mech Engn, 5th South ZhongGuanCun St, Beijing, Peoples R ChinaBeijing Inst Technol, Adv Technol Res Inst, 8366 Haitang Rd, Jinan, Shandong, Peoples R China				2023-02-21	WOS:000924567300001		
B	Sadiq, M.F.; Tasneem, N.; Ahmed, M.; Sultan, S.M.; Hasan, S.						Teen-Hang Meen				Towards Mitigating Probable Road Mishaps through DQN Based Deep Reinforcement Learning								2021 IEEE 3rd Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)								222	5				10.1109/ECBIOS51820.2021.9510732							Conference Paper	2021	2021	Road accident is a widespread problem in many urban areas nowadays. Reckless Driving (Over-Speed) is one of the attributes leading to the enormous destruction of pedestrian life. Autonomous vehicle speed control is an important research area in the Intelligent Transportation System (ITS) to mitigate road accidents in densely populated areas. Many existed works utilized Reinforcement Learning (RL) to control the vehicle speed intelligently. However, controlling the over-speed of the vehicle using RL under the speed limit of the different zones (i.e., Town, Rural Main Road, Highways, and so forth) is also another demanding task because of the speed limit variation. Therefore, we propose a Deep-Q-Network (DQN) based Deep Reinforcement Learning (Deep RL) method to overcome the over-speed problem in various speed limit areas. In this paper, the proposed system will reduce the vehicle speed intelligently if the driver passes over the speed limit in a particular zone. The simulation results show that our proposed solution can successfully reduce the vehicle speed under various zone-based speed limits.					2021 IEEE 3rd Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)2021 IEEE 3rd Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)	28-30 May 202128-30 May 2021		Tainan, TaiwanTainan, Taiwan	0	0	0	0	0	0	0					978-1-7281-9304-5									Univ. of Alberta, Edmonton, AB, CanadaDept. of Aeronaut. Eng., Nat. Formosa Univ., Huwei, Taiwan				2021-11-08	INSPEC:21079441		
J	Cao Jie; Shao Zixuan; Hou Liang							曹洁; 邵紫旋; 侯亮			Research on autonomous vehicle U-turn problem based on hierarchical reinforcement learning			基于分层强化学习的自动驾驶车辆掉头问题研究				计算机应用研究	Application Research of Computers				39	10			3008	3012,3045	1001-3695(2022)39:10<3008:JYFCQH>2.0.TX;2-S										Article	2022	2022	The U-turn task is one of the contents of autonomous driving research,and most of the solutions under the standard roads in cities cannot be implemented on non-standard roads.Aiming at solving this problem,this paper established a vehicle U-turn dynamical model and designed a multi-scale convolutional neural network to extract feature maps as the input of the agent.In addition,for the sparse reward problem in the U-turn task,this paper proposed a hierarchical proximal policy optimization algorithm that combined hierarchical reinforcement learning and proximal policy optimization algorithm.In experiments with simple and complex scena-rios,this algorithm learns policies faster and has a higher success rate of U-turn compared to other algorithms.			调头任务是自动驾驶研究的内容之一,大多数在城市规范道路下的方案无法在非规范道路上实施。针对这一问题,建立了一种车辆掉头动力学模型,并设计了一种多尺度卷积神经网络提取特征图作为智能体的输入。另外还针对调头任务中的稀疏奖励问题,结合分层强化学习和近端策略优化算法提出了分层近端策略优化算法。在简单和复杂场景的实验中,该算法相比于其他算法能够更快地学习到策略,并且具有更高的掉头成功率。						0	0	0	0	0	0	0			1001-3695											兰州理工大学计算机与通信学院, 兰州, 甘肃 730050, 中国Dept.of Computer & Communication, Lanzhou University of Technology, Lanzhou, Gansu 730050, China	兰州理工大学计算机与通信学院Dept.of Computer & Communication, Lanzhou University of Technology			2023-03-31	CSCD:7357117		
J	[Anonymous]										REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES, 53-57. SI								Mechatronic Systems and Control								5 pp.	5 pp.				10.2316/Journal.201.2023.1.201-0347							Journal Paper	2023	2023	Reinforcement learning (RL) is one of the most emerging domains of artificial intelligence. It is widely used in almost all sort of applications, including medical field, stock market, forecasting, and engineering field. One of the most effective uses of RL is in the control engineering domain owing to its learning by trial, for example, in building autonomous system like autonomous vehicle and robotics. In this paper, we have focused on the applications of RL in various control engineering problems. Stability of controller (or agent) using RL paradigm is a very crucial task due to exploration-exploitation policy used by any RL. Also, the unavailability of exact model of system or environment may lead to unsafe behaviour of the agent. Therefore, this paper focuses mainly on the stability aspect in RL-based controller. Many concepts are used to study and analyse the stability of a system viz Lyapunov method and Barrier function. This paper surveys the detailed application of these well-established stability certifier methods in various model-free and model-based RL framework.									0	0	0	0	0	0	0			2561-178X															2024-01-06	INSPEC:24238144		
C	Clemmons, Joseph; Jin, Yu-Fang			IEEE							Reinforcement Learning-Based Guidance of Autonomous Vehicles								2023 24TH INTERNATIONAL SYMPOSIUM ON QUALITY ELECTRONIC DESIGN, ISQED		International Symposium on Quality Electronic Design						496	501				10.1109/ISQED57927.2023.10129362							Proceedings Paper	2023	2023	Reinforcement learning (RL) has attracted significant research efforts to guide an autonomous vehicle (AV) for a collision-free path due to its advantages in investigating interactions among multiple vehicles and dynamic environments. This study deploys a Deep Q-Network (DQN) based RL algorithm with reward shaping to control an ego AV in an environment with multiple vehicles. Specifically, the state space of the RL algorithm depends on the desired destination, the ego vehicle's location and orientation, and the location of other vehicles in the system. The training time of the proposed RL algorithm is much shorter than most current image-based algorithms. The RL algorithm also provides an extendable framework to include a varying number of vehicles in the environment and can be easily adapted to different maps without changing the setup of the RL algorithm. Three scenarios were simulated in the Cars Learn to Act (CARLA) simulator to examine the effects of the proposed RL algorithm on guiding the ego AV interacting with multiple vehicles on straight and curvy roads. Our results showed that the ego AV could learn to reach its destination within 5000 episodes for all scenarios tested.					24th International Symposium on Quality Electronic Design (ISQED)24th International Symposium on Quality Electronic Design (ISQED)	APR 05-07, 2023APR 05-07, 2023	IEEE; IEEE Circuits & Syst Soc; IEEE Reliabil Soc; ACM SigDA; Synopsys; IEEE Electron Devices Soc; Silicon Valley Polytechn Inst; InnovoTek; Int Soc Qual Elect DesignIEEE; IEEE Circuits & Syst Soc; IEEE Reliabil Soc; ACM SigDA; Synopsys; IEEE Electron Devices Soc; Silicon Valley Polytechn Inst; InnovoTek; Int Soc Qual Elect Design	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			1948-3287		979-8-3503-3475-3									Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA				2023-08-10	WOS:001013619400070		
J	Kicki, Piotr; Gawron, Tomasz; Cwian, Krzysztof; Ozay, Mete; Skrzypczynski, Piotr				Kicki, Piotr/AAT-9497-2020; Ozay, Mete/L-4869-2013; Skrzypczynski, Piotr/M-1376-2014	Cwian, Krzysztof/0000-0003-2193-5684; Kicki, Piotr/0000-0003-0097-6612; Skrzypczynski, Piotr/0000-0002-9843-2404					Learning from experience for rapid generation of local car maneuvers								ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE				105						104399			10.1016/j.engappai.2021.104399					AUG 2021		Article; Early Access		2021	Being able to rapidly respond to the changing scenes and traffic situations by generating feasible local paths is of pivotal importance for car autonomy. We propose to train a deep neural network (DNN) to plan feasible and nearly-optimal paths for kinematically constrained vehicles in a small constant time. Our DNN model is trained using a novel weakly supervised approach and a gradient-based policy search. On real and simulated scenes and a large set of local planning problems, we demonstrate that our approach outperforms the existing planners with respect to the number of successfully completed tasks. While the path generation time is about 40 ms, the generated paths are smooth and comparable to those obtained from conventional path planners.									5	0	0	0	0	0	5			0952-1976	1873-6769										Poznan Univ Tech, Inst Robot & Machine Intelligence, Ul Piotrowo 3A, PL-60965 Poznan, Poland				2021-10-07	WOS:000701281200015		
C	Huang, Wenhui; Braghin, Francesco; Wang, Zhuo			IEEE	Huang, Wenhui/GPL-6227-2022	Huang, Wenhui/0000-0001-7212-027X; Braghin, Francesco/0000-0002-0476-4118					Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning								2019 IEEE 31ST INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE (ICTAI 2019)		Proceedings-International Conference on Tools With Artificial Intelligence						1536	1540				10.1109/ICTAI.2019.00220							Proceedings Paper	2019	2019	With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.					31st IEEE International Conference on Tools with Artificial Intelligence (ICTAI)31st IEEE International Conference on Tools with Artificial Intelligence (ICTAI)	NOV 04-06, 2019NOV 04-06, 2019	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	Portland, ORPortland, OR	3	0	0	0	0	0	4			1082-3409		978-1-7281-3798-8									Politecn Milan, Ind & Informat Engn, Milan, ItalyXidian Univ, Sch Commun Engn, Xian, Peoples R China				2020-09-03	WOS:000553441500211		
J	Xiang, J.; Guo, L.										Comfort Improvement for Autonomous Vehicles Using Reinforcement Learning with In-Situ Human Feedback								SAE Technical Papers								2022	01-0807				10.4271/2022-01-0807							Conference Paper; Journal Paper	2022	2022	In this paper, a reinforcement learning-based method is proposed to adapt autonomous vehicle passengers' expectation of comfort through in-situ human-vehicle interaction. Ride comfort has a significant influence on the user's experience and thus acceptance of autonomous vehicles. There is plenty of research about the motion planning and control of autonomous vehicles. However, limited studies have explicitly considered the comfort of passengers in autonomous vehicles. This paper studies the comfort of humans in autonomous vehicles longitudinal autonomous driving. The paper models and then improves passengers' feelings about autonomous driving behaviors. This proposed approach builds a control and adaptation strategy based on reinforcement learning using human's in-situ feedback on autonomous driving. It also proposes an adaptation of humans to autonomous vehicles to account for improper human driving expectations. The proposed approaches are implemented and tested with human-in-the-loop experiments and the results demonstrate that the proposed approaches can successfully adapt the vehicle behaviors, improve the ride comfort of humans in autonomous vehicles, and also correct improper human driving expectations.					WCX SAE World Congress ExperienceWCX SAE World Congress Experience	20222022		Virtual Conference, MI, USAVirtual Conference, MI, USA	0	0	0	0	0	0	0			2688-3627											Clemson Univ., Clemson, SC, USA				2023-09-14	INSPEC:23654850		
R	Lu, Chengjie; Shi, Yize; Zhang, Huihui; Zhang, Man; Wang, Tiexin; Yue, Tao; Ali, Shaukat										DeepCollision: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions								Zenodo													https://doi.org/10.5281/ZENODO.5906633							Data set	2022-09-06	2022	With the aim to test autonomous driving systems, we propose a novel reinforcement learning (RL)-based approach named DeepCollision to learn operating environment configurations of autonomous vehicles, including formalizing environment configuration learning as an MDP and adopting DQN algorithm as the RL solution;DeepCollisionlearns environment configurations to maximize collisions of an Autonomous Vehicle Under Test (AVUT). This dataset contains: algorithms- The algorithm of DeepCollision, which includes the network architecture and the DQN hyperparameter settings; pilot-study- All the raw data and plots for the pilot study; formal-experiment- A dataset contains all the raw data for analysis and the scenarios with detailed demand values; rest-api- The REST API endpoints for environment configuration and oneexampleto show the usage of the APIs. Copyright: Creative Commons Attribution 4.0 International Open Access									0	0	0	0	0	0	0														Nanjing University of Aeronautics & Astronautics, ChinaNanjing University of Aeronautics & Astronautics, ChinaQilu University of Technology (Shandong Academy of Sciences), China	Nanjing University of Aeronautics & AstronauticsNanjing University of Aeronautics & AstronauticsQilu University of Technology (Shandong Academy of Sciences)			2022-11-02	DRCI:DATA2022175024960112		
C	Ding, Guohui; Aghli, Sina; Heckman, Christoffer; Chen, Lijun	Kosecka, J				HECKMAN, CHRISTOFFER/0000-0002-9651-6866; CHEN, LIJUN/0000-0001-6694-4299	Maciejewski, AA; Okamura, A; Bicchi, A; Stachniss, C; Song, DZ; Lee, DH; Chaumette, F; Ding, H; Li, JS; Wen, J; Roberts, J; Masamune, K; Chong, NY; Amato, N; Tsagwarakis, N; Rocco, P; Asfour, T; Chung, WK; Yasuyoshi, Y; Sun, Y; Maciekeski, T; Althoefer, K; AndradeCetto, J; Chung, WK; Demircan, E; Dias, J; Fraisse, P; Gross, R; Harada, H; Hasegawa, Y; Hayashibe, M; Kiguchi, K; Kim, K; Kroeger, T; Li, Y; Ma, S; Mochiyama, H; Monje, CA; Rekleitis, I; Roberts, R; Stulp, F; Tsai, CHD; Zollo, L				Game-Theoretic Cooperative Lane Changing Using Data-Driven Models								2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						3640	3647											Proceedings Paper	2018	2018	Self-driving vehicles are being increasingly deployed in the wild. One of the most important next hurdles for autonomous driving is how such vehicles will optimally interact with one another and with their surroundings. In this paper, we consider the lane changing problem that is fundamental to road-bound multi-vehicle systems, and approach it through a combination of deep reinforcement learning (DRL) and game theory. We introduce a proactive-passive lane changing framework and formulate the lane changing problem as a Markov game between the proactive and passive vehicles. Based on different approaches to carry out DRL to solve the Markov game, we propose an asynchronous lane changing scheme as in a single-agent RL setting and a synchronous cooperative lane changing scheme that takes into consideration the adaptive behavior of the other vehicle in a vehicle's decision. Experimental results show that the synchronous scheme can effectively create and find proper merging moment after sufficient training. The framework and solution developed here demonstrate the potential of using reinforcement learning to solve multi-agent autonomous vehicle tasks such as the lane changing as they are formulated as Markov games.					25th IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)25th IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	OCT 01-05, 2018OCT 01-05, 2018	IEEE Robot & Automat Soc; IEEE Ind Elect Soc; Robot Soc Japan; Soc Instrument & Control Engineers; New Technol Fdn; IEEE; Adept MobileRobots; Willow Garage; Aldebaran Robot; Natl Instruments; Reflexxes GmbH; Schunk Intec S L U; Univ Carlos III Madrid; BOSCH; JD COM; Pal Robot; KUKA; Santander; Squirrel AI Learning; Baidu; Generat Robots; KINOVA Robot; Ouster; Univ Pablo Olavide Sevilla; Rapyuta Robot; SICK; TOYOTA; UP; Amazon; ARGO; Built Robot; Disney Res; Easy Mile; Hitachi; Robot; Khalifa Univ; Magazino; MathWorks; New Dexterity; Schunk; nuTonomy; PILZ; Prophesee; Rootnik; Saga Robot; Shadow; Soft Bank Robot; Anyverse; GalTech; Generat Robot; IEEE CAA Journal Automatica Sinica; Sci Robot, AAAS; TERASIEEE Robot & Automat Soc; IEEE Ind Elect Soc; Robot Soc Japan; Soc Instrument & Control Engineers; New Technol Fdn; IEEE; Adept MobileRobots; Willow Garage; Aldebaran Robot; Natl Instruments; Reflexxes GmbH; Schunk Intec S L U; Univ Carlos III Madrid; BOSCH; JD COM; Pal Robot; KUKA; Santander; Squirrel AI Learning; Baidu; Generat Robots; KINOVA Robot; Ouster; Univ Pablo Olavide Sevilla; Rapyuta Robot; SICK; TOYOTA; UP; Amazon; ARGO; Built Robot; Disney Res; Easy Mile; Hitachi; Robot; Khalifa Univ; Magazino; MathWorks; New Dexterity; Schunk; nuTonomy; PILZ; Prophesee; Rootnik; Saga Robot; Shadow; Soft Bank Robot; Anyverse; GalTech; Generat Robot; IEEE CAA Journal Automatica Sinica; Sci Robot, AAAS; TERAS	Madrid, SPAINMadrid, SPAIN	12	0	0	0	0	0	15			2153-0858		978-1-5386-8094-0									Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA				2019-03-07	WOS:000458872703061		
B	Abdalkarim, Mahmoud										Using V2X and Reinforcement Learning to Improve Autonomous Vehicles Algorithms with CARLA																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798380257244									Auburn University, Alabama, United States	Auburn University				PQDT:85149447		
J	Manjanna, S.; van Hoof, H.; Dudek, G.										Reinforcement learning with non-uniform state representations for adaptive search [arXiv]								arXiv								11 pp.	11 pp.											Journal Paper	15 June 2019	2019	Efficient spatial exploration is a key aspect of search and rescue. In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher. This should allow an autonomous vehicle find one or more lost targets as rapidly as possible. We do this by performing non-uniform sampling of the search region. The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning. We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target. Key features of our search algorithm are the ability to employ a very general non-deterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions. One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics. [doi:10.1109/SSRR.2018.8468649].									0	0	0	0	0	0	0														Mobile Robot. Lab., McGill Univ., Montreal, QC, CanadaAmsterdam Machine Learning Lab., Univ. of Amsterdam, Amsterdam, Netherlands				2019-10-25	INSPEC:19034952		
B	Ha, V.T.; Tu, T.N.; Dung, N.T.; Mien, T.L.; Thuy, C.T.				THANH HA, VO/AHA-6344-2022	THANH HA, VO/0000-0003-4023-260X					Deep Q-Network (DQN) Approach for Automatic Vehicles Applied in the Intelligent Transportation System (ITS)								2023 International Conference on System Science and Engineering (ICSSE)								527	32				10.1109/ICSSE58758.2023.10227206							Conference Paper	2023	2023	This paper presents the design of an intelligent controller applying reinforcement learning using a deep Q-network (DQN) algorithm for autonomous vehicles. The deep Q-network (DQN) algorithm is an online, model-free reinforcement learning approach. A DQN agent is a value-based reinforcement learning agent that teaches a critic to predict future rewards or returns. Deep Q-network is to replace the action-state Q table with a neural network. This solution applies to building a self-propelled agent capable of correcting static and moving obstacles according to the physical environment. As a result, the autonomous vehicle can move and avoid collisions with obstacles. The correctness of the theory is demonstrated through MATLAB simulation.					2023 International Conference on System Science and Engineering (ICSSE)2023 International Conference on System Science and Engineering (ICSSE)	27-28 July 202327-28 July 2023		Ho Chi Minh City, VietnamHo Chi Minh City, Vietnam	0	0	0	0	0	0	0					979-8-3503-2294-1									Fac. of Electr. & Electron. Eng., Univ. of Transp. & Commun., Hanoi, VietnamFac. of Electr. Eng., Univ. of Econ. - Technol. for Ind., Hanoi, Vietnam				2023-09-21	INSPEC:23684036		
B	Zhou, Jinkai										Autonomous Vehicle Fleet Operations and Planning With User Activity Scheduling Constraints																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798684655951									New York University Tandon School of Engineering, Civil Engineering, New York, United States	New York University Tandon School of Engineering				PQDT:66689079		
J	Nageshrao, S.; Tseng, E.; Filev, D.										Autonomous Highway Driving using Deep Reinforcement Learning [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	29 March 2019	2019	The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. This may lead to a scenario that was not postulated in the design phase. Due to this, formulating a rule based decision maker for selecting maneuvers may not be ideal. Similarly, it may not be effective to design an a-priori cost function and then solve the optimal control problem in real-time. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with simulated traffic. The decision maker for AV is implemented as a deep neural network providing an action choice for a given system state. In a critical application such as driving, an RL agent without explicit notion of safety may not converge or it may need extremely large number of samples before finding a reliable policy. To best address the issue, this paper incorporates reinforcement learning with an additional short horizon safety check (SC). In a critical scenario, the safety check will also provide an alternate safe action to the agent provided if it exists. This leads to two novel contributions. First, it generalizes the states that could lead to undesirable "near-misses" or "collisions ". Second, inclusion of safety check can provide a safe and stable training environment. This significantly enhances learning efficiency without inhibiting meaningful exploration to ensure safe and optimal learned behavior. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density in a highway setting.									0	0	0	0	0	0	0														Ford Greenfield Labs., Palo Alto, CA, USAFord Res. & Innovation Center, Dearborn, MI, USA				2019-08-08	INSPEC:18842172		
B	Fleicher, M.; Musicant, O.; Azaria, A.						Reformat, M.; Zhang, D.; Bourbakis, N.				Using Physiological Metrics to Improve Reinforcement Learning for Autonomous Vehicles								2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)								1223	30				10.1109/ICTAI56018.2022.00186							Conference Paper	2022	2022	Thanks to recent technological advances Autonomous Vehicles (AVs) are becoming available at some locations. Safety impacts of these devices have, however, been difficult to assess. In this paper we utilize physiological metrics to improve the performance of a reinforcement learning agent attempting to drive an autonomous vehicle in simulation. We measure the performance of our reinforcement learner in several aspects, including the amount of stress imposed on potential passengers, the number of training episodes required, and a score measuring the vehicle's speed as well as the distance successfully traveled by the vehicle, without traveling off-track or hitting a different vehicle. To that end, we compose a human model, which is based on a dataset of physiological metrics of passengers in an autonomous vehicle. We embed this model in a reinforcement learning agent by providing negative reward to the agent for actions that cause the human model an increase in heart rate. We show that such a "passenger-aware" reinforcement learner agent does not only reduce the stress imposed on hypothetical passengers, but, quite surprisingly, also drives safer and its learning process is more effective than an agent that does not obtain rewards from a human model.					2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)	31 Oct.-2 Nov. 202231 Oct.-2 Nov. 2022		Macao, ChinaMacao, China	0	0	0	0	0	0	0					979-8-3503-9744-4									Comput. Sci. Dept., Ariel Univ., Ariel, IsraelInd. Eng. Dept., Ariel Univ., Ariel, Israel				2023-05-05	INSPEC:22962736		
J	Yang, Jui-An; Kuo, Chung-Hsien										Integrating Vehicle Positioning and Path Tracking Practices for an Autonomous Vehicle Prototype in Campus Environment								ELECTRONICS				10	21					2703			10.3390/electronics10212703							Article	NOV 2021	2021	This paper presents the implementation of an autonomous electric vehicle (EV) project in the National Taiwan University of Science and Technology (NTUST) campus in Taiwan. The aim of this work was to integrate two important practices of realizing an autonomous vehicle in a campus environment, including vehicle positioning and path tracking. Such a project is helpful to the students to learn and practice key technologies of autonomous vehicles conveniently. Therefore, a laboratory-made EV was equipped with real-time kinematic GPS (RTK-GPS) to provide centimeter position accuracy. Furthermore, the model predictive control (MPC) was proposed to perform the path tracking capability. Nevertheless, the RTK-GPS exhibited some robust positioning concerns in practical application, such as a low update rate, signal obstruction, signal drift, and network instability. To solve this problem, a multisensory fusion approach using an unscented Kalman filter (UKF) was utilized to improve the vehicle positioning performance by further considering an inertial measurement unit (IMU) and wheel odometry. On the other hand, the model predictive control (MPC) is usually used to control autonomous EVs. However, the determination of MPC parameters is a challenging task. Hence, reinforcement learning (RL) was utilized to generalize the pre-trained datum value for the determination of MPC parameters in practice. To evaluate the performance of the RL-based MPC, software simulations using MATLAB and a laboratory-made, full-scale electric vehicle were arranged for experiments and validation. In a 199.27 m campus loop path, the estimated travel distance error was 0.82% in terms of UKF. The MPC parameters generated by RL also achieved a better tracking performance with 0.227 m RMSE in path tracking experiments, and they also achieved a better tracking performance when compared to that of human-tuned MPC parameters.									4	0	0	0	0	0	4				2079-9292										Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 106335, TaiwanNatl Taiwan Univ, Dept Mech Engn, Taipei 10617, Taiwan				2021-11-24	WOS:000719116500001		
B	Tianshu Chu; Kalabic, U.										Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoon								2019 IEEE 58th Conference on Decision and Control (CDC)								4079	84				10.1109/CDC40024.2019.9030110							Conference Paper	2019	2019	This paper proposes a model-based deep reinforcement learning (DRL) algorithm for cooperative adaptive cruise control (CACC) of connected vehicles. Differing from most existing CACC works, we consider a platoon consisting of both human-driven and autonomous vehicles. The humandriven vehicles are heterogeneous and connected via vehicleto-vehicle (V2V) communication and the autonomous vehicles are controlled by a cloud-based centralized DRL controller via vehicle-to-cloud (V2C) communication. To overcome the safety and robustness issues of RL, the algorithm informs lowerlevel controllers of desired headway signals instead of directly controlling vehicle accelerations. The lower-level behavior is modeled according to the optimal velocity model (OVM), which determines vehicle acceleration according to a headway input. Numerical experiments show that the model-based DRL algorithm outperforms its model-free version in both safety and stability of CACC. Furthermore, we study the impact of different penetration ratios of autonomous vehicles on the safety, stability, and optimality of the CACC policy.					2019 IEEE 58th Conference on Decision and Control (CDC)2019 IEEE 58th Conference on Decision and Control (CDC)	11-13 Dec. 201911-13 Dec. 2019		Nice, FranceNice, France	10	0	0	0	0	0	10					978-1-7281-1398-2									Stanford Univ., Palo Alto, CA, USAMitsubishi Electr. Res. Labs., Cambridge, MA, USA				2020-04-20	INSPEC:19450519		
B	Hossain, J.; Faridee, A.-Z.; Roy, N.; Basak, A.; Asher, D.E.										CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning								2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)								127	32				10.1109/ACSOS58161.2023.00030							Conference Paper	2023	2023	Autonomous navigation in off-road environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an under-explored area. In this paper, we propose CoverNav, a novel Deep Reinforcement Learning (DRL) based algorithm, for identifying covert and navigable trajectories with minimal cost in off-road terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low-cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL-based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state-of-the-art (SOTA) methods without compromising accuracy.					2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)	20232023	Lassonde School of Engineering; York University; Telus; OntarioTech University; HuaweiLassonde School of Engineering; York University; Telus; OntarioTech University; Huawei	Toronto, ON, CanadaToronto, ON, Canada	0	0	0	0	0	0	0					979-8-3503-3744-0									Dept. of Inf. Syst., Univ. of Maryland, Baltimore County, Baltimore, USADEVCOM Army Res. Lab, USA				2024-01-06	INSPEC:24218605		
J	Prathiba, Sahaya Beni; Raja, Gunasekaran; Kumar, Neeraj				Raja, Gunasekaran/AAD-8076-2022; Prathiba, Sahaya Beni/AAC-8905-2022; Kumar, Neeraj/L-3500-2016	Raja, Gunasekaran/0000-0002-2253-7648; Prathiba, Sahaya Beni/0000-0002-1299-0465; Kumar, Neeraj/0000-0002-3020-3947					Intelligent Cooperative Collision Avoidance at Overtaking and Lane Changing Maneuver in 6G-V2X Communications								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				71	1			112	122				10.1109/TVT.2021.3127219							Article	JAN 2022	2022	The rapid growth in Autonomous Vehicle (AV) technology endeavors increased attention towards road safety in recent days. Particularly, a higher number of road accidents occurs when the AV tries to overtake or change the lane. To cut down the number of accidents and improve traffic reliability, the AV should be capable of making intelligent decisions and communicating those to other AVs. Therefore, in this paper, a Cooperative Collision avoidance scheme for AVs at Overtaking and Lane Changing maneuver (CCAV-OLC) is proposed. The Inverse Reinforcement Learning (IRL) in the CCAV-OLC scheme, processes on the given number of expert demonstrations for automatically acquiring the reward function, and thereby imitating actual human driving strategy and decisions. However, the adaptability of IRL to a high-dimensional AV environment restricts the performance of the CCAV-OLC scheme. To overcome this, the IRL in CCAV-OLC leverages the Gaussian Process (GP) regression model (IRL-GP), which enables data-efficient Bayesian prediction even when the number of demonstrations is very low. After taking intelligent decisions in overtaking and lane changing maneuvers, the AVs cooperatively communicate and exchange the decisions with each other by 6th Generation Vehicle-to-Everything (6G-V2X) communications, which further improves the accuracy and lessens the time taken for making optimal decisions. The experimental results show that the AVs clone the expert's optimal driving strategy and avoid the collisions to a greater extent.									13	0	0	0	0	0	13			0018-9545	1939-9359										Anna Univ, Dept Comp Technol, NGNLab, Chennai 600025, Tamil Nadu, IndiaThapar Inst Engn & Technol, Patiala 147004, Punjab, IndiaKing Abdulaziz Univ, Jeddah, Saudi ArabiaAsia Univ, Dept Comp Sci & Informat Engn, Taichung 40704, TaiwanUniv Petr & Energy Studies, Sch Comp Sci, Dehra Dun 248007, Uttarakhand, India				2022-02-01	WOS:000745533700013		
B	Shoaraee, H.; Liang Chen; Fan Jiang										Decision-Making of an Autonomous Vehicle when Approached by an Emergency Vehicle using Deep Reinforcement Learning								2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)								185	91				10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00041							Conference Paper	2021	2021	Autonomous Vehicles (AVs) are the future of road transportation which can increase safety, efficiency, and productivity. Decision-making of AVs in a highway environment with different goals like overtaking, staying in a lane, and merging have been the focus of many studies. In this study, we want to address a new edge case in autonomous driving when the AV (ego) needs to make the best lateral and longitudinal decisions when approached by an emergency vehicle (emg). To achieve the desired behavior and learn the sequence decision process, we trained ego with the help of Deep Reinforcement Learning (DRL) algorithms and compared the results with rule-based algorithms. We proposed two neural networks as function approximators that help the ego to learn the optimum actions. The driving environment for this problem was developed by using Simulation Urban Mobility (SUMO) as an open-source traffic simulator. We will show our proposed solution based on the DRL outperforming the rule-based solution and demonstrate that it has a decent performance both in normal driving situations and when an emergency vehicle is approaching.					2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)	25-28 Oct. 202125-28 Oct. 2021		Virtual Conference, AB, CanadaVirtual Conference, AB, Canada	0	0	0	0	0	0	0					978-1-6654-2174-4									Dept. of Comput. Sci., Univ. of Northern British Columbia, Prince George, BC, Canada				2022-11-24	INSPEC:22233681		
J	Chen, Jing; Zhao, Cong; Jiang, Shengchuan; Zhang, Xinyuan; Li, Zhongxin; Du, Yuchuan					jiang, shengchuan/0000-0002-7212-7719; Zhao, Cong/0000-0002-1017-9118					Safe, Efficient, and Comfortable Autonomous Driving Based on Cooperative Vehicle Infrastructure System								INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH				20	1					893			10.3390/ijerph20010893							Article	JAN 2023	2023	Traffic crashes, heavy congestion, and discomfort often occur on rough pavements due to human drivers' imperfect decision-making for vehicle control. Autonomous vehicles (AVs) will flood onto urban roads to replace human drivers and improve driving performance in the near future. With the development of the cooperative vehicle infrastructure system (CVIS), multi-source road and traffic information can be collected by onboard or roadside sensors and integrated into a cloud. The information is updated and used for decision-making in real-time. This study proposes an intelligent speed control approach for AVs in CVISs using deep reinforcement learning (DRL) to improve safety, efficiency, and ride comfort. First, the irregular and fluctuating road profiles of rough pavements are represented by maximum comfortable speeds on segments via vertical comfort evaluation. A DRL-based speed control model is then designed to learn safe, efficient, and comfortable car-following behavior based on road and traffic information. Specifically, the model is trained and tested in a stochastic environment using data sampled from 1341 car-following events collected in California and 110 rough pavements detected in Shanghai. The experimental results show that the DRL-based speed control model can improve computational efficiency, driving efficiency, longitudinal comfort, and vertical comfort in cars by 93.47%, 26.99%, 58.33%, and 6.05%, respectively, compared to a model predictive control-based adaptive cruise control. The results indicate that the proposed intelligent speed control approach for AVs is effective on rough pavements and has excellent potential for practical application.									9	0	0	0	0	0	9				1660-4601										Tongji Univ, Key Lab Rd & Traff Engn, Minist Educ, Shanghai 201804, Peoples R ChinaShanghai Utopilot Technol Co Ltd, Shanghai 201306, Peoples R China	Shanghai Utopilot Technol Co Ltd			2023-01-22	WOS:000909110900001	36613215	
J	Kunming Li; Mao Shan; Narula, K.; Worrall, S.; Nebot, E.										Socially Aware Crowd Navigation with Multimodal Pedestrian Trajectory Prediction for Autonomous Vehicles [arXiv]								arXiv								8 pp.	8 pp.											Journal Paper	22 Nov. 2020	2020	Seamlessly operating an autonomous vehicle in a crowded pedestrian environment is a very challenging task. This is because human movement and interactions are very hard to predict in such environments. Recent work has demonstrated that reinforcement learning-based methods have the ability to learn to drive in crowds. However, these methods can have very poor performance due to inaccurate predictions of the pedestrians' future state as human motion prediction has a large variance. To overcome this problem, we propose a new method, SARL-SGAN-KCE, that combines a deep socially aware attentive value network with a human multimodal trajectory prediction model to help identify the optimal driving policy. We also introduce a novel technique to extend the discrete action space with minimal additional computational requirements. The kinematic constraints of the vehicle are also considered to ensure smooth and safe trajectories. We evaluate our method against the state of art methods for crowd navigation and provide an ablation study to show that our method is safer and closer to human behaviour.									0	0	0	0	0	0	0														Australian Centre for Field Robot., Univ. of Sydney, Sydney, NSW, Australia				2020-11-22	INSPEC:20342929		
C	Sama, Kyle; Morales, Yoichi; Akai, Naoki; Takeuchi, Eijiro; Takeda, Kazuya			IEEE							Learning How to Drive in Blind Intersections from Human Data								2018 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC)		IEEE International Conference on Systems Man and Cybernetics Conference Proceedings						317	324				10.1109/SMC.2018.00064							Proceedings Paper	2018	2018	In this paper we present a method to learn how to drive in different types of blind intersections using expert driving data. We cluster different intersections based on the velocity of how drivers approach them, and train a linear SVM classifier for each class of intersection. Through clustering we found that there were three different classes of intersections in typical residential areas in Japan. We used inverse reinforcement learning (IRL) to build a driving model for each type of intersection. The models were trained from 308 trajectories traversed by 5 different drivers. The models and policies were implemented and evaluated in a ROS simulator where the agent is provided a global path, and upon it reaching an intersection, it selects the appropriate trained policy. By doing this, the simulated autonomous vehicle can perform proactive safe driving behaviors when approaching blind intersections.					IEEE International Conference on Systems, Man, and Cybernetics (SMC)IEEE International Conference on Systems, Man, and Cybernetics (SMC)	OCT 07-10, 2018OCT 07-10, 2018	IEEE; Sci Council JapanIEEE; Sci Council Japan	IEEE Syst Man & Cybernet Soc, Miyazaki, JAPANIEEE Syst Man & Cybernet Soc, Miyazaki, JAPAN	3	0	0	0	0	0	3			1062-922X		978-1-5386-6650-0									Nagoya Univ, Inst Innovat Future Soc, Nagoya, Aichi 4648603, JapanNagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi 4638603, JapanNagoya Univ, Grad Sch Informat Sci, Takeda Lab, Nagoya, Aichi 4638603, Japan				2019-03-15	WOS:000459884800054		
J	Abeywickrama, Dhaminda B.; Griffiths, Nathan; Xu, Zhou; Mouzakitis, Alex					Griffiths, Nathan/0000-0002-6406-8632; Abeywickrama, Dhaminda/0000-0002-4423-0284					Emergence of norms in interactions with complex rewards								AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS				37	1					2			10.1007/s10458-022-09585-3							Article	JUN 2023	2023	Autonomous agents are becoming increasingly ubiquitous and are playing an increasing role in wide range of safety-critical systems, such as driverless cars, exploration robots and unmanned aerial vehicles. These agents operate in highly dynamic and heterogeneous environments, resulting in complex behaviour and interactions. Therefore, the need arises to model and understand more complex and nuanced agent interactions than have previously been studied. In this paper, we propose a novel agent-based modelling approach to investigating norm emergence, in which such interactions can be investigated. To this end, while there may be an ideal set of optimally compatible actions there are also combinations that have positive rewards and are also compatible. Our approach provides a step towards identifying the conditions under which globally compatible norms are likely to emerge in the context of complex rewards. Our model is illustrated using the motivating example of self-driving cars, and we present the scenario of an autonomous vehicle performing a left-turn at a T-intersection.									0	0	0	0	0	0	0			1387-2532	1573-7454										Univ Bristol, Dept Comp Sci, Trustworthy Syst Lab, Bristol, Avon, EnglandUniv West England, Bristol Robot Lab, Bristol, Avon, EnglandUniv Warwick, Dept Comp Sci, Coventry, W Midlands, EnglandJaguar Land Rover, Coventry, W Midlands, England				2022-11-07	WOS:000873863200001		
B	G, A.; Hemalatha, R.; N, S.; Kavitha, K.R.; Tamilselvi, M.; Kumar, A.K.				R, Hemalatha/AAU-8572-2021						MNNP: Design and Development of Traffic Sign Identification and Recognition System to Support Smart Vehicles using Modified Neural Network Principles								2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)								1	7				10.1109/RMKMATE59243.2023.10369847							Conference Paper	2023	2023	In recent times, the acceleration of automotive and computer vision technologies has sparked significant interest in autonomous vehicles. These vehicles' capacity to operate securely and effectively hinges on their adeptness at accurately identifying traffic signs. Consequently, traffic sign recognition has emerged as a pivotal element within autonomous driving systems. Scientists have been hard at work looking at various approaches to accomplishing this recognition, mostly drawing on machine learning and deep learning techniques. To address this issue, we offer a Modified Neural Network approach that combines the Haar cascade algorithm with a MobileNet model classifier. Our developed model is rigorously trained on the GTSRB dataset and then put to the test on a wide variety of traffic sign types. The culmination of our efforts yields a remarkable testing accuracy rate of 97.25%. This finding highlights the possible effectiveness of our technique in improving autonomously vehicle traffic sign identification, which in turn helps to improve the security and effectiveness of autonomous driving systems.					2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)	20232023		Chennai, IndiaChennai, India	0	0	0	0	0	0	0					979-8-3503-0570-8									Dept. of Electron. & Commun. Eng., SIMATSDept. of Comput. Sci. & Eng., St. Joseph's Coll. of Eng.Dept. of EEE, S. A. Eng. Coll., Chennai, IndiaDept. of ECE, Sona Coll. of Technol., SalemDept. of ECE, SRM Inst. of Sci. & Technol., Chennai, IndiaDept. of ECE, Mohan Babu Univ., India				2024-01-25	INSPEC:24350150		
J	Mosharafian, Sahand; Afzali, Shirin; Mohammadpour Velni, Javad					Mohammadpour Velni, Javad/0000-0001-8546-221X					Leveraging autonomous vehicles in mixed-autonomy traffic networks with reinforcement learning-controlled intersections								TRANSPORTATION LETTERS-THE INTERNATIONAL JOURNAL OF TRANSPORTATION RESEARCH				15	9			1218	1229				10.1080/19427867.2022.2146302					NOV 2022		Article; Early Access		2023	Development of new approaches to adaptive traffic signal control has received significant attention; an example is the reinforcement learning (RL), where training and implementation of an RL agent can allow adaptive signal control in real time, considering the agent's past experiences. Furthermore, autonomous vehicle (AV) technology has shown promise to enhancing the traffic mobility at highways and intersections. In this paper, delayed action deep Q-learning is developed for a vehicle network with signalized intersections to control the signal phase. A model predictive control (MPC) scheme is proposed to allow AVs to adapt their speed. Several case studies that consider mixed autonomy are examined aiming at reducing network traffic and fuel consumption in the traffic network with multiple intersections. Simulation studies reveal that even with a few AVs in the network, the waiting time, fuel consumption, and the number of stop-and-go movements are significantly reduced, while the travel time is increased.									1	0	0	0	0	0	1			1942-7867	1942-7875										Univ Georgia, Sch Elect & Comp Engn, Athens, GA 30602 USA				2022-12-08	WOS:000891489800001		
B	Zhang, Y.; Zhang, S.; Liang, Z.; Li, H.L.; Wu, H.; Liu, Q.										Dynamical Driving Interactions between Human and Mentalizing-designed Autonomous Vehicle								2022 IEEE International Conference on Development and Learning (ICDL)								178	83				10.1109/ICDL53763.2022.9962198							Conference Paper	2022	2022	Autonomous vehicle (AV) is progressing rapidly, but there are still many shortcomings when interacting with humans. To address this problem, it is necessary to study the human behaviors in human-AV interactions, and build a predictive model of human decision-making in the interaction. In turn, modelling human behavior in human-AV interaction can help us better understand human perception of AVs and human driving strategies. In this work, we first train multi-level AV agents using reinforcement learning (RL) models to imitate three mentalizing levels (i.e., level-0, level-1, and level-2), and then design a human-AV driving task that subjects interact with each level of AV agents in a two-lane merging scenario. Both human and AV driving behaviors are recorded. We found that conservative subjects obtain more rewards because of the randomness of the RL agents. Our results indicate that (i) human driving strategies are flexible and changeable, which allows to quickly adjust the strategy to maximize the reward when gaming against AV; (ii) human driving strategies are related to mentalizing ability, and subjects with higher mentalizing scores drive more conservatively. Our study shed lights on the relationship between human driving policy and mentalizing in human-AV interactions, and it can inspire the next-generation AV.					2022 IEEE International Conference on Development and Learning (ICDL)2022 IEEE International Conference on Development and Learning (ICDL)	12-15 Sept. 202212-15 Sept. 2022		London, UKLondon, UK	0	0	0	0	0	0	0					978-1-6654-1311-4									Dept. of Biomed. Eng., SUSTech, Shenzhen, ChinaDept. of Psychol., Univ. of Macau, Macau, China				2023-03-17	INSPEC:22332320		
C	Soni, Himanshu; Gupta, Vishu; Kumar, Rajesh			IEEE	Kumar, Rajesh/G-1408-2014	Kumar, Rajesh/0000-0002-6019-0702; Soni, Himanshu/0000-0003-3168-3011					Motion Planning using Reinforcement Learning for Electric Vehicle Battery Optimization(EVBO)								2019 INTERNATIONAL CONFERENCE ON POWER ELECTRONICS, CONTROL AND AUTOMATION (ICPECA-2019)								11	16				10.1109/icpeca47973.2019.8975684							Proceedings Paper	2019	2019	The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning. Motion planning is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour. In this paper, reinforcement learning techniques are explored for the motion planning of an electric vehicle(EV) while optimizing battery consumption. The EV travel time has also been evaluated under different reinforcement learning schemes. A traffic simulation network is developed for a high-trunk zone of Jaipur city using Simulation for Urban Mobility(SUMO) software. Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network. The results show that value iteration and q-learning have shown improved battery consumption. However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.					International Conference on Power Electronics, Control and Automation (ICPECA)International Conference on Power Electronics, Control and Automation (ICPECA)	NOV 16-17, 2019NOV 16-17, 2019	IEEE Delhi Sect; IEEE Ind Applicat SocIEEE Delhi Sect; IEEE Ind Applicat Soc	New Delhi, INDIANew Delhi, INDIA	0	0	0	0	0	0	0					978-1-7281-3958-6									Univ Rajasthan, Ctr Converging Technol, Jaipur, Rajasthan, IndiaMalaviya Natl Inst Technol, Dept Elect Engn, Jaipur, Rajasthan, India				2020-06-26	WOS:000540004400003		
B	Yujin Kim; Dong-Sung Pae; Sun-Ho Jang; Seong-Woo Kang; Myo-Taeg Lim										Reinforcement Learning for Autonomous Vehicle using MPC in Highway Situation								2022 International Conference on Electronics, Information, and Communication (ICEIC)								4 pp.	4 pp.				10.1109/ICEIC54506.2022.9748810							Conference Paper	2022	2022	Path planning for Autonomous Vehicle(AV) is a challenging problem, as the vehicle is required to obey the traffic rules while avoiding the collision with the other vehicles. Model Predictive Control(MPC) is one of the popular approach for proposing a feasible and stable path by reflecting vehicle dynamics in solving objective function and constraining the expected future control input. However, one of the drawbacks with this approach is that the demanded computational power increases proportionally to the number of considered future inputs. This paper presents a path planning algorithm using Reinforcement Learning(RL). RL is similar to MPC in finding the optimal solution that maximizes the reward function which can be seen as intrinsic objective function. In that respect, adequate employment of MPC path in training resulted in improved efficiency and performance. Through the simulations, proposed method showed 98% of similarity with path of MPC and reduced computation time by 91.13% on average, thus it is qualified for real-time path planning.					2022 International Conference on Electronics, Information, and Communication (ICEIC)2022 International Conference on Electronics, Information, and Communication (ICEIC)	6-9 Feb. 20226-9 Feb. 2022		Jeju, South KoreaJeju, South Korea	0	0	0	0	0	0	0					978-1-6654-0934-6									Dept. of Electr. Eng., Korea Univ., Seoul, South KoreaDept. of Software, Sangmyung Univ., Cheonan, South Korea				2022-11-01	INSPEC:21687031		
C	Salvato, Erica; Ghosh, Arnob; Fenu, Gianfranco; Parisini, Thomas			IEEE	Ghosh, Arnob/JZE-0748-2024; Salvato, Erica/GRE-6630-2022	Salvato, Erica/0000-0003-3815-6652					Control of a Mixed Autonomy Signalised Urban Intersection: An Action-Delayed Reinforcement Learning Approach								2021 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						2042	2047				10.1109/ITSC48978.2021.9564983							Proceedings Paper	2021	2021	We consider a mixed autonomy scenario where the traffic intersection controller decides whether the traffic light will be green or red at each lane for multiple traffic-light blocks. The objective of the traffic intersection controller is to minimize the queue length at each lane and maximize the outflow of vehicles over each block. We consider that the traffic intersection controller informs the autonomous vehicle (AV) whether the traffic light will be green or red for the future traffic-light block. Thus, the AV can adapt its dynamics by solving an optimal control problem. We model the decision process of the traffic intersection controller as a deterministic delayed Markov decision process owing to the delayed action by the traffic controller. We propose Reinforcement Learning based model-free algorithm to obtain the optimal policy. We show - by extensive simulations - that our algorithm converges and drastically reduces the energy costs of AVs as the traffic controller communicates with the AVs.					IEEE Intelligent Transportation Systems Conference (ITSC)IEEE Intelligent Transportation Systems Conference (ITSC)	SEP 19-22, 2021SEP 19-22, 2021	IEEEIEEE	Indianapolis, INIndianapolis, IN	1	0	0	0	0	0	1			2153-0009		978-1-7281-9142-3									Univ Trieste, Dept Engn & Architecture, Trieste, ItalyImperial Coll London, Dept Elect & Elect Engn Dept, London, EnglandImperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, EnglandUniv Cyprus, KIOS Res & Innovat Ctr Excellence, CY-1678 Nicosia, Cyprus				2022-09-24	WOS:000841862502008		
B	Siddique, U.; Sinha, A.; Cao, Y.										On Deep Reinforcement Learning for Target Capture Autonomous Guidance								AIAA SCITECH 2024 Forum								0957	0957				10.2514/6.2024-0957							Conference Paper	2024	2024	This paper explores the prospects of motion planning of autonomous vehicles using deep reinforcement learning (DRL). We are particularly interested in a goal-directed setting where the need is to design an optimal guidance strategy for a pursuing autonomous vehicle (the pursuer, which is also the DRL agent) to capture an adversary (the target). To this end, we first formulate the target capture guidance problem as a Markov Decision Process (MDP) wherein the kinematics of relative motion between the vehicles constitute the MDP, and the pursuer's lateral acceleration (chosen as its steering control to account for turn constraints) is the action of DRL agent. We show that a multifaceted reward function motivated by the collision conditions is sufficient and effective in designing the reinforcement learning action that enables the pursuer to capture the target regardless of the latter's motion. We then empirically evaluate the performance of the trained agent in various target capture scenarios.					AIAA SCITECH 2024 ForumAIAA SCITECH 2024 Forum	20242024	Jacobs; Caltech; Ball; Ennova Technologies; FAMU-FSU College of Engineering; Los Alamos National Laboratory; TXT PACE; Bastion TechnologiesJacobs; Caltech; Ball; Ennova Technologies; FAMU-FSU College of Engineering; Los Alamos National Laboratory; TXT PACE; Bastion Technologies	Orlando, FL, USAOrlando, FL, USA	0	0	0	0	0	0	0					978-1-62410-711-5									Univ. of Texas at San Antonio, San Antonio, TX, USA				2024-02-23	INSPEC:24573396		
B	Auxilius jude, M.J.; Diniesh, V.C.; K, P.K.; S, R.; S, N.K.; E n, S.										An Improved Retransmission Timeout Forecasting Algorithm for Vehicular Networks								2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT).								5 pp.	5 pp.				10.1109/ICAECT54875.2022.9807985							Conference Paper	2022	2022	The vehicular relay network remains a promising path of future intelligent transportation systems (ITS) that connects the autonomous vehicle with the internet for better traffic control and information sharing. The transmission control protocol (TCP) remains the backbone of data traffic on the internet as it harbors an enormous portion of global internet traffic between end devices. In VANET, TCP experiences drastic performance degradation during early or spurious RTO timeouts. This paper proposes a recursive learning retransmission timeout (RL-RTO), which efficiently reduces spurious timeouts and enhances fast recovery after timeouts. The RL-RTO forecast timer value is based on the three control parameters. The performance of RL-RTO is validated against recent RTO approaches under multi-hop vehicular environments. The proposed RL-RTO considerably regulates estimation errors and improves variance.					2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)	21-22 April 202221-22 April 2022		Bhilai, IndiaBhilai, India	0	0	0	0	0	0	0					978-1-6654-1120-2									Dept. of ECE, Kongu Eng. Coll., Perundurai, India				2022-11-01	INSPEC:21845507		
J	Kamel, Ahmed; Sayed, Tarek; Fu, Chuanyun				Sayed, Tarek/ABH-1224-2020	Sayed, Tarek/0000-0001-8797-0541; Kamel, Ahmed/0000-0003-1407-1961					Real-time safety analysis using autonomous vehicle data: a Bayesian hierarchical extreme value model								TRANSPORTMETRICA B-TRANSPORT DYNAMICS				11	1			826	846				10.1080/21680566.2022.2135634					OCT 2022		Article; Early Access		2023	This study proposes an approach for real-time road network safety analysis using autonomous vehicles (AVs) generated data. The approach utilises a Bayesian hierarchical spatial random parameter extreme value model (BHSRP). The model simultaneously addresses the scarcity and non-stationarity of conflict extremes and unobserved spatial heterogeneity. Two real-time safety metrics are estimated: the risk of crash (RC) and return level (RL). The RC and RL were applied to three months AVs data for evaluating the real-time safety level of an urban corridor in Palo Alto, California. The indicator time to collision (TTC) was used to characterise traffic conflicts. The conflict extreme was defined as the maxima of negated TTC in a 20-min interval (block). The results show that RC can differentiate the block-level risk level, while RL can reflect safety levels among blocks. For the RC, the hot (crash risk prone) segments and intersections are associated with more severe conflict frequency.									6	0	0	0	0	0	6			2168-0566	2168-0582										Univ British Columbia, Dept Civil Engn, 6250 Appl Sci Ln, Vancouver, BC V6T 1Z4, CanadaHarbin Inst Technol, Sch Transportat Sci & Engn, Harbin, Peoples R China				2022-10-28	WOS:000870588200001		
B	Wan, A.D.M.; Braspenning, P.J.						Soliman, J.I.; Roller, D.				Computational design principles for multiple autonomous vehicle organization								28th International Symposium on Automotive Technology and Automation. Proceedings for the Dedicated Conference on Robotics, Motion and Machine Vision in the Automotive Industries								383	94											Conference Paper	1995	1995	In distributed artificial intelligence (AI), a bifurcation in design principles can be observed. On the one hand there seems to be a resurgence of the `classical' AI approach of bestowing agents with human-like intelligent capacities and on the other hand there is an anti-representational school emerging. Classical AI has had to endure severe criticisms while anti-representational approaches fall short on providing a programme for developing and implementing higher-order intelligent functions. The approach taken in this paper is a middle-out strategy between these extremes and describes the associated principles of autonomous agent design including rigorous adaptive capacities based on reinforcement learning and possibilities of integration with higher-order intelligent functions. An outline of an architecture is presented that enables multiple autonomous vehicles (mobile agents), if equipped with it, to communicate and to coordinate thereby achieving a synthesis between the logico-linguistic and the reactive approaches.					Proceedings of Conference on Robotics, Motion and Machine Vision in the Automotive IndustriesProceedings of Conference on Robotics, Motion and Machine Vision in the Automotive Industries	18-22 Sept. 199518-22 Sept. 1995		Stuttgart, GermanyStuttgart, Germany	0	0	0	0	0	0	0					0-947719-73-3									Maastricht Univ., Maastricht, Netherlands				1995-01-01	INSPEC:5779853		
J	Ottoni, Andre L. C.; Nepomuceno, Erivelton G.; Oliveira, Marcos S. de; Oliveira, Daniela C. R. de				Santos de Oliveira, Marcos/JQQ-8403-2023; Ottoni, André Luiz Carvalho/HNR-9085-2023; Nepomuceno, Erivelton/F-4042-2016	Santos de Oliveira, Marcos/0000-0003-4395-4640; Ottoni, André Luiz Carvalho/0000-0003-2136-9870; Nepomuceno, Erivelton/0000-0002-5841-2193; Ramires de Oliveira, Daniela Carine/0000-0002-9573-8424					Reinforcement learning for the traveling salesman problem with refueling								COMPLEX & INTELLIGENT SYSTEMS				8	3	SI		2001	2015				10.1007/s40747-021-00444-4					JUN 2021		Article; Early Access		2022	The traveling salesman problem (TSP) is one of the best-known combinatorial optimization problems. Many methods derived from TSP have been applied to study autonomous vehicle route planning with fuel constraints. Nevertheless, less attention has been paid to reinforcement learning (RL) as a potential method to solve refueling problems. This paper employs RL to solve the traveling salesman problem With refueling (TSPWR). The technique proposes a model (actions, states, reinforcements) and RL-TSPWR algorithm. Focus is given on the analysis of RL parameters and on the refueling influence in route learning optimization of fuel cost. Two RL algorithms: Q-learning and SARSA are compared. In addition, RL parameter estimation is performed by Response Surface Methodology, Analysis of Variance and Tukey Test. The proposed method achieves the best solution in 15 out of 16 case studies.									10	0	0	0	1	0	10			2199-4536	2198-6053										Fed Univ Reconcavo Bahia UFRB, Technol & Exact Ctr, Cruz Das Almas, BrazilFed Univ Sao Joao Del Rei UFSJ, Dept Elect Engn, Control & Modelling Grp GCOM, Sao Joao Del Rei, BrazilFed Univ Sao Joao Del Rei UFSJ, Dept Math & Stat, Sao Joao Del Rei, Brazil	Fed Univ Reconcavo Bahia UFRB			2021-07-05	WOS:000662109300004		
J	Razzaq, Waleed; Hongwei, Mo										Neural Circuit Policies Imposing Visual Perceptual Autonomy								NEURAL PROCESSING LETTERS				55	7			9101	9116				10.1007/s11063-023-11194-4					FEB 2023		Article; Early Access		2023	We presented a hybrid deep learning architecture by combining biological brain-inspired Neural Circuit Policies with convolutional neural architecture. This allows the agent to learn key coherent features from various environments, express generalizability and interpret dynamics. We found that a combination of 12 inter-neurons and 8 command neurons can effectively perform high-stakes decision-making tasks such as autonomous vehicles. Our architecture not only has 305 times fewer trainable parameters than competing approaches but shows superior performance, robustness, and noise resiliency in famous autonomous vehicle simulations i.e. OpenAI-CarRacing and Udacity Simulator.									0	0	0	0	0	0	0			1370-4621	1573-773X										Harbin Engn Univ, Intelligent Syst Sci & Engn, Harbin 150001, Heilongjiang, Peoples R China				2023-03-24	WOS:000937563900001		
J	Kim, Hyunkun; Pyeon, Hyeongoo; Park, Jong Sool; Hwang, Jin Young; Lim, Sejoon					-Pyeon, Hyeongoo/0000-0003-1184-341X					Autonomous Vehicle Fuel Economy Optimization with Deep Reinforcement Learning								ELECTRONICS				9	11					1911			10.3390/electronics9111911							Article	NOV 2020	2020	The ever-increasing number of vehicles on the road puts pressure on car manufacturers to make their car fuel-efficient. With autonomous vehicles, we can find new strategies to optimize fuels. We propose a reinforcement learning algorithm that trains deep neural networks to generate a fuel-efficient velocity profile for autonomous vehicles given road altitude information for the planned trip. Using a highly accurate industry-accepted fuel economy simulation program, we train our deep neural network model. We developed a technique for adapting the heterogeneous simulation program on top of an open-source deep learning framework, and reduced dimension of the problem output with suitable parameterization to train the neural network much faster. The learned model combined with reinforcement learning-based strategy generation effectively generated the velocity profile so that autonomous vehicles can follow to control itself in a fuel efficient way. We evaluate our algorithm's performance using the fuel economy simulation program for various altitude profiles. We also demonstrate that our method can teach neural networks to generate useful strategies to increase fuel economy even on unseen roads. Our method improved fuel economy by 8% compared to a simple grid search approach.									5	0	0	0	0	0	5				2079-9292										Kookmin Univ, Grad Sch Automot Engn, Seoul 02707, South KoreaHyundai Motor Grp, Transmiss Res Lab, Hwaseong 18280, Gyeonggi Do, South KoreaKookmin Univ, Dept Automobile & IT Convergence, Seoul 02707, South Korea				2020-12-10	WOS:000592899600001		
C	Hester, Todd; Quinlan, Michael; Stone, Peter			IEEE							RTMBA: A Real-Time Model-Based Reinforcement Learning Architecture for Robot Control								2012 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						85	90											Proceedings Paper	2012	2012	Reinforcement Learning (RL) is a paradigm for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. Existing model-based RL methods learn in relatively few samples, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes in a novel way such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 14-18, 2012MAY 14-18, 2012	IEEEIEEE	St Paul, MNSt Paul, MN	26	0	0	0	0	0	29			1050-4729	2577-087X	978-1-4673-1405-3									Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA				2012-11-19	WOS:000309406700013		
J	Zhiqian Qiao; Schneider, J.; Dolan, J.M.										Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning [arXiv]								arXiv								7 pp.	7 pp.											Journal Paper	9 Nov. 2020	2020	For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but for some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.									0	0	0	0	0	0	0														Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USARobot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA				2021-02-26	INSPEC:20319267		
C	Chen, Wei-Lun; Lee, Kwan-Hung; Hsiung, Pao-Ann			IEEE							Intersection Crossing for Autonomous Vehicles based on Deep Reinforcement Learning								2019 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN (ICCE-TW)		IEEE International Conference on Consumer Electronics-Taiwan											10.1109/icce-tw46550.2019.8991738							Proceedings Paper	2019	2019	Future intersection crossings for autonomous vehicles will not be controlled by traffic signals, rather a controller will be used for communication among vehicles that need to cross an intersection. In this work, we propose an innovative management system called Deep Reinforcement Learning-based Autonomous Intersection Management (DRLAIM) system, which is the first system to use deep reinforcement learning. We train the system to learn a good intersection control policy by interacting with traffic environment through reinforcement learning. The brake-safe control model is used to ensure the safety of each autonomous vehicle while crossing. Experiment results show that after training using reinforcement learning, the throughput of intersection control model increased by 83%. In comparison with the Fast First Service (FFS) policy, the average waiting time of DRLAIM reduced by about 1.2% to 11.4%.					IEEE International Conference on Consumer Electronics-Taiwan (IEEE ICCE-TW)IEEE International Conference on Consumer Electronics-Taiwan (IEEE ICCE-TW)	MAY 20-22, 2019MAY 20-22, 2019	IEEEIEEE	Ilan, TAIWANIlan, TAIWAN	3	0	0	0	0	0	3			2381-5779		978-1-7281-3279-2									Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi, Taiwan				2019-01-01	WOS:000712323500056		
J	Wang, Kezhi; Wang, Liang; Pan, Cunhua; Ren, Hong				Wang, Kezhi/AAM-5657-2020; 任, 红/IUM-5764-2023; Pan, Cunhua/AAM-5342-2020	Wang, Kezhi/0000-0001-8602-0800; 任, 红/0000-0002-3477-1087; Pan, Cunhua/0000-0001-5286-7958					Deep Reinforcement Learning-Based Resource Management for Flexible Mobile Edge Computing Architectures, Applications, and Research Issues								IEEE VEHICULAR TECHNOLOGY MAGAZINE				17	2			85	93				10.1109/MVT.2022.3156745					APR 2022		Article; Early Access		2022										3	0	0	0	0	0	3			1556-6072	1556-6080										Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, EnglandCranfield Univ, Sch Aerosp Transport & Mfg, Cranfield MK43 0AL, Beds, EnglandSoutheast Univ, Natl Mobile Commun Res Lab, Nanjing 211189, Peoples R China				2022-05-03	WOS:000785743000001		
C	Ibn-Khedher, Hatem; Laroui, Mohammed; Ben Mabrouk, Mouna; Moungla, Hassine; Afifi, Hossam; Oleari, Alberto Nai; Kamal, Ahmed E.			IEEE							Edge Computing Assisted Autonomous Driving Using Artificial Intelligence								IWCMC 2021: 2021 17TH INTERNATIONAL WIRELESS COMMUNICATIONS & MOBILE COMPUTING CONFERENCE (IWCMC)		International Wireless Communications and Mobile Computing Conference						254	259				10.1109/IWCMC51323.2021.9498627							Proceedings Paper	2021	2021	The emergence of new vehicles generation such as connected and autonomous vehicles led to new challenges in the vehicular networking and computing managements to provide efficient services and guarantee the quality of service. The edge computing facility allows the decentralization of processing from the cloud to the edge of the network. In this paper, we design and propose an end-to-end, reliable and low latency communication architecture that allows the allocation of compute-intensive autonomous driving services, in particular autopilot, to shared resources on edge computing servers and improve the level of performance for autonomous vehicles. The reference architecture is used to design an Advanced Autonomous Driving (AAD) communication protocol between autonomous vehicles, edge computing servers, and the centralized cloud. Then, a mathematical programming approach using Integer Linear Programming (ILP) is formulated to model the autopilot chain resources Offloading at the network edge. Further, a deep reinforcement learning (DRL) approach is proposed to deal with dense Internet of Autonomous Vehicle (IoAV) networks. Moreover, several scenarios are considered to quantify the behavior of the optimization approaches. We compare their efficiency in terms of Total Edge Servers Utilization, Total Edge Servers Allocation Time, and Successfully Allocated Edge Autopilots.					17th IEEE International Wireless Communications and Mobile Computing Conference (IEEE IWCMC)17th IEEE International Wireless Communications and Mobile Computing Conference (IEEE IWCMC)	JUN 28-JUL 02, 2021JUN 28-JUL 02, 2021	IEEE; IEEE Harbin Sect; IEEE Commun Soc Harbin Chapter; HuaweiIEEE; IEEE Harbin Sect; IEEE Commun Soc Harbin Chapter; Huawei	ELECTR NETWORKELECTR NETWORK	4	0	0	0	0	0	5			2376-6492		978-1-7281-8616-0									ALTRAN Labs, F-78140 Velizy Villacoublay, FranceUniv Paris, LIPADE, F-75006 Paris, FranceInst Polytech Paris, Telecom SudParis Saclay, CNRS, UMR 5157, Paris, FranceIowa State Univ, Dept Elect Comp Engn, Ames, IA 50011 USA	ALTRAN Labs			2021-11-03	WOS:000707024100044		
B	Nian, G.; Xiao, J.; Zhou, X.										Dueling DQN-Rollout for Collision Avoidance Path Planning with Vehicle Speed Location								2023 3rd International Symposium on Computer Technology and Information Science (ISCTIS)								552	7				10.1109/ISCTIS58954.2023.10213163							Conference Paper	2023	2023	The rapid progress of artificial intelligence has led to significant advancements in the field of autonomous driving, yet effective collision avoidance path planning remains a challenging task. In response, deep reinforcement learning offers an efficient and modern alternative to traditional navigation strategies. This paper proposes a novel approach that incorporates vehicle speed location into the deep reinforcement learning process, utilizing the Dueling DQN-Rollout framework to consider both the distance of the road and obstacles ahead. The agent interacts with the environment to learn a policy, with a reward function that accounts for deviations from the intended path and collisions with obstacles. The training process focuses on imparting human-like driving skills to the autonomous vehicle. By employing the rollout algorithm, the rough Q-value is optimized to reduce training costs. Experimental results demonstrate that this approach can successfully plan a collision-free path for autonomous driving from origin to destination on a simulation platform.					2023 3rd International Symposium on Computer Technology and Information Science (ISCTIS)2023 3rd International Symposium on Computer Technology and Information Science (ISCTIS)	20232023	IEEEIEEE	Chengdu, ChinaChengdu, China	0	0	0	0	0	0	0					979-8-3503-2538-6									Sch. of Comput. Sci. & Eng. Southwest Minzu Univ., Chengdu, China				2023-09-01	INSPEC:23581719		
B	Yang, Q.; Li, G.; Li, G.; Liu, Y.										Trajectory Tracking Control of Autonomous Vehicles Based on Reinforcement Learning and Curvature Feedforward								2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	6				10.1109/CVCI56766.2022.9964583							Conference Paper	2022	2022	This paper proposes a trajectory tracking algorithm with adaptive parameter PID (Proportional-Integral-Derivative) controller based on reinforcement learning and curvature feedforward controller. This paper uses the deep reinforcement learning PPO algorithm (Proximal Policy Optimization) to adjust the parameters of the PID controller online, which is used for the lateral control of the autonomous vehicle. In addition, the maximum policy entropy is introduced based on PPO algorithm to enhance the exploration of policy. And a curvature feedforward controller is added, which will limit the speed according to the curvature of the road ahead, to avoid the problem that the vehicle is difficult to control when turning due to excessive speed. The training results show that compared with the traditional PPO algorithm, this method can get more rewards in the later stage of training. The simulation results show that this algorithm can reduce the average track error, reduce the overshoot and oscillation, and improve the tracking accuracy and robustness.					2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)	28-30 Oct. 202228-30 Oct. 2022	IEEE; Chinese Journal Mechanical Engineering; China Journal Highway and TransportIEEE; Chinese Journal Mechanical Engineering; China Journal Highway and Transport	Nanjing, ChinaNanjing, China	1	0	0	0	0	0	1					978-1-6654-5374-5									Sch. of Automobile & Traffic Eng., Liaoning Univ. of Technol., Jinzhou, China				2023-03-22	INSPEC:22361503		
C	Xu, Zhi; Liu, Shuncheng; Wu, Ziniu; Chen, Xu; Zeng, Kai; Zheng, Kai; Su, Han			ACM							PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning								PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021								2271	2280				10.1145/3459637.3482283							Proceedings Paper	2021	2021	The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution. It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks. The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e. harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane. This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles. Specifically, we propose a velocity control framework, called PATROL (sPAtial-Temporal ReinfOrcement Learning). First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g. velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment. Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time. At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB. We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments. Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.					30th ACM International Conference on Information and Knowledge Management (CIKM)30th ACM International Conference on Information and Knowledge Management (CIKM)	NOV 01-05, 2021NOV 01-05, 2021	Assoc Comp Machinery; ACM Special Interest Grp Informat Retrieval; ACM SIGWEBAssoc Comp Machinery; ACM Special Interest Grp Informat Retrieval; ACM SIGWEB	Univ Queensland, ELECTR NETWORKUniv Queensland, ELECTR NETWORK	3	0	0	0	0	0	3					978-1-4503-8446-9									Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R ChinaMIT, Dept Elect Engn & Comp Sci, Boston, MA USAAlibaba Grp, Hangzhou, Peoples R ChinaUniv Elect Sci & Technol China, Yangtze Delta Reg Inst, Quzhou, Peoples R China				2021-01-01	WOS:001054156202032		
C	Gonzalez-Miranda, Oscar; Miranda, Luis Antonio Lopez; Ibarra-Zannatha, Juan Manuel			IEEE							Q-Learning for autonomous vehicle navigation								2023 XXV ROBOTICS MEXICAN CONGRESS, COMROB								138	142				10.1109/COMROB60035.2023.10349747							Proceedings Paper	2023	2023	In this work, we proposed and developed a reinforcement Q-learning method to do the lane-keeping and obstacle evasion driving maneuvers. We detail how to design a simple car simulator and how to use it to do the training. For each problem, we define different states, actions, and reward functions to obtain a Q-table. Next, we use it as a driving maneuver controller in a different simulation environment. With this method, our car successfully droves on a road different to where it was training. An important conclusion is the possibility to build, more complex controllers to do passing or behavior selectors.					25th Robotics Mexican Congress (COMRob)25th Robotics Mexican Congress (COMRob)	NOV 15-17, 2023NOV 15-17, 2023	Univ Veracruzana, Fac Ingn Mecanica Electrica; Asociac Mexicana Robotica & Ind A C; IEEE Veracruz SectUniv Veracruzana, Fac Ingn Mecanica Electrica; Asociac Mexicana Robotica & Ind A C; IEEE Veracruz Sect	Xalapa, MEXICOXalapa, MEXICO	0	0	0	0	0	0	0					979-8-3503-0679-8									CINVESTAV, Dept Automat Control, Mexico City, DF, Mexico				2024-02-01	WOS:001138784400024		
J	Drungilas, Darius; Kurmis, Mindaugas; Senulis, Audrius; Lukosius, Zydrunas; Andziulis, Arunas; Januteniene, Jolanta; Bogdevicius, Marijonas; Jankunas, Valdas; Voznak, Miroslav				Janutėnienė, Jolanta/KBQ-1745-2024; Januteniene, Jolanta/Q-7525-2019	Drungilas, Darius/0000-0002-7821-3183					Deep reinforcement learning based optimization of automated guided vehicle time and energy consumption in a container terminal								ALEXANDRIA ENGINEERING JOURNAL				67				397	407				10.1016/j.aej.2022.12.057					JAN 2023		Article; Early Access		2023	The energy efficiency of port container terminal equipment and the reduction of CO2 emissions are among one of the biggest challenges facing every seaport in the world. The article pre-sents the modeling of the container transportation process in a terminal from the quay crane to the stack using battery-powered Automated Guided Vehicle (AGV) to estimate the energy consump-tion parameters. An AGV speed control algorithm based on Deep Reinforcement Learning (DRL) is proposed to optimize the energy consumption of container transportation. The results obtained and compared with real transportation measurements showed that the proposed DRL-based approach dynamically changing the driving speed of the AGV reduces energy consumption by 4.6%. The obtained results of the research provide the prerequisites for further research in order to find optimal strategies for autonomous vehicle movement including context awareness and infor-mation sharing with other vehicles in the terminal.(c) 2022 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Engineering, Alexandria University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/ licenses/by-nc-nd/4.0/).									6	0	0	0	1	0	6			1110-0168	2090-2670										Klaipeda Univ, H Manto Str 84, LT-92294 Klaipeda, LithuaniaVSB Tech Univ Ostrava, Dept Telecommun, 17 Listopadu 2172-15, Ostrava 70800, Czech Republic				2023-05-23	WOS:000918221700001		
J	Feher, Arpad; Aradi, Szilard; Becsi, Tamas				Bécsi, Tamás/H-9818-2012; Aradi, Szilard/AAE-2623-2019	Bécsi, Tamás/0000-0002-1487-9672; Aradi, Szilard/0000-0001-6811-2584					Online Trajectory Planning with Reinforcement Learning for Pedestrian Avoidance								ELECTRONICS				11	15					2346			10.3390/electronics11152346							Article	AUG 2022	2022	Planning the optimal trajectory of emergency avoidance maneuvers for highly automated vehicles is a complex task with many challenges. The algorithm needs to decrease accident risk by reducing the severity and keeping the car in a controllable state. Optimal trajectory generation considering all aspects of vehicle and environment dynamics is numerically complex, especially if the object to be avoided is moving. This paper presents a hierarchical method for the avoidance of moving objects in an autonomous vehicle, where a reinforcement learning agent is responsible for local planning, while longitudinal and lateral control is performed by the low-level model-predictive controller and Stanley controllers. In the developed architecture, the agent is responsible for the optimization. It is trained in various scenarios to provide the necessary parameters for a polynomial-based path and a velocity profile in a neural network output. The vehicle performs only the first step of the trajectory, which is redesigned repeatedly by the planner based on the new state. In the training phase, the vehicle executes the entire trajectory via low-level controllers to determine the reward value, which realizes a prediction for the future. The agent receives feedback and can further improve its performance. Finally, the proposed framework was tested in a simulation environment and was also compared to human drivers' abilities.									3	0	0	0	0	0	3				2079-9292										Budapest Univ Technol & Econ, Fac Transportat Engn & Vehicle Engn, Dept Control Transportat & Vehicle Syst, Muegyet Rkp 3, H-1111 Budapest, Hungary				2022-08-23	WOS:000840165400001		
C	Khaitan, Shivesh; Dolan, John M.			IEEE							State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections								2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						12219	12224				10.1109/IROS47612.2022.9981109							Proceedings Paper	2022	2022	Traversing intersections is a challenging problem for autonomous vehicles, especially when the intersections do not have traffic control. Recently deep reinforcement learning has received massive attention due to its success in dealing with autonomous driving tasks. In this work, we address the problem of traversing unsignalized intersections using a novel curriculum for deep reinforcement learning. The proposed curriculum leads to: 1) A faster training process for the reinforcement learning agent, and 2) Better performance compared to an agent trained without curriculum. Our main contribution is two-fold: 1) Presenting a unique curriculum for training deep reinforcement learning agents, and 2) demonstrating the performance improvement using the proposed curriculum in the unsignalized intersection traversal task. The framework expects processed observations of the surroundings from the perception system of the autonomous vehicle. We test our method in the CommonRoad motion planning simulator on T-intersections and four-way intersections.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	OCT 23-27, 2022OCT 23-27, 2022	IEEE; Royal Soc Japan; IEEE Robot & Automat Soc; IES; SICE; New Technol FdnIEEE; Royal Soc Japan; IEEE Robot & Automat Soc; IES; SICE; New Technol Fdn	Kyoto, JAPANKyoto, JAPAN	0	0	0	0	0	0	0			2153-0858		978-1-6654-7927-1									Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA				2023-03-12	WOS:000909405303107		
C	Guo, Youtian; Gao, Qi; Pan, Fengan					Guo, Youtian/0000-0002-0979-9344	Fu, J; Sun, J				Trained Model Reuse of Autonomous-Driving in Pygame with Deep Reinforcement Learning								PROCEEDINGS OF THE 39TH CHINESE CONTROL CONFERENCE		Chinese Control Conference						5660	5664											Proceedings Paper	2020	2020	Autonomous-Driving technology has begun to bring great convenience to daily trip, transportation, and surveying harsh environment. Considering that deep reinforcement learning has requirements for the convergence performance of the training results, and the actual training results sometimes cannot converge steadily or fail to reach the training goals, in this paper, the trained model reuse method was proposed, which can use the trained model generates Q(S-t, A(t)) and can he used as a part of Deep Reinforcement Learning model, and this model was built based on the value function that could predict the Q value corresponding to the various actions performed in the environment state. In the Pygame platform, a simplified traffic simulation environment was set, it is observed that the Autonomous-Driving vehicle could run smoothly without collision in a fixed-length test simulation environment, and this trained model reuse method could help autonomous vehicle accelerate the learning process, obtain better simulation results during most of the training process, save simulation time and computing resources.					39th Chinese Control Conference (CCC)39th Chinese Control Conference (CCC)	JUL 27-29, 2020JUL 27-29, 2020		Shenyang, PEOPLES R CHINAShenyang, PEOPLES R CHINA	1	0	0	0	0	0	1			2161-2927		978-988-15639-0-3									Beijing Inst Technol, Beijing 100081, Peoples R ChinaKunming BIT Ind Technol Res Inst INC, Kunming 650106, Yunnan, Peoples R China	Kunming BIT Ind Technol Res Inst INC			2021-04-23	WOS:000629243505136		
B	Cui, Z.; Li, M.; Huang, Y.; Wang, Y.; Chen, H.										An interpretation framework for autonomous vehicles decision-making via SHAP and RF								2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	7				10.1109/CVCI56766.2022.9964561							Conference Paper	2022	2022	Decision-making for autonomous vehicles is critical to achieving safe and efficient autonomous driving. In recent years, deep reinforcement learning (DRL) techniques have emerged as the most promising way to enable intelligent decision-making. However, DRL with 'black box' nature is not widely understood by humans, thus hindering their social acceptance. In this paper, we combine SHapley Additive exPlanation (SHAP) and random forest (RF) techniques to bring transparency to decision-making obtained by DRL. Specifically, we first implement decision-making of autonomous vehicles following in discrete action space based on DRL algorithm with the goal of safety and efficiency. Then we use the SHAP technique to simplify the feature space, which shows that relative distance, longitudinal speed of the ego vehicle, and longitudinal speed of the proceeding vehicle have a critical impact on vehicle following task. Finally, we collect the state-action pairs generated by the DRL model and perform feature filtering, and fit the decision model with an interpretable RF model. The simulation results show that the RF model achieves the behavioral explanation of autonomous vehicle following.					2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)	28-30 Oct. 202228-30 Oct. 2022	IEEE; Chinese Journal Mechanical Engineering; China Journal Highway and TransportIEEE; Chinese Journal Mechanical Engineering; China Journal Highway and Transport	Nanjing, ChinaNanjing, China	2	0	0	0	0	0	2					978-1-6654-5374-5									Clean Energy Automotive Eng. Center, Tongji Univ., Shanghai, ChinaDept. of Control Sci. & Eng., Tongji Univ., Shanghai, China				2023-03-22	INSPEC:22361652		
J	He, Rui; Lv, Haipeng; Zhang, Sumin; Zhang, Dong; Zhang, Hang										Lane Following Method Based on Improved DDPG Algorithm								SENSORS				21	14					4827			10.3390/s21144827							Article	JUL 2021	2021	In an autonomous vehicle, the lane following algorithm is an important component, which is a basic function of autonomous driving. However, the existing lane following system has a few shortcomings: first, the control method it adopts requires an accurate system model, and different vehicles have different parameters, which needs a lot of parameter calibration work. The second is that it may fail on road sections where the lateral acceleration requirements of vehicles are large, such as large curves. Third, its decision-making system is defined based on rules, which has disadvantages: it is difficult to formulate; human subjective factors cannot guarantee objectivity; coverage is difficult to guarantee. In recent years, the deep deterministic policy gradient (DDPG) algorithm has been widely used in the field of autonomous driving due to its strong nonlinear fitting ability and generalization performance. However, the DDPG algorithm has overestimated state action values and large cumulative errors, low training efficiency and other issues. Therefore, this paper improves the DDPG algorithm based on the double critic networks and priority experience replay mechanism. Then this paper proposes a lane following method based on this algorithm. Experiment shows that the algorithm can achieve excellent following results under various road conditions.									8	0	0	0	0	0	8				1424-8220										Jilin Univ, State Key Lab Automot Simulat & Control, Changchun 130025, Jilin, Peoples R China				2021-08-01	WOS:000677030600001	34300567	
C	Jafari, Rouhollah; Ashari, Alireza Esna; Huber, Marcus			IEEE							CHAMP: Integrated Logic with Reinforcement Learning for Hybrid Decision Making for Autonomous Vehicle Planning								2023 AMERICAN CONTROL CONFERENCE, ACC		Proceedings of the American Control Conference						3310	3315											Proceedings Paper	2023	2023	A Cognitive Hybrid Autonomous Motion Planner (CHAMP) is developed for autonomous driving applications in challenging driving scenarios. The proposed hybrid planner unifies a hierarchical rule-based decision-making architecture with Reinforcement Learning (RL). For challenging intersection scenarios, RL agents are trained to replace a subset of the rules in the logical planner. The hybrid planner is systematically tested and benchmarked to demonstrate its effectiveness in handling challenging road scenario with congested and chaotic traffic conditions.					American Control Conference (ACC)American Control Conference (ACC)	MAY 31-JUN 02, 2023MAY 31-JUN 02, 2023	Mitsubishi Elect Res Lab; Boeing; MathWorks; Quanser; ASML; Halliburton; Lockheed Martin; dSPACE; General Motors Co; Soc Ind & Appl Math; Springer; Collimator; JuliaHub; Unitree RobotMitsubishi Elect Res Lab; Boeing; MathWorks; Quanser; ASML; Halliburton; Lockheed Martin; dSPACE; General Motors Co; Soc Ind & Appl Math; Springer; Collimator; JuliaHub; Unitree Robot	San Diego, CASan Diego, CA	0	0	0	0	0	0	0			0743-1619	2378-5861	979-8-3503-2806-6									Gen Motors Res & Dev Ctr, Warren, MI 48092 USA				2023-09-07	WOS:001027160302147		
C	Boada, MJL; Salichs, MA				Boada, M.J.L./B-9206-2008	Boada, M.J.L./0000-0001-5377-0023	Asama, H; Inoue, H				Visual tracking skill reinforcement learning for a mobile robot								INTELLIGENT AUTONOMOUS VEHICLES 2001		IFAC PROCEEDINGS SERIES						173	178											Proceedings Paper	2002	2002	In this paper, we implement a reinforcement learning algorithm that allows an autonomous mobile robot, with a single CCD camera mounted on a pan-tilt platform, to learn simple skills such as watch and orientation, and to obtain the skill called approach combining the learned skills previously. The results obtained show the advantages of 1) decomposing complex skills in simpler skills due to the learning rate improves, 2) using the reinforcement as a learning mechanism because of the possibility to learn on-line without the robot having a previous knowledge of what it has to do for a given situation. We also present the neural network architecture used for the learning mechanism implementation. Copyright (C) 2001 IFAC.					4th IFAC Symposium on Intelligent Autonomous Vehicles (IAV 2001)4th IFAC Symposium on Intelligent Autonomous Vehicles (IAV 2001)	SEP 05-07, 2001SEP 05-07, 2001	Int Federat Automat Control; Intelligent Autonomous Vehicles TC; Aerospace, TVS; Intelligent Control Agr Automat; Marine Syst, TVM; Robotics, MIR; Hokkaido Prefecture; Sapporo CityInt Federat Automat Control; Intelligent Autonomous Vehicles TC; Aerospace, TVS; Intelligent Control Agr Automat; Marine Syst, TVM; Robotics, MIR; Hokkaido Prefecture; Sapporo City	SAPPORO, JAPANSAPPORO, JAPAN	0	0	0	0	0	0	0					0-08-043899-7									Univ Carlos III Madrid, Syst Engn & Automat Div, Leganes 28911, Madrid, Spain				2002-01-01	WOS:000184044700028		
J	Zihui Zhu; Zhengming Zhang; Wen Yan; Yongming Huang; Luxi Yang										Proactive caching in auto driving scene via deep reinforcement learning								2019 11th International Conference on Wireless Communications and Signal Processing (WCSP)								6 pp.	6 pp.				10.1109/WCSP.2019.8928131							Conference Paper	2019	2019	The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry. The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction. In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network. First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively. Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy. Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.					2019 11th International Conference on Wireless Communications and Signal Processing (WCSP)2019 11th International Conference on Wireless Communications and Signal Processing (WCSP)	23-25 Oct. 201923-25 Oct. 2019		Xi'an, ChinaXi'an, China	0	0	0	0	0	0	0														Sch. of Inf. Sci. & Eng., Southeast Univ., Nanjing, China				2020-01-30	INSPEC:19243133		
B	Wen Fu; Yanjie Li; Zhaohui Ye; Qi Liu					Liu, Qi/0000-0001-7485-6344					Decision Making for Autonomous Driving Via Multimodal Transformer and Deep Reinforcement Learning*								2022 IEEE International Conference on Real-time Computing and Robotics (RCAR)								481	6				10.1109/RCAR54675.2022.9872180							Conference Paper	2022	2022	On the basis of environmental information processed by the sensing module, the decision module in automatic driving integrates environmental and vehicle information to make the autonomous vehicle produce safe and reasonable driving behavior. Considering the complexity and variability of the driving environment of autonomous vehicles, researchers have begun to apply deep reinforcement learning (DRL) in the study of autonomous driving control strategies in recent years. In this paper, we apply an algorithm framework combining multimodal transformer and DRL to solve the autonomous driving decision problem in complex scenarios. We use ResNet and transformer to extract the features of LiDAR point cloud and image. We use Deep Deterministic Policy Gradient (DDPG) algorithm to complete the subsequent autonomous driving decision-making task. And we use information bottleneck to improve the sampling efficiency of RL. We use CARLA simulator to evaluate our approach. The results show that our approach allows the agent to learn better driving strategies.					2022 IEEE International Conference on Real-time Computing and Robotics (RCAR)2022 IEEE International Conference on Real-time Computing and Robotics (RCAR)	17-22 July 202217-22 July 2022	IEEE; Shenzhen Institute of Advanced Technology; Shanghai Jiao Tong UniversityIEEE; Shenzhen Institute of Advanced Technology; Shanghai Jiao Tong University	Guiyang, ChinaGuiyang, China	1	0	0	0	0	0	1					978-1-6654-6983-8									Dept. of Control Sci. & Eng., Harbin Inst. of Technol., Shenzhen, China				2022-11-01	INSPEC:22026951		
C	Mekala, M. S.; Zhang, Haolin; Park, Ju H.; Jung, Ho-Youl			IEEE	Mekala, M S/AAD-1253-2020	Mekala, M S/0000-0002-1313-285X					Quantum-based Offloading Strategy for Intelligent Vehicle Network								2023 IEEE 20TH CONSUMER COMMUNICATIONS & NETWORKING CONFERENCE, CCNC		IEEE Consumer Communications and Networking Conference											10.1109/CCNC51644.2023.10059954							Proceedings Paper	2023	2023	Role of RSUs become essential in enhancing the service reliability rate of the end vehicle to strengthen autonomous vehicle technology. Computation-intensive services are delaysensitive, and most existing methods attempted comprehensively to meet the application deadline but have not reached the expectations due to classical computing. In this regard, we design a novel offloading decision-making method based on quantum theory through reinforcement learning. Grover's algorithm is employed to select a feasible device based on cost and energy usage probability ratio. Theoretical and mathematical validations and simulation outcomes confine the impact of novel decisionmaking methods on the statistical constraints of the heterogeneous framework.					IEEE 20th Consumer Communications and Networking Conference (CCNC)IEEE 20th Consumer Communications and Networking Conference (CCNC)	JAN 08-11, 2023JAN 08-11, 2023	IEEEIEEE	Las Vegas, NVLas Vegas, NV	0	0	0	0	0	0	0			2331-9852		978-1-6654-9734-3									Yeugnam Univ, Informat Commun Engn, Gyongsan 38544, South KoreaYeugnam Univ, RLRC Autonomous Vehicle Arts & Mat Innovat, Gyongsan, South KoreaXi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R ChinaYeungnam Univ, Dept Elect Engn, Gyongsan 38544, South Korea	Yeugnam UnivYeugnam Univ			2023-06-01	WOS:000982339100235		
B	Shengduo Chen; Yaowei Sun; Dachuan Li; Qiang Wang; Qi Hao; Sifakis, J.				HAO, QI/IST-5581-2023	HAO, QI/0000-0002-2792-5965					Runtime Safety Assurance for Learning-enabled Control of Autonomous Driving Vehicles								2022 International Conference on Robotics and Automation (ICRA)								8978	84				10.1109/ICRA46639.2022.9812177							Conference Paper	2022	2022	Providing safety guarantees for Autonomous Vehicle (AV) systems with machine-learning based controllers remains a challenging issue. In this work, we propose Simplex-Drive, a framework that can achieve runtime safety assurance for machine-learning enabled controllers of AVs. The proposed Simplex-Drive consists of an unverified Deep Reinforcement Learning (DRL)-based advanced controller (AC) that achieves desirable performance in complex scenarios, a Velocity-Obstacle (VO) based baseline safe controller (BC) with provably safety guarantees, and a verified mode management unit that monitors the operation status and switches the control authority between AC and BC based on safety-related conditions. We provide a formal correctness proof of Simplex-Drive and conduct a lane-changing case study in dense traffic scenarios. The simulation experiment results demonstrate that Simplex-Drive can always ensure the operation safety without sacrificing control performance, even if the DRL policy may lead to deviations from the safe status.					2022 IEEE International Conference on Robotics and Automation (ICRA)2022 IEEE International Conference on Robotics and Automation (ICRA)	23-27 May 202223-27 May 2022		Philadelphia, PA, USAPhiladelphia, PA, USA	0	0	0	0	0	0	0					978-1-7281-9681-7									Dept. of Comput. Sci. & Eng., Southern Univ. of Sci. & Technol., Shenzhen, ChinaRes. Inst. for Trustworthy Autonomous Syst., Shenzhen, ChinaArtificial Intelligence Res. Center, Chinese Acad. of Mil. Sci., Beijing, China				2022-10-20	INSPEC:22116398		
J	Folkers, A.; Rick, M.; Buskens, C.										Controlling an Autonomous Vehicle with Deep Reinforcement Learning [arXiv]								arXiv								7 pp.	7 pp.											Journal Paper	24 Sept. 2019	2019	We present a control approach for autonomous vehicles based on deep reinforcement learning. A neural network agent is trained to map its estimated state to acceleration and steering commands given the objective of reaching a specific target state while considering detected obstacles. Learning is performed using state-of-the-art proximal policy optimization in combination with a simulated environment. Training from scratch takes five to nine hours. The resulting agent is evaluated within simulation and subsequently applied to control a full-size research vehicle. For this, the autonomous exploration of a parking lot is considered, including turning maneuvers and obstacle avoidance. Altogether, this work is among the first examples to successfully apply deep reinforcement learning to a real vehicle. [doi:10.1109/IVS.2019.8814124].									0	0	0	0	0	0	0														Center for Ind. Math., Univ. of Bremen, Bremen, Germany				2020-04-10	INSPEC:19423203		
B	Soni, H.; Gupta, V.; Kumar, R.										Motion Planning using Reinforcement Learning for Electric Vehicle Battery optimization(EVBO)								2019 International Conference on Power Electronics, Control and Automation (ICPECA). Proceedings								6 pp.	6 pp.				10.1109/ICPECA47973.2019.8975684							Conference Paper	2019	2019	The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning. Motion planning is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour. In this paper, reinforcement learning techniques are explored for the motion planning of an electric vehicle(EV) while optimizing battery consumption. The EV travel time has also been evaluated under different reinforcement learning schemes. A traffic simulation network is developed for a high-traffic zone of Jaipur city using Simulation for Urban Mobility(SUMO) software. Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network. The results show that value iteration and q-learning have shown improved battery consumption. However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.					2019 International Conference on Power Electronics, Control and Automation (ICPECA)2019 International Conference on Power Electronics, Control and Automation (ICPECA)	16-17 Nov. 201916-17 Nov. 2019		New Delhi, IndiaNew Delhi, India	0	0	0	0	0	0	0					978-1-7281-3958-6									Centre for Converging Technol., Univ. of Rajasthan, Jaipur, IndiaDept. of Electr. Eng., Malaviya Nat. Inst. of Technol., Jaipur, India				2020-02-27	INSPEC:19316743		
C	Gu, Ziqing; Yang, Yujie; Duan, Jingliang; Li, Shengbo Eben; Chen, Jianyu; Cao, Wenhan; Zheng, Sifa			IEEE	Duan, Jingliang/JTV-0943-2023	Duan, Jingliang/0000-0002-3697-1576; Yang, Yujie/0000-0001-7222-0019					Belief state separated reinforcement learning for autonomous vehicle decision making under uncertainty								2021 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						586	592				10.1109/ITSC48978.2021.9564576							Proceedings Paper	2021	2021	In autonomous driving, the ego vehicle and its surrounding traffic environments always have uncertainties like parameter and structural errors, behavior randomness of road users, etc. Furthermore, environmental sensors are noisy or even biased. This problem can be formulated as a partially observable Markov decision process. Existing methods lack a good representation of historical information, making it very challenging to find an optimal policy. This paper proposes a belief state separated reinforcement learning (RL) algorithm for decision-making of autonomous driving in uncertain environments. We extend the separation principle from linear Gaussian systems to general nonlinear stochastic environments, where the belief state, defined as the posterior distribution of the true state, is found to be a sufficient statistic of historical information. This belief state is estimated by action-enhanced variational inference from historical information and is proved to satisfy the Markovian property, thus allowing us to obtain the optimal policy using traditional RL algorithms for Markov decision processes. The policy gradient of a task-specific prior model is mixed with that of the interaction data to improve learning performance. The proposed algorithm is evaluated in a multi-lane autonomous driving task, where the surrounding vehicles are subject to behavior uncertainty and observation noise. The simulation results show that compared with existing RL algorithms, the proposed method can achieve a higher average return with better driving performance.					IEEE Intelligent Transportation Systems Conference (ITSC)IEEE Intelligent Transportation Systems Conference (ITSC)	SEP 19-22, 2021SEP 19-22, 2021	IEEEIEEE	Indianapolis, INIndianapolis, IN	0	0	0	0	0	0	0			2153-0009		978-1-7281-9142-3									Tsinghua Univ, Sch Vehicle & Mobil, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R ChinaTsinghua Univ, Inst Interdiscriplinary Informat Sci, Beijing, Peoples R China				2022-09-24	WOS:000841862500084		
C	Lin, Yuan; McPhee, John; Azad, Nasser L.			IEEE							Longitudinal Dynamic versus Kinematic Models for Car-Following Control Using Deep Reinforcement Learning								2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						1504	1510											Proceedings Paper	2019	2019	The majority of current studies on autonomous vehicle control via deep reinforcement learning (DRL) utilize point-mass kinematic models, neglecting vehicle dynamics which includes acceleration delay and acceleration command dynamics. The acceleration delay, which results from sensing and actuation delays, results in delayed execution of the control inputs. The acceleration command dynamics dictates that the actual vehicle acceleration does not rise up to the desired command acceleration instantaneously due to dynamics. In this work, we investigate the feasibility of applying DRL controllers trained using vehicle kinematic models to more realistic driving control with vehicle dynamics. We consider a particular longitudinal car-following control, i.e., Adaptive Cruise Control (ACC), problem solved via DRL using a point-mass kinematic model. When such a controller is applied to car following with vehicle dynamics, we observe significantly degraded car-following performance. Therefore, we redesign the DRL framework to accommodate the acceleration delay and acceleration command dynamics by adding the delayed control inputs and the actual vehicle acceleration to the reinforcement learning environment state, respectively. The training results show that the redesigned DRL controller results in near-optimal control performance of car following with vehicle dynamics considered when compared with dynamic programming solutions.					IEEE Intelligent Transportation Systems Conference (IEEE-ITSC)IEEE Intelligent Transportation Systems Conference (IEEE-ITSC)	OCT 27-30, 2019OCT 27-30, 2019	IEEE; Intel; Griffith Univ; Auckland Tourism Events & Econ DevIEEE; Intel; Griffith Univ; Auckland Tourism Events & Econ Dev	Auckland, NEW ZEALANDAuckland, NEW ZEALAND	25	2	0	0	0	0	27			2153-0009		978-1-5386-7024-8									Univ Waterloo, Syst Design Engn Dept, Waterloo, ON N2L 3G1, Canada				2020-04-22	WOS:000521238101087		
J	Jiqian Dong; Sikai Chen; Ha, P.Y.J.; Yujie Li; Labi, S.										A DRL-based Multiagent Cooperative Control Framework for CAV Networks: a Graphic Convolution Q Network [arXiv]								arXiv								20 pp.	20 pp.											Journal Paper	11 Oct. 2020	2020	Connected Autonomous Vehicle (CAV) Network can be defined as a collection of CAVs operating at different locations on a multilane corridor, which provides a platform to facilitate the dissemination of operational information as well as control instructions. Cooperation is crucial in CAV operating systems since it can greatly enhance operation in terms of safety and mobility, and high-level cooperation between CAVs can be expected by jointly plan and control within CAV network. However, due to the highly dynamic and combinatory nature such as dynamic number of agents (CAVs) and exponentially growing joint action space in a multiagent driving task, achieving cooperative control is NP hard and cannot be governed by any simple rule-based methods. In addition, existing literature contains abundant information on autonomous driving's sensing technology and control logic but relatively little guidance on how to fuse the information acquired from collaborative sensing and build decision processor on top of fused information. In this paper, a novel Deep Reinforcement Learning (DRL) based approach combining Graphic Convolution Neural Network (GCN) and Deep Q Network (DQN), namely Graphic Convolution Q network (GCQ) is proposed as the information fusion module and decision processor. The proposed model can aggregate the information acquired from collaborative sensing and output safe and cooperative lane changing decisions for multiple CAVs so that individual intention can be satisfied even under a highly dynamic and partially observed mixed traffic. The proposed algorithm can be deployed on centralized control infrastructures such as road-side units (RSU) or cloud platforms to improve the CAV operation.									0	0	0	0	0	0	0														Center for Connected & Automated Transp., Purdue Univ., West Lafayette, IN, USA				2021-01-29	INSPEC:20236516		
C	Chen, Shengduo; Sun, Yaowei; Li, Dachuan; Wang, Qiang; Hao, Qi; Sifakis, Joseph			IEEE							Runtime Safety Assurance for Learning-enabled Control of Autonomous Driving Vehicles								2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022)								8978	8984				10.1109/ICRA46639.2022.9812177							Proceedings Paper	2022	2022	Providing safety guarantees for Autonomous Vehicle (AV) systems with machine-learning based controllers remains a challenging issue. In this work, we propose Simplex-Drive, a framework that can achieve runtime safety assurance for machine-learning enabled controllers of AVs. The proposed Simplex-Drive consists of an unverified Deep Reinforcement Learning (DRL)-based advanced controller (AC) that achieves desirable performance in complex scenarios, a Velocity-Obstacle (VO) based baseline safe controller (BC) with provably safety guarantees, and a verified mode management unit that monitors the operation status and switches the control authority between AC and BC based on safety-related conditions. We provide a formal correctness proof of Simplex-Drive and conduct a lane-changing case study in dense traffic scenarios. The simulation experiment results demonstrate that Simplex-Drive can always ensure the operation safety without sacrificing control performance, even if the DRL policy may lead to deviations from the safe status.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 23-27, 2022MAY 23-27, 2022	IEEE; IEEE Robot & Automat SocIEEE; IEEE Robot & Automat Soc	Philadelphia, PAPhiladelphia, PA	4	0	0	0	0	0	4					978-1-7281-9681-7									Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen 518055, Peoples R ChinaRes Inst Trustworthy Autonomous Syst, Shenzhen 518055, Peoples R ChinaChinese Acad Mil Sci, Def Innovat Inst, Artificial Intelligence Res Ctr, Beijing 100072, Peoples R China	Res Inst Trustworthy Autonomous SystChinese Acad Mil Sci			2022-01-01	WOS:000941277601122		
J	Dai, X; Li, CK; Rad, AB										An approach to tune fuzzy controllers based on reinforcement learning for autonomous vehicle control								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				6	3			285	293				10.1109/TITS.2005.853698							Article	SEP 2005	2005	In this paper, we suggest a new approach for tuning parameters of fuzzy controllers based on reinforcement learning. The architecture of the proposed approach is comprised of a Q estimator network (QEN) and a Takagi-Sugeno-type fuzzy inference system (TSK-FIS). Unlike other fuzzy Q-learning approaches that select an optimal action based on finite discrete actions, the proposed controller obtains the control output directly from TSK-FIS. With the proposed architecture, the learning algorithms for all the parameters of the QEN and the FIS are developed based on the temporal-difference (TD) methods as well as the gradient-descent algorithm. The performance of the proposed design technique is illustrated by simulation studies of a vehicle longitudinal-control system.									75	5	0	0	0	0	85			1524-9050	1558-0016										Rockwell Automat Res Ctr, Shanghai 2002333, Peoples R ChinaHong Kong Polytech Univ, Dept Elect & Informat Engn, Kowloon, Hong Kong, Peoples R ChinaHong Kong Polytech Univ, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China	Rockwell Automat Res Ctr			2005-09-01	WOS:000231999900004		
J	Hwang, Seulbin; Lee, Kibeom; Jeon, Hyeongseok; Kum, Dongsuk				; Kum, Dongsuk/G-7362-2012	Jeon, Hyeongseok/0000-0002-4596-6437; Kum, Dongsuk/0000-0002-2590-4845; Lee, Kibeom/0000-0002-9745-7116					Autonomous Vehicle Cut-In Algorithm for Lane-Merging Scenarios via Policy-Based Reinforcement Learning Nested Within Finite-State Machine								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	10			17594	17606				10.1109/TITS.2022.3153848					MAR 2022		Article; Early Access		2022	Lane-merging scenarios pose highly challenging problems for autonomous vehicles due to conflicts of interest between the human-driven and cutting-in autonomous vehicles. Such conflicts become severe when traffic increases, and cut-in algorithms suffer from a steep trade-off between safety and cut-in performance. In this study, a reinforcement learning (RL)-based cut-in policy network nested within a finite state machine (FSM)--which is a high-level decision maker, is proposed to achieve high cut-in performance without sacrificing safety. This FSM-RL hybrid approach is proposed to obtain 1) a strategic and adjustable algorithm, 2) optimal safety and cut-in performance, and 3) robust and consistent performance. In the high-level decision making algorithm, the FSM provides a framework for four cut-in phases (ready for safe gap selection, gap approach, negotiation, and lane-change execution) and handles the transitions between these phases by calculating the collision risks associated with target vehicles. For the lane-change phase, a policy-based deep-RL approach with a soft actor-critic network is employed to get optimal cut-in performance. The results of simulations show that the proposed FSM-RL cut-in algorithm consistently achieves a high cut-in success rate without sacrificing safety. In particular, as the traffic increases, the cut-in success rate and safety are significantly improved over existing optimized rule-based cut-in algorithms and end-to-end RL algorithm.									13	0	0	0	0	0	14			1524-9050	1558-0016										NAVER LABS, Dept Robot SW & Deep Learning, Gyeonggi 13591, South KoreaGachon Univ, Dept Future Mobil, Autonomous Syst Lab, Gyeonggi 13120, South KoreaKorea Adv Inst Sci & Technol KAIST, Vehicular Syst Design & Control Lab, Grad Sch Green Transportat, Daejeon 34141, South Korea				2022-03-24	WOS:000767805200001		
C	Wang, X. N.; Xu, X.; Wu, T.; He, H. G.; Zhang, M.										Hybrid reinforcement learning combined with SVMs and its applications								DYNAMICS OF CONTINUOUS DISCRETE AND IMPULSIVE SYSTEMS-SERIES B-APPLICATIONS & ALGORITHMS				13E				3740	3745										S	Proceedings Paper	DEC 2006	2006	A new learning control method with hybrid learning strategies is presented. The learning controller combines reinforcement learning (RL) with support vector machines (SVMs) so that the performance of the learning controller can be optimized not only by a priori knowledge from human experts but also by online reinforcement learning. In the controller, supervised learning based on SVMs is employed to construct the initial policy of reinforcement learning and a policy gradient reinforcement learning algorithm is implemented to make use of the initial policy for online optimization. Thus, the learning control approach has three advantages: (1) Prior knowledge can be easily used only by providing training examples to SVM-based supervised learning. (2) The controller performance can be optimized using online RL to compensate unknown disturbances. (3) The controller structure is determined by SVMs, which is data-driven, not predefined. We tested the performance of the learning controller on several path tracking tasks of a four-wheeled autonomous vehicle. The simulation results demonstrate the effectiveness of the learning controller.					International Conference on Sensing, Computing and AutomationInternational Conference on Sensing, Computing and Automation	MAY 08-11, 2006MAY 08-11, 2006		Chongqing, PEOPLES R CHINAChongqing, PEOPLES R CHINA	0	0	0	0	0	0	0			1492-8760											Natl Univ Def Technol, Coll Mechatron Engn & Automat, Changsha 410073, Hunan, Peoples R China				2006-12-01	WOS:000248842900033		
C	Hejase, Bilal; Yurtsever, Ekim; Han, Teawon; Singh, Baljeet; Filev, Dimitar P.; Tseng, H. Eric; Ozguner, Umit										Dynamic and Interpretable State Representation for Deep Reinforcement Learning in Automated Driving								IFAC PAPERSONLINE				55	24			129	134				10.1016/j.ifaco1.2022.10.273							Proceedings Paper	2022	2022	Understanding the causal relationship between an autonomous vehicle's input state and its output action is important for safety mitigation and explainable automated driving. However, reinforcement learning approaches have the drawback of being black box models. This work proposes an interpretable state representation that can capture state-action causalities for an automated driving agent, while also allowing for the underlying formulation to be general enough to be adapted to different driving scenarios. It also proposes encoding temporally-extended information in the state representation for better driving performance. We test this approach on a reinforcement learning agent in a highway simulation environment and demonstrate that the proposed state representation can capture state-action causalities in an interpretable manner. Experimental results show that the formulation and interpretation can be used to adapt the behavior of the driving agent to achieve desired, even unseen, driving behaviors after training. Copyright (c) 2022 The Authors. This is an open access article under the CC BY-NC-ND license					10th IFAC Symposium on Advances in Automotive Control (AAC)10th IFAC Symposium on Advances in Automotive Control (AAC)	AUG 29-31, 2022AUG 29-31, 2022	Int Federat Automat Control, Tech Comm 7 1 Automot Control; Gen Motors Co; Cummins Inc; PACCAR Inc; Ford Motor Co; Honda R & D Americas; Jobs Ohio; Schaeffler Grp; BorgWarner Inc; Int Federat Automat Control, Tech Comm 7 4 Transportat Syst; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; IEEE CSS Tech Comm Automot ControlsInt Federat Automat Control, Tech Comm 7 1 Automot Control; Gen Motors Co; Cummins Inc; PACCAR Inc; Ford Motor Co; Honda R & D Americas; Jobs Ohio; Schaeffler Grp; BorgWarner Inc; Int Federat Automat Control, Tech Comm 7 4 Transportat Syst; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; IEEE CSS Tech Comm Automot Controls	Ohio State Univ, Columbus, OHOhio State Univ, Columbus, OH	0	0	0	0	0	0	0			2405-8963											Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USAFord Greenfield Labs, Palo Alto, CA 94304 USAFord Motor Co, Dearborn, MI 48121 USA	Ford Greenfield Labs			2022-11-09	WOS:000872024300021		
J	Perez-Gil, Oscar; Barea, Rafael; Lopez-Guillen, Elena; Bergasa, Luis M.; Gomez-Huelamo, Carlos; Gutierrez, Rodrigo; Diaz-Diaz, Alejandro				Barea, Rafael/R-5760-2016; Diaz-Diaz, Alejandro/AAF-3672-2022; Diaz-Diaz, Alejandro/AFI-1982-2022; Bergasa, Luis M./H-9810-2013	Diaz-Diaz, Alejandro/0000-0002-7232-3197; Diaz-Diaz, Alejandro/0000-0002-7232-3197; Bergasa, Luis M./0000-0002-0087-3077					Deep reinforcement learning based control for Autonomous Vehicles in CARLA								MULTIMEDIA TOOLS AND APPLICATIONS				81	3			3553	3576				10.1007/s11042-021-11437-3					JAN 2022		Article; Early Access		2022	Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them. This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle. More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them. The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route. In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands. For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world. The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance. DDPG perfoms trajectories very similar to classic controller as LQR. In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m. To conclude, some conclusions and future works are commented.									25	0	0	0	0	0	27			1380-7501	1573-7721										Univ Alcala, Elect Dept, Alcala De Henares, Spain				2022-01-13	WOS:000742319000007		
J	Zhang, X.; Wu, L.; Liu, H.; Wang, Y.; Li, H.; Xu, B.										High-Speed Ramp Merging Behavior Decision for Autonomous Vehicles Based on Multiagent Reinforcement Learning								IEEE Internet of Things Journal								22664	72				10.1109/JIOT.2023.3304890							Journal Paper	2023	2023	To improve the decision success rate of a multiagent reinforcement learning algorithm in merging high-speed ramps of autonomous vehicles, the independent proximal policy optimization (IPPO) method is presented. The Markov decision process (MDP) model for autonomous vehicle behavioral decision making is developed. Moreover, the state space, reward function, and action space are all designed. An IPPO method is proposed using independent learning and parameter-sharing strategies based on the proximal policy optimization algorithm. And further, a decision-making model for autonomous driving behavior is built. For simulation experiments, a highway ramp scenario is set. The experiment findings indicate that the IPPO algorithm can significantly increase the decision success rate of autonomous vehicles in the ramp merging assignment. Also, as compared to the MAACKTR and GPPO algorithms, the IPPO algorithm can achieve a better average reward and finish the ramp merging more rapidly.									0	0	0	0	0	0	0			2327-4662											Sch. of Automobile, Chang'an Univ., Xian, China				2024-01-11	INSPEC:24260018		
B	Chakraborty, S.; Kumar, S.; Bhatt, N.; Pasumarthy, R.										End-to-end Autonomous Driving in Heterogeneous Traffic Scenario Using Deep Reinforcement Learning								2023 European Control Conference (ECC)								1	6											Conference Paper	2023	2023	In this paper, we propose an end-to-end autonomous driving architecture for safe maneuvering in heterogeneous traffic using a reinforcement learning (RL) algorithm. Using the proposed architecture we develop an RL agent that can make driving decisions directly from the sensor data. We formulate the autonomous driving problem as a Markov Decision Process and propose different architectures using Deep Q -Networks for two types of sensor data - top view images of the autonomous vehicle (AV) and its surrounding vehicles and information on relative position and velocities of the surrounding vehicles w.r.t the AV. We consider a highway scenario and analyze the performance of the RL agent using the proposed architectures using the highway-env simulator. We compare the driving performance of the AV for both sensor types and discuss their efficacy under varying traffic densities.					2023 European Control Conference (ECC)2023 European Control Conference (ECC)	13-16 June 202313-16 June 2023		Bucharest, RomaniaBucharest, Romania	0	0	0	0	0	0	0					978-3-907144-08-4									Dept. of Electr. Eng., Indian Instiute of Technol. Madras, Chennai, IndiaDept. of Electr. Eng., Indian Inst. of Technol. Madras, Chennai, IndiaDept. of Biotechnol., Indian Inst. of Technol. Madras, Chennai, IndiaDept. of Electr. Eng., Robert Bosch (India), Bengaluru, India				2023-08-17	INSPEC:23514337		
B				IEEE							Proceedings of 2020 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)								Proceedings of 2020 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)																				Book; Meeting	2020	2020						3rd IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)3rd IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)	November 04 -06, 2020November 04 -06, 2020	IEEE; Univ Trento; Minist Politiche Agricole Alimentari ForestaliIEEE; Univ Trento; Minist Politiche Agricole Alimentari Forestali		0	0	0	0	0	0	0					978-1-7281-8783-9(P)													2021-06-16	BIOSIS:PREV202100582887		
J	Zhou, Zejian; Xu, Hao				xu, hao/GWQ-7394-2022; Zhou, Zejian/AAJ-5205-2020; xu, haodong/IAM-9132-2023; Xu, Hao Ran/JED-9378-2023	Zhou, Zejian/0000-0002-0252-6765; 					Decentralized Adaptive Optimal Tracking Control for Massive Autonomous Vehicle Systems With Heterogeneous Dynamics: A Stackelberg Game								IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS				32	12			5654	5663				10.1109/TNNLS.2021.3100417							Article	DEC 2021	2021	In this article, a decentralized optimal tracking control problem has been studied for a large-scale autonomous vehicle system with heterogeneous system dynamics. Due to the ultralarge number of agents, the notorious "curse of dimension" problem as well as the unrealistic assumption of the existence of reliable very large-scale communication links in uncertain environments have challenged the traditional multiagent system (MAS) algorithms for decades. The emerging mean-field game (MFG) theory has recently been widely adopted to generate a decentralized control method that deals with those challenges by encoding the large scale MASs' information into a novel time-varying probability density functions (PDF) which can be obtained locally. However, the traditional MFG methods assume all agents are homogeneous, which is unrealistic in practical industrial applications, e.g., Internet of Things (IoTs), and so on. Therefore, a novel mean-field Stackelberg game (MFSG) is formulated based on the Stackelberg game, where all the agents have been classified as two different categories where one major leader's decision dominates the other minor agents. Moreover, a hierarchical structure that treats all minor agents as a mean-field group is developed to tackle the assumption of homogeneous agents. Then, the actor-actor-critic-critic-mass (A(2)C(2) M) algorithm with five neural networks is designed to learn the optimal policies by solving the MFSG. The Lyapunov theory is utilized to prove the convergence of A(2)C(2) M neural networks and the closed-loop system's stability. Finally, a series of numerical simulations are conducted to demonstrate the effectiveness of the developed method.									4	0	0	0	0	0	4			2162-237X	2162-2388										Univ Nevada, Dept Elect & Biomed Engn, Reno, NV 89557 USA				2021-12-11	WOS:000724480600041	34370674	
B	Liu, X.; Ren, Y.						S. Shmaliy, Y.; Nayyar, A.				Supervised Learning Guided Reinforcement Learning for Humanized Autonomous Driving Following Decision Making								7th International Conference on Computing, Control and Industrial Engineering (CCIE 2023): Advances in Computing, Control and Industrial Engineering VII. Lecture Notes in Electrical Engineering (1047)								457	64				10.1007/978-981-99-2730-2_45							Conference Paper	2023	2023	In order to improve the humanization of the autonomous vehicle following function and reduce the reinforcement learning exploration space, the adaptive following control strategy based on human experience supervised deep reinforcement learning is proposed in this paper. First, an end-to-end human driving reference model based on a temporal LSTM network is built to learn the driver's policy function from temporal state information to action. The model is trained by imitation learning using the driver-following data, and the action signals are output online through the input temporal state, and the output action signals are introduced into reinforcement learning through the reward function. The reward function is designed by combining the safety and comfort of the following process. The decision model is trained by a dual delay depth deterministic policy gradient. The simulation results based on CARLA simulator show that the proposed control strategy is closer to the human driver's driving habits and converges faster than the traditional reinforcement learning.					International Conference on Computing, Control and Industrial EngineeringInternational Conference on Computing, Control and Industrial Engineering	25-26 Feb. 202325-26 Feb. 2023		Hangzhou, ChinaHangzhou, China	0	0	0	0	0	0	0					978-981-99-2730-2									Coll. of Artificial Intelligence, Southwest Univ., Chongqing, ChinaColl. of Eng. & Technol., Southwest Univ., Chongqing, ChinaUniv. of Guanajuato, Salamanca, MexicoSch. of Comput. Sci., Duy Tan Univ., Da Nang, Vietnam				2023-09-29	INSPEC:23716716		
B	Yuchuan Zhang; Hui Xie; Kang Song										An Optimal Vehicle Speed Planning Algorithm for Regenerative Braking at Traffic Lights Intersections based on Reinforcement Learning								Proceedings of the 2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)								193	8				10.1109/CVCI51460.2020.9338590							Conference Paper	2020	2020	For electric vehicle or hybrid electric vehicles, the regenerative braking is one of the important means to realize energy saving, for which braking ahead of a traffic light intersection is a representative scenario. The uncertainty in driver behavior and future traffic flow, however, make it challenging to achieve optimal dynamic energy recovery through conventional braking operation by drivers. Therefore, in this paper, an energy recovery optimization-oriented vehicle speed planning algorithm ahead of traffic lights intersection is proposed, for autonomous vehicle or driving assistance system. First, the reward function is designed, taking the energy recovery amount, traffic efficiency and driving smoothness into consideration. Then, the information of traffic lights at intersections is obtained in advance through V2I (vehicle to infrastructure) communication. Finally, the q-table and neural network are trained in the framework of reinforcement learning, deriving optimal vehicle speed profile. Simulation results on a high-fidelity model show that the amount of recovered electrical energy using q-learning algorithm is 45.08% higher than that of uniform deceleration. The amount of electrical energy using DQN (Deep Q-network) algorithm is 2.24% higher than q-learning, showing to be a better candidate in terms of comprehensive optimality than q-learning.					2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)	18-20 Dec. 202018-20 Dec. 2020	IEEE Syst. Man & Cybern. Soc.IEEE Syst. Man & Cybern. Soc.	Hangzhou, ChinaHangzhou, China	5	1	0	0	0	0	6					978-1-7281-8497-5									Sch. of Mech. Eng., Tianjin Univ., Tianjin, China				2021-04-03	INSPEC:20424401		
B	Mengqi Li; Ziyao Geng; Yi Wang										Research on Vehicle Dispatch Problem Based on Kuhn-Munkres and Reinforcement Learning Algorithm								Proceedings of the 2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)								986	92				10.1109/ICPECA51329.2021.9362615							Conference Paper	2021	2021	With the development of artificial intelligence and 5G communication technology, autonomous vehicles are gradually becoming more achievable. Autonomous vehicles are used in urban transportation to provide taxi service, which effectively reduces labor cost and realizes intelligent transportation systems. The vehicle system combined with 5G technology can quickly obtain traffic information, which provides a decision basis for the vehicle dispatching. Thus, it is necessary to develop an efficient way to distribute and allocate these vehicles to maximize the potential income for the system. This paper studies the vehicle dispatching based on the travel data from the 2016 New York City Green Taxi data and propose two dispatching methods. First, we consider the dispatching problem as a maximum weight value matching problem. Then a distance-based dispatching method is proposed with the goal of minimizing the waiting time of passengers by using the Kuhn and Munkres (KM) algorithm. Finally, we formulate the decision of vehicle dispatch with a Markov Decision Process (MDP) and introduce a Reinforcement Learning (RL)-based dispatching method, which combines RL algorithm and KM algorithm to solve the dispatching problem with the goal of maximizing long-term revenue of divers. In the experiment, KM algorithm is compared with the full permutation algorithm to prove the effectiveness of KM algorithm. The performance of the distance-based dispatching method and RL-based dispatching method are presented in a small-scale dispatching and a large-scale dispatching. Experiment results show that the total revenue of vehicles is improved by about 20% by using RL-based dispatching method, compared to dispatching method based on the distance. Thus, RL-based dispatching method is more effective in a dispatching platform. It could be used by future public autonomous vehicle companies to achieve the fulfill the need of maximizing the potential income for the system.					2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)	22-24 Jan. 202122-24 Jan. 2021		Shenyang, ChinaShenyang, China	0	0	0	0	0	0	0					978-1-7281-9004-4									Abington Friends Sch., Jenkintown, PA, USA				2021-01-01	INSPEC:20509171		
J	Makantasis, Konstantinos; Kontorinaki, Maria; Nikolos, Ioannis					Nikolos, Ioannis/0000-0002-0675-5880					Deep reinforcement-learning-based driving policy for autonomous road vehicles								IET INTELLIGENT TRANSPORT SYSTEMS				14	1			13	24				10.1049/iet-its.2019.0249							Article	JAN 2020	2020	In this work, the problem of path planning for an autonomous vehicle that moves on a freeway is considered. The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics. On the contrary, this work proposes the development of a driving policy based on reinforcement learning. In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required. Driving scenarios where the road is occupied both by autonomous and manual driving vehicles are considered. To the best of the authors' knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments. The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established SUMO microscopic traffic flow simulator. Finally, some initial results regarding the effect of autonomous vehicles' behaviour on the overall traffic flow are presented.									15	2	0	0	0	0	17			1751-956X	1751-9578										Tech Univ Crete, Sch Prod Engn & Management, Khania, GreeceUniv Malta, Inst Digital Games, Msida, MaltaUniv Malta, Fac Sci, Dept Stat & Operat Res, Msida, Malta				2020-02-12	WOS:000510641500002		
J	Hu, Jinchao; Li, Xu; Cen, Yanqing; Xu, Qimin; Zhu, Xuefen; Hu, Weiming					Jinchao, Hu/0000-0003-0174-9737; Hu, Weiming/0000-0001-9483-5559					A Roadside Decision-Making Methodology Based on Deep Reinforcement Learning to Simultaneously Improve the Safety and Efficiency of Merging Zone								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	10			18620	18631				10.1109/TITS.2022.3157910					MAR 2022		Article; Early Access		2022	The safety and efficiency of the merging zone is particularly important for traffic networks. Although autonomous vehicle improves the safety and efficiency from vehicle view, traffic controlling in merging zone mostly focus on improving efficiency from roadside view. Lacking of detailed driving recommendation, it ignores the safety of merging zone where commercial vehicle pose a high collision risk in real traffic. This paper proposes a roadside decision-making methodology to simultaneously improve the safety and efficiency of merging zone. We have built two modules, namely assessment and decision-making. Assessment module takes advantage of Bayesian inference to evaluate dynamic collision risk. Decision-making module based on deep reinforcement learning recommends the actions to commercial vehicles by roadside unit. A series of typical simulation tests show that our method increases the TTC of commercial vehicles by an average of 62.7%. In the free flow, the overall travel time of vehicles in the merging zone is reduced by 11.68%. Most notably, when congestion occurred, the average jam length is reduced by 59.68% on the premise of safety. Moreover, the average accuracy of the roadside decision-making method on the evaluation metrics of TTC, travel time, and jam length are 93.73%, 91.65%, and 94.45%, respectively. The experimental results show that the roadside decision-making methodology simultaneously improves safety and efficiency, and it dynamically adapts free and congested traffic flow.									4	0	0	0	0	0	4			1524-9050	1558-0016										Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R ChinaRes Inst Highway Minist Transport, Beijing 100011, Peoples R China	Res Inst Highway Minist Transport			2022-03-26	WOS:000770591900001		
J	Liu, Jiaxin; Wang, Hong; Peng, Liang; Cao, Zhong; Yang, Diange; Li, Jun				Liu, Jiaxin/JNS-7966-2023	Liu, Jiaxin/0000-0002-9486-1084; Cao, Zhong/0000-0002-2243-5705; Peng, Liang/0000-0002-8494-2382					PNNUAD: Perception Neural Networks Uncertainty Aware Decision-Making for Autonomous Vehicle								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	12			24355	24368				10.1109/TITS.2022.3197602					AUG 2022		Article; Early Access		2022	Most environment perception methods in autonomous vehicles rely on deep neural networks because of their impressive performance. However, neural networks have black-box characteristics in nature, which may lead to perception uncertainty and untrustworthy autonomous vehicles. Thus, this work proposes a decision-making method to adapt the potential perception uncertainty due to the sensor noises, fuzzy features, and unfamiliar inputs. The whole method is named as Perception Neural Networks Uncertainty Aware Decision-Making (PNNUAD) method. PNNUAD first uses the Monte Carlo dropout method to estimate the perception neural network uncertainty into a distribution around the original output. Then, the perception uncertainty will be considered in a designed reinforcement learning-based planner using a distributed value function. Finally, a backup policy will maintain the vehicle's performance to avoid disastrous perception uncertainty. The evaluation section uses an augmented reality urban driving scenario; namely, the scenario builds in the CARLA simulator while the perception uncertainty comes from the real dataset. This case study focuses on the object class uncertainty of a widely used neural network, i.e., YOLO-V3. The results indicate that the proposed method can maintain AV safety even with poor perception performance. Meanwhile, the AV has not become too conservative by defending the perception uncertainty. This work is necessary for applying the statistics neural networks to safety-critical autonomous vehicles, and the source code will be open-source in this work.									7	0	0	0	1	0	7			1524-9050	1558-0016										Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R China				2022-08-28	WOS:000843249000001		
B	Yadavalli, S.R.; Das, L.C.; Won, M.										RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging								2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)								5514	21				10.1109/IROS55552.2023.10341918							Conference Paper	2023	2023	A platoon refers to a group of vehicles traveling together in very close proximity using automated driving technology. Owing to its immense capacity to improve fuel efficiency, driving safety, and driver comfort, platooning technology has garnered substantial attention from the autonomous vehicle research community. Although highly advantageous, recent research has uncovered that an excessively small intra-platoon gap can impede traffic flow during highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a challenge due to the massive computational complexity. In this paper, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The framework's state space has been meticulously designed in consultation with the transportation literature to take into account critical traffic parameters that bear direct relevance to merging efficiency. An intra-platoon gap decision making method based on the deep deterministic policy gradient algorithm is created to incorporate the continuous action space to ensure precise and continuous adaptation of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway on-ramp merging scenarios.					2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	20232023		Detroit, MI, USADetroit, MI, USA	0	0	0	0	0	0	0					978-1-6654-9190-7									Dept. of Comput. Sci., Univ. of Memphis, Memphis, TN, USA				2024-01-11	INSPEC:24257381		
B	Han, S.; Wang, H.; Su, S.; Shi, Y.; Miao, F.				Han, Songyang/HNI-9860-2023	Han, Songyang/0000-0001-8314-4832					Stable and Efficient Shapley Value-Based Reward Reallocation for Multi-Agent Reinforcement Learning of Autonomous Vehicles								2022 International Conference on Robotics and Automation (ICRA)								8765	71				10.1109/ICRA46639.2022.9811626							Conference Paper	2022	2022	With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs). However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability. When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process. In this work, we propose to reallocate the system's total reward efficiently to motivate stable cooperation among autonomous vehicles. We formally define and quantify how to reallocate the system's total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system's total reward. We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game. Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group. We then propose a cooperative policy learning algorithm with Shapley value reward reallocation. In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.					2022 IEEE International Conference on Robotics and Automation (ICRA)2022 IEEE International Conference on Robotics and Automation (ICRA)	23-27 May 202223-27 May 2022		Philadelphia, PA, USAPhiladelphia, PA, USA	2	0	0	0	0	0	2					978-1-7281-9681-7									Dept. of Comput. Sci. & Eng., Univ. of Connecticut, Storrs, CT, USASch. of Inf. Sci. & Technol., ShanghaiTech Univ., Shanghai, ChinaElectr. & Comput. Eng. Dept., Univ. of California, San Diego, La Jolla, CA, USA				2022-10-28	INSPEC:22136434		
J	Mackay, Andrew K.; Riazuelo, Luis; Montano, Luis					Riazuelo Latas, Luis Miguel/0000-0002-6722-5541; Mackay Parrott, Andrew Keon/0000-0003-2032-948X; /0000-0002-0449-2300					RL-DOVS: Reinforcement Learning for Autonomous Robot Navigation in Dynamic Environments								SENSORS				22	10					3847			10.3390/s22103847							Article	MAY 2022	2022	Autonomous navigation in dynamic environments where people move unpredictably is an essential task for service robots in real-world populated scenarios. Recent works in reinforcement learning (RL) have been applied to autonomous vehicle driving and to navigation around pedestrians. In this paper, we present a novel planner (reinforcement learning dynamic object velocity space, RL-DOVS) based on an RL technique for dynamic environments. The method explicitly considers the robot kinodynamic constraints for selecting the actions in every control period. The main contribution of our work is to use an environment model where the dynamism is represented in the robocentric velocity space as input to the learning system. The use of this dynamic information speeds the training process with respect to other techniques that learn directly either from raw sensors (vision, lidar) or from basic information about obstacle location and kinematics. We propose two approaches using RL and dynamic obstacle velocity (DOVS), RL-DOVS-A, which automatically learns the actions having the maximum utility, and RL-DOVS-D, in which the actions are selected by a human driver. Simulation results and evaluation are presented using different numbers of active agents and static and moving passive agents with random motion directions and velocities in many different scenarios. The performance of the technique is compared with other state-of-the-art techniques for solving navigation problems in environments such as ours.									3	0	0	0	0	0	3				1424-8220										Univ Zaragoza, Aragon Inst Engn Res I3A, E-50009 Zaragoza, Spain				2022-06-06	WOS:000801697000001	35632257	
B	D'Orazio, T.; Cicirelli, G.; Attolico, G.; Distante, C.						Kumar, A.N.; Russell, I.				A reinforcement learning approach for a goal-reaching behavior								Proceedings of the Twelfth International Florida AI Research Society Conference								79	83											Conference Paper	1999	1999	Developing elementary behavior is the starting point for the realization of complex systems. In this paper we will describe a learning algorithm that realizes a simple goal-reaching behavior for an autonomous vehicle when a-priori knowledge of the environment is not provided. The state of the system is based on information received by a visual sensor. A Q-learning algorithm associates the optimal action to each state, developing the optimal state-action rules (optimal policy). A few training trials are sufficient, in simulation, to learn the optimal policy since during the test trials the set of actions is initially limited. The state and action sets are then enlarged, introducing fuzzy variables with their membership functions to the extent of tackling errors in state estimation due to the noise in the vision measurements. Experimental results, both in simulated and real environment, are shown.					Proceedings of 12th International FLAIRS ConferenceProceedings of 12th International FLAIRS Conference	3-5 May 19993-5 May 1999		Orlando, FL, USAOrlando, FL, USA	0	0	0	0	0	0	0					0-57735-080-4									Ist. Elaborazione Segnali ed Immagini, Bari, Italy				1999-01-01	INSPEC:6536209		
J	Chen, I-Ming; Chan, Ching-Yao					Chen, I-Ming/0000-0002-2753-9167					Deep reinforcement learning based path tracking controller for autonomous vehicle								PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING				235	2-3			541	551	0954407020954591			10.1177/0954407020954591					SEP 2020		Article; Early Access		2021	Path tracking is an essential task for autonomous vehicles (AV), for which controllers are designed to issue commands so that the AV will follow the planned path properly to ensure operational safety, comfort, and efficiency. While solving the time-varying nonlinear vehicle dynamic problem is still challenging today, deep neural network (NN) methods, with their capability to deal with nonlinear systems, provide an alternative approach to tackle the difficulties. This study explores the potential of using deep reinforcement learning (DRL) for vehicle control and applies it to the path tracking task. In this study, proximal policy optimization (PPO) is selected as the DRL algorithm and is combined with the conventional pure pursuit (PP) method to structure the vehicle controller architecture. The PP method is used to generate a baseline steering control command, and the PPO is used to derive a correction command to mitigate the inaccuracy associated with the baseline from PP. The blend of the two controllers makes the overall operation more robust and adaptive and attains the optimality to improve tracking performance. In this paper, the structure, settings and training process of the PPO are described. Simulation experiments are carried out based on the proposed methodology, and the results show that the path tracking capability in a low-speed driving condition is significantly enhanced.									22	4	0	0	0	0	25			0954-4070	2041-2991										Univ Calif Berkeley, Calif PATH, 1357 S 46th St,Bldg 452,MC 3580, Richmond, CA 94804 USA				2020-10-01	WOS:000570637100001		
C	Folkers, Andreas; Rick, Matthias; Bueskens, Christof			IEEE	Buskens, Christof/O-9948-2016	Buskens, Christof/0000-0001-7385-4670; Folkers, Andreas/0000-0002-1071-9145					Controlling an Autonomous Vehicle with Deep Reinforcement Learning								2019 30TH IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV19)		IEEE Intelligent Vehicles Symposium						2025	2031											Proceedings Paper	2019	2019	We present a control approach for autonomous vehicles based on deep reinforcement learning. A neural network agent is trained to map its estimated state to acceleration and steering commands given the objective of reaching a specific target state while considering detected obstacles. Learning is performed using state-of-the-art proximal policy optimization in combination with a simulated environment. Training from scratch takes five to nine hours. The resulting agent is evaluated within simulation and subsequently applied to control a full-size research vehicle. For this, the autonomous exploration of a parking lot is considered, including turning maneuvers and obstacle avoidance. Altogether, this work is among the first examples to successfully apply deep reinforcement learning to a real vehicle.					30th IEEE Intelligent Vehicles Symposium (IV)30th IEEE Intelligent Vehicles Symposium (IV)	JUN 09-12, 2019JUN 09-12, 2019	IEEE; Intel; Intempora; Denso; Codeplay; Positics; Easy Mile; Natl Instruments; IAU; Toyota Res InstIEEE; Intel; Intempora; Denso; Codeplay; Positics; Easy Mile; Natl Instruments; IAU; Toyota Res Inst	Paris, FRANCEParis, FRANCE	28	0	0	0	0	0	30			1931-0587		978-1-7281-0560-4									Univ Bremen, Ctr Ind Math, WG Optimizat & Optimal Control, Bremen, Germany				2020-02-05	WOS:000508184100266		
B	Ben Elallid, B.; El Alaoui, H.; Benamar, N.										Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation								2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)								308	13				10.1109/3ICT60104.2023.10391453							Conference Paper	2023	2023	In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforcement learning applications in autonomous driving and highlights the potential of single-agent, cost-effective methods for addressing more complex driving scenarios and advancing reinforcement learning algorithms in the future.					2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)	20232023		Sakhir, BahrainSakhir, Bahrain	0	0	0	0	0	0	0					979-8-3503-0777-1									Moulay Ismail Univ., Meknes, MoroccoAL Akhawayn Univ., Ifrane, Morocco				2024-02-08	INSPEC:24460025		
J	Manchella, Kaushik; Umrawal, Abhishek K.; Aggarwal, Vaneet				Umrawal, Abhishek Kumar/AAO-3802-2021; Aggarwal, Vaneet/A-4843-2017	Umrawal, Abhishek Kumar/0000-0003-4460-7499; Aggarwal, Vaneet/0000-0001-9131-4723					FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers and Goods Transportation								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				22	4			2035	2047				10.1109/TITS.2020.3048361							Article	APR 2021	2021	The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries. On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching. The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems. This article considers combining passenger transportation with goods delivery to improve vehicle-based transportation. We propose FlexPool: a distributed model-free deep reinforcement learning algorithm that jointly serves passengers & goods workloads by learning optimal dispatch policies from its interaction with the environment. The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method. These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods. The dispatching algorithm based on deep reinforcement learning is integrated with an efficient matching algorithm for passengers and goods. Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers & goods. FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers & goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.									37	2	0	0	0	0	39			1524-9050	1558-0016										Purdue Univ, Sch Ind Engn, W Lafayette, IN 47907 USAPurdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA				2021-04-28	WOS:000637194100009		
C	Bcrceanu, Andrei; Leba, Monica; Risteiu, Marius; Mija, Neln; Lonica, Andreea			IEEE	; Ionica, Andreea/B-9657-2015	Mija, Nelu/0000-0002-5974-2848; Ionica, Andreea/0000-0003-1988-9340					Simulation of an Autonomous Car Drive Scenario								2022 7TH INTERNATIONAL CONFERENCE ON MATHEMATICS AND COMPUTERS IN SCIENCES AND INDUSTRY, MCSI								170	174				10.1109/MCSI55933.2022.00034							Proceedings Paper	2022	2022	In everyday life, people are dependent on public or private means of transport. Day by day, this phenomenon leads to a continuous increase in road traffic, a fact that implies the appearance of urban agglomerations, jeopardizing traffic safety, but also a major impact on the environment. Thanks to modern information and communication technologies, these aforementioned problems will be able to be given at least one pertinent advanced solution to lessen the number of transportation related problems we face today. These intelligent transport systems have a wide area of applicability in increasing road safety, minimizing the impact on the environment but also in improving traffic management and maximizing the benefits due to transport. By autonomous vehicle it is understood that the system assists the driver by maintaining a constant speed according to the requirements and conditions of the road segment on which the movement is carried out, by correctly entering the lane without inadvertently leaving it, but also by maintaining a safe distances from surrounding vehicles and obstacles, all this to ensure a safe and incident-free journey. By using a simulation environment such as Carla, we can create an autonomous driving scenario and its training model that we can later use, if it meets all the requirements, on a real vehicle. The simulation allows us to see if the model can be validated or where it still needs improvement without endangering someone's life and without material damage.					7th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)7th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)	AUG 22-24, 2022AUG 22-24, 2022		Athens, GREECEAthens, GREECE	0	0	0	0	0	0	0					978-1-6654-8190-8									Univ Petrosani, Comp & Elect Engn Dept, Petrosani, RomaniaUniv Petrosani, Management Dept, Petrosani, Romania				2023-06-14	WOS:000995248300027		
J	Karunakaran, D.; Worrall, S.; Nebot, E.										Efficient falsification approach for autonomous vehicle validation using a parameter optimisation technique based on reinforcement learning [arXiv]								arXiv								11 pp.	11 pp.											Journal Paper	15 Nov. 2020	2020	The widescale deployment of Autonomous Vehicles (AV) appears to be imminent despite many safety challenges that are yet to be resolved. It is well-known that there are no universally agreed Verification and Validation (VV) methodologies guarantee absolute safety, which is crucial for the acceptance of this technology. The uncertainties in the behaviour of the traffic participants and the dynamic world cause stochastic reactions in advanced autonomous systems. The addition of ML algorithms and probabilistic techniques adds significant complexity to the process for real-world testing when compared to traditional methods. Most research in this area focuses on generating challenging concrete scenarios or test cases to evaluate the system performance by looking at the frequency distribution of extracted parameters as collected from the real-world data. These approaches generally employ Monte-Carlo simulation and importance sampling to generate critical cases. This paper presents an efficient falsification method to evaluate the System Under Test. The approach is based on a parameter optimisation problem to search for challenging scenarios. The optimisation process aims at finding the challenging case that has maximum return. The method applies policy-gradient reinforcement learning algorithm to enable the learning. The riskiness of the scenario is measured by the well established RSS safety metric, euclidean distance, and instance of a collision. We demonstrate that by using the proposed method, we can more efficiently search for challenging scenarios which could cause the system to fail in order to satisfy the safety requirements.									0	0	0	0	0	0	0														Australian Centre for Field Robot., Univ. of Sydney, Sydney, NSW, Australia				2020-11-15	INSPEC:20338505		
J	Samak, T.V.; Samak, C.V.; Krovi, V.										Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem [arXiv]								Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem [arXiv]																				Preprint	2023	2023	This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the approaches in stochastic environments, since the agents were mutually independent and exhibited asynchronous motion behavior. The problems were further aggravated by providing the agents with sparse observation spaces and requiring them to sample control commands that implicitly satisfied the imposed kinodynamic as well as safety constraints. The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training as well as deployment phases.									0	0	0	0	0	0	0																		2023-10-12	INSPEC:23782180		
J	Farooq, Muhammad Shoaib; Khalid, Haris; Arooj, Ansif; Umer, Tariq; Asghar, Aamer Bilal; Rasheed, Jawad; Shubair, Raed M. M.; Yahyaoui, Amani				Farooq, Muhammad Shoaib/AAC-8760-2020; Rasheed, Jawad/AAY-5207-2020	Farooq, Muhammad Shoaib/0000-0002-4095-8868; Rasheed, Jawad/0000-0003-3761-1641; Arooj, Ansif/0000-0001-7116-4545; Umer, Tariq/0000-0002-3333-8142					A Conceptual Multi-Layer Framework for the Detection of Nighttime Pedestrian in Autonomous Vehicles Using Deep Reinforcement Learning								ENTROPY				25	1					135			10.3390/e25010135							Article	JAN 2023	2023	The major challenge faced by autonomous vehicles today is driving through busy roads without getting into an accident, especially with a pedestrian. To avoid collision with pedestrians, the vehicle requires the ability to communicate with a pedestrian to understand their actions. The most challenging task in research on computer vision is to detect pedestrian activities, especially at nighttime. The Advanced Driver-Assistance Systems (ADAS) has been developed for driving and parking support for vehicles to visualize sense, send and receive information from the environment but it lacks to detect nighttime pedestrian actions. This article proposes a framework based on Deep Reinforcement Learning (DRL) using Scale Invariant Faster Region-based Convolutional Neural Networks (SIFRCNN) technologies to efficiently detect pedestrian operations through which the vehicle, as agents train themselves from the environment and are forced to maximize the reward. The SIFRCNN has reduced the running time of detecting pedestrian operations from road images by incorporating Region Proposal Network (RPN) computation. Furthermore, we have used Reinforcement Learning (RL) for optimizing the Q-values and training itself to maximize the reward after getting the state from the SIFRCNN. In addition, the latest incarnation of SIFRCNN achieves near-real-time object detection from road images. The proposed SIFRCNN has been tested on KAIST, City Person, and Caltech datasets. The experimental results show an average improvement of 2.3% miss rate of pedestrian detection at nighttime compared to the other CNN-based pedestrian detectors.									3	0	0	0	0	0	3				1099-4300										Univ Management & Technol, Sch Syst & Technol, Dept Comp Sci, Lahore 54000, PakistanUniv Educ, Dept Informat Sci, Div Sci & Technol, Lahore 54000, PakistanCOMSATS Univ Islamabad, Dept Comp Sci, Lahore Campus, Lahore 54000, PakistanCOMSATS Univ Islamabad, Dept Elect & Comp Engn, Lahore 54000, PakistanNisantasi Univ, Dept Software Engn, TR-34398 Istanbul, TurkiyeNew York Univ NYU, Dept Elect & Comp Engn, Abu Dhabi, U Arab EmiratesIstanbul Sabahattin Zaim Univ, Dept Software Engn, TR-34303 Istanbul, Turkiye	Univ EducNew York Univ NYU			2023-02-05	WOS:000915061100001	36673276	
J	Fu, Yuchuan; Li, Changle; Yu, F. Richard; Luan, Tom H.; Zhang, Yao				Luan, Tom H./HLG-0711-2023						A Selective Federated Reinforcement Learning Strategy for Autonomous Driving								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	2			1655	1668				10.1109/TITS.2022.3219644					NOV 2022		Article; Early Access		2023	Currently, the complex traffic environment challenges the fast and accurate response of a connected autonomous vehicle (CAV). More importantly, it is difficult for different CAVs to collaborate and share knowledge. To remedy that, this paper proposes a selective federated reinforcement learning (SFRL) strategy to achieve online knowledge aggregation strategy to improve the accuracy and environmental adaptability of the autonomous driving model. First, we propose a federated reinforcement learning framework that allows participants to use the knowledge of other CAVs to make corresponding actions, thereby realizing online knowledge transfer and aggregation. Second, we use reinforcement learning to train local driving models of CAVs to cope with collision avoidance tasks. Third, considering the efficiency of federated learning (FL) and the additional communication overhead it brings, we propose a CAVs selection strategy before uploading local models. When selecting CAVs, we consider the reputation of CAVs, the quality of local models, and time overhead, so as to select as many high-quality users as possible while considering resources and time constraints. With above strategic processes, our framework can aggregate and reuse the knowledge learned by CAVs traveling in different environments to assist in driving decisions. Extensive simulation results validate that our proposal can improve model accuracy and learning efficiency while reducing communication overhead.									9	1	0	0	0	0	9			1524-9050	1558-0016										Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R ChinaXidian Univ, Res Inst Smart Transportat, Xian 710071, Shaanxi, Peoples R ChinaCarleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, CanadaXidian Univ, Sch Cyber Engn, Xian 710071, Shaanxi, Peoples R ChinaNorthwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China				2022-11-24	WOS:000881971400001		
J	Kumar, D.; Muhammad, N.				Muhammad, Naveed/HOH-2448-2023	Muhammad, Naveed/0000-0001-5965-1965					A Survey on Localization for Autonomous Vehicles								IEEE Access								115865	83				10.1109/ACCESS.2023.3326069							Journal Paper	2023	2023	Research on autonomous vehicles has made significant advances in recent years. To operate an autonomous vehicle safely and effectively, precise localization is essential. This study aims to present the state of the art in localization to scientists new to the area. It presents and summarizes works from the field of localization and suggests a classification for the works. Approaches to localization are mainly divided into three categories: conventional localization, machine-learning-based localization, and vehicle-to-everything (V2X) localization. Conventional localization primarily depends on high-definition (HD) maps or certain marks, such as landmarks and road marks. Machine-learning-based localization approaches include using neural networks, end-to-end approaches, as well as reinforcement learning for performing or improving localization. Moreover, V2X localization methods localize vehicles by communicating with other vehicles (V2V) or infrastructures (V2I). This study not only presents a bigger picture of the area of localization in autonomous driving but also presents the potentials and drawbacks of different localization methods. At the end of the review, some research areas open for future research are also highlighted.									1	0	0	0	0	0	1			2169-3536											Inst. of Comput. Sci., Univ. of Tartu, Tartu, Estonia				2023-11-09	INSPEC:23944822		
C	Li, Yan; Zhang, Hao; Wang, Zhuping			IEEE	li, jinsong/HJH-9559-2023; Zhang, Hao/HHM-1940-2022						Data-Driven Lateral Fault-tolerance Control of Autonomous Vehicle System Using Reinforcement Learning								2020 IEEE 16TH INTERNATIONAL CONFERENCE ON CONTROL & AUTOMATION (ICCA)		IEEE International Conference on Control and Automation ICCA						1410	1415											Proceedings Paper	2020	2020	In this paper, a data driven lateral fault-tolerant control (LFTC) method is proposed for four wheels drive vehicles, which considering both vehicle velocity and trajectory tracking performance. The novel design of the lateral control employs with zero-sum game and adaptive dynamics programming technique to solve the Riccati equation without requiring the knowledge of system, only using online data. The LFTC consists of the data driven off-policy and adaptive control. The adaptive parameters adjusted online to compensate the actuator faults automatically. The tracking system is asymptotically stable with the disturbance attenuation level gamma. Finally, simulation is provided to show the effectiveness of the proposed method.					16th IEEE International Conference on Control and Automation (ICCA)16th IEEE International Conference on Control and Automation (ICCA)	OCT 09-11, 2020OCT 09-11, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1			1948-3449		978-1-7281-9093-8									Tongji Univ, Dept Control Sci & Engn, Shanghai 200092, Peoples R China				2021-05-28	WOS:000646357300239		
B	Qi Zhu; Zhenhua Huang; Zhenping Sun; Daxue Liu; Bin Dai					Dai, Bin/0000-0001-9405-2626					Reinforcement learning based throttle and brake control for autonomous vehicle following								2017 Chinese Automation Congress (CAC). Proceedings								6657	62				10.1109/CAC.2017.8243976							Conference Paper	2017	2017	In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower. A reinforcement learning based throttle and brake control approach is developed for the follower vehicle. Near optimal control law is directly learned by trial and error with the neural dynamic programming algorithm. According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower. Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim.					2017 Chinese Automation Congress (CAC)2017 Chinese Automation Congress (CAC)	20-22 Oct. 201720-22 Oct. 2017		Jinan, ChinaJinan, China	0	0	0	0	0	0	0					978-1-5386-3524-7									Coll. of Mechatron. Eng. & Autom., Nat. Univ. of Defense Technol., Changsha, China				2017-01-01	INSPEC:17469207		
B	Cai, H.										Pursuit-escape Strategy for Autonomous Agents								2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)								1622	7				10.1109/EEBDA56825.2023.10090700							Conference Paper	2023	2023	The pursuit-escape strategy is the cutting-edge research related to autonomous vehicle. This paper focuses on training 2 agents based on the Q-Learning algorithm model in reinforce learning, in a pursuit-escape game confrontation to solve the game strategy selection problem for a single pursuer and a single escapee in a simulation environment. First of all, the simulation model uses the gym API library provided by the OpenAI research organization to call the pre-defined environment to build the simulation environment. At the same time, the training model of the pursuit-escape game would be built and trained in the cloud computing platform of Baidu PaddlePaddle, and the final training effect that meets the expectation proves that the algorithm is verified. This research also introduces the current interference in to increase the realism of the simulation and optimizes the algorithm related to obstacle judgment. The simulation and experimental results demonstrate that the sensor model can assist in providing "rewards and punishments" in the pursuit process of an agent, which can be applied to improve the autonomous motion strategy selection of the agent.					2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)	24-26 Feb. 202324-26 Feb. 2023		Changchun, ChinaChangchun, China	0	0	0	0	0	0	0					978-1-6654-6253-2									Logistics Eng. Coll., Shanghai Maritime Univ., Shanghai, China				2023-04-28	INSPEC:22932047		
B	Clemmons, J.; Jin, Y.-F.										Reinforcement Learning-Based Guidance of Autonomous Vehicles								2023 24th International Symposium on Quality Electronic Design (ISQED)								1	6				10.1109/ISQED57927.2023.10129362							Conference Paper	2023	2023	Reinforcement learning (RL) has attracted significant research efforts to guide an autonomous vehicle (AV) for a collision-free path due to its advantages in investigating interactions among multiple vehicles and dynamic environments. This study deploys a Deep Q-Network (DQN) based RL algorithm with reward shaping to control an ego AV in an environment with multiple vehicles. Specifically, the state space of the RL algorithm depends on the desired destination, the ego vehicle's location and orientation, and the location of other vehicles in the system. The training time of the proposed RL algorithm is much shorter than most current image-based algorithms. The RL algorithm also provides an extendable framework to include a varying number of vehicles in the environment and can be easily adapted to different maps without changing the setup of the RL algorithm. Three scenarios were simulated in the Cars Learn to Act (CARLA) simulator to examine the effects of the proposed RL algorithm on guiding the ego AV interacting with multiple vehicles on straight and curvy roads. Our results showed that the ego AV could learn to reach its destination within 5000 episodes for all scenarios tested.					2023 24th International Symposium on Quality Electronic Design (ISQED)2023 24th International Symposium on Quality Electronic Design (ISQED)	20232023		San Francisco, CA, USASan Francisco, CA, USA	0	0	0	0	0	0	0					979-8-3503-3475-3									Dept. of Electr. & Comput. Eng., Univ. of Texas at San Antonio, San Antonio, USA				2023-08-12	INSPEC:23163115		
C	Zhu, Zihui; Zhang, Zhengming; Yan, Wen; Huang, Yongming; Yang, Luxi			IEEE							Proactive Caching in Auto Driving Scene via Deep Reinforcement Learning								2019 11TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS AND SIGNAL PROCESSING (WCSP)		International Conference on Wireless Communications and Signal Processing								UNSP 1570568743			10.1109/wcsp.2019.8928131							Proceedings Paper	2019	2019	The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry. The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction. In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network. First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively. Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy. Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.					11th IEEE International Conference on Wireless Communications and Signal Processing (WCSP)11th IEEE International Conference on Wireless Communications and Signal Processing (WCSP)	OCT 23-25, 2019OCT 23-25, 2019		Xian, PEOPLES R CHINAXian, PEOPLES R CHINA	3	0	0	0	0	0	3			2325-3746		978-1-7281-3555-7									Southeast Univ, Sch Informat Sci & Engn, Nanjing, Peoples R China				2020-04-13	WOS:000522040500284		
J	Qian, Lilin; Xu, Xin; Zeng, Yujun; Huang, Junwen				徐, 昕/JNS-1298-2023; Qian, Lilin/X-6353-2018	徐, 昕/0000-0003-3238-745X; Qian, Lilin/0000-0002-6227-9970; Zeng, Yujun/0000-0002-5765-684X					Deep, Consistent Behavioral Decision Making with Planning Features for Autonomous Vehicles								ELECTRONICS				8	12					1492			10.3390/electronics8121492							Article	DEC 2019	2019	Autonomous driving promises to be the main trend in the future intelligent transportation systems due to its potentiality for energy saving, and traffic and safety improvements. However, traditional autonomous vehicles' behavioral decisions face consistency issues between behavioral decision and trajectory planning and shows a strong dependence on the human experience. In this paper, we present a planning-feature-based deep behavior decision method (PFBD) for autonomous driving in complex, dynamic traffic. We used a deep reinforcement learning (DRL) learning framework with the twin delayed deep deterministic policy gradient algorithm (TD3) to exploit the optimal policy. We took into account the features of topological routes in the decision making of autonomous vehicles, through which consistency between decision making and path planning layers can be guaranteed. Specifically, the features of a route extracted from path planning space are shared as the input states for the behavioral decision. The actor-network learns a near-optimal policy from the feasible and safe candidate emulated routes. Simulation tests on three typical scenarios have been performed to demonstrate the performance of the learning policy, including the comparison with a traditional rule-based expert algorithm and the comparison with the policy considering partial information of a contour. The results show that the proposed approach can achieve better decisions. Real-time test on an HQ3 (HongQi the third ) autonomous vehicle also validated the effectiveness of PFBD.									12	1	0	0	0	0	13				2079-9292										Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China				2019-12-01	WOS:000506678200118		
J	Masmitja, I.; Martin, M.; Katija, K.; Gomariz, S.; Navarro, J.										A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles [arXiv]								arXiv																				Journal Paper	17 Jan. 2023	2023	Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems. Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position. Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption. To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms. Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies. The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g. the median predicted error at the beginning of the target's localisation is 17% less. These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles. This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios									0	0	0	0	0	0	0																		2023-03-17	INSPEC:22629038		
J	Lichtle, N.; Jang, K.; Shah, A.; Vinitsky, E.; Lee, J.W.; Bayen, A.M.										Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data [arXiv]								Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data [arXiv]																				Preprint	2024	2024	Designing traffic-smoothing cruise controllers that can be deployed onto autonomous vehicles is a key step towards improving traffic flow, reducing congestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass the common issue of having to carefully fine-tune a large traffic microsimulator by leveraging real-world trajectory data from the I-24 highway in Tennessee, replayed in a one-lane simulation. Using standard deep reinforcement learning methods, we train energy-reducing wave-smoothing policies. As an input to the agent, we observe the speed and distance of only the vehicle in front, which are local states readily available on most recent vehicles, as well as non-local observations about the downstream state of the traffic. We show that at a low 4% autonomous vehicle penetration rate, we achieve significant fuel savings of over 15% on trajectories exhibiting many stop-and-go waves. Finally, we analyze the smoothing effect of the controllers and demonstrate robustness to adding lane-changing into the simulation as well as the removal of downstream information.									0	0	0	0	0	0	0																		2024-02-08	INSPEC:24464585		
J	Everett, Michael; Lutjens, Bjorn; How, Jonathan P.					Everett, Michael/0000-0001-9377-6745; How, Jonathan/0000-0001-8576-1930					Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning								IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS				33	9			4184	4198				10.1109/TNNLS.2021.3056046					FEB 2021		Article; Early Access		2022	Deep neural network-based systems are now state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a deep Q-network (DQN) policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios, a classic control task, and Atari Pong. This article extends our prior work with new performance guarantees, extensions to other reinforcement learning algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.									9	1	0	0	0	0	11			2162-237X	2162-2388										MIT, Aerosp Controls Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA				2021-12-25	WOS:000732352700001	33587714	
B	Gao, L.; Wu, Y.; Wang, L.; Wang, L.; Zhang, J.; Li, K.										End-to-end autonomous vehicle navigation control method guided by the dynamic window approach								2023 IEEE 6th International Electrical and Energy Conference (CIEEC)								4472	6				10.1109/CIEEC58067.2023.10167001							Conference Paper	2023	2023	Existing end-to-end vehicle navigation control methods based on deep reinforcement learning generally have low exploration efficiency and difficulty converging the model to the ideal state. To address these problems, this paper proposes a hybrid reinforcement learning framework that can fuse the traditional path planning algorithm (dynamic window approach, DWA) with the deep reinforcement learning approach. By taking advantage of DWA's ability to plan a collision-free trajectory with guaranteed vehicle dynamics constraints quickly, giving positive guidance to the DRL module at the early stage of its training, thus improving exploration efficiency while ensuring exploration breadth. To verify the effectiveness of the algorithm, a joint CARLA and ROS simulation environment is built and simulated in a typical scenario. The simulation results show that compared with existing deep reinforcement learning methods, the proposed method in this paper has significantly improved in terms of model convergence speed, stability, and pre-mid-term decision performance, in which the training time of TD3 decision network can be shortened by more than 85%.					2023 IEEE 6th International Electrical and Energy Conference (CIEEC)2023 IEEE 6th International Electrical and Energy Conference (CIEEC)	12-14 May 202312-14 May 2023		Hefei, ChinaHefei, China	0	0	0	0	0	0	0					979-8-3503-4667-1									Chinese Acad. of Sci., Beijing, ChinaSch. of Vehicles & Mobility, Tsinghua Univ., Beijing, China				2023-07-28	INSPEC:23419959		
C	Yang, Fan; Wang, Ping; Wang, XinHong			ACM	zhang, zhi/HPH-4905-2023; Wang, Xin/HZL-4695-2023; Wang, Xin/GYU-1129-2022						Continuous Control in Car Simulator with Deep Reinforcement Learning								PROCEEDINGS OF 2018 THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2018) / 2018 THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND MULTIMEDIA TECHNOLOGY (ICIMT 2018)								566	570				10.1145/3297156.3297217							Proceedings Paper	2018	2018	Deep reinforcement learning (DRL), which can be trained without abundant labeled data required in supervised learning, plays an important role in autonomous vehicle researches. According to action space, DRL can be further divided into two classes: discrete domain and continuous domain. In this work, we focus on continuous steering control since it's impossible to switch among different discrete steering values at short intervals in reality. We first define the steering smoothness to quantify the degree of continuity. Then we propose a new penalty in reward shaping. We carry experiments based on Deep Deterministic Policy Gradient (DDPG) and Asynchronous Advantage Actor Critic (A3C), which are the state of the art in continuous domain. Results show that the proposed penalty improves the steering smoothness with both algorithms.					2nd International Conference on Computer Science and Artificial Intelligence (CSAI) / 10th International Conference on Information and Multimedia Technology (ICIMT)2nd International Conference on Computer Science and Artificial Intelligence (CSAI) / 10th International Conference on Information and Multimedia Technology (ICIMT)	DEC 08-10, 2018DEC 08-10, 2018	Shenzhen Univ, Coll Comp Sci & Software Engn; Kalbis InstShenzhen Univ, Coll Comp Sci & Software Engn; Kalbis Inst	Shenzhen, PEOPLES R CHINAShenzhen, PEOPLES R CHINA	1	0	0	0	0	0	4					978-1-4503-6606-9									Tongji Univ, Sch Elect & Informat Engn, Caoan Rd 4800, Shanghai, Peoples R China				2019-06-14	WOS:000469786300109		
B	Guo, H.; Li, G.; Liu, J.; Tan, Z.; Yu, D.; Zhang, Y.										Autonomous Vehicle Path Tracking Control based on Adaptive Dynamic Programming								2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	6				10.1109/CVCI59596.2023.10397352							Conference Paper	2023	2023	This paper presents a path tracking control method for autonomous vehicles by utilizing Adaptive Dynamic Programming (ADP). The primary goal is to enhance the path tracking performance under various longitudinal speeds. The proposed approach incorporates the consideration of nonlinear tire characteristics and establishes a nonlinear vehicle system model based on the fundamental principles of vehicle dynamics. By formulating a performance index function that captures the correlation between the vehicle's position and desired paths, the optimal control strategy is obtained using the Dual Heuristic Programming algorithm (DHP), known for its computational efficiency, within the ADP framework. The simulation results verify the effectiveness of the proposed path tracking control scheme, which can have better control performance under different vehicle speeds.					2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)	20232023		Changsha, ChinaChangsha, China	0	0	0	0	0	0	0					979-8-3503-4048-8									State Key Lab. of Automotive Simulation & Control & the Coll. of Commun. Eng., Jilin Univ., Changchun, ChinaJilin Univ. Coll. of Commun. Eng., Changchun, ChinaJilin Univ. State Key Lab. of Automotive Simulation & Control, Changchun, ChinaSAIC Motor R&D Innovation Headquarter, Shanghai, China				2024-02-15	INSPEC:24508366		
C	Cao, Mingcong; Chen, Jiayi; Wang, Junmin			IEEE	chen, jiayi/IAP-9353-2023; Chen, Jia/HQZ-3908-2023; Chen, Jiayi/GZM-5106-2022; chen, jia/JDW-7660-2023; chen, jia/JLM-4733-2023						A Novel Vehicle Tracking Method for Cross-Area Sensor Fusion with Reinforcement Learning Based GMM								2020 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						442	447				10.23919/acc45564.2020.9147318							Proceedings Paper	2020	2020	Radars, LiDARs and cameras have been widely adopted in autonomous driving applications due to their complementary capabilities of environment perception. However, one problem lies in how to effectively improve the cross-area tracking accuracy with massive data from multiple sensors. This paper proposes a novel tracking solution that is composed of a reinforcement-learning-based Gaussian mixture model (GMM), submodel center realignment, and data-driven trajectory association. Specifically, developed with a Q-learning-based cluster number, an improved GMM-EM algorithm is firstly investigated to cluster the dense short-range radar data points. Subsequently, an innovative kinetic-energy-aware approach is presented to realign the Q-learning GMM cluster centers for position error mitigation. In addition to Q-learning GMM clustering, a weight-scheduled method is presented to associate the data from a long-range radar and cameras for cross-area object extraction and trajectory fusion. Eighteen experiments for training and one experiment for verification were conducted on a fully-instrumented autonomous vehicle. Experimental results demonstrate that a better tracking performance in crossing detection areas can be achieved by the proposed method.					American Control Conference (ACC)American Control Conference (ACC)	JUL 01-03, 2020JUL 01-03, 2020	Amer Automat Control Council; Int Federat Automat ControlAmer Automat Control Council; Int Federat Automat Control	Denver, CODenver, CO	8	1	0	0	0	0	9			0743-1619	2378-5861	978-1-5386-8266-1									Southeast Univ, Sch Mech Engn, Nanjing 211189, Peoples R ChinaUniv Texas Austin, Austin, TX 78712 USAUniv Michigan, Elect Engn & Comp Sci Dept, Ann Arbor, MI 48109 USAUniv Texas Austin, Dept Mech Engn, Austin, TX 78712 USA				2021-03-02	WOS:000618079800065		
B	Jamgochian, A.; Buehrle, E.; Fischer, J.; Kochenderfer, M.J.										SHAIL: Safety-Aware Hierarchical Adversarial Imitation Learning for Autonomous Driving in Urban Environments								2023 IEEE International Conference on Robotics and Automation (ICRA)								1530	6				10.1109/ICRA48891.2023.10161449							Conference Paper	2023	2023	Designing a safe and human-like decision-making system for an autonomous vehicle is a challenging task. Generative imitation learning is one possible approach for automating policy-building by leveraging both real-world and simulated decisions. Previous work that applies generative imitation learning to autonomous driving policies focuses on learning a low-level controller for simple settings. However, to scale to complex settings, many autonomous driving systems combine fixed, safe, optimization-based low-level controllers with high-level decision-making logic that selects the appropriate task and associated controller. In this paper, we attempt to bridge this gap in complexity by employing Safety-Aware Hierarchical Adversarial Imitation Learning (SHAIL), a method for learning a high-level policy that selects from a set of low-level controller instances in a way that imitates low-level driving data on-policy. We introduce an urban roundabout simulator that controls non-ego vehicles using real data from the Interaction dataset. We then demonstrate empirically that even with simple controller options, our approach can produce better behavior than previous approaches in driver imitation that have difficulty scaling to complex environments. Our implementation is available at https://github.com/sisl/InteractionImitation.					2023 IEEE International Conference on Robotics and Automation (ICRA)2023 IEEE International Conference on Robotics and Automation (ICRA)	29 May-2 June 202329 May-2 June 2023		London, UKLondon, UK	0	0	0	0	0	0	0					979-8-3503-2365-8									Stanford Univ., Stanford, CA, USAKarlsruhe Inst. of Technol., Karlsruhe, Germany				2023-07-20	INSPEC:23387658		
C	Kang, Liuwang; Shen, Haiying			IEEE	shen, haiying/AAA-4995-2021						A Reinforcement Learning based Decision-making System with Aggressive Driving Behavior Consideration for Autonomous Vehicles								2021 18TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON SENSING, COMMUNICATION, AND NETWORKING (SECON)		Annual IEEE International Conference on Sensing, Communication, and Networking Workshops											10.1109/SECON52354.2021.9491587							Proceedings Paper	2021	2021	With the fast development of autonomous vehicle (AV) technology and possible popularity of AVs in the near future, a mixed-vehicle type driving environment where both AVs and their surrounding human-driving vehicles drive on the same road will exist and last for a long time. An AV measures its driving environments in real time and make control decisions to ensure driving safety. However, surrounding human-driving vehicles may conduct aggressive driving behaviors (e.g., sudden deceleration, sudden acceleration, sudden left or right lane change) in practice, which requires an AV to make correct control decisions to eliminate the effect of aggressive driving behaviors on its driving safety. In this paper, we propose a rein-inrcement learning based decision-making system (ReDS) which considers aggressive driving behaviors of surrounding human-driving vehicles during the decision making process. In ReDS, we firstly build a mixture density network based aggressive driving behavior detection method to detect possible aggressive driving behaviors among surrounding vehicles of an AV. We then build a reward function based on aggressive driving behavior detection results and incorporate the reward function into a reinforcement learning model to make optimal control decisions considering aggressive driving behaviors. We use a real-world traffic dataset from the United States Department of Transportation Federal Highway Administration to evaluate optimal control decision determination performance of ReDS in comparison with the state-of-the-art methods. The comparison results show that ReDS can improve optimal control decision success rate by 43% compared with existing methods, which demonstrates that ReDS has good optimal control decision determination performance.					18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)	JUL 06-09, 2021JUL 06-09, 2021	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			2473-0440		978-1-6654-4108-7									Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA				2021-01-01	WOS:000709091000009		
J	Mo, Shuojie; Pei, Xiaofei; Wu, Chaoxian				Wu, Chaoxian/AAT-1048-2020	Wu, Chaoxian/0000-0003-3384-8916					Safe Reinforcement Learning for Autonomous Vehicle Using Monte Carlo Tree Search								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	7			6766	6773				10.1109/TITS.2021.3061627					MAR 2021		Article; Early Access		2022	Reinforcement learning has gradually demonstrated its decision-making ability in autonomous driving. Reinforcement learning is learning how to map states to actions by interacting with environment so as to maximize the long-term reward. Within limited interactions, the learner will get a suitable driving policy according to the designed reward function. However there will be a lot of unsafe behaviors during training in traditional reinforcement learning. This paper proposes a RL-based method combined with RI, agent and Monte Carlo tree search algorithm to reduce unsafe behaviors. The proposed safe reinforcement learning framework mainly consists of two modules: risk state estimation module and safe policy search module. Once the future state will he risky calculated by the risk state estimation module using current state information and the action outputted by the RL agent, the MCTS based safe policy search module will activate to guarantee a safer exploration by adding an additional reward for risk actions. We test the approach in several random overtake scenarios, resulting in faster convergence and safer behaviors compared to traditional reinforcement learning.									24	1	0	0	0	0	26			1524-9050	1558-0016										Wuhan Univ Technol, Dept Automot Engn, Wuhan 430070, Peoples R ChinaWuhan Univ Technol, Hubei Res Ctr New Energy & Intelligent Connected, Wuhan 430070, Peoples R ChinaWuhan Univ Technol, Hubei Key Lab Adv Technol Automot Components, Wuhan 430070, Peoples R China				2021-12-25	WOS:000732210800001		
J	de Morais, Gustavo A. P.; Marcos, Lucas B.; Bueno, Jose Nuno A. D.; de Resende, Nilo F.; Terra, Marco Henrique; Grassi Jr, Valdir				Grassi Jr, Valdir/I-7595-2014	Grassi Jr, Valdir/0000-0001-6753-139X; Marcos, Lucas Barbosa/0000-0003-1857-8938; Terra, Marco Henrique/0000-0002-4477-1769					Vision-based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles								CONTROL ENGINEERING PRACTICE				104						104630			10.1016/j.conengprac.2020.104630							Article	NOV 2020	2020	Given the recent advances in computer vision, image processing and control systems, self-driving vehicles has been one of the most promising and challenging research topics nowadays. The design of vision -based robust controllers to keep an autonomous car in the center of the lane, despite uncertainties and disturbances, is still an ongoing challenge. This paper presents a hybrid control architecture that combines Deep Reinforcement Learning (DRL) and Robust Linear Quadratic Regulator (RLQR) for vision-based lateral control of an autonomous vehicle. Evolutionary estimation is used to model the vehicle uncertainties. For performance comparison, a DRL method and three other hybrid controllers are also evaluated. The inputs for each controller are real-time semantically segmented RGB camera images which serve as the basis to calculate continuous steering actions to keep the vehicle on the center of the lane with a constant velocity. Simulation results show that the proposed hybrid RLQR with evolutionary estimation of uncertainties architecture outperforms the other algorithms implemented. It presents lower tracking errors, smoother steering inputs, total collision avoidance and better generalization in new urban environments. Furthermore, it significantly decreases the required training time.									21	0	0	0	0	0	21			0967-0661	1873-6939										Univ Sao Paulo, Sao Carlos Sch Engn, Dept Elect & Comp Engn, Sao Carlos, BrazilUniv Sao Paulo, Sao Carlos Sch Engn, Dept Mech Engn, Sao Carlos, Brazil				2020-10-30	WOS:000579014700003		
C	Landgraf, Daniel; Voelz, Andreas; Kontes, Georgios; Mutschler, Christopher; Graichen, Knut					Landgraf, Daniel/0000-0002-0698-0666					Hierarchical Learning for Model Predictive Collision Avoidance								IFAC PAPERSONLINE				55	20			355	360				10.1016/j.ifacol.2022.09.121					SEP 2022		Proceedings Paper; Early Access		2022	Recent progress in model predictive control (MPC) has shown great potential to control complex nonlinear systems in real-time. However, if parts of the controlled system cannot be modeled exactly by differential equations, the performance of MPC can decrease significantly. This paper approaches this problem by combining MPC with deep reinforcement learning (DRL) to a hierarchical control system, which is applied to control the motion of an autonomous vehicle. While the DRL algorithm is responsible for the decision-making with regard to obstacles on the street, the model predictive controller deals with the nonlinear dynamics of the vehicle. To this end, the vehicle dynamics are modeled by differential equations and the decision-making problem is modeled as a Markov decision process (MDP). The decisions are considered in the optimization problem of the controller, whose cost function, in turn, is considered in the reward function of the MDP. The performance of the hierarchical vehicle control is evaluated in scenarios with static and moving obstacles. Furthermore, it is examined whether adding information about the predicted trajectory to the state space of the MDP can increase the convergence speed. Copyright (C) 2022 The Authors.					10th Vienna International Conference on Mathematical Modelling (MATHMOD)10th Vienna International Conference on Mathematical Modelling (MATHMOD)	JUL 27-29, 2022JUL 27-29, 2022	Int Federat Automat Control; VDI VDE Soc Measurement & Automat Control; ARGESIM Arbeitsgemeinschaft Simulat; Osterreichische Gesell Mess Automatisierung & Robotertechnik; Gesell Angewandte Mathematik &Mechanik; Oesterreichische Comp Gesel; Osterreichische Mathematische Gesell; EUROSIM Federat European Simulat Soc; Osterreichischer Verband Elektrotechnik; ASIM Arbeitsgemeinschaft Simulat; MathWorks Inc; Vienna Convent BurInt Federat Automat Control; VDI VDE Soc Measurement & Automat Control; ARGESIM Arbeitsgemeinschaft Simulat; Osterreichische Gesell Mess Automatisierung & Robotertechnik; Gesell Angewandte Mathematik &Mechanik; Oesterreichische Comp Gesel; Osterreichische Mathematische Gesell; EUROSIM Federat European Simulat Soc; Osterreichischer Verband Elektrotechnik; ASIM Arbeitsgemeinschaft Simulat; MathWorks Inc; Vienna Convent Bur	Tech Univ Wien, ELECTR NETWORKTech Univ Wien, ELECTR NETWORK	0	0	0	0	0	0	0			2405-8963											Friedrich Alexander Univ Erlangen Nurnberg, Chair Automat Control, Erlangen, GermanyFraunhofer Inst Integrated Circuits IIS, Nurnberg, Germany				2022-10-19	WOS:000860842100060		
J	Di, Xuan; Shi, Rongye				Shi, Rongye/D-2035-2011	Shi, Rongye/0000-0003-4298-9358; Di, Xuan/0000-0003-2925-7697					A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				125						103008			10.1016/j.trc.2021.103008					MAR 2021		Article; Early Access		2021	This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV). It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions. We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AVdominated, and the pure AVs. This paper is primarily focused on the latter three phases. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? We also provide a list of public datasets and simulation software related to AVs. Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.									79	0	0	0	2	0	82			0968-090X	1879-2359										Columbia Univ, Dept Civil Engn & Engn Mech, New York, NY 10027 USAColumbia Univ, Ctr Smart Cities, Data Sci Inst, New York, NY USA				2021-03-11	WOS:000636098100006		
C	Chakraborty, Soumyajit; Kumar, Subhadeep; Bhatt, Nirav; Pasumarthy, Ramkrishna			IEEE							End-to-end Autonomous Driving in Heterogeneous Traffic Scenario Using Deep Reinforcement Learning								2023 EUROPEAN CONTROL CONFERENCE, ECC																				Proceedings Paper	2023	2023	In this paper, we propose an end-to-end autonomous driving architecture for safe maneuvering in heterogeneous traffic using a reinforcement learning (RL) algorithm. Using the proposed architecture we develop an RL agent that can make driving decisions directly from the sensor data. We formulate the autonomous driving problem as a Markov Decision Process and propose different architectures using Deep Q-Networks for two types of sensor data - top view images of the autonomous vehicle (AV) and its surrounding vehicles and information on relative position and velocities of the surrounding vehicles w.r.t the AV. We consider a highway scenario and analyze the performance of the RL agent using the proposed architectures using the highway-env simulator. We compare the driving performance of the AV for both sensor types and discuss their efficacy under varying traffic densities.					European Control Conference (ECC)European Control Conference (ECC)	JUN 13-16, 2023JUN 13-16, 2023		Bucharest, ROMANIABucharest, ROMANIA	0	0	0	0	0	0	0					978-3-907144-08-4									Indian Inst Technol Madras, Dept Elect Engn, Chennai 600036, IndiaIndian Inst Technol Madras, Dept Elect Engn, Robert Bosch Ctr Data Sci & Artificial Intelligen, Chennai 600036, IndiaIndian Inst Technol Madras, Prospect Ctr Excellence Network Syst Learning, Robert Bosch Ctr Data Sci & Artificial Intelligen, Dept Biotechnol,Control & Evolut Grp, Chennai 600036, IndiaIndian Inst Technol Madras, Dept Elect Engn, Robert Bosch Ctr Data Sci & Artificial Intelligen, Chennai 600036, IndiaIndian Inst Technol Madras, Prospect Ctr Excellence Network Syst Learning, Control & Evolut Grp, Chennai 600036, India				2023-09-01	WOS:001035589000130		
C	Jude, M. Joseph Auxilius; Diniesh, V. C.; Kumar, Prathap K.; Rahul, S.; Kumar, Nithish S.; Shanjeev, E. N.			IEEE	Jude, M. Joseph Auxilius/L-7563-2019	Jude, M. Joseph Auxilius/0000-0001-7684-4435					An Improved Retransmission Timeout Forecasting Algorithm for Vehicular Networks								2022 SECOND INTERNATIONAL CONFERENCE ON ADVANCES IN ELECTRICAL, COMPUTING, COMMUNICATION AND SUSTAINABLE TECHNOLOGIES (ICAECT)													10.1109/ICAECT54875.2022.9807985							Proceedings Paper	2022	2022	The vehicular relay network remains a promising path of future intelligent transportation systems ( ITS) that connects the autonomous vehicle with the internet for better traffic control and information sharing. The transmission control protocol (TCP) remains the backbone of data traffic on the internet as it harbors an enormous portion of global internet traffic between end devices. In VANET, TCP experiences drastic performance degradation during early or spurious RTO timeouts. This paper proposes a recursive learning retransmission timeout (RL-RTO), which efficiently reduces spurious timeouts and enhances fast recovery after timeouts. The RL-RTO forecast timer value is based on the three control parameters. The performance of RL-RTO is validated against recent RTO approaches under multi-hop vehicular environments. The proposed RL-RTO considerably regulates estimation errors and improves variance.					2nd IEEE International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (IEEE ICAECT)2nd IEEE International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (IEEE ICAECT)	APR 21-22, 2022APR 21-22, 2022		Shri Shankaracharya Grp Inst, Bhilai, INDIAShri Shankaracharya Grp Inst, Bhilai, INDIA	0	0	0	0	0	0	0					978-1-6654-1120-2									Kongu Engn Coll, Dept ECE, Perundurai, India				2022-09-17	WOS:000848762200136		
B	Sun, Q.; Zhang, L.; Yu, H.; Zhang, W.; Mei, Y.; Xiong, H.					Zhang, Le/0000-0003-0894-9651					Hierarchical Reinforcement Learning for Dynamic Autonomous Vehicle Navigation at Intelligent Intersections								KDD '23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining								4852	61				10.1145/3580305.3599839							Conference Paper	2023	2023	Recent years have witnessed the rapid development of the Cooperative Vehicle Infrastructure System (CVIS), where road infrastructures such as traffic lights (TL) and autonomous vehicles (AVs) can share information among each other and work collaboratively to provide safer and more comfortable transportation experience to human beings. While many efforts have been made to develop efficient and sustainable CVIS solutions, existing approaches on urban intersections heavily rely on domain knowledge and physical assumptions, preventing them from being practically applied. To this end, this paper proposes NavTL, a learning-based framework to jointly control traffic signal plans and autonomous vehicle rerouting in mixed traffic scenarios where human-driven vehicles and AVs co-exist. The objective is to improve travel efficiency and reduce total travel time by minimizing congestion at the intersections while guiding AVs to avoid the temporally congested roads. Specifically, we design a graph-enhanced multi-agent decentralized bi-directional hierarchical reinforcement learning framework by regarding TLs as manager agents and AVs as worker agents. At lower temporal resolution timesteps, each manager sets a goal for the workers within its controlled region. Simultaneously, managers learn to take the signal actions based on the observation from the environment as well as an intention information extracted from its workers. At higher temporal resolution timesteps, each worker makes rerouting decisions along its way to the destination based on its observation from the environment, an intention-enhanced manager state representation, and a goal from its present manager. Finally, extensive experiments on one synthetic and two real-world network-level datasets demonstrate the effectiveness of our proposed framework in terms of improving travel efficiency.					KDD '23: The 29th ACM SIGKDD Conference on Knowledge Discovery and Data MiningKDD '23: The 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining	6-10 Aug. 20236-10 Aug. 2023	SIGMOD; SIGKDDSIGMOD; SIGKDD	Long Beach, CA, USALong Beach, CA, USA	0	0	0	0	0	0	0					979-8-4007-0103-0									Hong Kong Univ. of Sci. & Technol., Hong Kong, ChinaBaidu Res., Beijing, ChinaHong Kong Univ. of Sci. & Technol., Guangzhou, ChinaBaidu Inc., Beijing, China				2024-01-11	INSPEC:24293943		
B	E, S.; Krishnan, M.S.; S, K.; Tanguturi, R.C.; Arunagirinathan, S.; Gunasekaran, K.N.; Ramkumar, M.S.				Ramkumar, Dr.M.Siva/S-4935-2017; tanguturi, rama chaithanya/GQH-9493-2022	Ramkumar, Dr.M.Siva/0000-0001-7264-4786; tanguturi, rama chaithanya/0000-0002-9923-7360					Reinforcement Learning Based Autonomous E-Vehicle Speed Control								2023 International Conference on Inventive Computation Technologies (ICICT)								192	8				10.1109/ICICT57646.2023.10134037							Conference Paper	2023	2023	Greater efficiency in both energy use and traffic flow are two benefits of autonomous self-driving cars. Due to their superior performance, effectiveness, and lack of carbon emission, electric vehicles (EVs) have recently become popular and used in an autonomous vehicle. As EVs are making a splash in the automotive industry, researchers are taking a greater interest in studying, modelling, and simulating them. Controlling EV speed is not an easy task. Modelling and Conventional Proportional Integral Derivative (PID) controller tuning for EV speed regulation are presented in this paper. To design and simulation of EV speed control in MATLAB, the transfer function of EV is derived and considered. PID controller is found to be easy to implement, practical, and provide superior closed-loop performance. Two PID tuning techniques, Ziegler-Nichols (ZN) and a Reinforcement Learning (RL) method are designed to regulate the EV speed. Characteristics of the time domain have been used to perform a comparative analysis. Additionally, the Integral Square Error (ISE) is analysed to determine the optimal PID tuning strategy for EV speed regulation.					2023 International Conference on Inventive Computation Technologies (ICICT)2023 International Conference on Inventive Computation Technologies (ICICT)	26-28 April 202326-28 April 2023		Lalitpur, NepalLalitpur, Nepal	0	0	0	0	0	0	0					979-8-3503-9849-6									Dept. of Inf. Technol., Mahendra Inst. of Technol. (Autonomo us), Namakkal, IndiaDept. of EEE, Karpagam Coll. of Eng., Coimbatore, IndiaDept. of Electr. & Electron. Eng., Coimbatore, IndiaDept. of Comput. Sci. & Eng., Pace Inst. of Technol. & Sci., Ongole, IndiaDept. of Electr. & Electron. Eng., Gov. Coll. of Technol., Coimbatore, IndiaDept. of Mech. Eng., Sri Krishna Coll. of Eng. & Technol., Coimbatore, IndiaDept of EEE, Karpagam Acad. of Higher Educ., Coimbatore, India				2023-06-15	INSPEC:23203968		
B	Zhiqian Qiao; Schneider, J.; Dolan, J.M.										Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning*								2021 IEEE International Conference on Robotics and Automation (ICRA)								2667	73				10.1109/ICRA48506.2021.9561095							Conference Paper	2021	2021	For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure [1] allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.					2021 IEEE International Conference on Robotics and Automation (ICRA)2021 IEEE International Conference on Robotics and Automation (ICRA)	30 May-5 June 202130 May-5 June 2021		Xi'an, ChinaXi'an, China	0	0	0	0	0	0	0					978-1-7281-9077-8									Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA				2021-12-22	INSPEC:21257341		
C	Zhu, Qi; Huang, Zhenhua; Sun, Zhenping; Liu, Daxue; Dai, Bin			IEEE							Reinforce lent Learning based Throttle and Brake Control for Autonomous Vehicle Following								2017 CHINESE AUTOMATION CONGRESS (CAC)		Chinese Automation Congress						6657	6662											Proceedings Paper	2017	2017	In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower. A reinforcement learning based throttle and brake control approach is developed for the follower vehicle. Near optimal control law is directly learned by "trial and error" with the neural dynamic programming algorithm. According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower. Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim.					Chinese Automation Congress (CAC)Chinese Automation Congress (CAC)	OCT 20-22, 2017OCT 20-22, 2017	IEEE; CAA; IEEE Syst Man & Cybernet SocIEEE; CAA; IEEE Syst Man & Cybernet Soc	Jinan, PEOPLES R CHINAJinan, PEOPLES R CHINA	6	0	0	0	0	0	6			2688-092X	2688-0938	978-1-5386-3524-7									Natl Univ Def Technol, Coll Mechatron Engn & Automat, Changsha, Hunan, Peoples R China				2018-04-27	WOS:000427816106133		
J	Chen, Longquan; He, Ying; Wang, Qiang; Pan, Weike; Ming, Zhong					chen, longquan/0000-0002-4340-1360					Joint Optimization of Sensing, Decision-Making and Motion-Controlling for Autonomous Vehicles: A Deep Reinforcement Learning Approach								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				71	5			4642	4654				10.1109/TVT.2022.3150793							Article	MAY 2022	2022	The three main modules of autonomous vehicles, i.e., sensing, decision making, and motion controlling, have been studied separately in most existing works on autonomous driving, which overlook the correlations among these modules, leading to a result of unsatisfactory performance. In this paper, we propose a novel scheme that first tactfully processes the sensing data, then jointly learns and optimizes the decision-making and motion-controlling using reinforcement learning (RL). Specifically, the proposed scheme designs a novel state representation mechanism, where the sensing data goes through the attention layer and the convolutional neural network (CNN) layer sequentially. The attention layer focuses on extracting the most important local information and then CNN layer takes a broad view to comprehensively consider the global information for a better representation. Furthermore, the proposed scheme jointly learns decision-making and motion-controlling, therefore, the relevance of these two modules is implicitly considered, which helps to achieve a better autonomous driving policy. Extensive simulation results show that the proposed scheme is better than classic control methods and some RL methods in terms of safety, velocity, etc. We also demonstrate the respective functions of the attention layer and the CNN layer through ablation studies. Finally, we construct a traffic scene with a real autonomous vehicle, and verified the feasibility of the proposed scheme.									18	0	0	0	0	0	18			0018-9545	1939-9359										Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R ChinaChinese Acad Mil Sci, Inst Syst Engn, Natl Key Lab Sci & Technol Informat Syst Secur, Beijing 100864, Peoples R China	Chinese Acad Mil Sci			2022-06-10	WOS:000799654900014		
J	Fu, Yuchuan; Li, Changle; Yu, Fei Richard; Luan, Tom H.; Zhang, Yao				Luan, Tom/ABA-2407-2021; Luan, Tom H./HLG-0711-2023; Yu, F. Richard/B-3182-2018	Luan, Tom/0000-0002-5215-7443; Zhang, Yao/0000-0002-1276-5399					A Decision-Making Strategy for Vehicle Autonomous Braking in Emergency via Deep Reinforcement Learning								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				69	6			5876	5888				10.1109/TVT.2020.2986005							Article	JUN 2020	2020	Autonomous braking through vehicle precise decision-making and control to reduce accidents is a key issue, especially in the early diffusion phase of autonomous vehicle development. This paper proposes a deep reinforcement learning (DRL)-based autonomous braking decision-making strategy in an emergency situation. Three key influencing factors, including efficiency, accuracy and passengers' comfort, are fully considered and satisfied by the proposed strategy. First, the vehicle lane-changing process and the braking process are analyzed in detail, which include the critical factors in the design of the autonomous braking strategy. Second, we propose a DRL process that determines the optimal strategy for autonomous braking. Particularly, a multi-objective reward function is designed, which can compromise the rewards achieved of different brake moments, the degree of the accident, and the comfort of the passenger. Third, a typical actor-critic (AC) algorithm named deep deterministic policy gradient (DDPG) is adopted for solving the autonomous braking problem, which can improve the efficiency of the optimal strategy and be stable in continuous control tasks. Once the strategy is well trained, the vehicle can automatically take optimal braking behavior in an emergency to improve driving safety. Extensive simulations validate the effectiveness and efficiency of our proposal in terms of learning effectiveness, decision-making accuracy and driving safety.									63	5	0	0	1	0	69			0018-9545	1939-9359										Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R ChinaCarleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, CanadaXidian Univ, Sch Cyber Engn, Xian 710071, Shaanxi, Peoples R China				2020-09-01	WOS:000558744200013		
J	김문종; 최기창; 오병화; 양지훈										Local Path Generation Method for Unmanned Autonomous Vehicles Using Reinforcement Learning			강화학습을 이용한 무인 자율주행 차량의 지역경로 생성 기법					KIPS Transactions on Software and Data Engineering	정보처리학회논문지. 소프트웨어 및 데이터 공학			3	9			369	374				10.3745/KTSDE.2014.3.9.369							research-article	2014	2014	Path generation methods are required for safe and efficient driving in unmanned autonomous vehicles. There are two kinds ofpaths: global and local. A global path consists of all the way points including the source and the destination. A local path is thetrajectory that a vehicle needs to follow from a way point to the next in the global path. In this paper, we propose a novel methodfor local path generation through machine learning, with an effective curve function used for initializing the trajectory. First,reinforcement learning is applied to a set of candidate paths to produce the best trajectory with maximal reward. Then the optimalsteering angle with respect to the trajectory is determined by training an artificial neural network. Our method outperformed existingapproaches and successfully found quality paths in various experimental settings, including the cases with obstacles.				무인 자율주행 차량에서의 경로 생성 기법은 차량이 자동적으로 안전하고 효율적인 경로를 생성하고 주행할 수 있도록 해 준다. 경로에는 크게 전역경로와 지역경로가 있다. 전역경로는 차량이 출발점으로부터 도착점까지 가기 위해 주행해야 하는 구간을, 지역경로는 전역경로에서 얻은 구간을 주행하기 위해서 차량이 실제로 주행해야 할 경로를 의미한다. 본 논문에서는 지역경로 생성을 위하여 효율성 높은 곡선 함수를 사용하는 기존연구에서 더 나아가 학습을 통해 경로를 생성하는 방법을 제안한다. 먼저 강화학습을 통해서 후보경로에 대한 예측 보상 값을 얻고 보상 값이 최고가 되는 경로를 찾는 작업을 한다. 또한 인공 신경망을 통해서는 생성된 경로에 최적화된 조향 명령을 주기 위해 조향 각을 학습하는 작업을 한다. 더 나아가 주행하는 경로에 장애물이 발견되더라도 이를 효율적으로 회피하는 최적의 경로를 학습기법을 통해 만들어낸다. 본 논문에서 제안된 알고리즘의 우수성은 실제 주행 환경으로 모델링한 시뮬레이션 실험을 통해 검증되었다.					1	0	0	0	0	0	1			2287-5905															2014-01-01	KJD:ART001918161		
J	Guan, Jiayi; Chen, Guang; Huang, Jin; Li, Zhijun; Xiong, Lu; Hou, Jing; Knoll, Alois				Knoll, Alois/AAN-8417-2021; Guan, Jiayi/HDO-3257-2022	Knoll, Alois/0000-0003-4840-076X; Hou, Jing/0000-0003-4778-137X; Huang, Jin/0000-0001-8774-2936					A Discrete Soft Actor-Critic Decision-Making Strategy With Sample Filter for Freeway Autonomous Driving								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				72	2			2593	2598				10.1109/TVT.2022.3212996							Article	FEB 2023	2023	Autonomous driving is a promising technology to reduce traffic accidents and improve driving efficiency. Although significant progress has been achieved, existing decision-making systems of autonomous vehicle still cannot meet the safety and driving efficiency requirements in highly dynamic environments. In this work, we design a discrete decision-making strategy based on the discrete soft actor-critic with sample filter algorithm (DSAC-SF) to improve driving efficiency and safety on freeways with dynamics traffic. Specifically, we first propose a sample filter method for discrete soft actor-critic, which improves the sample efficiency and stability of the algorithm via enhancing the utilization of effective samples. Subsequently, we construct the discrete decision-making strategy for autonomous driving based on the DSAC-SF algorithm, and further design the area observation method and the multi-objective reward function to improve the driving safety and efficiency. Finally, we carry out comparison and ablation experiments on the the scalable multi-agent reinforcement learning training school (SMARTS) simulation environment. Experimental results indicate that our strategy obtains a high success rate and a fast vehicle speed in the decision-making tasks on freeways. Moreover, our DSAC-SF algorithm also achieves improved performance in training efficiency and stability compared to the commonly used discrete reinforcement learning algorithm.									5	0	0	0	0	0	5			0018-9545	1939-9359										Tongji Univ, Shanghai 200070, Peoples R ChinaTech Univ Munich, Chair Robot Artificial Intelligence & Real time Sy, D-80333 Munich, GermanyTsinghua Univ, Sch Vehicle & Mobil, Beijing 100190, Peoples R ChinaUniv Sci & Technol China, Hefei 230052, Peoples R China				2023-03-30	WOS:000944202400091		
J	Bhattacharyya, Raunak; Wulfe, Blake; Phillips, Derek J.; Kuefler, Alex; Morton, Jeremy; Senanayake, Ransalu; Kochenderfer, Mykel J.				; Kochenderfer, Mykel/E-7069-2010	Senanayake, Ransalu/0000-0002-7913-6116; Kochenderfer, Mykel/0000-0002-7238-9663					Modeling Human Driving Behavior Through Generative Adversarial Imitation Learning								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	3			2874	2887				10.1109/TITS.2022.3227738					DEC 2022		Article; Early Access		2023	An open problem in autonomous vehicle safety validation is building reliable models of human driving behavior in simulation. This work presents an approach to learn neural driving policies from real world driving demonstration data. We model human driving as a sequential decision making problem that is characterized by non-linearity and stochasticity, and unknown underlying cost functions. Imitation learning is an approach for generating intelligent behavior when the cost function is unknown or difficult to specify. Building upon work in inverse reinforcement learning (IRL), Generative Adversarial Imitation Learning (GAIL) aims to provide effective imitation even for problems with large or continuous state and action spaces, such as modeling human driving. This article describes the use of GAIL for learning-based driver modeling. Because driver modeling is inherently a multi-agent problem, where the interaction between agents needs to be modeled, this paper describes a parameter-sharing extension of GAIL called PS-GAIL to tackle multi-agent driver modeling. In addition, GAIL is domain agnostic, making it difficult to encode specific knowledge relevant to driving in the learning process. This paper describes Reward Augmented Imitation Learning (RAIL), which modifies the reward signal to provide domain-specific knowledge to the agent. Finally, human demonstrations are dependent upon latent factors that may not be captured by GAIL. This paper describes Burn-InfoGAIL, which allows for disentanglement of latent variability in demonstrations. Imitation learning experiments are performed using NGSIM, a real-world highway driving dataset. Experiments show that these modifications to GAIL can successfully model highway driving behavior, accurately replicating human demonstrations and generating realistic, emergent behavior in the traffic flow arising from the interaction between driving agents.									9	2	0	0	0	0	12			1524-9050	1558-0016										Univ Oxford, Oxford Robot Inst, Oxford OX2 6NN, EnglandStanford Univ, Dept Comp Sci, Stanford, CA 94305 USAStanford Univ, Dept Symbol Syst, Stanford, CA 94305 USAStanford Univ, Dept Aeronaut & Astronaut, Stanford, CA 94305 USA				2023-01-11	WOS:000903731300001		
C	Xing, Jinwei; Zou, Xinyun; Krichmar, Jeffrey L.			IEEE	Xing, Jinwei/JFS-1206-2023; Zou, Xinyun/AAQ-2542-2021	Zou, Xinyun/0000-0002-5387-6562; Xing, Jinwei/0000-0002-8085-2769					Neuromodulated Patience for Robot and Self-Driving Vehicle Navigation								2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)		IEEE International Joint Conference on Neural Networks (IJCNN)											10.1109/ijcnn48605.2020.9206642							Proceedings Paper	2020	2020	Robots and self-driving vehicles face a number of challenges when navigating through real environments. Successful navigation in dynamic environments requires prioritizing subtasks and monitoring resources. Animals are under similar constraints. It has been shown that the neuromodulator serotonin (5-HT) regulates impulsiveness and patience in animals. In the present paper, we take inspiration from the serotonergic system and apply it to the task of robot navigation. In a set of outdoor experiments, we show how changing the level of patience can affect the amount of time the robot will spend searching for a desired location. To navigate GPS compromised environments, we introduce a deep reinforcement learning paradigm in which the robot learns to follow sidewalks. This may further regulate a tradeoff between a smooth long route and a rough shorter route. Using patience as a parameter may be beneficial for autonomous systems under time pressure.					International Joint Conference on Neural Networks (IJCNN) held as part of the IEEE World Congress on Computational Intelligence (IEEE WCCI)International Joint Conference on Neural Networks (IJCNN) held as part of the IEEE World Congress on Computational Intelligence (IEEE WCCI)	JUL 19-24, 2020JUL 19-24, 2020	IEEE; IEEE Computat Intelligence Soc; Int Neural Network SocIEEE; IEEE Computat Intelligence Soc; Int Neural Network Soc	ELECTR NETWORKELECTR NETWORK	4	0	0	0	1	0	4			2161-4393		978-1-7281-6926-2									Univ Calif Irvine, Dept Cognit Sci, Irvine, CA 92697 USAUniv Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA				2021-04-08	WOS:000626021400051		
J	Cho, Kyunghoon				Cho, Kyunghoon/HQY-8586-2023	Cho, Kyunghoon/0000-0002-4679-6660					A Hierarchical Learning Approach to Autonomous Driving Using Rule Specifications								IEEE ACCESS				10				74815	74824				10.1109/ACCESS.2022.3191434							Article	2022	2022	Understanding the movement of surrounding objects and controlling robot platforms (such as autonomous vehicles and social robots) in a safe way are challenging problems. In the autonomous driving problem, autonomous vehicles must take into account the future behaviors of nearby vehicles and make appropriate controls accordingly. This is the biggest factor that makes autonomous driving problems difficult. In this work, this problem is tackled by combining benefits from both sequence prediction and deep reinforcement learning in a hierarchical manner. The driver's behavior is classified according to the driving style defined by the rules selected in the autonomous driving situation. High-level behavior represents driving style, and the vehicle's movement model is trained to condition itself to high-level behavior. Instead of directly finding low-level controls, we focus on finding high-level behaviors to increase efficiency. For example, if an autonomous vehicle needs to change lanes in certain situations, the high-level behavior "change lanes" is first selected and the corresponding vehicle movement model is used to find the appropriate low-level controls. Reinforcement learning is used to help select the best high-level behavior, and future behaviors of nearby vehicles are jointly reasoned to lead to better understanding of the current situation. The feasibility of the proposed approach is tested in publicly available datasets. The proposed method shows better efficiency and performance compared to existing learning-based control algorithms.									0	0	0	0	0	0	0			2169-3536											Incheon Natl Univ, Dept Informat & Telecommun Engn, Inchoen 22012, South Korea				2022-08-20	WOS:000838520000001		
C	Martinson, Michael; Skrynnik, Alexey; Panov, Aleksandr, I				Skrynnik, Alexey/U-3497-2018; Panov, Aleksandr I./L-9171-2013	Skrynnik, Alexey/0000-0001-9243-1622; Panov, Aleksandr I./0000-0002-9747-3837	Kuznetsov, SO; Panov, AI; Yakovlev, KS				Navigating Autonomous Vehicle at the Road Intersection Simulator with Reinforcement Learning								ARTIFICIAL INTELLIGENCE		Lecture Notes in Artificial Intelligence		12412				71	84				10.1007/978-3-030-59535-7_6							Proceedings Paper	2020	2020	In this paper, we consider the problem of controlling an agent that simulates the behavior of an self-driving car when passing a road intersection together with other vehicles. We consider the case of using smart city systems, which allow the agent to get full information about what is happening at the intersection in the form of video frames from surveillance cameras. The paper proposes the implementation of a control system based on a trainable behavior generation module. The agent's model is implemented using reinforcement learning (RL) methods. In our work, we analyze various RL methods (PPO, Rainbow, TD3), and variants of the computer vision subsystem of the agent. Also, we present our results of the best implementation of the agent when driving together with other participants in compliance with traffic rules.					18th Russian Conference on Artificial Intelligence (RCAI)18th Russian Conference on Artificial Intelligence (RCAI)	OCT 10-16, 2020OCT 10-16, 2020	Russian Assoc Artificial Intelligence; Russian Acad Sci, Fed Res Ctr Comp Sci & Control; Moscow Inst Phys & Technol; Natl Res Nucl UnivRussian Assoc Artificial Intelligence; Russian Acad Sci, Fed Res Ctr Comp Sci & Control; Moscow Inst Phys & Technol; Natl Res Nucl Univ	ELECTR NETWORKELECTR NETWORK	3	0	0	0	0	0	3			0302-9743	1611-3349	978-3-030-59535-7; 978-3-030-59534-0									Moscow Inst Phys & Technol, Moscow, RussiaRussian Acad Sci, Artificial Intelligence Res Inst, Fed Res Ctr Comp Sci & Control, Moscow, Russia				2021-11-18	WOS:000714953300006		
J	Sanmin, Kim; Kim, Youngseok; Jeon, Hyeongseok; Kum, Dongsuk; Lee, Kibeom										Autonomous Driving Technology Trend and Future Outlook: Powered by Artificial Intelligence			자율주행 기술 동향 및 발전 방향: AI를 중심으로					Transactions of KSAE	한국자동차공학회 논문집			30	10			819	830											research-article	2022	2022	Autonomous driving is not a new concept, and relevant technology has been developed for a long time. However, in recent years, autonomous driving technology has been leaping forward, fueled by the advance of AI-based technologies. In particular, the essential components of autonomous driving, such as perception, prediction, and planning, deliver entirely different performances from those of the pre-AI era. In this study, the trends and development of autonomous driving technology will be analyzed by decomposing it into element technologies ranging from perception, prediction, and planning, focusing on AI-based research. For the perception part, LiDAR and camera-based research and sensor fusion technologies will be examined. For the prediction part, we will look into various prediction paradigms such as interaction-aware and map-based prediction. The planning part will cover maneuver decisions, motion planning, and reinforcement learning-based methods.									0	0	0	0	0	0	0			1225-6382															2022-10-25	KJD:ART002879618		
J	Kumar, Abhishek				Kumar, Abhishek/AGF-4591-2022	Kumar, Abhishek/0000-0002-6633-0294					REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES								MECHATRONIC SYSTEMS AND CONTROL				51	1			53	57				10.2316/J.2023.201-0347							Article	2023	2023	Reinforcement learning (RL) is one of the most emerging domains of artificial intelligence. It is widely used in almost all sort of applications, including medical field, stock market, forecasting, and engineering field. One of the most effective uses of RL is in the control engineering domain owing to its learning by trial, for example, in building autonomous system like autonomous vehicle and robotics. In this paper, we have focused on the applications of RL in various control engineering problems. Stability of controller (or agent) using RL paradigm is a very crucial task due to exploration-exploitation policy used by any RL. Also, the unavailability of exact model of system or environment may lead to unsafe behaviour of the agent. Therefore, this paper focuses mainly on the stability aspect in RL-based controller. Many concepts are used to study and analyse the stability of a system viz Lyapunov method and Barrier function. This paper surveys the detailed application of these well-established stability certifier methods in various model-free and model-based RL framework.									2	0	0	0	0	0	2			2561-1771	2561-178X										CMR Univ, SOET, Bengaluru, India	CMR Univ			2023-07-16	WOS:001021517200006		
J	Irshayyid, Ali; Chen, Jun				Chen, Jun/H-4506-2011	Chen, Jun/0000-0002-0934-8519; Irshayyid, Ali/0000-0001-9695-7680					Comparative Study of Cooperative Platoon Merging Control Based on Reinforcement Learning								SENSORS				23	2					990			10.3390/s23020990							Article	JAN 2023	2023	The time that a vehicle merges in a lane reduction can significantly affect passengers' safety, comfort, and energy consumption, which can, in turn, affect the global adoption of autonomous electric vehicles. In this regard, this paper analyzes how connected and automated vehicles should cooperatively drive to reduce energy consumption and improve traffic flow. Specifically, a model-free deep reinforcement learning approach is used to find the optimal driving behavior in the scenario in which two platoons are merging into one. Several metrics are analyzed, including the time of the merge, energy consumption, and jerk, etc. Numerical simulation results show that the proposed framework can reduce the energy consumed by up to 76.7%, and the average jerk can be decreased by up to 50%, all by only changing the cooperative merge behavior. The present findings are essential since reducing the jerk can decrease the longitudinal acceleration oscillations, enhance comfort and drivability, and improve the general acceptance of autonomous vehicle platooning as a new technology.									0	0	0	0	0	0	0				1424-8220										Oakland Univ, Dept Elect & Comp Engn, Rochester, MI 48309 USA				2023-02-10	WOS:000916253400001	36679787	
B	Ziqing Gu; Yujie Yang; Jingliang Duan; Shengbo Li, E.; Jianyu Chen; Wenhan Cao; Sifa Zheng										Belief state separated reinforcement learning for autonomous vehicle decision making under uncertainty								2021 IEEE International Intelligent Transportation Systems Conference (ITSC)								586	92				10.1109/ITSC48978.2021.9564576							Conference Paper	2021	2021	In autonomous driving, the ego vehicle and its surrounding traffic environments always have uncertainties like parameter and structural errors, behavior randomness of road users, etc. Furthermore, environmental sensors are noisy or even biased. This problem can be formulated as a partially observable Markov decision process. Existing methods lack a good representation of historical information, making it very challenging to find an optimal policy. This paper proposes a belief state separated reinforcement learning (RL) algorithm for decision-making of autonomous driving in uncertain environments. We extend the separation principle from linear Gaussian systems to general nonlinear stochastic environments, where the belief state, defined as the posterior distribution of the true state, is found to be a sufficient statistic of historical information. This belief state is estimated by action-enhanced variational inference from historical information and is proved to satisfy the Markovian property, thus allowing us to obtain the optimal policy using traditional RL algorithms for Markov decision processes. The policy gradient of a task-specific prior model is mixed with that of the interaction data to improve learning performance. The proposed algorithm is evaluated in a multi-lane autonomous driving task, where the surrounding vehicles are subject to behavior uncertainty and observation noise. The simulation results show that compared with existing RL algorithms, the proposed method can achieve a higher average return with better driving performance.					2021 IEEE International Intelligent Transportation Systems Conference (ITSC)2021 IEEE International Intelligent Transportation Systems Conference (ITSC)	19-22 Sept. 202119-22 Sept. 2021		Indianapolis, IN, USAIndianapolis, IN, USA	0	0	0	0	0	0	0					978-1-7281-9142-3									Sch. of Vehicle & Mobility, Tsinghua Univ., Beijing, ChinaInst. of Interdiscriplinary Inf. Sci., Tsinghua Univ., Beijing, China				2022-01-07	INSPEC:21259396		
J	Clark, E.; Hunter, A.; Isupova, O.; Donnelly, M.					Isupova, Olga/0000-0002-8506-6871; , Edward/0000-0001-8621-826X					Exploring the use of AI in marine acoustic sensor management								Proceedings of Meetings on Acoustics				47	1			070008 (11 pp.)	070008 (11 pp.)				10.1121/2.0001601							Journal Paper	2022	2022	Underwater passive acoustic source detection and tracking is important for various marine applications, including marine mammal monitoring and naval surveillance. The performance in these applications is dependent on the placement and operation of sensing assets, such as autonomous underwater vehicles. Conventionally, these decisions have been made by human operators aided by acoustic propagation modelling tools, situational and environmental data, and experience. However, this is time-consuming and computationally expensive. We consider a 'toy problem' of a single autonomous vehicle (agent) in search of a stationary source of low frequency within a reinforcement learning (RL) architecture. We initially choose the observation space to be the agent's current position. The agent is allowed to explore the environment with a limited action space, taking equal distance steps in one of $n$ directions. Rewards are received for positive detections of the source. Using OpenAI's PPO algorithm an increase in median episode reward of approximately 20 points in the RL environment developed is seen when the agent is given a history of it's previous moves and signal-to-noise ratio compared to the simple state. The future expansion of the RL framework is discussed in terms of the observation and action spaces, reward function and RL architecture. [The copyright for the referenced work is owned by Acoustical Society of America. Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]									0	0	0	0	0	0	0			1939-800X											Dept. of Mech. Eng., Univ. of Bath, Bath, UKDept. of Comput. Sci., Univ. of Bath, Bath, UKSyst. Eng. & Assesment Ltd, UK				2022-10-28	INSPEC:22141509		
C	De Silva, Varuna; Wang, Xiongzhao; Aladagli, Deniz; Kondoz, Ahmet; Ekmekcioglu, Erhan						Arai, K; Kapoor, S; Bhatia, R				An Agent-Based Modelling Framework for Driving Policy Learning in Connected and Autonomous Vehicles								INTELLIGENT SYSTEMS AND APPLICATIONS, INTELLISYS, VOL 2		Advances in Intelligent Systems and Computing		869				113	125				10.1007/978-3-030-01057-7_10							Proceedings Paper	2019	2019	Due to the complexity of the natural world, a programmer cannot foresee all possible situations, a connected and autonomous vehicle (CAV) will face during its operation, and hence, CAVs will need to learn to make decisions autonomously. Due to the sensing of its surroundings and information exchanged with other vehicles and road infrastructure, a CAV will have access to large amounts of useful data. While different control algorithms have been proposed for CAVs, the benefits brought about by connectedness of autonomous vehicles to other vehicles and to the infrastructure, and its implications on policy learning has not been investigated in literature. This paper investigates a data driven driving policy learning framework through an agent-based modelling approaches. The contributions of the paper are two-fold. A dynamic programming framework is proposed for in-vehicle policy learning with and without connectivity to neighboring vehicles. The simulation results indicate that while a CAV can learn to make autonomous decisions, vehicle-to-vehicle (V2V) communication of information improves this capability. Furthermore, to overcome the limitations of sensing in a CAV, the paper proposes a novel concept for infrastructure-led policy learning and communication with autonomous vehicles. In infrastructure-led policy learning, road-side infrastructure senses and captures successful vehicle maneuvers and learns an optimal policy from those temporal sequences, and when a vehicle approaches the road-side unit, the policy is communicated to the CAV. Deep-imitation learning methodology is proposed to develop such an infrastructure-led policy learning framework.					Intelligent Systems Conference (IntelliSys)Intelligent Systems Conference (IntelliSys)	SEP 06-07, 2018SEP 06-07, 2018		London, ENGLANDLondon, ENGLAND	1	0	0	0	0	0	1			2194-5357	2194-5365	978-3-030-01057-7; 978-3-030-01056-0									Loughborough Univ, Inst Digital Technol, London, England				2020-12-22	WOS:000591539000010		
B	Xu, L.; Sun, S.; Zhang, Y.D.; Petropulu, A.										Joint Antenna Selection and Beamforming in Integrated Automotive Radar Sensing-Communications with Quantized Double Phase Shifters								ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)								1	5				10.1109/ICASSP49357.2023.10097184							Conference Paper	2023	2023	We consider an integrated sensing-communication system operating in a dynamic environment, such as an autonomous vehicle scenario. We propose a novel, low-cost, low power consumption and low-computation approach for designing a beam that can simultaneously reach the radar target of interest and the desired communication destination. The transmitter is a uniform linear array, equipped with quantized double phase shifters, which enables a flexible beam design while using analog only processing. Only a small number of antennas are selected to transmit in each channel use, in order to save system power and reduce antenna coupling. We propose a deep reinforcement learning approach to adaptively adjust the double phase shifters and select the active antennas in order to optimize the transmit beamforming, through a transmission and feedback trail. The actor-critic network strategy together with the Wolpertinger policy is adopted to obtain the optimal solutions efficiently and effectively. Numerical results demonstrate the feasibility of the proposed method.					ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	20232023		Rhodes, GreeceRhodes, Greece	0	0	0	0	0	0	0					978-1-7281-6327-7									Dept. of Electr. & Comput. Eng., Univ. of Alabama, Tuscaloosa, AL, USADept. of Electr. & Comput. Eng., Temple Univ., Philadelphia, PA, USADept. of Electr. & Comput. Eng., Rutgers Univ., Piscataway, NJ, USA				2023-11-11	INSPEC:23945912		
J	Hart, F.; Waltz, M.; Okhrin, O.										Two-step dynamic obstacle avoidance [arXiv]								Two-step dynamic obstacle avoidance [arXiv]																				Preprint	2024	2024	Dynamic obstacle avoidance (DOA) is a fundamental challenge for any autonomous vehicle, independent of whether it operates in sea, air, or land. This paper proposes a two-step architecture for handling DOA tasks by combining supervised and reinforcement learning (RL). In the first step, we introduce a data-driven approach to estimate the collision risk of an obstacle using a recurrent neural network, which is trained in a supervised fashion and offers robustness to non-linear obstacle movements. In the second step, we include these collision risk estimates into the observation space of an RL agent to increase its situational awareness.~We illustrate the power of our two-step approach by training different RL agents in a challenging environment that requires to navigate amid multiple obstacles. The non-linear movements of obstacles are exemplarily modeled based on stochastic processes and periodic patterns, although our architecture is suitable for any obstacle dynamics. The experiments reveal that integrating our collision risk metrics into the observation space doubles the performance in terms of reward, which is equivalent to halving the number of collisions in the considered environment. Furthermore, we show that the architecture's performance improvement is independent of the applied RL algorithm.									0	0	0	0	0	0	0																		2024-01-06	INSPEC:24221630		
J	Rais, Mohamed Saber; Boudour, Rachid; Zouaidia, Khouloud; Bougueroua, Lamine				Rais, Mohamed Saber/GLU-6889-2022; khouloud, zouaidia/AAV-3782-2021	khouloud, zouaidia/0000-0003-2702-8022; LAMINE, BOUGUEROUA/0000-0002-5322-8231					Decision making for autonomous vehicles in highway scenarios using Harmonic SK Deep SARSA								APPLIED INTELLIGENCE				53	3			2488	2505				10.1007/s10489-022-03357-y					MAY 2022		Article; Early Access		2023	The complexity of taking decisions for an autonomous vehicle (AV) to avoid road accident fatalities, provide safety, comfort, and reduce traffic raises the need for improvements in the field of decision making. To solve these challenges, many algorithms and techniques were applied, and the most common ones were reinforcement learning (RL) algorithms combined with deep learning techniques. Therefore, in this paper we proposed a novel extension of the popular "SARSA" (State-Action-Reward-State-Action) RL technique called "Harmonic SK Deep SARSA" that takes advantage of the stability which SARSA algorithm provides and uses the notion of similar and cumulative states saved in an alternative memory to enhance the stability of the algorithm and achieve remarkable performance that SARSA could not accomplish due to its on policy nature. Through the investigation of our novel extension the adaptability of the algorithm to unexpected situations during learning and to unforeseen changes in the environment was proved while reducing the computational load in the learning process and increasing the convergence rate that plays a key role in upgrading decision making application that require numerous real time consecutive decisions, including autonomous vehicles, industrial robots, gaming, aerial navigation... The novel algorithm was tested in a gym environment simulator called "Highway-env" with multiple highway situations (multiple lanes configurations, highway with dynamic number of lanes (from 4-lane to 2-lane, from 4-lane to 6-lane), merge) with numerous dynamic obstacles. For the purpose of comparison, we used a benchmark of cutting edge algorithms known for their prominent performance. The experimental results showed that the proposed algorithm outperformed the comparison algorithms in learning stability and performance that were validated by the following metrics: average loss value per episode, average accuracy per episode, maximum speed value reached per episode, average speed per episode, and the total reward per episode.									2	0	0	0	0	0	2			0924-669X	1573-7497										Badji Mokhtar Univ, Embedded Syst Lab, Annaba, AlgeriaEfrei Paris, Allianst Res Lab, Villejuif, France	Efrei Paris			2022-05-18	WOS:000793086800001		
J	Hester, Todd; Stone, Peter										TEXPLORE: real-time sample-efficient reinforcement learning for robots								MACHINE LEARNING				90	3			385	429				10.1007/s10994-012-5322-7							Article	MAR 2013	2013	The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations online. RL is a paradigm for learning sequential decision making tasks, usually formulated as a Markov Decision Process (MDP). For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. In addition, the algorithm must learn efficiently in the face of noise, sensor/actuator delays, and continuous state features. In this article, we present texplore, the first algorithm to address all of these challenges together. texplore is a model-based RL method that learns a random forest model of the domain which generalizes dynamics to unseen states. The agent explores states that are promising for the final policy, while ignoring states that do not appear promising. With sample-based planning and a novel parallel architecture, texplore can select actions continually in real-time whenever necessary. We empirically evaluate the importance of each component of texplore in isolation and then demonstrate the complete algorithm learning to control the velocity of an autonomous vehicle in real-time.									52	3	0	0	0	0	64			0885-6125	1573-0565										Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA				2013-03-06	WOS:000314716200003		
B	Lakshmanan, M.; Anandha Mala, G.S.										Augmented Random Search based Autonomous Driving System								2023 5th International Conference on Smart Systems and Inventive Technology (ICSSIT)								1421	4				10.1109/ICSSIT55814.2023.10061010							Conference Paper	2023	2023	The development of autonomous driving technology led to a rise in the popularity of self-driving automobiles. CARLA is an open-source simulator for autonomous driving research and is used in these autonomous driving systems. From the beginning, CARLA has supported the creation, instruction, and testing of autonomous driving systems. Along with this, CARLA offers free to use open digital assets (such as city plans, structures, and vehicles). To perform addition and deletion of random sounds from the weight and tracking the entire rewards received as a result of this weight modification, the Augmented Random Search (ARS) approach trains a supervised learning on input data. The Augmented Random Generator is then used to measure the weights depending on these incentives in a predetermined number of episodes at a predestined learning rate. The Algorithm for Robotic Search (ARS) will be used to train self-driving cars. Automobiles based on data received from each car's front cameras. As a result of this research, a framework for training self-driving cars has been developed. Carla's policy employing ARS will be created with it as the core algorithm, giving a more realistic picture of how things work. Because of its novelty, ARS is an extremely light tool for analyzing difficult control tasks, and the authors of the study discovered that ARS had at least 15 times the computing efficiency of the fastest competitive learning approaches. CARLA has been used to provide more consistent results from autonomous vehicle training, and this research has proved successful considering how many unique circumstances there are, in opening up the majority of the chances for additional study on the same issue.					2023 5th International Conference on Smart Systems and Inventive Technology (ICSSIT)2023 5th International Conference on Smart Systems and Inventive Technology (ICSSIT)	23-25 Jan. 202323-25 Jan. 2023		Tirunelveli, IndiaTirunelveli, India	0	0	0	0	0	0	0					978-1-6654-7467-2									Comput. Sci. & Eng., Anna Univ., Chennai, IndiaComput. Sci. & Eng., Easwari Eng. Coll., Chennai, India				2023-03-30	INSPEC:22777002		
J	Pin Wang; Hanhan Li; Ching-Yao Chan										Quadratic Q-network for Learning Continuous Control for Autonomous Vehicles [arXiv]								arXiv								9 pp.	9 pp.											Journal Paper	29 Nov. 2019	2019	Reinforcement Learning algorithms have recently been proposed to learn time-sequential control policies in the field of autonomous driving. Direct applications of Reinforcement Learning algorithms with discrete action space will yield unsatisfactory results at the operational level of driving where continuous control actions are actually required. In addition, the design of neural networks often fails to incorporate the domain knowledge of the targeting problem such as the classical control theories in our case. In this paper, we propose a hybrid model by combining Q-learning and classic PID (Proportion Integration Differentiation) controller for handling continuous vehicle control problems under dynamic driving environment. Particularly, instead of using a big neural network as Q-function approximation, we design a Quadratic Q-function over actions with multiple simple neural networks for finding optimal values within a continuous space. We also build an action network based on the domain knowledge of the control mechanism of a PID controller to guide the agent to explore optimal actions more efficiently.We test our proposed approach in simulation under two common but challenging driving situations, the lane change scenario and ramp merge scenario. Results show that the autonomous vehicle agent can successfully learn a smooth and efficient driving behavior in both situations.									0	0	0	0	0	0	0																		2020-06-05	INSPEC:19587363		
J	Xiaoyuan Fu; Quan Yuan; Shifan Liu; Baozhu Li; Qi Qi; Jingyu Wang										Communication-efficient decision-making of digital twin assisted Internet of vehicles: A hierarchical multi-agent reinforcement learning approach								China Communications								55	68				10.23919/JCC.2023.03.005							Journal Paper	2023	2023	The connected autonomous vehicle is considered an effective way to improve transport safety and efficiency. To overcome the limited sensing and computing capabilities of individual vehicles, we design a digital twin assisted decision-making framework for Internet of Vehicles, by leveraging the integration of communication, sensing and computing. In this framework, the digital twin entities residing on edge can effectively communicate and cooperate with each other to plan sub-targets for their respective vehicles, while the vehicles only need to achieve the sub-targets by generating a sequence of atomic actions. Furthermore, we propose a hierarchical multiagent reinforcement learning approach to implement the framework, which can be trained in an end-to-end way. In the proposed approach, the communication interval of digital twin entities could adapt to time-varying environment. Extensive experiments on driving decision-making have been performed in traffic junction scenarios of different difficulties. The experimental results show that the proposed approach can largely improve collaboration efficiency while reducing communication overhead.									0	0	0	0	0	0	0			1673-5447											State key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Xi'an, ChinaState key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Beijing, ChinaInternet of Things & Smart City Innovation Platform, Zhuhai Fudan Innovation Inst., Zhuhai, China				2023-04-22	INSPEC:22886850		
J	Meneses-Cime, Karina; Guvenc, Bilin Aksun; Guvenc, Levent					Guvenc, Levent/0000-0001-8823-1820					Optimization of On-Demand Shared Autonomous Vehicle Deployments Utilizing Reinforcement Learning								SENSORS				22	21					8317			10.3390/s22218317							Article	NOV 2022	2022	Ride-hailed shared autonomous vehicles (SAV) have emerged recently as an economically feasible way of introducing autonomous driving technologies while serving the mobility needs of under-served communities. There has also been corresponding research work on optimization of the operation of these SAVs. However, the current state-of-the-art research in this area treats very simple networks, neglecting the effect of a realistic other traffic representation, and is not useful for planning deployments of SAV service. In contrast, this paper utilizes a recent autonomous shuttle deployment site in Columbus, Ohio, as a basis for mobility studies and the optimization of SAV fleet deployment. Furthermore, this paper creates an SAV dispatcher based on reinforcement learning (RL) to minimize passenger wait time and to maximize the number of passengers served. The created taxi-dispatcher is then simulated in a realistic scenario while avoiding generalization or over-fitting to the area. It is found that an RL-aided taxi dispatcher algorithm can greatly improve the performance of a deployment of SAVs by increasing the overall number of trips completed and passengers served while decreasing the wait time for passengers.									2	0	0	0	0	0	3				1424-8220										Ohio State Univ, Automated Driving Lab, Columbus, OH 43210 USA				2022-11-27	WOS:000884119800001	36366014	
C	Chronis, Christos; Sardianos, Christos; Varlamis, Iraklis; Michail, Dimitrios; Tserpes, Konstantinos				Tserpes, Konstantinos/A-8449-2016	Tserpes, Konstantinos/0000-0001-5183-1443	Vassilakopoulos, MG; Karanikolas, NN				A driving profile recommender system for autonomous driving using sensor data and reinforcement learning								25TH PAN-HELLENIC CONFERENCE ON INFORMATICS WITH INTERNATIONAL PARTICIPATION (PCI2021)								33	38				10.1145/3503823.3503830							Proceedings Paper	2021	2021	The design of algorithms for autonomous vehicles includes a wide range of machine learning tasks including scene perception by the visual input from cameras and other sensors, monitoring and prediction of the driver and passengers' state, and others. The aim of the present work is to study the task of personalizing the driving experience in an autonomous vehicle, taking into account the particularities and differences of each person in how he/she perceives the vehicle's velocity. For this purpose, we employ the Actor-Critic Reinforcement Learning technique in order to automatically select the best driving mode during driving. The input to the actor-critic model comprises the driver's stress and excitement, which are affected by the route conditions, and the vehicle velocity and angular velocity. The output at each step is the best mode for each driver, which better balances stress, excitement, and route completion time. The whole setup is simulated and tested within the Carla open-source simulator for autonomous driving research.					25th Pan-Hellenic Conference on Informatics with International Participation (PCI)25th Pan-Hellenic Conference on Informatics with International Participation (PCI)	NOV 26-28, 2021NOV 26-28, 2021	Univ Thessaly, Dept Elect & Comp Engn; Greek Comp Soc; Univ W AtticaUniv Thessaly, Dept Elect & Comp Engn; Greek Comp Soc; Univ W Attica	Volos, GREECEVolos, GREECE	3	0	0	0	0	0	3					978-1-4503-9555-7									Harokopio Univ Athens, Athens, Greece				2021-01-01	WOS:000927593100007		
C	Zheng, Jinkai; Mu, Phil K.; Man, Ziqian; Luan, Tom H.; Cai, Lin X.; Shan, Hangguan				LI, YUN/JTV-7108-2023; Luan, Tom H./HLG-0711-2023	Zheng, Jinkai/0000-0002-7171-2668; Luan, Tom/0000-0002-5215-7443; Mu, Kangle/0000-0002-4509-1861	Zheng, J; Liu, X; Luan, TH; Jayaraman, PP; Dai, H; Mitra, K; Qin, K; Ranjan, R; Wen, S				Device Placement for Autonomous Vehicles using Reinforcement Learning								IEEE CONGRESS ON CYBERMATICS / 2021 IEEE INTERNATIONAL CONFERENCES ON INTERNET OF THINGS (ITHINGS) / IEEE GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) / IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING (CPSCOM) / IEEE SMART DATA (SMARTDATA)								190	196				10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics53846.2021.00041							Proceedings Paper	2021	2021	Autonomous driving is a complex function consisting of multiple parallel AI tasks running at the same time for information sensing, fusion, and decision making. To process the complex computing tasks, an autonomous vehicle is typically equipped with different processing units at the same time, such as CPU, GPU, FPGA, of the different computing capabilities. As the AI tasks have different requirements on the computing resources, a fundamental issue is how to optimally allocate the real-time computation tasks to different processing units (as known as device placement) on board towards maximum utility for autonomous driving. Towards the issue, this paper develops a reinforcement learning algorithm which is based on the proximal policy optimization (PPO) specifically for finding the optimal device placement for running a neural network model. A sequence-to-sequence model is proposed to allocate the operations of a neural network model on appropriate computing units in the vehicle. The execution time and energy consumption of the placement solution is used as a reward signal to further optimize the parameters. We implement our algorithm in different benchmarks, and compare it with different baselines. Experiments have demonstrated that our algorithm can find the optimal device placement position, and its performance is better than previous state-of-the-art RL algorithm as well as traditional methods.					IEEE Congress on Cybermat / 14th IEEE Int Conf on Internet of Things / 14th IEEE Int Conf on Cyber, Phys, and Social Comp / 17th IEEE Int Conf on Green Comp and Commun / 7th IEEE Int Conf on Smart DataIEEE Congress on Cybermat / 14th IEEE Int Conf on Internet of Things / 14th IEEE Int Conf on Cyber, Phys, and Social Comp / 17th IEEE Int Conf on Green Comp and Commun / 7th IEEE Int Conf on Smart Data	DEC 06-08, 2021DEC 06-08, 2021	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1					978-1-6654-1762-4									Xidian Univ, Sch Cyber Engn, Xian, Shaanxi, Peoples R ChinaUniv Michigan, Elect Engn & Comp Sci Dept, Ann Arbor, MI 48109 USAIIT, Elect & Comp Engn, Chicago, IL 60616 USAZhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China				2022-06-02	WOS:000795905900026		
J	Bharilya, V.; Kumar, N.										Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions [arXiv]								arXiv																				Journal Paper	12 July 2023	2023	Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future behavior of nearby traffic participants, similar to the predictive driving abilities of human drivers. Building upon existing literature is crucial to advance the field and develop a comprehensive understanding of trajectory prediction methods in the context of automated driving. To address this need, we have undertaken a comprehensive review that focuses on trajectory prediction methods for AVs, with a particular emphasis on machine learning techniques including deep learning and reinforcement learning-based approaches. We have extensively examined over two hundred studies related to trajectory prediction in the context of AVs. The paper begins with an introduction to the general problem of predicting vehicle trajectories and provides an overview of the key concepts and terminology used throughout. After providing a brief overview of conventional methods, this review conducts a comprehensive evaluation of several deep learning-based techniques. Each method is summarized briefly, accompanied by a detailed analysis of its strengths and weaknesses. The discussion further extends to reinforcement learning-based methods. This article also examines the various datasets and evaluation metrics that are commonly used in trajectory prediction tasks. Encouraging an unbiased and objective discussion, we compare two major learning processes, considering specific functional features. By identifying challenges in the existing literature and outlining potential research directions, this review significantly contributes to the advancement of knowledge in the domain of AV trajectory prediction.									0	0	0	0	0	0	0																		2023-08-10	INSPEC:23492706		
B	Forbes, Jeffrey Roderick Norman										Reinforcement learning for autonomous vehicles																												Dissertation/Thesis	Jan 01 2002	2002										0	0	0	0	0	0	0					978-0-493-82227-3									University of California, Berkeley, California, United States	University of California, Berkeley				PQDT:65358902		
B	Azadani, M.N.; Boukerche, A.						Zorzi, M.; Tao, M.; Saad, W.				A Context-Aware Path Forecasting Method for Connected Autonomous Vehicles								ICC 2023 - IEEE International Conference on Communications								247	52				10.1109/ICC45041.2023.10279653							Conference Paper	2023	2023	Forecasting the future paths of surrounding vehicles of a Connected Autonomous Vehicle (CAV) can enhance connectivity and efficiency of vehicular networks, and accurate motion forecasting of nearby vulnerable road users can advance road safety and urban mobility. This task needs a high-level situational awareness for the CAV. Early methods rely solely on vehicle kinematics and overlook the uncertainty within agents behavior and the effects of surrounding context on the behavior of nearby agents, resulting in lower performance or infeasible predictions. In the current work, we introduce a novel context-aware forecasting approach for CAVs that benefits from inverse reinforcement learning (IRL) to condition the future motions of nearby agents on scene-based state sequences defined using a Markov Decision Process. More precisely, we map the images of the surrounding context and the behavior history of agents into rewards and learn optimal expert behaviors using IRL. We validate the path forecasting efficiency of our model using two large motion prediction benchmarks with different scenes and achieve state-of-the-art results in terms of FDE and ADE metrics.					ICC 2023 - IEEE International Conference on CommunicationsICC 2023 - IEEE International Conference on Communications	20232023		Rome, ItalyRome, Italy	0	0	0	0	0	0	0					978-1-5386-7462-8									Sch. of Electr. Eng. & Comput. Sci., Univ. of Ottawa, Ottawa, Canada				2023-11-09	INSPEC:23947109		
J	Zhang, Ruixian; Han, Yining; Su, Man; Lin, Zefeng; Li, Haowei; Zhang, Lixian										Robust reinforcement learning with UUB guarantee for safe motion control of autonomous robots								SCIENCE CHINA-TECHNOLOGICAL SCIENCES													10.1007/s11431-023-2435-3					DEC 2023		Article; Early Access		2023	This paper addresses the issue of safety in reinforcement learning (RL) with disturbances and its application in the safety-constrained motion control of autonomous robots. To tackle this problem, a robust Lyapunov value function (rLVF) is proposed. The rLVF is obtained by introducing a data-based LVF under the worst-case disturbance of the observed state. Using the rLVF, a uniformly ultimate boundedness criterion is established. This criterion is desired to ensure that the cost function, which serves as a safety criterion, ultimately converges to a range via the policy to be designed. Moreover, to mitigate the drastic variation of the rLVF caused by differences in states, a smoothing regularization of the rLVF is introduced. To train policies with safety guarantees under the worst disturbances of the observed states, an off-policy robust RL algorithm is proposed. The proposed algorithm is applied to motion control tasks of an autonomous vehicle and a cartpole, which involve external disturbances and variations of the model parameters, respectively. The experimental results demonstrate the effectiveness of the theoretical findings and the advantages of the proposed algorithm in terms of robustness and safety.									0	0	0	0	0	0	0			1674-7321	1869-1900										Harbin Inst Technol, Sch Astronaut, Harbin 150001, Peoples R ChinaHarbin Inst Technol, Sch Management, Harbin 150001, Peoples R ChinaBeijing Inst Tracking & Telecommun Technol, Beijing 100094, Peoples R China	Beijing Inst Tracking & Telecommun Technol			2024-01-15	WOS:001133130300006		
J	Fu, Yuchuan; Li, Changle; Luan, Tom H. H.; Zhang, Yao				Luan, Tom H./HLG-0711-2023						IoV and blockchain-enabled driving guidance strategy in complex traffic environment								CHINA COMMUNICATIONS													10.23919/JCC.ea.2020-0174.202302					MAY 2023		Article; Early Access		2023	Diversified traffic participants and complex traffic environment (e.g., roadblocks or road damage exist) challenge the decision-making accuracy of a single connected and autonomous vehicle (CAV) due to its limited sensing and computing capabilities. Using Internet of Vehicles (IoV) to share driving rules between CAVs can break limitations of a single CAV, but at the same time may cause privacy and safety issues. To tackle this problem, this paper proposes to combine IoV and blockchain technologies to form an efficient and accurate autonomous guidance strategy. Specifically, we first use reinforcement learning for driving decision learning, and give the corresponding driving rule extraction method. Then, an architecture combining IoV and blockchain is designed to ensure secure driving rule sharing. Finally, the shared rules will form an effective autonomous driving guidance strategy through driving rules selection and action selection. Extensive simulation proves that the proposed strategy performs well in complex traffic environment, mainly in terms of accuracy, safety, and robustness.									0	0	0	0	0	0	0			1673-5447											Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R ChinaXidian Univ, Res Inst Smart Transportat, Xian 710071, Peoples R ChinaNorthwestern Polytech Univ, Xian 710072, Peoples R China				2023-06-10	WOS:000988436200001		
J	Lee, R.; Puig-Navarro, J.; Agogino, A.K.; Giannakopoulou, D.; Mengshoel, O.J.; Kochenderfer, M.J.; Allen, B.D.										Adaptive Stress Testing of Trajectory Planning Systems								2019 AIAA Science and Technology Forum and Exposition (SciTech)								16 pp.	16 pp.											Conference Paper	2019	2019	Trajectory planners play a critical role in many autonomous vehicle systems, such as unmanned aircraft and driverless cars. However, analyzing the safety of these systems is very challenging due to their complexity. Existing validation approaches are generally focused on the correctness of the plans in isolation, while ignoring that the planner may not be able to generate a valid plan at all due to unanticipated operational conditions. While large scale simulation testing can find some failures, they are extremely unlikely to uncover complex failure modes due to the complexity of the system and the rarity of failures. This paper proposes to use adaptive stress testing to efficiently search for failure scenarios. Adaptive stress testing is a black box testing method that uses reinforcement learning to sample paths from a simulator of the system under test and its operating environment. The algorithm adversarially optimizes the stochastic elements in the system's environment to find the most likely sequence of states that results in a failure event. We provide a detailed discussion of sources of uncertainty and failure modes in trajectory planning systems. We apply our approach to stress test a prototype trajectory planner and show that adaptive stress testing can find a variety of operational failures including collisions with obstacles and various planning failures.					2019 AIAA Science and Technology Forum and Exposition (SciTech)2019 AIAA Science and Technology Forum and Exposition (SciTech)	7-11 Jan. 20197-11 Jan. 2019		San Diego, CA, USASan Diego, CA, USA	0	0	0	0	0	0	0																		2020-03-19	INSPEC:19363600		
J	Zhang, Mei; Chen, Kai; Zhu, Jinhui										An efficient planning method based on deep reinforcement learning with hybrid actions for autonomous driving on highway								INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS				14	10			3483	3499				10.1007/s13042-023-01845-2					JUN 2023		Article; Early Access		2023	Due to the complexity and uncertainty of the traffic, planning for autonomous driving (AD) on highway is challenging. Traditional planning algorithms have the problems of low and unstable efficiency, which reduces the real-time performance of the autonomous vehicle (AV). Deep reinforcement learning (DRL) is an emerging and promising method that has achieved amazing performance in many fields. In this paper, we propose a novel planning approach based on soft actor critic (SAC) with hybrid actions. The algorithm takes the structured information of the ego vehicle and the surroundings as input, and generates a termination state on the Frenet space for ego vehicle, then a feasible and continuous spatiotemporal trajectory will be output by a polynomial planner based on the intermediate state. Different from other sampling-based planning methods, only single polynomial planning is required, which improves planning efficiency significantly. Experiments show that DRL agent with hybrid actions is more secure than the agents with only continuous or discrete actions. Compared with other planning methods, the proposed algorithm has the least and most robust time for planning in different scenarios.									1	0	0	0	0	0	1			1868-8071	1868-808X										South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510640, Peoples R ChinaSouth China Univ Technol, Sch Software Engn, Guangzhou, Peoples R ChinaSouth China Univ Technol, Minist Educ, Key Lab Big Data & Intelligent Robot, Guangzhou, Peoples R China				2023-07-11	WOS:001017703100002		
C	Friji, Hamdi; Ghazzai, Hakim; Besbes, Hichem; Massoud, Yehia			IEEE	FRIJI, Hamdi/ABB-9977-2021; Ghazzai, Hakim/K-2518-2019	Ghazzai, Hakim/0000-0002-8636-4264					A DQN-based Autonomous Car-following Framework Using RGB-D Frames								2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT)								45	50				10.1109/GCAIOT51063.2020.9345899							Proceedings Paper	2020	2020	Modeling car-following behavior has recently garnered much attention due to the wide variety of applications it may be utilized in, such as accident analysis, driver assessment, and support systems. Some of the latest approaches investigate scenario-based autonomous driving algorithms. In this paper, we propose an end-to-end car-following framework that, based on high dimensional RGB-D features only, it ensures autonomous driving by following the actions of a leader car while taking into account other environmental factors (e.g. pedestrians, sidewalk crashing, etc.) To this end, a reinforcement learning (RL) algorithm, precisely an improved Deep Q-Network algorithm, is designed to avoid crashes with the leader car and its detection loss while effectively driving on road. The model is trained and tested using the CARLA simulator in different environments. Our preliminary tests show promising results for enhancing the driving capabilities of autonomous vehicles in many situations such as highways, one-way roads, and no-overtaking roads.					IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT)IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT)	DEC 12-16, 2020DEC 12-16, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0					978-1-7281-8420-3									Stevens Inst Technol, Sch Syst & Enterprises, Hoboken, NJ 07030 USAUniv Carthage, Higher Sch Commun Tunis, Tunis, Tunisia				2021-08-04	WOS:000675459100008		
C	Kimura, Hikaru; Takahashi, Masaki; Nishiwaki, Kazuhiro; Iezawa, Masahiro										Decision-Making Based on Reinforcement Learning and Model Predictive Control Considering Space Generation for Highway On-Ramp Merging								IFAC PAPERSONLINE				55	27			241	246				10.1016/j.ifacol.2022.10.519							Proceedings Paper	2022	2022	Reducing traffic accidents pertaining to autonomous vehicles has garnered attention. Merging on a highway is one of the most challenging problems that must be addressed for the realization of autonomous vehicles. It is difficult because an agent must decide where to merge in a complex and everchanging environment. Merging with congested highway traffic involves significant interaction with vehicles in the main lane. If there is no space for the autonomous vehicle to merge, it needs to work on vehicles in the main lane to create space and subsequently decide to merge or not. Reinforcement learning (RL) is a promising method for solving decision -making problems. However, it is difficult to guarantee the safety of the controller obtained using RL. Therefore, we propose a combined method in which decision -making is performed by RL and vehicle control by model predictive control (MPC) to ensure safety. The performance of the proposed system is tested by simulations. The proposed system made appropriate decisions according to the situation, and by controlling the vehicle in consideration of collision avoidance constraints, it showed a high merge success rate even in a crowded situation. Copyright 2022 The Authors. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)					9th IFAC Symposium on Mechatronic Systems (MECHATRONICS) / 16th International Conference on Motion and Vibration Control (MoViC)9th IFAC Symposium on Mechatronic Systems (MECHATRONICS) / 16th International Conference on Motion and Vibration Control (MoViC)	SEP 06-09, 2022SEP 06-09, 2022	Int Federat Automat Control, Tech Comm 4 2 Mechatron Syst; Int Federat Automat Control, Tech Comm 4 3 Robot; Int Federat Automat Control, Tech Comm 4 5 Human Machine Syst; Int Federat Automat Control, Tech Comm 6 2 Min, Mineral & Met Proc; Int Federat Automat Control, Tech Comm 6 3 Power & Energy Syst; Int Federat Automat Control, Tech Comm 7 1 Automot Control; Int Federat Automat Control, Tech Comm 7 2 Marine Syst; Int Federat Automat Control, Tech Comm 7 3 Aerosp; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; Int Federat Automat Control, Tech Comm 8 2 Biol & Med Syst; Int Federat Automat Control, Tech Comm 9 4 Control Educ; Amer Automat Control CouncilInt Federat Automat Control, Tech Comm 4 2 Mechatron Syst; Int Federat Automat Control, Tech Comm 4 3 Robot; Int Federat Automat Control, Tech Comm 4 5 Human Machine Syst; Int Federat Automat Control, Tech Comm 6 2 Min, Mineral & Met Proc; Int Federat Automat Control, Tech Comm 6 3 Power & Energy Syst; Int Federat Automat Control, Tech Comm 7 1 Automot Control; Int Federat Automat Control, Tech Comm 7 2 Marine Syst; Int Federat Automat Control, Tech Comm 7 3 Aerosp; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; Int Federat Automat Control, Tech Comm 8 2 Biol & Med Syst; Int Federat Automat Control, Tech Comm 9 4 Control Educ; Amer Automat Control Council	Univ Calif Los Angeles, Los Angeles, CAUniv Calif Los Angeles, Los Angeles, CA	0	0	0	0	0	0	0			2405-8963											Keio Univ, Dept Syst Design Engn, Yokohama, Kanagawa, JapanMitsubishi Electr Corp, Adv Technol R&D Ctr, Tokyo, Japan				2022-12-13	WOS:000889047000041		
J	Makantasis, K.; Kontorinaki, M.; Nikolos, I.										A deep reinforcement-learning-based driving policy for autonomous road vehicles [arXiv]								arXiv								17 pp.	17 pp.											Journal Paper	10 July 2019	2019	In this work we consider the problem of path planning for an autonomous vehicle that moves on a freeway. The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics. On the contrary, we propose the development of a driving policy based on reinforcement learning. In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required. We consider driving scenarios where the road is occupied both by autonomous and manual driving vehicles. To the best of our knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments. The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established SUMO microscopic traffic flow simulator. Finally, we present some initial results regarding the effect of autonomous vehicles' behavior on the overall traffic flow.									0	0	0	0	0	0	0														Sch. of Production Eng. & Manage., Tech. Univ. of Crete, Chania, Greece				2019-11-22	INSPEC:19104186		
J	Chen, Ziqin; Anglea, Timothy; Zhang, Yuanzhao; Wang, Yongqiang				Zhang, Yuanzhao/J-7344-2019	Zhang, Yuanzhao/0000-0002-2056-7755					Optimal synchronization in pulse-coupled oscillator networks using reinforcement learning								PNAS NEXUS				2	4								10.1093/pnasnexus/pgad102							Article	APR 3 2023	2023	Spontaneous synchronization is ubiquitous in natural and man-made systems. It underlies emergent behaviors such as neuronal response modulation and is fundamental to the coordination of robot swarms and autonomous vehicle fleets. Due to its simplicity and physical interpretability, pulse-coupled oscillators has emerged as one of the standard models for synchronization. However, existing analytical results for this model assume ideal conditions, including homogeneous oscillator frequencies and negligible coupling delays, as well as strict requirements on the initial phase distribution and the network topology. Using reinforcement learning, we obtain an optimal pulse-interaction mechanism (encoded in phase response function) that optimizes the probability of synchronization even in the presence of nonideal conditions. For small oscillator heterogeneities and propagation delays, we propose a heuristic formula for highly effective phase response functions that can be applied to general networks and unrestricted initial phase distributions. This allows us to bypass the need to relearn the phase response function for every new network.									1	0	0	0	0	0	1				2752-6542										Clemson Univ, Dept Elect & Comp Engn, Clemson, SC 29634 USASanta Fe Inst, 1399 Hyde Pk Rd, Santa Fe, NM 87501 USA				2023-10-14	WOS:001053440200022	37077885	
J	Huang Hui; Wei Hanbing							黄辉; 隗寒冰			Lane Changing Trajectory Planning of Autonomous Vehicle Based on Driving Characteristic Learning			基于驾驶特征学习的自动驾驶车辆换道轨迹规划				汽车技术	Automobile Technology					4			19	25	1000-3703(2021)4<19:JYJSTZ>2.0.TX;2-#										Article	2021	2021	Large deviation between vehicle planning trajectory and driver decision trajectory exists in the process of lane change for autonomous vehicles.To solve this problem,a lane change trajectory planning algorithm is developed based on learning trajectory feature.Based on the sampling and cost optimization combination of trajectory planning,the algorithm collects the driver's lane changing trajectory function characteristics.By means of the maximum entropy inverse reinforcement learning,cost function weight is updated iteratively.According to the achieved cost function,the alternative sampling paths are designated to generate lane changing trajectory of autonomous vehicles which reflect the characteristics of drivers' trajectories.The experimental results show that the lane changing trajectory after learning of drivers' characteristics are incorporated in the lane changing trajectory area of the driver.The trajectory's features are more similar to the real lane changing trajectory's features of the driver,and can reflect driver's subjective feeling.			针对自动驾驶车辆换道过程中存在的车辆规划轨迹与人类驾驶员决策轨迹偏差较大问题,开发了一种基于驾驶员轨迹特征学习的换道轨迹规划算法。采集驾驶员换道轨迹曲线函数特征,在轨迹采样及成本优化相结合的轨迹规划基础上,采用最大熵逆强化学习策略迭代更新成本函数权重,并依据学习的成本函数筛选备选采样轨迹,生成反映驾驶员轨迹特征的自动驾驶车辆换道轨迹。试验结果表明,进行驾驶员特征学习后的换道轨迹基本包含在驾驶员换道轨迹区域内,且轨迹特征更为接近人类驾驶员换道轨迹特征,更能反映驾驶员主观感受。						0	0	0	0	0	0	0			1000-3703											重庆交通大学, 重庆 400074, 中国Chongqing Jiaotong University, Chongqing 400074	重庆交通大学Chongqing Jiaotong University			2021-07-09	CSCD:6948319		
B	Tsai, J.; Chang, Y.-T.; Chuang, P.-H.; You, Z.										An Autonomous Vehicle-Following Technique for Self-Driving Cars Based on the Semantic Segmentation Technique								2023 IEEE International Symposium on Robotic and Sensors Environments (ROSE)								1	7				10.1109/ROSE60297.2023.10410810							Conference Paper	2023	2023	Self-driving cars are an increasingly concerned and researched popular field. In this paper, we utilize the CARLA (Car Learning to Act) self-driving car simulator to build a custom simulation environment. The adopted environment consists of a figure-8-shaped driveway with two lanes, where the training car will follow a lead reference car with a varying speed on the inner lane and multiple reference cars will intermittently cut into the inner lane from the outer lane so as to interfere with the training car. This setup aims to more accurately emulate real-world traffic conditions. Specifically, we employ the Convolutional Neural Network (CNN) combined with the Deep Reinforcement Learning (DRL) technology to achieve autonomous control of the self-driving car. A semantic segmentation camera is installed beside the rearview mirror of the training car to capture the observation image of the road ahead while the car is moving, which is then fed into the DRL models for training and decision making, along with the car speed. Additionally, we design an appropriate reward mechanism for our models according to the current situation of the self-driving car to improve driving safety and comfort. We adopt the Deep Deterministic Policy Gradient (DDPG) and time-cycling Recurrent Deterministic Policy Gradient (RDPG) learning algorithms in DRL to train the self-driving car, aiming to achieve the goal of autonomously determining safe and comfortable driving paths while following other vehicles.					2023 IEEE International Symposium on Robotic and Sensors Environments (ROSE)2023 IEEE International Symposium on Robotic and Sensors Environments (ROSE)	20232023	IEEE; IEEE Instrumentation & Measurement SocietyIEEE; IEEE Instrumentation & Measurement Society	Tokyo, JapanTokyo, Japan	0	0	0	0	0	0	0					979-8-3503-0804-4									Dept. of Electr. Eng., Nat. Chung Hsing Univ., Taichung, Taiwan				2024-02-23	INSPEC:24556903		
J	Lelko, A.; Nemeth, B.; Fenyes, D.; Gaspar, P.										Integration of Robust Control with Reinforcement Learning for Safe Autonomous Vehicle Motion								IFAC PapersOnLine								1101	6				10.1016/j.ifacol.2023.10.1711							Journal Paper	2023	2023	This paper presents a control design framework for the integration of robust control and reinforcement learning-based (RL) control agent. The proposed integration method is applied for motion control of autonomous road vehicles, providing safe motion. In the integrated motion control, longitudinal and lateral dynamics are incorporated. The high-performance motion of the vehicle, e.g., high-velocity motion, path following, and reduction of lateral acceleration, through the RL-based control agent is achieved. The training through Proximal Policy Optimization during episodes is performed. Safe motion with guaranteed performances, i.e., keeping limits on lateral error, through the robust control and the supervisor is achieved. The robust control is designed through the H method, and in the supervisor, a constrained quadratic programming task is performed. As a result, lateral and longitudinal control inputs of the vehicle are calculated by the integrated control system. The effectiveness of the proposed control method using simulation scenarios and test scenarios on a small-scaled test vehicle is illustrated. All rights reserved Elsevier.									0	0	0	0	0	0	0			2405-8963											Inst. for Comput. Sci. & Control (SZTAKI), Eotvos Lorand Res. Network, Budapest, HungaryDept. of Control for Transp. & Vehicle Syst., Budapest Univ. of Technol. & Econ., Budapest, Hungary				2024-02-01	INSPEC:24420999		
C	Greenwood, Garrison W.; Elsayed, Saber; Sarker, Ruhul; Abbass, Hussein A.			IEEE	Sarker, Ruhul A/B-5677-2012; Abbass, Hussein A/C-9563-2009	Sarker, Ruhul A/0000-0002-1363-2774; Abbass, Hussein A/0000-0002-8837-0748; Elsayed, Saber/0000-0003-0836-6122					Online Generation of Trajectories for Autonomous Vehicles using a Multi-Agent System								2014 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC)								1218	1224											Proceedings Paper	2014	2014	Autonomous vehicles are frequently deployed in environments where only certain trajectories are feasible. Classical trajectory generation methods attempt to find a feasible trajectory that satisfies a set of constraints. In some cases the optimal trajectory may be known, but it is hidden from the autonomous vehicle. Under such circumstance the vehicle must discover a feasible trajectory. This paper describes a multi-agent system that uses a combination of reinforcement learning and differential evolution to generate a trajectory that is epsilon-close to a target trajectory that is hidden.					IEEE Congress on Evolutionary Computation (CEC)IEEE Congress on Evolutionary Computation (CEC)	JUL 06-11, 2014JUL 06-11, 2014	IEEEIEEE	Beijing, PEOPLES R CHINABeijing, PEOPLES R CHINA	0	0	0	0	0	0	0					978-1-4799-1488-3									Portland State Univ, Dept Elect & Comp Engn, Portland, OR 97207 USAUNSW Australia Canberra, Sch Engn & Informat Technol, Canberra, ACT 2600, Australia				2015-08-09	WOS:000356684601081		
J	Tsai, Jichiang; Chang, Che-Cheng; Li, Tzu										Autonomous Driving Control Based on the Technique of Semantic Segmentation								SENSORS				23	2					895			10.3390/s23020895							Article	JAN 2023	2023	Advanced Driver Assistance Systems (ADAS) are only applied to relatively simple scenarios, such as highways. If there is an emergency while driving, the driver should take control of the car to deal properly with the situation at any time. Obviously, this incurs the uncertainty of safety. Recently, in the literature, several studies have been proposed for the above-mentioned issue via Artificial Intelligence (AI). The achievement is exactly the aim that we look forward to, i.e., the autonomous vehicle. In this paper, we realize the autonomous driving control via Deep Reinforcement Learning (DRL) based on the CARLA (Car Learning to Act) simulator. Specifically, we use the ordinary Red-Green-Blue (RGB) camera and semantic segmentation camera to observe the view in front of the vehicle while driving. Then, the captured information is utilized as the input for different DRL models so as to evaluate the performance, where the DRL models include DDPG (Deep Deterministic Policy Gradient) and RDPG (Recurrent Deterministic Policy Gradient). Moreover, we also design an appropriate reward mechanism for these DRL models to realize efficient autonomous driving control. According to the results, only the RDPG strategies can finish the driving mission with the scenario that does not appear/include in the training scenario, and with the help of the semantic segmentation camera, the RDPG control strategy can further improve its efficiency.									2	0	0	0	0	0	2				1424-8220										Natl Chung Hsing Univ, Dept Elect Engn, Taichung 402, TaiwanNatl Chung Hsing Univ, Grad Inst Commun Engn, Taichung 402, TaiwanFeng Chia Univ, Dept Informat Engn & Comp Sci, Taichung 407, TaiwanNatl Chung Hsing Univ, Dept Elect Engn, Taichung 402, Taiwan				2023-02-09	WOS:000915928300001	36679688	
J	Wang, Yue; Sarkar, Esha; Li, Wenqing; Maniatakos, Michail; Jabari, Saif Eddin					wang, Yue/0000-0002-9260-3970; Maniatakos, Michail/0000-0001-6899-0651					Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems								IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY				16				4772	4787				10.1109/TIFS.2021.3114024							Article	2021	2021	Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.									12	1	0	0	0	0	16			1556-6013	1556-6021										NYU, Dept Elect & Comp Engn, Tandon Sch Engn, New York, NY 10013 USANew York Univ Abu Dhabi, Div Engn, Abu Dhabi, U Arab EmiratesNYU, Dept Civil & Urban Engn, Tandon Sch Engn, New York, NY 10013 USA				2021-10-20	WOS:000704109600009		
C	Chalaki, Behdad; Beaver, Logan E.; Remer, Ben; Jang, Kathy; Vinitsky, Eugene; Bayen, Alexandre M.; Malikopoulos, Andreas A.			IEEE	Chalaki, Behdad/AAQ-5101-2020; Beaver, Logan E./AAA-8568-2022	Chalaki, Behdad/0000-0002-3055-1693; Beaver, Logan E./0000-0002-9770-2740; Vinitsky, Eugene/0000-0003-2372-4944; Malikopoulos, Andreas/0000-0003-4817-0976					Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning								2020 IEEE 16TH INTERNATIONAL CONFERENCE ON CONTROL & AUTOMATION (ICCA)		IEEE International Conference on Control and Automation ICCA						35	40											Proceedings Paper	2020	2020	In this article, we demonstrate a zero-shot transfer of an autonomous driving policy from simulation to University of Delaware's scaled smart city with adversarial multi-agent reinforcement learning, in which an adversary attempts to decrease the net reward by perturbing both the inputs and outputs of the autonomous vehicles during training. We train the autonomous vehicles to coordinate with each other while crossing a roundabout in the presence of an adversary in simulation. The adversarial policy successfully reproduces the simulated behavior and incidentally outperforms, in terms of travel time, both a human-driving baseline and adversary-free trained policies. Finally, we demonstrate that the addition of adversarial training considerably improves the performance of the policies after transfer to the real world compared to Gaussian noise injection.					16th IEEE International Conference on Control and Automation (ICCA)16th IEEE International Conference on Control and Automation (ICCA)	OCT 09-11, 2020OCT 09-11, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	19	0	0	0	0	0	21			1948-3449		978-1-7281-9093-8									Univ Delaware, Dept Mech Engn, Newark, DE 19716 USAUniv Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USAUniv Calif Berkeley, Dept Mech Engn, Berkeley, CA 94720 USA				2021-05-28	WOS:000646357300007		
J	Lee, Dongcheul										A Comparative Analysis of Reinforcement Learning Activation Functions for Parking of Autonomous Vehicles			자율주행 자동차의 주차를 위한 강화학습 활성화 함수 비교 분석					The Journal of The Institute of Internet, Broadcasting and Communication	한국인터넷방송통신학회 논문지			21	6			75	81				10.7236/JIIBC.2022.22.6.75							research-article	2022	2022	Autonomous vehicles, which can dramatically solve the lack of parking spaces, are making great progress through deep reinforcement learning. Activation functions are used for deep reinforcement learning, and various activation functions have been proposed, but their performance deviations were large depending on the application environment. Therefore, finding the optimal activation function depending on the environment is important for effective learning. This paper analyzes 12 functions mainly used in reinforcement learning to compare and evaluate which activation function is most effective when autonomous vehicles use deep reinforcement learning to learn parking. To this end, a performance evaluation environment was established, and the average reward of each activation function was compared with the success rate, episode length, and vehicle speed. As a result, the highest reward was the case of using GELU, and the ELU was the lowest. The reward difference between the two activation functions was 35.2%.				주차 공간의 부족함을 획기적으로 해결할 수 있는 자율주행 자동차는 심층 강화 학습을 통해 큰 발전을 이루고있다. 심층 강화 학습에는 활성화 함수가 사용되는데, 그동안 다양한 활성화 함수가 제안되어 왔으나 적용 환경에 따라그 성능 편차가 심했다. 따라서 환경에 따라 최적의 활성화 함수를 찾는 것이 효과적인 학습을 위해 중요하다. 본 논문은자율주행 자동차가 주차를 학습하기 위해 심층 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 가장 효과적인지 비교 평가하기 위해 강화 학습에 주로 사용되는 12개의 함수를 분석하였다. 이를 위해 성능 평가 환경을 구축하고각 활성화 함수의 평균 보상을 성공률, 에피소드 길이, 자동차 속도와 비교하였다. 그 결과 가장 높은 보상은 GELU를사용한 경우였고, ELU는 가장 낮았다. 두 활성화 함수의 보상 차이는 35.2%였다.					0	0	0	0	0	0	0			2289-0238															2023-01-25	KJD:ART002908827		
J	Ye, Qiming; Feng, Yuxiang; Macias, Jose Javier Escribano; Stettler, Marc; Angeloudis, Panagiotis					Angeloudis, Panagiotis/0000-0002-6778-8264; YE, Qiming/0000-0003-4732-2042; Feng, Yuxiang (Felix)/0000-0002-5530-2503; Stettler, Marc/0000-0002-2066-9380					Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions Using Reinforcement Learning								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	2			2024	2034				10.1109/TITS.2022.3220110					NOV 2022		Article; Early Access		2023	The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure. In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed. Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand. Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions. We implement a centralised learning paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations. Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians. Furthermore, the distributive learning algorithm outperforms its centralised counterpart regarding computational cost (49.55%), benchmark rewards (25.35%), best cumulative rewards (24.58%), optimal actions (13.49%) and rate of convergence. This novel road management technique could potentially contribute to the flow-adaptive and active mobility-friendly streets in the AVs era.									0	0	0	0	0	0	0			1524-9050	1558-0016										Imperial Coll London, Ctr Transport Studies, Dept Civil & Environm Engn, London SW72AZ, England				2022-12-01	WOS:000886766700001		
J	Yan, Ruidong; Li, Penghui; Gao, Hongbo; Huang, Jin; Wang, Chengbo				zhang, ly/JMB-7214-2023						Car-following strategy of intelligent connected vehicle using extended disturbance observer adjusted by reinforcement learning								CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY													10.1049/cit2.12252					JUL 2023		Article; Early Access		2023	Disturbance observer-based control method has achieved good results in the car-following scenario of intelligent and connected vehicle (ICV). However, the gain of conventional extended disturbance observer (EDO)-based control method is usually set manually rather than adjusted adaptively according to real time traffic conditions, thus declining the car-following performance. To solve this problem, a car-following strategy of ICV using EDO adjusted by reinforcement learning is proposed. Different from the conventional method, the gain of proposed strategy can be adjusted by reinforcement learning to improve its estimation accuracy. Since the "equivalent disturbance" can be compensated by EDO to a great extent, the disturbance rejection ability of the car-following method will be improved significantly. Both Lyapunov approach and numerical simulations are carried out to verify the effectiveness of the proposed method.									0	0	0	0	0	0	0			2468-6557	2468-2322										Beijing Jiaotong Univ, Sch Traff & Transportat, Beijing, Peoples R ChinaUniv Sci & Technol China, Sch Informat Sci & Technol, Dept Automat, Hefei, Peoples R ChinaTsinghua Univ, Sch Vehicle & Mobil, Beijing, Peoples R ChinaLiverpool John Moores Univ, Offshore & Marine Res Inst LOOM, Liverpool Logist, Liverpool, England				2023-07-08	WOS:001017997000001		
B	Mohammed, S.T.; Kastouri, M.; Niederfahrenhorst, A.; Ascheid, G.										Video representation learning for decoupled deep reinforcement learning applied to autonomous driving								2023 IEEE/SICE International Symposium on System Integration (SII)								1	6				10.1109/SII55687.2023.10039291							Conference Paper	2023	2023	This work focuses on using Deep Reinforcement Learning (DRL) to control an autonomous vehicle in the hyper-realistic urban simulation LGSVL. Classical control systems such as MPC maneuver vehicles based on a given trajectory, current velocity, position, distances, and more. Our approach does not pass this information to the DRL agent but only images provided by the camera. Current DRL efforts also exploit similar approaches for autonomous driving, but they are only suitable for small, simple tasks using simple simulations. Our approach consists of two differently trained neural networks (NN), a perceptual NN for representation learning and an actor NN for selecting the correct action. The perception NN will be trained via representation and self-supervised learning to strengthen our DRL agent's understanding of the scene. It can recognize temporal information and the dynamics of a complex environment. This work shows the importance of decoupling the perception and decision (actor) model for autonomous driving. All in all, we could drive autonomously in a hyper-realistic urban simulation using our modular DRL framework. Moreover, our approach also provides a solution for other similar tasks in the field of robotics based on images.					2023 IEEE/SICE International Symposium on System Integration (SII)2023 IEEE/SICE International Symposium on System Integration (SII)	17-20 Jan. 202317-20 Jan. 2023		Atlanta, GA, USAAtlanta, GA, USA	0	0	0	0	0	0	0					979-8-3503-9868-7									Fac. of Electr. Eng., RWTH Aachen Univ., Aachen, Germany				2023-04-26	INSPEC:22626456		
J	Jin Lisheng; Han Guangde; Xie Xianyi; Guo Baicang; Liu Guofeng; Zhu Wentao							金立生; 韩广德; 谢宪毅; 郭柏苍; 刘国峰; 朱文涛			Review of Autonomous Driving Decision-Making Research Based on Reinforcement Learning			基于强化学习的自动驾驶决策研究综述				汽车工程	Automotive Engineering				45	4			527	540	1000-680X(2023)45:4<527:JYQHXX>2.0.TX;2-8										Article	2023	2023	Decision-making technology of autonomous vehicle is promoted by the development of reinforcement learning, and intelligent decision-making technology has become a key issue of high concern in the field of autonomous driving. Taking the development of reinforcement learning algorithm as the main line in this paper, the indepth application of this algorithm in the field of single-car autonomous driving decision-making is summarized. Traditional reinforcement learning algorithms, classic algorithms and frontier algorithms are summarized and compared from the aspect of basic principles and theoretical modeling methods. According to the classification of autonomous driving decision-making methods in different scenarios, the impact of environmental state observability on modeling is analyzed, and the application technology routes of typical reinforcement learning algorithms at different levels are emphasized. The research prospects for the autonomous driving decision-making method are proposed in order to provide a useful reference for the research of autonomous driving decision-making.			强化学习的发展推动了自动驾驶决策技术的进步,智能决策技术已成为自动驾驶领域高度关注的要点问题。本文以强化学习算法发展为主线,综述该算法在单车自动驾驶决策领域的深入应用。对强化学习传统算法、经典算法和前沿算法从基本原理和理论建模等方面进行归纳总结与对比分析。针对不同场景的自动驾驶决策方法分类,分析环境状态可观测性对建模的影响,重点阐述了不同层次强化学习典型算法的应用技术路线,并对自动驾驶决策方法提出研究展望,以期为自动驾驶决策方案研究提供有益参考。						0	0	0	0	0	0	0			1000-680X											燕山大学车辆与能源学院, 秦皇岛, 河北 066004, 中国School of Vehicle and Energy,Yanshan University, Qinhuangdao, Hebei 066004, China	燕山大学车辆与能源学院School of Vehicle and Energy,Yanshan University			2023-11-24	CSCD:7533449		
C	Du, Yali; Ma, Chengdong; Liu, Yuchen; Lin, Runji; Dong, Hao; Wang, Jun; Yang, Yaodong			IEEE	Liu, Yuchen/AAH-6098-2021	Du, Yali/0000-0001-5683-2621					Scalable Model-based Policy Optimization for Decentralized Networked Systems								2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						9019	9026				10.1109/IROS47612.2022.9982253							Proceedings Paper	2022	2022	Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks. Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly, requiring communications or shifting or resources. This work aims to improve data efficiency of multi-agent control by model-based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO). In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts. To alleviate the bias of model-generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation. To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traffic signal control (ATSC). Empirical results show that our method achieves superior data efficiency and matches the performance of model-free methods using true models.The source code of our algorithm and baselines can be found at https://github.com/PKU-MARL/Model-Based-MARL.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	OCT 23-27, 2022OCT 23-27, 2022	IEEE; Royal Soc Japan; IEEE Robot & Automat Soc; IES; SICE; New Technol FdnIEEE; Royal Soc Japan; IEEE Robot & Automat Soc; IES; SICE; New Technol Fdn	Kyoto, JAPANKyoto, JAPAN	0	0	0	0	0	0	0			2153-0858		978-1-6654-7927-1									Kings Coll London, 30 Aldwych, London, EnglandXiamen Univ, 422 Siming Rd, Xiamen, Peoples R ChinaPeking Univ, Beijing, Peoples R ChinaChinese Acad Sci, 95 Zhongguancun East Rd, Beijing, Peoples R ChinaUCL, 66-72 Gower St, London, EnglandPeking Univ, 5 Yiheyuan Rd, Beijing, Peoples R ChinaPeking Univ, Sch CS, CFCS, Beijing, Peoples R ChinaPeking Univ, Inst AI, Beijing, Peoples R ChinaBIGAI, Beijing, Peoples R China	BIGAI			2023-03-12	WOS:000909405301089		
J	Wang, Jianfei; Lv, Tiejun; Huang, Pingmu; Mathiopoulos, P. Takis				Mathiopoulos, P. Takis/L-8370-2013	Mathiopoulos, P. Takis/0000-0002-3332-4699					Mobility-Aware Partial Computation Offloading in Vehicular Networks: A Deep Reinforcement Learning Based Scheme								CHINA COMMUNICATIONS				17	10			31	49											Article	OCT 2020	2020	Encouraged by next-generation networks and autonomous vehicle systems, vehicular networks must employ advanced technologies to guarantee personal safety, reduce traffic accidents and ease traffic jams. By leveraging the computing ability at the network edge, multi-access edge computing (MEC) is a promising technique to tackle such challenges. Compared to traditional full offloading, partial offloading offers more flexibility in the perspective of application as well as deployment of such systems. Hence, in this paper, we investigate the application of partial computing offloading in-vehicle networks. In particular, by analyzing the structure of many emerging applications, e.g., AR and online games, we convert the application structure into a sequential multi-component model. Focusing on shortening the application execution delay, we extend the optimization problem from the single-vehicle computing offloading (SVCOP) scenario to the multi-vehicle computing offloading (MVCOP) by taking multiple constraints into account. A deep reinforcement learning (DRL) based algorithm is proposed as a solution to this problem. Various performance evaluation results have shown that the proposed algorithm achieves superior performance as compared to existing offloading mechanisms in deducing application execution delay.									20	4	0	0	0	0	22			1673-5447											Beijing Univ Posts & Telecommun BUPT, Sch Informat & Commun Engn, Beijing 100876, Peoples R ChinaNatl & Kapodistrian Univ Athens, Dept Informat & Telecommun, Athens 15784, Greece				2020-12-22	WOS:000587726900004		
B	Krodel, N.; Kuhnert, K.-D.										Pattern matching as the nucleus for either autonomous driving or driver assistance systems								IV'2002. IEEE Intelligent Vehicle Symposium. Proceedings (Cat. No.02TH8607)								135	40 vol.1				10.1109/IVS.2002.1187941							Conference Paper	2003	2003	Concerns autonomous vehicle driving by pattern matching combined with reinforcement learning. In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicle or obstacle but with the strict requirement to self-improve driving behaviour. A camera is used to build quickly an abstract complete description (ACSD) of vehicle's current situation. This combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an approximate nearest neighbour pattern matching algorithm with a O(n log m) characteristic compared to O(n·m) for the conventional nearest neighbour calculation. In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.					IV'2002. IEEE Intelligent Vehicle Symposium. ProceedingsIV'2002. IEEE Intelligent Vehicle Symposium. Proceedings	17-21 June 200217-21 June 2002	IEEE; ITSCIEEE; ITSC	Versailles, FranceVersailles, France	0	0	0	0	0	0	0					0-7803-7346-4									Inst. for Real-Time-Syst., Siegen Univ., Siegen, Germany				2003-01-01	INSPEC:7712618		
C	Baheri, Ali; Kolmanovsky, Ilya; Girard, Anouck; Tseng, H. Eric; Filev, Dimitar			IEEE		Kolmanovsky, Ilya/0000-0002-7225-4160					Vision-Based Autonomous Driving: A Model Learning Approach								2020 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						2520	2525				10.23919/acc45564.2020.9147510							Proceedings Paper	2020	2020	We present an integrated approach for perception and control for an autonomous vehicle and demonstrate this approach in a high-fidelity urban driving simulator. Our approach first builds a model for the environment, then trains a policy exploiting the learned model to identify the action to take at each time-step. To build a model for the environment, we leverage several deep learning algorithms. To that end, first we train a variational autoencoder to encode the input image into an abstract latent representation. We then utilize a recurrent neural network to predict the latent representation of the next frame and handle temporal information. Finally, we utilize an evolutionary-based reinforcement learning algorithm to train a controller based on these latent representations to identify the action to take. We evaluate our approach in CARLA, a high-fidelity urban driving simulator, and conduct an extensive generalization study. Our results demonstrate that our approach outperforms several previously reported approaches in terms of the percentage of successfully completed episodes for a lane keeping task.					American Control Conference (ACC)American Control Conference (ACC)	JUL 01-03, 2020JUL 01-03, 2020	Amer Automat Control Council; Int Federat Automat ControlAmer Automat Control Council; Int Federat Automat Control	Denver, CODenver, CO	2	0	0	0	0	0	3			0743-1619	2378-5861	978-1-5386-8266-1									West Virginia Univ, Dept Aerosp & Mech Engn, Morgantown, WV 26505 USAUniv Michigan, Dept Aerosp Engn, Ann Arbor, MI 48109 USAFord Res & Innovat Ctr, 2101 Village Rd, Dearborn, MI 48124 USA				2020-01-01	WOS:000618079802079		
J	Nguyen, Hung Duy; Han, Kyoungseok				Nguyen, Hung Duy/JXL-7245-2024	Nguyen, Hung Duy/0000-0001-9495-489X					Safe Reinforcement Learning-based Driving Policy Design for Autonomous Vehicles on Highways								INTERNATIONAL JOURNAL OF CONTROL AUTOMATION AND SYSTEMS				21	12			4098	4110				10.1007/s12555-023-0255-4							Article	DEC 2023	2023	Safe decision-making strategy of autonomous vehicles (AVs) plays a critical role in avoiding accidents. This study develops a safe reinforcement learning (safe-RL)-based driving policy for AVs on highways. The hierarchical framework is considered for the proposed safe-RL, where an upper layer executes a safe exploration-exploitation by modifying the exploring process of the epsilon-greedy algorithm, and a lower layer utilizes a finite state machine (FSM) approach to establish the safe conditions for state transitions. The proposed safe-RL-based driving policy improves the vehicle's safe driving ability using a Q-table that stores the values corresponding to each action state. Moreover, owing to the trade-off between the epsilon-greedy values and safe distance threshold, the simulation results demonstrate the superior performance of the proposed approach compared to other alternative RL approaches, such as the epsilon-greedy Q-learning (GQL) and decaying epsilon-greedy Q-learning (DGQL), in an uncertain traffic environment. This study's contributions are twofold: it improves the autonomous vehicle's exploration-exploitation and safe driving ability while utilizing the advantages of FSM when surrounding cars are inside safe-driving zones, and it analyzes the impact of safe-RL parameters in exploring the environment safely.									0	0	0	0	0	0	0			1598-6446	2005-4092										Kyungpook Natl Univ, Sch Mech Engn, Daegu 41566, South KoreaTU Wien, Automat & Control Inst ACIN, A-1040 Vienna, Austria				2024-02-14	WOS:001112720200025		
J	Zarrouki, B.; Wang, C.; Betz, J.										Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control [arXiv]								Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control [arXiv]																				Preprint	2023	2023	In this paper, we present a Deep Reinforcement Learning (RL)-driven Adaptive Stochastic Nonlinear Model Predictive Control (SNMPC) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, we conceive an RL agent to proactively anticipate upcoming control tasks and to dynamically determine the most suitable combination of key SNMPC parameters - foremost the robustification factor $\kappa$ and the Uncertainty Propagation Horizon (UPH) $T_u$. We analyze the trained RL agent's decision-making process and highlight its ability to learn context-dependent optimal parameters. One key finding is that adapting the constraints robustification factor with the learned policy reduces conservatism and improves closed-loop performance while adapting UPH renders previously infeasible SNMPC problems feasible when faced with severe disturbances. We showcase the enhanced robustness and feasibility of our Adaptive SNMPC (aSNMPC) through the real-time motion control task of an autonomous passenger vehicle to follow an optimal race line when confronted with significant time-variant disturbances. Experimental findings demonstrate that our look-ahead RL-driven aSNMPC outperforms its Static SNMPC (sSNMPC) counterpart in minimizing the lateral deviation both with accurate and inaccurate disturbance assumptions and even when driving in previously unexplored environments.									0	0	0	0	0	0	0																		2023-12-07	INSPEC:24145424		
J	Ben Naveed, K.; Zhiqian Qiao; Dolan, J.M.										Trajectory Planning for Autonomous Vehicles Using Hierarchical Reinforcement Learning [arXiv]								arXiv								7 pp.	7 pp.											Journal Paper	9 Nov. 2020	2020	Planning safe trajectories under uncertain and dynamic conditions makes the autonomous driving problem significantly complex. Current sampling-based methods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this problem because of the high computational cost. Supervised learning methods such as Imitation Learning lack generalization and safety guarantees. To address these problems and in order to ensure a robust framework, we propose a Hierarchical Reinforcement Learning (HRL) structure combined with a Proportional-Integral-Derivative (PID) controller for trajectory planning. HRL helps divide the task of autonomous vehicle driving into sub-goals and supports the network to learn policies for both high-level options and low-level trajectory planner choices. The introduction of sub-goals decreases convergence time and enables the policies learned to be reused for other scenarios. In addition, the proposed planner is made robust by guaranteeing smooth trajectories and by handling the noisy perception system of the ego-car. The PID controller is used for tracking the waypoints, which ensures smooth trajectories and reduces jerk. The problem of incomplete observations is handled by using a Long-Short-Term-Memory (LSTM) layer in the network. Results from the high-fidelity CARLA simulator indicate that the proposed method reduces convergence time, generates smoother trajectories, and is able to handle dynamic surroundings and noisy observations.									0	0	0	0	0	0	0														Electron. & Inf. Eng, Hong Kong Polytech. Univ., Hong Kong, ChinaElectr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USARobot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA				2021-01-29	INSPEC:20226647		
J	Lee, Cheonghwa; An, Dawn					lee, cheonghwa/0000-0002-0119-7293					Decision-Making in Fallback Scenarios for Autonomous Vehicles: Deep Reinforcement Learning Approach								APPLIED SCIENCES-BASEL				13	22					12258			10.3390/app132212258							Article	NOV 2023	2023	This paper proposes a decision-making algorithm based on deep reinforcement learning to support fallback techniques in autonomous vehicles. The fallback technique attempts to mitigate or escape risky driving conditions by responding to appropriate avoidance maneuvers essential for achieving a Level 4+ autonomous driving system. However, developing a fallback technique is difficult because of the innumerable fallback situations to address and eligible optimal decision-making among multiple maneuvers. We employed a decision-making algorithm utilizing a scenario-based learning approach to address these issues. First, we crafted a specific fallback scenario encompassing the challenges to be addressed and matched the anticipated optimal maneuvers as determined by heuristic methods. In this scenario, the ego vehicle learns through trial and error to determine the most effective maneuver. We conducted 100 independent training sessions to evaluate the proposed algorithm and compared the results with those of heuristic-derived maneuvers. The results were promising; 38% of the training sessions resulted in the vehicle learning lane-change maneuvers, whereas 9% mastered slow following. Thus, the proposed algorithm successfully learned human-equivalent fallback capabilities from scratch within the provided scenario.									0	0	0	0	0	0	0				2076-3417										Korea Inst Ind Technol, Daegyeong Div, Adv Mechatron R&D Grp, Daegu 42994, South Korea				2023-12-23	WOS:001120279800001		
B	Hao Zeng; Lintao Zhang; Yuliang Tang; Yanglong Sun; Yuqi Ruan										Intelligent Resource Allocation Based on Reinforcement Learning for NOMA Vehicular Cooperative Communication Networks								2021 16th International Conference on Computer Science & Education (ICCSE)								622	7				10.1109/ICCSE51940.2021.9569512							Conference Paper	2021	2021	There exits difficulty in environment sensing for autonomous vehicles in Internet of vehicles (IoV) networks. The autonomous vehicle can obtain a full sense of the environment from connected vehicles by vehicle-to-vehicle (V2V) communications and make better decisions for driving safety. However, limited spectrum resources cannot adapt to the increasing number of communication links and the traditional frequency multiplexing will lead to serious co-frequency interference and fail to connect more vehicles. Therefore, we develop a resource allocation scheme through jointly optimizing sub-band selection and power control based on V2X Network. Meanwhile, the non-orthogonal multiple access (NOMA) technology is adopted for multi-V2V communication that shares the spectrum allocated to vehicle-to-infrastructure (V2I) links. Considering time-varying channel conditions caused by vehicle movement and power control over a continuous range in a vehicular environment, we investigate a deep deterministic policy gradient algorithm to maximize the sum transmission rate of the V2I links and V2V links while ensuring the quality of service of V2V links. Simulation results demonstrate the proposed solution can effectively optimize the total throughput.					2021 16th International Conference on Computer Science & Education (ICCSE)2021 16th International Conference on Computer Science & Education (ICCSE)	17-19 Aug. 202117-19 Aug. 2021		Lancaster, UKLancaster, UK	0	0	0	0	0	0	0					978-1-6654-1468-5									Informatics & Commun. Eng. Dept., Xiamen Univ., Xiamen, China				2022-02-17	INSPEC:21402889		
B	Manjanna, S.; van Hoof, H.; Dudek, G.										Reinforcement Learning with Non-uniform State Representations for Adaptive Search								2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)								7 pp.	7 pp.				10.1109/SSRR.2018.8468649							Conference Paper	2018	2018	Efficient spatial exploration is a key aspect of search and rescue. In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher. This should allow an autonomous vehicle find one or more lost targets as rapidly as possible. We do this by performing non-uniform sampling of the search region. The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning. We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target. Key features of our search algorithm are the ability to employ a very general nondeterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions. One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics. We compare the paths generated by our algorithm with other accepted spatial coverage techniques such as distribution independent boustrophedonic coverage and model dependent spiral search. We present a proof showing that rewarding for clearing probability mass instead of locating the target does not bias the objective function. The experiments show that the learned policy outperforms several well-known baselines even in scenarios different from the one it has been trained on.					2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	6-8 Aug. 20186-8 Aug. 2018		Philadelphia, PA, USAPhiladelphia, PA, USA	0	0	0	0	0	0	0					978-1-5386-5572-6									Mobile Robot. Lab., McGill Univ., Montreal, QC, CanadaAmsterdam Machine Learning Lab., Univ. of Amsterdam, Amsterdam, Netherlands				2018-01-01	INSPEC:18098764		
B	Thakkar, H.K.; Desai, A.; Singh, P.; Samhitha, K.						Garg, D.; Jagannathan, S.; Gupta, A.; Garg, L.; Gupta, S.				ReLearner: a reinforcement learning-based self driving car model using gym environment								Advanced Computing: 11th International Conference, IACC 2021, Revised Selected Papers. Communications in Computer and Information Science (1528)								399	409				10.1007/978-3-030-95502-1_30							Conference Paper	2022	2022	In the recent past, Artificial intelligence and its sister technology such as Machine Learning, Deep Learning, and Reinforcement learning have grown rapidly in several applications. The self-driving car is one of the applications, which is the need of the hour. In this paper, we describe the trends in autonomous vehicle technology for the self-driving car. There are many different approaches to mathematically formulate a design for the self-driving car such as deep Q-learning, Q-learning, and machine learning. However, in this paper, we propose a very basic and less compute-intensive simplistic self-driving car model called "ReLearner" using the Gym environment. To simulate the self-driving car model, we preferred to create a simple small environment OpenAi gym which is a deterministic environment. The OpenAi gym provides the virtual simulation environment and parameter tuning to train and test the model. We have focused on two methods to test our model. The basic approach is to compare the performance of the car when tested using Q-Learning and another using a random action agent, i.e., No reinforcement learning. We have derived a theoretical model and analyzed how to use Q-learning to train cars to drive. We have carried out a simulation and on evaluating the performance and found that Q-learning is a more optimal approach to solve the issue of a self-driving car.					International Advanced Computing ConferenceInternational Advanced Computing Conference	18-19 Dec. 202118-19 Dec. 2021		Msida, MaltaMsida, Malta	0	0	0	0	0	0	0					978-3-030-95502-1									Dept. of Comput. Eng., Marwadi Univ., Rajkot, IndiaDept. of Comput. Sci. & Eng., SRM Univ., Amaravati, IndiaUniv. of Malta, Msida, MaltaBennett Univ., Noida, IndiaMissouri Univ. of Sci. & Technol., Rolla, MO, USAModel Inst. of Eng. & Technol., Kot Bhalwal, India				2022-11-18	INSPEC:22230046		
C	Ferdowsi, Aidin; Challita, Ursula; Saad, Walid; Mandayam, Narayan B.			IEEE	Ferdowsi, Aidin/AAN-8892-2020; Saad, Walid/C-7978-2018						Robust Deep Reinforcement Learning for Security and Safety in Autonomous Vehicle Systems								2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						307	312											Proceedings Paper	2018	2018	The dependence of autonomous vehicles (AVs) on sensors and communication links exposes them to cyber-physical (CP) attacks by adversaries that seek to take control of the AVs by manipulating their data. In this paper, the state estimation process for monitoring AV dynamics, in presence of CP attacks, is analyzed and a novel adversarial deep reinforcement learning (RL) algorithm is proposed to maximize the robustness of AV dynamics control to CP attacks. The attacker's action and the AV's reaction to CP attacks are studied in a game-theoretic framework. In the formulated game, the attacker seeks to inject faulty data to AV sensor readings so as to manipulate the inter-vehicle optimal safe spacing and potentially increase the risk of AV accidents or reduce the vehicle flow on the roads. Meanwhile, the AV, acting as a defender, seeks to minimize the deviations of spacing so as to ensure robustness to the attacker's actions. Since the AV has no information about the attacker's action and due to the infinite possibilities for data value manipulations, each player uses long short term memory (LSTM) blocks to learn the expected spacing deviation resulting from its own action and feeds this deviation to a reinforcement learning (RL) algorithm. Then, the attacker's RL algorithm chooses the action which maximizes the spacing deviation, while the AV's RL algorithm seeks to find the optimal action that minimizes such deviation. Simulation results show that the proposed adversarial deep RL algorithm can improve the robustness of the AV dynamics control as it minimizes the intra-AV spacing deviation.					21st IEEE International Conference on Intelligent Transportation Systems (ITSC)21st IEEE International Conference on Intelligent Transportation Systems (ITSC)	NOV 04-07, 2018NOV 04-07, 2018	IEEE; IEEE Intelligent Transportat Syst SocIEEE; IEEE Intelligent Transportat Syst Soc	Maui, HIMaui, HI	68	3	0	0	0	0	72			2153-0009		978-1-7281-0323-5									Virginia Tech, Bradley Dept Elect & Comp Engn, Wireless VT, Blacksburg, VA 24061 USAEricsson Res, Stockholm, SwedenUniv Edinburgh, Edinburgh, Midlothian, ScotlandRutgers State Univ, Dept ECE, WINLAB, New Brunswick, NJ USA				2019-02-19	WOS:000457881300048		
J	Du, Danfeng; Shen, Mingyu; Guo, Xiurong; Sun, Chaowei					Shen, Mingyu/0000-0003-2242-0180					Hierarchical Driving Strategy for Connected and Autonomous Vehicles Making a Protected Left Turn at Signalized Intersections								JOURNAL OF TRANSPORTATION ENGINEERING PART A-SYSTEMS				149	3					04022154			10.1061/JTEPBS.TEENG-7243							Article	MAR 1 2023	2023	Left-turn execution in autonomous driving at urban intersections is often complex and characterized by unpredicted events, such as vehicles speeding and running a red light. Despite these hazards, autonomous vehicles must drive through intersections safely and efficiently. To solve this problem, a new hierarchical driving strategy (HDS) is proposed for connected and autonomous vehicles making a protected left turn at signalized intersections, which combines the rule-based method and deep reinforcement learning (DRL). The high level of the HDS is the rule-based decision-making module, and the low level is the driving skill, which is dependent on the status. Specifically, the vehicle status when turning left is divided into safe and alert statuses. Further, DRL is used to train the driving skill of the vehicle for each status. The HDS design effectively combines the advantages of end-to-end driving. Moreover, the algorithm was experimentally evaluated in multiple protected left-turn scenarios. Compared with the pure rule-based method, the HDS achieved a 34% reduction in failure rate in the test scenario, and the driving behavior of the autonomous vehicle employing HDS was more intelligent. Moreover, the HDS is highly robust to complex scenarios.									0	0	0	0	0	0	0			2473-2907	2473-2893										Northeast Forestry Univ, Coll Transportat, Harbin 150000, Peoples R ChinaNortheast Forestry Univ, Vehicle Operat Engn, Harbin 150000, Peoples R ChinaNortheast Forestry Univ, Harbin 150000, Peoples R China				2023-02-22	WOS:000913198300007		
J	Xu, Xin; Zuo, Lei; Li, Xin; Qian, Lilin; Ren, Junkai; Sun, Zhenping				Qian, Lilin/X-6353-2018; 徐, 昕/JNS-1298-2023; Li, Xin/ABC-3060-2021	Qian, Lilin/0000-0002-6227-9970; 徐, 昕/0000-0003-3238-745X; 					A Reinforcement Learning Approach to Autonomous Decision Making of Intelligent Vehicles on Highways								IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS				50	10			3884	3897				10.1109/TSMC.2018.2870983							Article	OCT 2020	2020	Autonomous decision making is a critical and difficult task for intelligent vehicles in dynamic transportation environments. In this paper, a reinforcement learning approach with value function approximation and feature learning is proposed for autonomous decision making of intelligent vehicles on highways. In the proposed approach, the sequential decision making problem for lane changing and overtaking is modeled as a Markov decision process with multiple goals, including safety, speediness, smoothness, etc. In order to learn optimized policies for autonomous decision-making, a multiobjective approximate policy iteration (MO-API) algorithm is presented. The features for value function approximation are learned in a data-driven way, where sparse kernel-based features or manifold-based features can be constructed based on data samples. Compared with previous RL algorithms such as multiobjective Q-learning, the MO-API approach uses data-driven feature representation for value and policy approximation so that better learning efficiency can be achieved. A highway simulation environment using a 14 degree-of-freedom vehicle dynamics model was established to generate training data and test the performance of different decision-making methods for intelligent vehicles on highways. The results illustrate the advantages of the proposed MO-API method under different traffic conditions. Furthermore, we also tested the learned decision policy on a real autonomous vehicle to implement overtaking decision and control under normal traffic on highways. The experimental results also demonstrate the effectiveness of the proposed method.									88	3	0	0	0	0	91			2168-2216	2168-2232										Natl Univ Def Technol, Inst Unmanned Syst, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China				2020-10-01	WOS:000571736100035		
R	Lu, Chengjie; Shi, Yize; Zhang, Huihui; Zhang, Man; Wang, Tiexin; Yue, Tao; Ali, Shaukat										DeepCollision: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions								Zenodo													https://doi.org/10.5281/ZENODO.5906634							Data set	2022-09-06	2022	With the aim to test autonomous driving systems, we propose a novel reinforcement learning (RL)-based approach named DeepCollision to learn operating environment configurations of autonomous vehicles, including formalizing environment configuration learning as an MDP and adopting DQN algorithm as the RL solution;DeepCollisionlearns environment configurations to maximize collisions of an Autonomous Vehicle Under Test (AVUT). This dataset contains: algorithms- The algorithm of DeepCollision, which includes the network architecture and the DQN hyperparameter settings; pilot-study- All the raw data and plots for the pilot study; formal-experiment- A dataset contains all the raw data for analysis and the scenarios with detailed demand values; rest-api- The REST API endpoints for environment configuration and oneexampleto show the usage of the APIs. Copyright: Creative Commons Attribution 4.0 International Open Access									0	0	0	0	0	0	0														Nanjing University of Aeronautics & Astronautics, ChinaNanjing University of Aeronautics & Astronautics, ChinaQilu University of Technology (Shandong Academy of Sciences), China	Nanjing University of Aeronautics & AstronauticsNanjing University of Aeronautics & AstronauticsQilu University of Technology (Shandong Academy of Sciences)			2022-11-02	DRCI:DATA2022175024960113		
J	Jang, K.; Beaver, L.; Chalaki, B.; Remer, B.; Vinitsky, E.; Malikopoulos, A.; Bayen, A.										Simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles [arXiv]								arXiv								9 pp.	9 pp.											Journal Paper	14 Dec. 2018	2018	Using deep reinforcement learning, we train control policies for autonomous vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library for deep reinforcement learning in micro-simulators, we train two policies, one policy with noise injected into the state and action space and one without any injected noise. In simulation, the autonomous vehicle learns an emergent metering behavior for both policies in which it slows to allow for smoother merging. We then directly transfer this policy without any tuning to the University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for connected and automated vehicles. We characterize the performance of both policies on the scaled city. We show that the noise-free policy winds up crashing and only occasionally metering. However, the noise-injected policy consistently performs the metering behavior and remains collision-free, suggesting that the noise helps with the zero-shot policy transfer. Additionally, the transferred, noise-injected policy leads to a 5% reduction of average travel time and a reduction of 22% in maximum travel time in the UDSSC. Videos of the controllers can be found at https://sites.google.com/view/iccps-policy-transfer.									0	0	0	0	0	0	0														Univ. of California, Berkeley, Berkeley, CA, USAUniv. of Delaware, Newark, DE, USA				2019-02-27	INSPEC:18413385		
B	Zhou, C.; Liao, M.; Jiao, L.; Tao, F.						Sun, F.; Meng, Q.; Fu, Z.; Fang, B.				Lane Change Decision Control of Autonomous Vehicle Based on A3C Algorithm								Cognitive Systems and Information Processing: 8th International Conference, ICCSIP 2023, Revised Selected Papers. Communications in Computer and Information Science (1918)								217	29				10.1007/978-981-99-8018-5_16							Conference Paper	2024	2024	Coordinating the lateral and longitudinal control of vehicles during lane changing, while considering the vehicle's operational state and the surrounding environment, poses a highly challenging task. In recent years, deep reinforcement learning (DRL) technology has experienced rapid development and has found widespread applications in the traditional automatic control industry. With the improvement of driving safety requirements, DRL technology provides a new research direction and an effective way for the development of vehicle autonomous lane change. This paper investigates automated decision control for lane changing in autonomous vehicles using the Asynchronous Advantage Actor-Critic (A3C) algorithm, while also proposing reasonable multi-objective performance evaluation metrics. Nevertheless, the traditional A3C algorithm frequently encounters convergence oscillation or degradation issues, hindering agents from attaining the highest reward. To address the mentioned issues, an improved parameter updating method based on a weighted average of advantage value is proposed. The simulation results on the highway simulation platform demonstrate that the enhanced A3C algorithm offers increased stability in comparison to the traditional A3C algorithm. Moreover, in comparison to the Deep Q-Network (DQN) algorithm and the Deep Deterministic Policy Gradient (DDPG) algorithm, the enhanced A3C algorithm showcases faster convergence speed and a higher success rate, hence confirming the superiority of the proposed improvement.					International Conference on Cognitive Systems and Signal ProcessingInternational Conference on Cognitive Systems and Signal Processing	20232023		Luoyang, ChinaLuoyang, China	0	0	0	0	0	0	0					978-981-99-8018-5									Sch. of Inf. Eng., Henan Univ. of Sci. & Technol., Luoyang, ChinaLongmen Lab., Hengyang, ChinaAerosp. Syst. Eng. Shanghai, Shanghai, ChinaTsinghua Univ., Beijing, ChinaSouthern Univ. of Sci. & Technol., Shenzhen, ChinaHenan Univ. of Sci. & Technol., Luoyang, China				2024-02-08	INSPEC:24468216		
C	Chandramohan, Aashik; Poel, Mannes; Meijerink, Bernd; Heijenk, Geert			IEEE							Machine Learning for Cooperative Driving in a Multi-Lane Highway Environment								2019 WIRELESS DAYS (WD)		IFIP Wireless Days											10.1109/wd.2019.8734192							Proceedings Paper	2019	2019	Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them. In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment. A driving algorithm is designed using deep Q learning, a type of reinforcement learning. In order to train and test driving algorithms, we deploy a simulated traffic system, using SUMO (Simulation of Urban Mobility). The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles. Furthermore, the impact of limited communication range and random packet loss is investigated. Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high. We propose directions for additional research to improve the performance of the algorithm.					Wireless Days Conference (WD)Wireless Days Conference (WD)	APR 24-26, 2019APR 24-26, 2019		Manchester, ENGLANDManchester, ENGLAND	2	0	0	0	0	0	2			2156-9711		978-1-7281-0117-0									Univ Twente, Dept Comp Sci, Enschede, Netherlands				2019-01-01	WOS:000477698000008		
C	Chu, Tianshu; Kalabic, Uros			IEEE							Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoons								2019 IEEE 58TH CONFERENCE ON DECISION AND CONTROL (CDC)		IEEE Conference on Decision and Control						4079	4084											Proceedings Paper	2019	2019	This paper proposes a model-based deep reinforcement learning (DRL) algorithm for cooperative adaptive cruise control (CACC) of connected vehicles. Differing from most existing CACC works, we consider a platoon consisting of both human-driven and autonomous vehicles. The human-driven vehicles are heterogeneous and connected via vehicle-to-vehicle (V2V) communication and the autonomous vehicles are controlled by a cloud-based centralized DRL controller via vehicle-to-cloud (V2C) communication. To overcome the safety and robustness issues of RL, the algorithm informs lower-level controllers of desired headway signals instead of directly controlling vehicle accelerations. The lower-level behavior is modeled according to the optimal velocity model (OVM), which determines vehicle acceleration according to a headway input. Numerical experiments show that the model-based DRL algorithm outperforms its model-free version in both safety and stability of CACC. Furthermore, we study the impact of different penetration ratios of autonomous vehicles on the safety, stability, and optimality of the CACC policy.					58th IEEE Conference on Decision and Control (CDC)58th IEEE Conference on Decision and Control (CDC)	DEC 11-13, 2019DEC 11-13, 2019	IEEEIEEE	Nice, FRANCENice, FRANCE	16	0	0	0	1	0	18			0743-1546		978-1-7281-1398-2									Stanford Univ, Palo Alto, CA 94305 USAMitsubishi Elect Res Labs, Cambridge, MA 02139 USA	Mitsubishi Elect Res Labs			2020-09-03	WOS:000560779003114		
C	Mohammed, Shawan Taha; Kastouri, Mohamed; Niederfahrenhorst, Artur; Ascheid, Gerd			IEEE							Video Representation Learning for Decoupled Deep Reinforcement Learning Applied to Autonomous Driving								2023 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION, SII		IEEE/SICE International Symposium on System Integration											10.1109/SII55687.2023.10039291							Proceedings Paper	2023	2023	This work focuses on using Deep Reinforcement Learning (DRL) to control an autonomous vehicle in the hyper-realistic urban simulation LGSVL. Classical control systems such as MPC maneuver vehicles based on a given trajectory, current velocity, position, distances, and more. Our approach does not pass this information to the DRL agent but only images provided by the camera. Current DRL efforts also exploit similar approaches for autonomous driving, but they are only suitable for small, simple tasks using simple simulations. Our approach consists of two differently trained neural networks (NN), a perceptual NN for representation learning and an actor NN for selecting the correct action. The perception NN will be trained via representation and self-supervised learning to strengthen our DRL agent's understanding of the scene. It can recognize temporal information and the dynamics of a complex environment. This work shows the importance of decoupling the perception and decision (actor) model for autonomous driving. All in all, we could drive autonomously in a hyperrealistic urban simulation using our modular DRL framework. Moreover, our approach also provides a solution for other similar tasks in the field of robotics based on images.					IEEE/SICE International Symposium on System Integration (SII)IEEE/SICE International Symposium on System Integration (SII)	JAN 17-20, 2023JAN 17-20, 2023	IEEE; SICEIEEE; SICE	Atlanta, GAAtlanta, GA	0	0	0	0	0	0	0			2474-2317		979-8-3503-9868-7									Rhein Westfal TH Aachen, Fac Elect Engn Commun Technol & Embedded Syst, D-52074 Aachen, GermanyRhein Westfal TH Aachen, Fac Elect Engn Math & Comp Sci, D-52074 Aachen, Germany				2023-05-24	WOS:000972217000097		
B	Farkas, P.; Szoke, L.; Aradi, S.										Defining metrics for scenario-based evaluation of autonomous vehicle models								2022 IEEE 1st International Conference on Cognitive Mobility (CogMob)								000155	60				10.1109/CogMob55547.2022.10117768							Conference Paper	2022	2022	The paper deals with the evaluation of autonomous vehicles along with the quantification of their behavior and maneuvers. The article outlines the positive aspects of autonomy and lists several arguments in their favor, e.g. convenience and efficiency considerations. Furthermore, it also addresses the associated difficulties including the feasibility of road testing and the establishment of appropriate simulations. The current work aims to define methods providing objective indicators to compare algorithms solving the complex tasks of road transport. Rule-based, supervised and reinforcement learning control models, test environments, accelerated test methods and assessment indicators of the corresponding literature are reviewed and evaluated. After investigating the different metrics, we formulate an evaluation framework that can be applied in the development and assessment process of new artificial intelligence controlled models. As an outcome of this work, we aim to aid a missing sector in the field of autonomous driving function development by collecting and defining metrics that intend to help qualitatively evaluate and compare algorithms. The key aspect during the definition of the suggested method was to ensure its extensive applicability by selecting only metrics that can be obtained from the already installed sensors of the vehicles. Additionally, we also assess multiple agents to observe how their behavior compares and whether the proposed metrics reflect the expected behavior.					2022 IEEE 1st International Conference on Cognitive Mobility (CogMob)2022 IEEE 1st International Conference on Cognitive Mobility (CogMob)	20222022		Budapest, HungaryBudapest, Hungary	1	0	0	0	0	0	1					978-1-6654-7631-7									Dept. of Control for Transp. & Vehicle Syst., Budapest Univ. of Technol. & Econ., Budapest, HungaryRobert Bosch Kft., Budapest, Hungary				2023-06-03	INSPEC:23123037		
B	Bogosyan, S.; Gokasan, M.; Vamvoudakis, K.G.										Zero-Sum Game (ZSG) based Integral Reinforcement Learning for Trajectory Tracking Control of Autonomous Smart Car								2022 IEEE 31st International Symposium on Industrial Electronics (ISIE)								1	4				10.1109/ISIE51582.2022.9948217							Conference Paper	2022	2022	The ultimate aim of our research study is the development, practical implementation, and benchmarking of continuous-time, online reinforcement learning (RL) schemes for the trajectory tracking control (TTC) of fully autonomous vehicles (AVs) in real-world scenarios. The adaptive optimality and model-free nature offered by RL has a stronger promise against its model-based counterparts, such as MPC, against uncertainties related to the vehicle, road, tire-terrain and environmental dynamics. The existing studies on RL based AV control are mostly theoretical, often dealing with high-level TTC, and perform evaluations in simulations considering simplified or linear models with no disturbance and slip effects. The literature also demonstrates the lack of practical implementations in overall RL based autonomous vehicle control. Our ultimate goal is to fill these theoretical and practical gaps by designing and practically evaluating novel RL strategies that will improve the performance of TTC against uncertainties at all levels. This paper presents the simulation results of our preliminary studies in the online, longitudinal tracking control of a realistic AV (with uncertain nonlinear dynamics, as well as disturbance, and slip effects), which we treat as a Zero-Sum Game (ZSG) problem using an Integral Reinforcement Learning (IRL) approach with synchronous actor and critic updates (SyncIRL). The results are promising and motivate the practical implementation of the approach for combined longitudinal and lateral control of AV.					2022 IEEE 31st International Symposium on Industrial Electronics (ISIE)2022 IEEE 31st International Symposium on Industrial Electronics (ISIE)	1-3 June 20221-3 June 2022		Anchorage, AK, USAAnchorage, AK, USA	0	0	0	0	0	0	0					978-1-6654-8240-0									Electr. & Electron. Eng., Istanbul Tech. Univ., Istanbul, TurkeyAerosp. Eng., Georgia Inst. of Technol., Atlanta, GA, USA				2023-03-22	INSPEC:22295363		
J	Crumpacker, James B.; Robbins, Matthew J.; Jenkins, Phillip R.					Robbins, Matthew/0000-0002-1718-6839; Jenkins, Phillip/0000-0002-2425-9151					An approximate dynamic programming approach for solving an air combat maneuvering problem								EXPERT SYSTEMS WITH APPLICATIONS				203						117448			10.1016/j.eswa.2022.117448					MAY 2022		Article; Early Access		2022	Within visual range air combat involves execution of highly complex and dynamic activities, requiring rapid, sequential decision-making to achieve success. Fighter pilots spend years perfecting tactics and maneuvers for these types of combat engagements, yet the ongoing emergence of unmanned, autonomous vehicle technologies elicits a natural question - can an autonomous unmanned combat aerial vehicle (AUCAV) be imbued with the necessary artificial intelligence to perform challenging air combat maneuvering tasks independently? We formulate and solve the air combat maneuvering problem (ACMP) to examine this important question, developing a Markov decision process (MDP) model to control a defending AUCAV seeking to destroy an attacking adversarial vehicle. The MDP model includes a 5-degree-of-freedom, point-mass aircraft state transition model to accurately represent both kinematics and energy while maneuvering. An approximate dynamic programming (ADP) approach is proposed wherein we develop and test an approximate policy iteration algorithm that implements value function approximation via neural network regression to attain high-quality maneuver policies for the AUCAV. A representative intercept scenario is specified for testing purposes wherein the AUCAV must engage and destroy an adversary aircraft attempting to penetrate the defended airspace. Several designed experiments are conducted to determine how aircraft velocity and adversary maneuvering tactics impact the efficacy of the proposed ADP solution approach and to enable efficient algorithm parameter tuning. ADP-generated policies are compared to two benchmark maneuver policies constructed from two reward shaping functions found in the ACMP literature, attaining improved mean probabilities of kill for 24 of 36 air combat situations considered									7	0	0	0	0	0	7			0957-4174	1873-6793										AF Inst Technol, Dept Operat Sci, 2950 Hobson Way Wright Patterson AFB, Dayton, OH 45433 USA				2022-07-08	WOS:000806335700011		
C	Liu, Yongyang; Zhou, Anye; Wang, Yu; Peeta, Srinivas				Zhou, Anye/AAX-4857-2021; Liu, Yongyang/AAN-1552-2021; Wang, Yu/JBS-1645-2023						Proactive Longitudinal Control to Manage Disruptive Lane Changes of Human-Driven Vehicles in Mixed-Flow Traffic								IFAC PAPERSONLINE				54	2			321	326				10.1016/j.ifacol.2021.06.037					JUL 2021		Proceedings Paper; Early Access		2021	Connected and autonomous vehicles (CAVs) can be leveraged to enable cooperative platooning control to alleviate traffic oscillations. However, in the near future, CAVs and human-driven vehicles (HDV5) will coexist on roads, creating a mixed-flow traffic environment. In mixed-flow traffic, CAV platoons would inevitably encounter lane changes by HDVs in adjacent lanes. These lane changes can generate disturbances and oscillations upstream, jeopardizing the performance of platooning control. Hence, it is necessary to explore the interactions between CAVs and HDVs in the lane -change process, to analyze how CAVs can be used to manage disruptive lane changes of HDVs in mixed-flow traffic. This study proposes deep reinforcement learning-based proactive longitudinal control for CAVs to counteract disruptive HDV lane -change behaviors that can induce disturbances, such that the smoothness of traffic flow can be preserved in the platooning control process. Results from numerical experiments suggest that CAVs controlled by the proposed control strategy can effectively reduce the occurrence of disruptive lane change maneuvers of HDVs to improve string stability performance in mixed-flow traffic. Further, the reliability of the proposed control strategy for different HDV driver types is illustrated. Copyright (c) 2021 The Authors.					16th IFAC Symposium on Control in Transportation Systems (CTS)16th IFAC Symposium on Control in Transportation Systems (CTS)	JUN 08-10, 2021JUN 08-10, 2021	Int Federat Automat Control, Tech Comm 7 4 Transportat Syst; Int Federat Automat Control, Tech Comm 6 4 Fault Detection, Supervis & Safety Tech Proc; Int Federat Automat Control, Tech Comm 7 1 Marine Syst; Int Federat Automat Control, Tech Comm 7 3 Aerosp; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; Univ Gustave Eiffel, IFSTTAR COSYSInt Federat Automat Control, Tech Comm 7 4 Transportat Syst; Int Federat Automat Control, Tech Comm 6 4 Fault Detection, Supervis & Safety Tech Proc; Int Federat Automat Control, Tech Comm 7 1 Marine Syst; Int Federat Automat Control, Tech Comm 7 3 Aerosp; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; Univ Gustave Eiffel, IFSTTAR COSYS	Lille, FRANCELille, FRANCE	4	0	0	0	1	0	4			2405-8963											Georgia Inst Technol, Sch Civil & Environm Engn, Atlanta, GA 30332 USA				2021-08-24	WOS:000680570200051		
B	Xu, Z.; Liu, S.; Wu, Z.; Chen, X.; Zeng, K.; Zheng, K.; Su, H.										PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning								CIKM '21: Proceedings of the 30th ACM International Conference on Information & Knowledge Management								2271	80				10.1145/3459637.3482283							Conference Paper	2021	2021	The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution. It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks. The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e. harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane. This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles. Specifically, we propose a velocity control framework, called PATROL (sPAtial-temporal ReinfOrcement Learning). First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g. velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment. Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time. At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB. We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments. Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.					CIKM '21: The 30th ACM International Conference on Information and Knowledge ManagementCIKM '21: The 30th ACM International Conference on Information and Knowledge Management	1-5 Nov. 20211-5 Nov. 2021	SIGWEB; SIGIRSIGWEB; SIGIR	Virtual Event, QLD, AustraliaVirtual Event, QLD, Australia	0	0	0	0	0	0	0					978-1-4503-8446-9									Sch. of Comput. Sci. & Eng., Univ. of Electron. Sci. & Technol. of China, Chengdu, ChinaMassachusetts Inst. of Technol., Boston, MA, USAAlibaba Group, Hangzhou, China				2021-01-01	INSPEC:22747705		
J	Gao, Weinan; Gao, Jingqin; Ozbay, Kaan; Jiang, Zhong-Ping				Gao, Weinan/ABA-9794-2021; Jiang, Zhong-Ping/AAH-4981-2019; Gao, Jingqin/AAD-1491-2020	Gao, Weinan/0000-0001-7921-018X; Jiang, Zhong-Ping/0000-0002-4868-9359; Gao, Jingqin/0000-0002-1718-2432					Reinforcement-Learning-Based Cooperative Adaptive Cruise Control of Buses in the Lincoln Tunnel Corridor with Time-Varying Topology								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				20	10			3796	3805				10.1109/TITS.2019.2895285							Article	OCT 2019	2019	The exclusive bus lane (XBL) is one of the most popular bus transit systems in the U.S. The Lincoln Tunnel utilizes an XBL through the tunnel in the AM peak period. This paper proposes a novel data-driven cooperative adaptive cruise control (CACC) algorithm that aims to minimize a cost function for connected and autonomous buses along the XBL. Different from existing model-based CACC algorithms, the proposed approach employs the idea of reinforcement learning, which does not rely on accurate knowledge of bus dynamics. Considering a time-varying topology, where each autonomous vehicle can only receive information from preceding vehicles that are within its communication range, a distributed controller is learned real-time by online headway, velocity, and acceleration data collected from the system trajectories. The convergence of the proposed algorithm and the stability of the closed-loop system are rigorously analyzed. The effectiveness of the proposed approach is demonstrated using a well-calibrated Paramics microscopic traffic simulation model of the XBL corridor. The simulation results show that the travel time in the autonomous version of the XBL are close to the present day travel time even when the bus volume is increased by 30%.									40	6	0	0	0	0	44			1524-9050	1558-0016										Georgia Southern Univ, Allen E Paulson Coll Engn & Comp, Dept Elect & Comp Engn, Statesboro, GA 30460 USANYU, Tandon Sch Engn, Dept Civil & Urban Engn, Brooklyn, NY 11201 USANYU, Tandon Sch Engn, C2SMART Tier 1 Univ Transportat Ctr, Brooklyn, NY 11201 USANYU, Tandon Sch Engn, Dept Elect & Comp Engn, Control & Networks Lab, Brooklyn, NY 11201 USA				2019-10-22	WOS:000489747100020		
C	Brunnbauer, Axel; Berducci, Luigi; Brandstatter, Andreas; Lechner, Mathias; Hasani, Ramin; Rus, Daniela; Grosu, Radu			IEEE							Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing								2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022)								7513	7520				10.1109/ICRA46639.2022.9811650							Proceedings Paper	2022	2022	World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms. While learning world models for high-dimensional observations (e.g., pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored. In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model. We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 23-27, 2022MAY 23-27, 2022	IEEE; IEEE Robot & Automat SocIEEE; IEEE Robot & Automat Soc	Philadelphia, PAPhiladelphia, PA	1	0	0	0	0	0	1					978-1-7281-9681-7									Tech Univ Wien TU Wien, CPS, Vienna, AustriaInst Sci & Technol Austria IST Austria, Klosterneuburg, AustriaMassachusetts Inst Technol MIT, CSAIL, Cambridge, MA USA	Massachusetts Inst Technol MIT			2022-01-01	WOS:000941277600124		
J	Berbar, Anas; Gastli, Adel; Meskin, Nader; Al-Hitmi, Mohammed A.; Ghommam, Jawhar; Mesbah, Mostefa; Mnif, Faical				Meskin, Nader/AAR-9342-2020	Meskin, Nader/0000-0003-3098-9369; Gastli, Adel/0000-0002-6818-3320; Al-Hitmi, Mohammed/0000-0002-5344-1294					Reinforcement Learning-Based Control of Signalized Intersections Having Platoons								IEEE ACCESS				10				17683	17696				10.1109/ACCESS.2022.3149161							Article	2022	2022	Smart transportation cities are based on intelligent systems and data sharing, whereas human drivers generally have limited capabilities and imperfect traffic observations. The perception of Connected and Autonomous Vehicle (CAV) utilizes data sharing through Vehicle-To-Vehicle (V2V) and Vehicle-To-Infrastructure (V2I) communications to improve driving behaviors and reduce traffic delays and fuel consumption. This paper proposes a Double Agent (DA) intelligent traffic signal module based on the Reinforcement Learning (RL) method, where the first agent, the Velocity Agent (VA) aims to minimize the fuel consumption by controlling the speed of platoons and single CAVs crossing a signalized intersection, while the second agent, the Signal Agent (SA) proceeds to efficiently reduce traffic delays through signal sequencing and phasing. Several simulation studies have been conducted for a signalized intersection with different traffic flows and the performance of the single-agent with only VA, DA with both VA and SA, and Intelligent Driver Model (IDM) are compared. It is shown that the proposed DA solution improves the average delay by 47.3% and the fuel efficiency by 13.6% compared to the Intelligent Driver Model (IDM).									5	0	0	0	0	0	5			2169-3536											Qatar Univ, Coll Engn, Elect Engn Dept, Doha, QatarSultan Qaboos Univ, Coll Engn, Dept Elect & Comp Engn, Muscat 123, Oman				2022-03-05	WOS:000757822200001		
B	Liu, T.; Lei, L.; Liu, Z.; Zheng, K.										Jointly Learning V2X Communication and Platoon Control with Deep Reinforcement Learning								2023 IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)								1	6				10.1109/PIMRC56721.2023.10293991							Conference Paper	2023	2023	In autonomous vehicle platooning, Vehicle-to-Everything (V2X) communications are leveraged in cooperative adaptive cruise control (CACC) to improve control performance. Since exchanging information at all times incurs significant communication overhead in vehicular networks, it is important to determine when V2X communication is necessary. To solve this problem, we propose a Deep Reinforcement Learning (DRL)-based algorithm named Attention-DDPG, which learns platoon control with Deep Deterministic Policy Gradient (DDPG), and learns when to communicate with an attention network. Specifically, each preceding vehicle is equipped with a deep neural network (DNN), which takes as input its local state and platoon control action and determines whether to transmit its acceleration or not to the following vehicle at each time step. The attention network of a preceding vehicle is trained using the feedback from the following vehicle on the value of V2X information in the form of an advantage function. In order to evaluate Attention-DDPG, simulations are performed using real driving data, and performance is compared with those of two baselines that communicate and do not communicate at all times, respectively. The results demonstrate that Attention-DDPG strikes a competitive tradeoff between control performance and communication overhead while ensuring platoon string stability.					2023 IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)2023 IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)	20232023		Toronto, ON, CanadaToronto, ON, Canada	0	0	0	0	0	0	0					978-1-6654-6483-3									Key Lab. of Universal Wireless Commun., Beijing Univ. of Posts & Telecommun., Beijing, ChinaSch. of Eng., Univ. of Guelph, Guelph, CanadaColl. of Electr. Eng. & Comput. Sci., Ningbo Univ., Ningbo, China				2023-11-23	INSPEC:24063867		
B	Brunnbauer, A.; Berducci, L.; Brandstatter, A.; Lechner, M.; Hasani, R.; Rus, D.; Grosu, R.										Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing								2022 International Conference on Robotics and Automation (ICRA)								7513	20				10.1109/ICRA46639.2022.9811650							Conference Paper	2022	2022	World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms. While learning world models for high-dimensional observations (e.g., pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored. In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model. We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.					2022 IEEE International Conference on Robotics and Automation (ICRA)2022 IEEE International Conference on Robotics and Automation (ICRA)	23-27 May 202223-27 May 2022		Philadelphia, PA, USAPhiladelphia, PA, USA	6	0	0	0	0	0	6					978-1-7281-9681-7									CPS, Tech. Univ. Wien, Vienna, AustriaInst. of Sci. & Technol. AustriaComput. Sci. & Artificial Intelligence Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA				2022-10-20	INSPEC:22115883		
J	Zhu, Tong; Li, Xiaohu; Fan, Wei; Wang, Changshuai; Liu, Haoxue; Zhao, Runqing				WANG, Changshuai/JZD-5603-2024	WANG, Changshuai/0000-0003-0872-6087; Zhu, Tong/0000-0003-3978-3869; Zhao, Runqing/0000-0001-6297-3447					Trajectory Optimization of CAVs in Freeway Work Zone considering Car-Following Behaviors Using Online Multiagent Reinforcement Learning								JOURNAL OF ADVANCED TRANSPORTATION				2021						9805560			10.1155/2021/9805560							Article	NOV 3 2021	2021	Work zone areas are frequent congested sections considered as the freeway bottleneck. Connected and autonomous vehicle (CAV) trajectory optimization can improve the operating efficiency in bottleneck areas by harmonizing vehicles' manipulations. This study presents a joint trajectory optimization of cooperative lane changing, merging, and car-following actions for CAV control at a local merging point together with upstream points. The multiagent reinforcement learning (MARL) method is applied in this system, with one agent providing a merging advisory service at the merging point and controlling the inner-lane vehicles' headway for smooth outer-lane vehicle merging, while other agents provide lane-changing advisory services at advance lane-changing points to control how vehicles make lane changes in advance and perform corresponding headway adjustment, similar to and jointly with the merging advisory service. Uniting all agents, the coordination graph (CG) method is applied to seek the global optimum, overcoming the exponential growth problem in MARL. Using MATLAB and the VISSIM COM interface, an online simulation platform is established. The simulation results show that MARL is effective for online computation with in-timing response. More importantly, comparisons of the results obtained in various scenarios demonstrate that the proposed system obtained smoother vehicle trajectories in all controlled sections, rather than only in the merging area, indicating that it can achieve better traffic conditions in freeway work zone areas.									3	0	0	0	0	0	3			0197-6729	2042-3195										Changan Univ, Coll Transportat Engn, Xian, Peoples R ChinaChina Automot Technol & Res Ctr, Beijing, Peoples R ChinaBeijing Jingdong Century Trading Co Ltd, Beijing, Peoples R ChinaSoutheast Univ, Sch Transportat, Nanjing, Peoples R ChinaChangan Univ, Sch Automobile, Xian, Peoples R ChinaUNSW Sydney, Sch Aviat, High St, Kensington, NSW 2052, Australia	China Automot Technol & Res CtrBeijing Jingdong Century Trading Co Ltd			2021-11-25	WOS:000719668100001		
J	Benvenuti, A.; Hawkins, C.; Fallin, B.; Chen, B.; Bialy, B.; Dennis, M.; Hale, M.										Differentially Private Reward Functions for Multi-Agent Markov Decision Processes [arXiv]								Differentially Private Reward Functions for Multi-Agent Markov Decision Processes [arXiv]																				Preprint	2023	2023	Reward functions encode desired behavior in multi-agent Markov decision processes, but onlookers may learn reward functions by observing agents, which can reveal sensitive information. Therefore, in this paper we introduce and compare two methods for privatizing reward functions in policy synthesis for multi-agent Markov decision processes. Reward functions are privatized using differential privacy, a statistical framework for protecting sensitive data. Both methods we develop rely on the Gaussian mechanism, which is a method of randomization we use to perturb (i) each agent's individual reward function or (ii) the joint reward function shared by all agents. We prove that both of these methods are differentially private and compare the abilities of each to provide accurate reward values for policy synthesis. We then develop an algorithm for the numerical computation of the performance loss due to privacy on a case-by-case basis. We also exactly compute the computational complexity of this algorithm in terms of system parameters and show that it is inherently tractable. Numerical simulations are performed on a gridworld example and in waypoint guidance of an autonomous vehicle, and both examples show that privacy induces only negligible performance losses in practice.									0	0	0	0	0	0	0																		2023-10-12	INSPEC:23783255		
J	Gao, Weinan; Jiang, Zhong-Ping; Ozbay, Kaan				Gao, Weinan/ABA-9794-2021; Jiang, Zhong-Ping/AAH-4981-2019	Gao, Weinan/0000-0001-7921-018X; Jiang, Zhong-Ping/0000-0002-4868-9359					Data-Driven Adaptive Optimal Control of Connected Vehicles								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				18	5			1122	1133				10.1109/TITS.2016.2597279							Article	MAY 2017	2017	In this paper, a data-driven non-model-based approach is proposed for the adaptive optimal control of a class of connected vehicles that is composed of n human-driven vehicles only transmitting motional data and an autonomous vehicle in the tail receiving the broadcasted data from preceding vehicles by wireless vehicle-to-vehicle (V2V) communication devices. Considering the cases of range-limited V2V communication and input saturation, several optimal control problems are formulated to minimize the errors of distance and velocity and to optimize the fuel usage. By employing an adaptive dynamic programming technique, the optimal controllers are obtained without relying on the knowledge of system dynamics. The effectiveness of the proposed approaches is demonstrated via the online learning control of the connected vehicles in Paramics' traffic microsimulation.									118	6	0	0	0	0	127			1524-9050	1558-0016										NYU, Control & Networks Lab, Dept Elect & Comp Engn, Tandon Sch Engn, Brooklyn, NY 11201 USANYU, Dept Civil & Urban Engn, Brooklyn, NY 11201 USANYU, CUSP, Brooklyn, NY 11201 USA				2017-05-01	WOS:000400901400008		
J	Yadavalli, S.R.; Das, L.C.; Won, M.										RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging [arXiv]								arXiv																				Journal Paper	07 Dec. 2022	2022	A platoon refers to a group of vehicles traveling together in very close proximity. It has received significant attention from the autonomous vehicle research community due to its strong potential to significantly enhance fuel efficiency, driving safety, and driver comfort. Despite these advantages, recent research has revealed a detrimental effect of the extremely small intra-platoon gap on traffic flow for highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a significant challenge due to the massive computational complexity. To this end, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The state space of the framework is carefully designed in consultation with the transportation literature to incorporate critical traffic parameters relevant to merging efficiency. A deep deterministic policy gradient algorithm is adopted to account for the continuous action space to ensure precise and continuous adjustment of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway merging scenarios.									0	0	0	0	0	0	0																		2023-05-12	INSPEC:23019313		
J	Liu, Yuqi; Gao, Yinfeng; Zhang, Qichao; Ding, Dawei; Zhao, Dongbin										Multi-task safe reinforcement learning for navigating intersections in dense traffic								JOURNAL OF THE FRANKLIN INSTITUTE-ENGINEERING AND APPLIED MATHEMATICS				360	17			13737	13760				10.1016/j.jfranklin.2022.06.052					NOV 2023		Article; Early Access		2023	Multi-task intersection navigation, which includes unprotected turning left, turning right, and going straight in heavy traffic, remains a difficult task for autonomous vehicles. For the human driver, negotiation skills with other interactive vehicles are the key to guaranteeing safety and efficiency. However, it is hard to balance the safety and efficiency of the autonomous vehicle for multi-task intersection navigation. In this paper, we formulate a multi-task safe reinforcement learning framework with social attention to improve the safety and efficiency when interacting with other traffic participants. Specifically, the social attention module is used to focus on the states of negotiation vehicles. In addition, a safety layer is added to the multi-task reinforcement learning framework to guarantee safe negotiation. We deploy experiments in the simulators SUMO, which has abundant traffic flows, and CARLA, which has high-fidelity vehicle models. Both show that the proposed algorithm improves safety while maintaining stable traffic efficiency for the multi-task intersection navigation problem. More details and demonstrations are available at https:// github.com/ liuyuqi123/ SAT.(c) 2022 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.									5	0	0	0	0	0	5			0016-0032	1879-2693										Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R ChinaUniv Chinese Acad Sci, Coll Artificial Intelligence, Beijing 100049, Peoples R ChinaUniv Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 1000839, Peoples R China				2023-12-17	WOS:001111154800001		
B	Weingertner, P.; Ho, M.; Timofeev, A.; Aubert, S.; Pita-Gil, G.										Monte Carlo Tree Search With Reinforcement Learning for Motion Planning								2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)								7 pp.	7 pp.				10.1109/ITSC45102.2020.9294697							Conference Paper	2020	2020	Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.					2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)	20-23 Sept. 202020-23 Sept. 2020		Rhodes, GreeceRhodes, Greece	0	0	0	0	0	0	0					978-1-7281-4149-7									Renault Software Labs., Sophia Antipolis, FranceStanford Univ., Stanford, CA, USAExperis Switzerland				2021-02-19	INSPEC:20302816		
B	Hunor toth, S.; Janos viharos, Z.; Bardos, A.										Autonomous Vehicle Drift With a Soft Actor-critic Reinforcement Learning Agent								2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI).								000015	000020				10.1109/SAMI54271.2022.9780803							Conference Paper	2022	2022	Self-driving vehicles have become a more and more important field in recent years. Supported by the techniques of Artificial Intelligence (AI), the current tendency of positive results in applications is making it a promising area to focus further research. Additionally, Reinforcement Learning (RL) is already proved to be an efficient approach for complex problems, e.g. robots, industrial systems and also games (like chess, Go), etc.Drifting is a driving technique at handling limits where the driver intentionally oversteers, with loss of traction, while driving the vehicle through the entirety of a corner. It is a very challenging control task and often results in an accident when it occurs on public roads, consequently, the efficient control of this motion is especially important in the safety of autonomous vehicles.The paper reports novel research results whose main goal is to develop a self-driving agent for drift motion control based on vehicle simulation by Matlab/Simulink. Longitudinal and lateral velocity together with the yaw rate formed the state representation of the vehicle. The agent action space consists of two continuous actuator values: pedal ratio and roadwheel angle. The goal of the agent is twofold: first, it has to jump into a drifting state, second, it has to keep the vehicle in drift.The simulation results show that the proposed Soft Actor-Critic (SAC) RL agent is capable of learning to approach a pre-determined drift equilibrium from cornering and staying in this drift situation as well. For the training, the solution excluded using any kind of prior data, it only works with information gained from the simulation model, which is a remarkable difference from the actual state-of-the-art RL-based solutions.					2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI)2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI)	2-5 March 20222-5 March 2022		Poprad, SlovakiaPoprad, Slovakia	0	0	0	0	0	0	0					978-1-6654-9704-6									Dept. of Automotive Technol., Budapest Univ. of Technol. & Econ., Budapest, HungaryRes. Lab. on Eng. & Manage. Intelligence Intelligent Processes Res. Group, Inst. for Comput. Sci. & Control, Budapest, HungaryFac. of Econ., John von Neumann Univ., Kecskemet, Hungary				2022-11-01	INSPEC:21779445		
J	Fu, Xiaoyuan; Yuan, Quan; Liu, Shifan; Li, Baozhu; Qi, Qi; Wang, Jingyu				wang, jing/HJA-5384-2022; Liu, Jing/IQX-0664-2023						Delayed-onset immune-related adverse events involving the thyroid gland by immune checkpoint inhibitors in combination with chemotherapy: a case report and retrospective cohort study								CHINA COMMUNICATIONS				20	3			55	68				10.23919/JCC.2023.03.005							Article	MAR 2023	2023	The connected autonomous vehicle is considered an effective way to improve transport safety and efficiency. To overcome the limited sensing and computing capabilities of individual vehicles, we design a digital twin assisted decision-making framework for Internet of Vehicles, by leveraging the integration of communication, sensing and computing. In this framework, the digital twin entities residing on edge can effectively communicate and cooperate with each other to plan sub-targets for their respective vehicles, while the vehicles only need to achieve the sub-targets by generating a sequence of atomic actions. Furthermore, we propose a hierarchical multi-agent reinforcement learning approach to implement the framework, which can be trained in an end-to-end way. In the proposed approach, the communication interval of digital twin entities could adapt to time-varying environment. Extensive experiments on driving decision-making have been performed in traffic junction scenarios of different difficulties. The experimental results show that the proposed approach can largely improve collaboration efficiency while reducing communication overhead.									0	0	0	0	0	0	0			1673-5447											Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R ChinaXidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R ChinaZhuhai Fudan Innovat Inst, Internet Things & Smart City Innovat Platform, Zhuhai 519000, Peoples R ChinaState Key Lab High End Server & Storage Technol, Jinan 250101, Peoples R China	Zhuhai Fudan Innovat InstState Key Lab High End Server & Storage Technol			2023-04-26	WOS:000960532300006		
B	[Anonymous]										2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)								2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)																				Conference Proceedings	2010	2010	The following topics are dealt with: stroke patient rehabilitation; augmented reality; autonomous vehicle control; human-robot interaction; mobile surveillance; epileptic seizure onset detection; tutorial dialogues; wireless sensor network routing; fuzzy quadtree; mobile robots; landscape visualization; fuzzy control; induction motor drives; handwriting classification; reinforcement learning; power system operations; nonlinear state estimation; dynamic programming; fingerprint image segmentation; sensorless control; and speech recognition.					2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)	21-23 June 201021-23 June 2010		Povoa de Varzim, PortugalPovoa de Varzim, Portugal	0	0	0	0	0	0	0					978-1-4244-7104-1													2010-01-01	INSPEC:11473980		
J	Josef, Shirel; Degani, Amir					Josef, Shirel/0000-0003-3863-4837; Degani, Amir/0000-0002-4813-8506					Deep Reinforcement Learning for Safe Local Planning of a Ground Vehicle in Unknown Rough Terrain								IEEE ROBOTICS AND AUTOMATION LETTERS				5	4			6748	6755				10.1109/LRA.2020.3011912							Article	OCT 2020	2020	Safe unmanned ground vehicle navigation in unknown rough terrain is crucial for various tasks such as exploration, search and rescue and agriculture. Offline global planning is often not possible when operating in harsh, unknown environments, and therefore, online local planning must be used. Most online rough terrain local planners require heavy computational resources, used for optimal trajectory searching and estimating vehicle orientation in positions within the range of the sensors. In this work, we present a deep reinforcement learning approach for local planning in unknown rough terrain with zero-range to local-range sensing, achieving superior results compared to potential fields or local motion planning search spaces methods. Our approach includes reward shaping which provides a dense reward signal. We incorporate self-attention modules into our deep reinforcement learning architecture in order to increase the explainability of the learnt policy. The attention modules provide insight regarding the relative importance of sensed inputs during training and planning. We extend and validate our approach in a dynamic simulation, demonstrating successful safe local planning in environments with a continuous terrain and a variety of discrete obstacles. By adding the geometric transformation between two successive timesteps and the corresponding action as inputs, our architecture is able to navigate on surfaces with different levels of friction. Reinforcement learning, autonomous vehicle navigation, motion and path planning.									43	3	0	0	1	0	49			2377-3766											Technion Israel Inst Technol, Technion Autonomous Syst Program, IL-3200003 Haifa, IsraelTechnion Israel Inst Technol, Fac Civil & Environm Engn, IL-3200003 Haifa, Israel				2020-09-18	WOS:000566332100001		
J	Schwarting, Wilko; Pierson, Alyssa; Alonso-Mora, Javier; Karaman, Sertac; Rus, Daniela				Pierson, Alyssa/AAG-4800-2019; Alonso-Mora, Javier/AAO-7590-2020	Pierson, Alyssa/0000-0002-4885-9119; Alonso-Mora, Javier/0000-0003-0058-570X					Social behavior for autonomous vehicles								PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA				116	50			24972	24978				10.1073/pnas.1820676116							Article	DEC 10 2019	2019	Deployment of autonomous vehicles on public roads promises increased efficiency and safety. It requires understanding the intent of human drivers and adapting to their driving styles. Autonomous vehicles must also behave in safe and predictable ways without requiring explicit communication. We integrate tools from social psychology into autonomous-vehicle decision making to quantify and predict the social behavior of other drivers and to behave in a socially compliant way. A key component is Social Value Orientation (SVO), which quantifies the degree of an agent's selfishness or altruism, allowing us to better predict how the agent will interact and cooperate with others. We model interactions between agents as a best-response game wherein each agent negotiates to maximize their own utility. We solve the dynamic game by finding the Nash equilibrium, yielding an online method of predicting multiagent interactions given their SVOs. This approach allows autonomous vehicles to observe human drivers, estimate their SVOs, and generate an autonomous control policy in real time. We demonstrate the capabilities and performance of our algorithm in challenging traffic scenarios: merging lanes and unprotected left turns. We validate our results in simulation and on human driving data from the NGSIM dataset. Our results illustrate how the algorithm's behavior adapts to social preferences of other drivers. By incorporating SVO, we improve autonomous performance and reduce errors in human trajectory predictions by 25%.									164	5	0	0	12	0	181			0027-8424											MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USADelft Univ Technol, Cognit Robot, NL-2628 CD Delft, NetherlandsMIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA				2019-12-10	WOS:000502577500013	31757853	
C	Li, Huayi; Li, Nan; Kolmanovsky, Ilya; Girard, Anouck			IEEE	Li, Nan/Q-5511-2019	Kolmanovsky, Ilya/0000-0002-7225-4160					Energy-Efficient Autonomous Vehicle Control Using Reinforcement Learning and Interactive Traffic Simulations								2020 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						3029	3034				10.23919/acc45564.2020.9147717							Proceedings Paper	2020	2020	Connected and autonomous vehicles are expected to improve mobility and transportation, as well as to provide energy efficiency benefits. The integration of safety and energy efficiency aspects is challenging as there are certain trade-offs between them, and also because the assessment of these attributes requires different time horizons. This paper illustrates the development of a controller for highway driving that, through reinforcement learning, can simultaneously address requirements of safety, comfort, performance and energy efficiency for battery electric vehicles. The training process of the decision policy exploits traffic simulations that are capable of representing the interactive behavior of vehicles in traffic based on game theory. Results indicate the potential for improved energy efficiency by adding powertrain-related states in the decision policy and by suitably defining the reward function.					American Control Conference (ACC)American Control Conference (ACC)	JUL 01-03, 2020JUL 01-03, 2020	Amer Automat Control Council; Int Federat Automat ControlAmer Automat Control Council; Int Federat Automat Control	Denver, CODenver, CO	3	1	0	0	0	0	6			0743-1619	2378-5861	978-1-5386-8266-1									Univ Michigan, Dept Aerosp Engn, Ann Arbor, MI 48109 USA				2021-03-02	WOS:000618079802155		
J	Yao, Yu; Zhao, Junhui; Li, Zeqing; Cheng, Xu; Wu, Lenan				Liu, Yuxuan/JVO-7759-2024; zhang, wb/JGM-5316-2023						Jamming and Eavesdropping Defense Scheme Based on Deep Reinforcement Learning in Autonomous Vehicle Networks								IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY				18				1211	1224				10.1109/TIFS.2023.3236788							Article	2023	2023	As a legacy from conventional wireless services, illegal eavesdropping is regarded as one of the critical security challenges in Connected and Autonomous Vehicles (CAVs) network. Our work considers the use of Distributed Kalman Filtering (DKF) and Deep Reinforcement Learning (DRL) techniques to improve anti-eavesdropping communication capacity and mitigate jamming interference. Aiming to improve the security performance against smart eavesdropper and jammer, we first develop a DKF algorithm that is capable of tracking the attacker more accurately by sharing state estimates among adjacent nodes. Then, a design problem for controlling transmission power and selecting communication channel is established while ensuring communication quality requirements of the authorized vehicular user. Since the eavesdropping and jamming model is uncertain and dynamic, a hierarchical Deep Q-Network (DQN)-based architecture is developed to design the anti-eavesdropping power control and possibly channel selection policy. Specifically, the optimal power control scheme without prior information of the eavesdropping behavior can be quickly achieved first. Based on the system secrecy rate assessment, the channel selection process is then performed when necessary. Simulation results confirm that our jamming and eavesdropping defense technique enhances the secrecy rate as well as achievable communication rate compared with currently available techniques.									38	0	0	0	0	0	38			1556-6013	1556-6021										East China Jiaotong Univ, Sch Informat Engn, Nanchang 330013, Peoples R ChinaBeijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R ChinaSun Yat sen Univ, Dept Elect & Commun Engn, Shenzhen 518000, Peoples R ChinaSoutheast Univ, Coll Informat Sci & Engn, Nanjing 210096, Peoples R China				2023-02-20	WOS:000922862600002		
B	Khalid, M.; Aslam, N.; Liang Wang										A Reinforcement Learning based Path Guidance Scheme for Long-range Autonomous Valet Parking in Smart Cities								2020 IEEE Eighth International Conference on Communications and Networking (ComNet)								7 pp.	7 pp.				10.1109/ComNet47917.2020.9306103							Conference Paper	2020	2020	Finding a parking slot in the city centre has always been a great challenge. In many cases, drivers spend a lot of time roaming around looking for an empty and suitable parking slot. The emerging machine learning technologies in intelligent transport system has made it more flexible for Electric Autonomous Vehicle (EAV) to find a parking slot and get parked. The Long-range Autonomous Valet Parking (LAVP) allows an EAV to drop user at a suitable drop-off spot and select an economical parking slot. With the evolution of battery operated vehicles, the primary concern is efficient use of battery resources. This can be done either by maximizing battery capacity or by smartly using battery with existing capacity. During the parking process, most of the energy is consumed by finding an optimal path to parking slot. The work proposed in this paper guides EAV from a random starting point to nearest drop-off spot and CP. A Reinforcement Learning based Autonomous Valet Parking technique (RL-LAVP) has been designed to guide EAV to drop-off spot, CP and minimize the total distance covered during this process. The RL-LAVP results show a significant improvement towards minimizing covered distance and consumed energy when compared with RaNdom (RN) parking and LAVP parking techniques.					2020 IEEE Eighth International Conference on Communications and Networking (ComNet)2020 IEEE Eighth International Conference on Communications and Networking (ComNet)	27-30 Oct. 202027-30 Oct. 2020		Hammamet, TunisiaHammamet, Tunisia	0	0	0	0	0	0	0					978-1-7281-5320-9									Dept. of Comput. & Inf. Sci., Northumbria Univ., Newcastle upon Tyne, UK				2020-01-01	INSPEC:20346190		
C	Vasquez, Dizan; Yu, Yufeng; Kumar, Suryansh; Laugier, Christian			IEEE	Kumar, Suryansh/JAO-0244-2023	Kumar, Suryansh/0000-0003-2755-8744					An open framework for human-like autonomous driving using Inverse Reinforcement Learning								2014 IEEE VEHICLE POWER AND PROPULSION CONFERENCE (VPPC)		IEEE Vehicle Power and Propulsion Conference																		Proceedings Paper	2014	2014	Research on autonomous car driving and advanced driving assistance systems has come to occupy a very significant place in robotics research. On the other hand, there are significant entry barriers (eg cost, legislation, logistics) that make it very difficult for small research groups and individual researchers to have access to a real autonomous vehicle for their experiments. This paper proposes to leverage an existing driving simulator (Torcs) by developing a ROS communication bridge for it. We use is as the basis for an experimental framework for the development and evaluation of Human-like autonomous driving based on Inverse Reinforce Learning (IRL). Based on an extensible and open architecture, this framework provides efficient GPU-based implementations of state-of the art IRL algorithms, as well as two challenging test environments and a set of evaluation metrics as a first step toward a benchmark.					2014 IEEE Vehicle Power and Propulsion Conference2014 IEEE Vehicle Power and Propulsion Conference	OCT 27-30, 2014OCT 27-30, 2014	VTS; INESC; UNIV DE COIMBRA; POLITECNICO DE COIMBRA; ISEC ENGENHARIA; IEEE Power & Energy; EPE; MEGEVH; Associacao Portuguesa do Velculo Elect; SEW EURODRIVE; OPAL RT; efacecVTS; INESC; UNIV DE COIMBRA; POLITECNICO DE COIMBRA; ISEC ENGENHARIA; IEEE Power & Energy; EPE; MEGEVH; Associacao Portuguesa do Velculo Elect; SEW EURODRIVE; OPAL RT; efacec	Coimbra, PORTUGALCoimbra, PORTUGAL	3	0	0	0	0	0	4			1938-8756		978-1-4799-6782-7									Inria Rhne Alpes, Le Chesnay, FranceBeihang Univ, Beijing 100191, Peoples R ChinaIIIT Hyderabad, Hyderabad, Andhra Pradesh, India	Inria Rhne Alpes			2014-01-01	WOS:000380610000020		
J	Wang, Shupei; Wang, Ziyang; Jiang, Rui; Zhu, Feng; Yan, Ruidong; Shang, Ying					Wang, Ziyang/0000-0003-4104-1573					A multi-agent reinforcement learning-based longitudinal and lateral control of CAVs to improve traffic efficiency in a mandatory lane change scenario								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				158						104445			10.1016/j.trc.2023.104445					DEC 2023		Article; Early Access		2024	Bottleneck areas are prone to severe traffic congestion due to the sudden drop in capacity. To improve traffic efficiency in the bottleneck area, this paper proposes a multi-agent deep reinforcement learning framework integrating collision avoidance strategies to improve traffic efficiency in a mandatory lane change scenario. The proposed method considers distance-keeping and lane-changing coordination in a connected autonomous vehicle (CAV) environment, by controlling vehicles' longitudinal and lateral movement to effectively reduce traffic congestion in a mandatory lane change scenario. This framework was trained and tested in a simulation environment that is the same as the natural driving environment. Compared with real-world data and the benchmark model (a Dueling Double Deep Q-Network-based model), the proposed model shows better performance in terms of average speed, travel time, throughput, and safety in the bottleneck area. The results show that the proposed model can effectively reduce traffic congestion and improve traffic efficiency in a mandatory lane change scenario.									0	0	0	0	0	0	0			0968-090X	1879-2359										Beijing Jiaotong Univ, Sch Traff & Transportat, Beijing 100044, Peoples R ChinaBeijing Jiaotong Univ, Sch Syst Sci, Beijing 100044, Peoples R ChinaNanyang Technol Univ, Sch Civil & Environm Engn, Singapore, Singapore				2024-01-08	WOS:001132779400001		
C	Weingertner, Philippe; Ho, Minnie; Timofeev, Andrey; Aubert, Sebastien; Pita-Gil, Guillermo			IEEE							Monte Carlo Tree Search With Reinforcement Learning for Motion Planning								2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC											10.1109/itsc45102.2020.9294697							Proceedings Paper	2020	2020	Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.					23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)	SEP 20-23, 2020SEP 20-23, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1			2153-0009		978-1-7281-4149-7									Renault Software Labs, Grp Renault, Toulouse, FranceStanford Univ, Stanford, CA 94305 USAZoox Corp, Foster City, CA USAExperis Switzerland, Zurich, Switzerland	Renault Software LabsZoox CorpExperis Switzerland			2021-09-23	WOS:000682770703040		
J	Kuutti, Sampo; Bowden, Richard; Jin, Yaochu; Barber, Phil; Fallah, Saber				Jin, Yaochu/GRY-7004-2022; Bowden, Richard/AAF-8283-2019	Jin, Yaochu/0000-0003-1100-0631; Bowden, Richard/0000-0003-3285-8020; Fallah, Saber/0000-0002-1298-1040; Kuutti, Sampo/0000-0002-7020-4370					A Survey of Deep Learning Applications to Autonomous Vehicle Control								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				22	2			712	733				10.1109/TITS.2019.2962338							Article	FEB 2021	2021	Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.									256	6	0	0	3	0	276			1524-9050	1558-0016										Univ Surrey, Ctr Automot Engn, Guildford GU2 7XH, Surrey, EnglandUniv Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, EnglandUniv Surrey, Dept Comp Sci, Guildford GU2 7XH, Surrey, EnglandJaguar Land Rover Ltd, Coventry CV4 7JJ, W Midlands, EnglandPhil Barber Ind, Leeds DL7 9UN, Yorks, England	Phil Barber Ind			2021-03-14	WOS:000615045000003		
J	Wang, Rong; Yang, Peng; Gong, Yeming; Chen, Cheng				GONG, Yeming/I-7148-2012	GONG, Yeming/0000-0001-9270-5507					Operational policies and performance analysis for overhead robotic compact warehousing systems with bin reshuffling								INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH													10.1080/00207543.2023.2289643					DEC 2023		Article; Early Access		2023	This paper studies a novel robotic warehousing system called the overhead robotic compact storage and retrieval system, which can free up the floor space occupation at a low cost. Bins, as basic storage containers, are stacked on top of each other to form a bin stack. Along overhead tracks, bin-picking robots transport bins between storage/retrieval positions and workstations with the aid of track-changing robots. Little research has been done to study operational policies and performance analysis for this new robotic compact warehousing system. We propose a nested queuing network model that considers two transportation resources and performs reinforcement learning using real data to improve the reshuffling efficiency. We find that reinforcement learning based reshuffling policy greatly reduces the reshuffling distance and saves computation time compared to existing policies. We find that the storage policy of stacks affects the optimal width/length ratio regardless of the system height. Interestingly, we obtain the number of robots that can stabilise the system to avoid an explosion of the order queue; two more robots than that number will produce relatively low throughput times. Compared to an AutoStore system, using our system reduces cost by 30% with a slight increase in throughput time.									0	0	0	0	0	0	0			0020-7543	1366-588X										Tsinghua Univ, Shenzhen Int Grad Sch, Div Logist & Transportat, Shenzhen, Peoples R ChinaTsinghua Univ, Inst Data & Informat, Shenzhen Int Grad Sch, Shenzhen, Peoples R ChinaTsinghua Univ, Dept Ind Engn, Beijing, Peoples R ChinaEMLYON Business Sch, AIM Artificial Intelligence Management Inst, Ecully, FranceTsinghua Univ, Shenzhen Int Grad Sch, Div Logist & Transportat, Shenzhen 518055, Peoples R ChinaTsinghua Univ, Inst Data & Informat, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China				2023-12-16	WOS:001115915500001		
B	Ekechi, Chijioke Cyriacus										Intelligent Control of a Swarm of Unmanned Aerial Vehicles in Turbulent Environments Using Clustering-PPO Algorithm																												Dissertation/Thesis	Jan 01 2023	2023										0	0	0	0	0	0	0					9798381169027									Tennessee Technological University, Electrical Engineering, Tennessee, United States	Tennessee Technological University				PQDT:86766304		
C	Koren, Mark; Kochenderfer, Mykel J.			IEEE	Kochenderfer, Mykel/E-7069-2010	Kochenderfer, Mykel/0000-0002-7238-9663					Adaptive Stress Testing without Domain Heuristics using Go-Explore								2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC											10.1109/itsc45102.2020.9294729							Proceedings Paper	2020	2020	Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more likely actions, in order to to find more likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.					23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)	SEP 20-23, 2020SEP 20-23, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	2	0	0	0	0	0	2			2153-0009		978-1-7281-4149-7									Stanford Univ, Aeronaut & Astronaut, Stanford, CA 94305 USA				2020-01-01	WOS:000682770703070		
C	Li, Xiaohu; Cao, Zehong; Bai, Quan			Assoc Advancement Artificial Intelligence	Cao, Zehong/O-7351-2016	Cao, Zehong/0000-0003-3656-0328					A Novel Mountain Driving Unity Simulated Environment for Autonomous Vehicles								THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE		AAAI Conference on Artificial Intelligence		35				16075	16077											Proceedings Paper	2021	2021	The simulated driving environment provides a low cost and time-saving platform to test the performance of the autonomous vehicle by linkage with existing machine learning approaches. However, most of existing simulated driving environments focus on building flat roads in urban areas. Still, they neglected to endeavour the tough steep, curvy hill roads, such as mountain paths around suburban areas. In this study, by deploying in Unity engine, we developed the first complex mountain driving simulated environment with characterizing continuous curves and up/downhill. Then, two state-of-art reinforcement learning (RL) algorithms are used to train a vehicle agent and test the performance of autonomous vehicles in our developed simulated environment. Also, we set 5 different levels of vehicle's speeds and observe the cumulative rewards during the vehicle agent training. Our demonstration presents the developed environment supports for complex mountain scenario configurations and RL-based autonomous vehicles, and our findings show that the vehicle agent could achieve high cumulative rewards during the training stage, suggesting that our work is a potential new simulation environment for autonomous vehicles research. The demonstration video can be viewed via the link. https://youtu.be/0wSqGeCn-NU.					35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence	FEB 02-09, 2021FEB 02-09, 2021	Assoc Advancement Artificial IntelligenceAssoc Advancement Artificial Intelligence	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			2159-5399	2374-3468	978-1-57735-866-4									Univ Tasmania, Sch ICT, Hobart, Tas, Australia				2021-09-20	WOS:000681269807217		
C	Chen, Luyao; Xie, Shaorong; Pang, Tao; Yu, Hang; Luo, Xiangfeng; Zhang, Zhenyu						Reformat, M; Zhang, D; Bourbakis, N				Learning from Suboptimal Demonstration via Trajectory-Ranked Adversarial Imitation								2022 IEEE 34TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI		Proceedings-International Conference on Tools With Artificial Intelligence						486	493				10.1109/ICTAI56018.2022.00078							Proceedings Paper	2022	2022	Robots trained by Imitation Learning(IL) are used in many tasks(e.g., autonomous vehicle manipulation). Generative Adversarial Imitation Learning (GAIL) assumes that the demonstration set used for training is of high quality. However, such demonstrations are difficult and expensive to obtain. GAIL-related methods fail to learn effective strategies if non-high quality demonstrations are used because the performance of agents trained by this method is limited by the demonstrator's operations. Our idea is to enable the agent to learn strategy with better performance than the demonstrator from a suboptimal demonstration set, which contains non-high quality demonstrations that are easier to obtain. Inspired by this, we propose the Trajectory-Ranked Adversarial Imitation Learning (TRAIL) method. First, for demonstration set processing, we introduce a ranking process and define the concept of Performance Relative Advantage of suboptimal demonstrations to specify the ranking order. Second, for model training, we reconstruct the objective function of GAIL and use an experience replay buffer, enabling the agent to learn implicit features and ranking information from the ranked suboptimal demonstration set and possess the ability to outperform the demonstrator. Experiments show that in Mujoco's tasks, our method can learn from a suboptimal demonstration set and can achieve better performance than baseline methods.					34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)	OCT 31-NOV 02, 2022OCT 31-NOV 02, 2022	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			1082-3409		979-8-3503-9744-4									Shanghai Univ, Sch Comp Engn & Sci, Shanghai, Peoples R ChinaChina Elect Technol Grp Corp, Res Inst 32, Beijing, Peoples R China				2023-06-18	WOS:000992544400070		
J	Mahmoud, Sara; Billing, Erik; Svensson, Henrik; Thill, Serge				Svensson, Henrik/IUQ-2969-2023	Svensson, Henrik/0000-0003-3129-4892					How to train a self-driving vehicle: On the added value (or lack thereof) of curriculum learning and replay buffers								FRONTIERS IN ARTIFICIAL INTELLIGENCE				6						1098982			10.3389/frai.2023.1098982							Article	JAN 25 2023	2023	Learning from only real-world collected data can be unrealistic and time consuming in many scenario. One alternative is to use synthetic data as learning environments to learn rare situations and replay buffers to speed up the learning. In this work, we examine the hypothesis of how the creation of the environment affects the training of reinforcement learning agent through auto-generated environment mechanisms. We take the autonomous vehicle as an application. We compare the effect of two approaches to generate training data for artificial cognitive agents. We consider the added value of curriculum learning-just as in human learning-as a way to structure novel training data that the agent has not seen before as well as that of using a replay buffer to train further on data the agent has seen before. In other words, the focus of this paper is on characteristics of the training data rather than on learning algorithms. We therefore use two tasks that are commonly trained early on in autonomous vehicle research: lane keeping and pedestrian avoidance. Our main results show that curriculum learning indeed offers an additional benefit over a vanilla reinforcement learning approach (using Deep-Q Learning), but the replay buffer actually has a detrimental effect in most (but not all) combinations of data generation approaches we considered here. The benefit of curriculum learning does depend on the existence of a well-defined difficulty metric with which various training scenarios can be ordered. In the lane-keeping task, we can define it as a function of the curvature of the road, in which the steeper and more occurring curves on the road, the more difficult it gets. Defining such a difficulty metric in other scenarios is not always trivial. In general, the results of this paper emphasize both the importance of considering data characterization, such as curriculum learning, and the importance of defining an appropriate metric for the task.									1	0	0	0	0	0	1				2624-8212										Univ Skovde, Sch Informat, Interact Lab, Skovde, SwedenRadboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands				2023-03-08	WOS:000928959000001	36762255	
J	Desjardins, Charles; Chaib-draa, Brahim				Chaib-draa, Brahim/A-1157-2008	Chaib-draa, Brahim/0000-0001-7615-5154					Cooperative Adaptive Cruise Control: A Reinforcement Learning Approach								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				12	4			1248	1260				10.1109/TITS.2011.2157145							Article	DEC 2011	2011	Recently, improvements in sensing, communicating, and computing technologies have led to the development of driver-assistance systems (DASs). Such systems aim at helping drivers by either providing a warning to reduce crashes or doing some of the control tasks to relieve a driver from repetitive and boring tasks. Thus, for example, adaptive cruise control (ACC) aims at relieving a driver from manually adjusting his/her speed to maintain a constant speed or a safe distance from the vehicle in front of him/her. Currently, ACC can be improved through vehicle-to-vehicle communication, where the current speed and acceleration of a vehicle can be transmitted to the following vehicles by intervehicle communication. This way, vehicle-to-vehicle communication with ACC can be combined in one single system called cooperative adaptive cruise control (CACC). This paper investigates CACC by proposing a novel approach for the design of autonomous vehicle controllers based on modern machine-learning techniques. More specifically, this paper shows how a reinforcement-learning approach can be used to develop controllers for the secure longitudinal following of a front vehicle. This approach uses function approximation techniques along with gradient-descent learning algorithms as a means of directly modifying a control policy to optimize its performance. The experimental results, through simulation, show that this design approach can result in efficient behavior for CACC.									211	19	0	0	0	0	269			1524-9050	1558-0016										Univ Laval, Dept Comp Sci & Software Engn, Quebec City, PQ G1K 7P4, Canada				2011-12-01	WOS:000297588500030		
J	Salvi, A.; Coleman, J.; Buzhardt, J.; Krovi, V.; Tallapragada, P.										Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning*								IFAC PapersOnLine								276	81				10.1016/j.ifacol.2022.11.197							Journal Paper	2022	2022	Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort. The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle. The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled. The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios. Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance. This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action. In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity. All rights reserved Elsevier.									0	0	0	0	0	0	0			2405-8963											Dept. of Automotive Eng., Clemson Univ., Clemson, SC, USADept. of Mech. Eng., Clemson Univ., Clemson, SC, USA				2023-05-19	INSPEC:23074700		
B	Liuwang Kang; Haiying Shen										A Reinforcement Learning based Decision-making System with Aggressive Driving Behavior Consideration for Autonomous Vehicles								2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)								9 pp.	9 pp.				10.1109/SECON52354.2021.9491587							Conference Paper	2021	2021	With the fast development of autonomous vehicle (AV) technology and possible popularity of AVs in the near future, a mixed-vehicle type driving environment where both AVs and their surrounding human-driving vehicles drive on the same road will exist and last for a long time. An AV measures its driving environments in real time and make control decisions to ensure driving safety. However, surrounding human-driving vehicles may conduct aggressive driving behaviors (e.g., sudden deceleration, sudden acceleration, sudden left or right lane change) in practice, which requires an AV to make correct control decisions to eliminate the effect of aggressive driving behaviors on its driving safety. In this paper, we propose a reinforcement learning based decision-making system (ReDS) which considers aggressive driving behaviors of surrounding human-driving vehicles during the decision making process. In ReDS, we firstly build a mixture density network based aggressive driving behavior detection method to detect possible aggressive driving behaviors among surrounding vehicles of an AV. We then build a reward function based on aggressive driving behavior detection results and incorporate the reward function into a reinforcement learning model to make optimal control decisions considering aggressive driving behaviors. We use a real-world traffic dataset from the United States Department of Transportation Federal Highway Administration to evaluate optimal control decision determination performance of ReDS in comparison with the state-of-the-art methods. The comparison results show that ReDS can improve optimal control decision success rate by 43% compared with existing methods, which demonstrates that ReDS has good optimal control decision determination performance.					2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)	6-9 July 20216-9 July 2021		Rome, ItalyRome, Italy	0	0	0	0	0	0	0					978-1-6654-4108-7									Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA				2021-10-08	INSPEC:20968112		
C	Zhou, Weitao; Jiang, Kun; Cao, Zhong; Deng, Nanshan; Yang, Diange			IEEE		Jiang, Kun/0000-0003-4995-7244					Integrating Deep Reinforcement Learning with Optimal Trajectory Planner for Automated Driving								2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC											10.1109/itsc45102.2020.9294275							Proceedings Paper	2020	2020	Trajectory planning in the intersection is a challenging problem due to the strong uncertain intentions of surrounding agents. Conventional methods may fail in some corner cases when the ad-hoc parameters or predictions do not match the real traffic. This paper proposes a trajectory planning method, adaptive to the uncertain interactions, called Value-Estimation-Guild (VEG) trajectory planner. The method builds on the Frenet frame trajectory planner, in the meantime, uses the deep reinforcement learning to deal with the high uncertainty. The deep reinforcement learning learns from past failures and adjusts the sample direction of the optimal planner under the Frenet frame. In this way, the generated trajectory can be partially optimal and adapt to the stochastic as well. This method drives the automated vehicle through intersections and completes the unprotected left turn mission. During the testing, traffic density, surrounding vehicles' types, and intentions are all generated randomly. The statistics results show that the proposed trajectory planner works well under high uncertainty. It helps the automatic vehicles to finish the unprotected left turn with a success rate of 94.4 %, compared with the baseline method of 90%.					23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)	SEP 20-23, 2020SEP 20-23, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	11	2	0	0	0	0	12			2153-0009		978-1-7281-4149-7									Tsinghua Univ, Ctr Intelligent Connected Vehicles & Transportat, Sch Vehicle & Mobil, Beijing 100084, Peoples R ChinaUniv Michigan, Dept Mech Engn, Ann Arbor, MI 48105 USA				2021-09-23	WOS:000682770700099		
J	Ngai, Chi Kit										Reinforcement-learning-based autonomous vehicle navigation in a dynamically changing environment																												Dissertation/Thesis	Jan 01 2008	2008										0	0	0	0	0	0	0														University of Hong Kong (Hong Kong), Hong Kong	University of Hong Kong (Hong Kong)				PQDT:64631811		
J	Ibn-Khedher, Hatem; Laroui, Mohammed; Moungla, Hassine; Afifi, Hossam; Abd-Elrahman, Emad				Abd-Elrahman, Emad/GLT-2572-2022; Laroui, Mohammed/R-1473-2018	Abd-Elrahman, Emad/0000-0002-8340-9164; Laroui, Mohammed/0000-0002-6761-8417					Next-Generation Edge Computing Assisted Autonomous Driving Based Artificial Intelligence Algorithms								IEEE ACCESS				10				53987	54001				10.1109/ACCESS.2022.3174548							Article	2022	2022	Edge Computing and Network Function Virtualization (NFV) concepts can improve network processing and multi-resources allocation when intelligent optimization algorithms are deployed. Multiservice offloading and allocation approaches pose interesting challenges in the current and next-generation vehicle networks. The state-of-the-art optimization approaches still formulate exact algorithms, and tune approximation methods to get sufficient solutions. These approaches are data-centric that aim to use heterogeneous data inputs to find the near optimal solutions. In the context of connected and autonomous vehicles (CAVs), these techniques show an exponential computational time and deal only with small and medium scale networks. Therefore, we are motivated by using recent Deep Reinforcement Learning (DRL) techniques to learn the behavior of exact optimization algorithms while enhancing the Quality of Service (QoS) of network operators and satisfying the requirements of the next-generation Autonomous Vehicles (AVs). DRL algorithms can improve AVs service offloading and optimize edge resources. An Optimal Virtual Edge Autopilot Placement (OVEAP) algorithm is proposed using Integer Linear Programming (ILP). Moreover, an autopilot placement protocol is presented to support the algorithm. Optimal allocation and Virtual Network Function (VNF) placement and chaining of the autopilot, based on several new constraints such as computing and networking loads, network edge infrastructure, and placement cost, are designed. Further, a DRL approach is formulated to deal with dense Internet of Autonomous Vehicle (IoAV) networks. Extensive simulations and evaluations are carried out. Results show that the proposed allocation strategies outperform the state-of-the-art solutions and give better performance in terms of Total Edge Servers Utilization, Total Edge Servers Allocation Time, and Successfully Allocated autopilots.									1	0	0	0	0	0	1			2169-3536											Capgemini Engn, F-75017 Paris, FranceUniv Djillali Liabes Sidi Bel Abbes, Comp Sci Dept, Sidi Bel Abbes 22000, AlgeriaUniv Paris Cite, Lab Informat Paris Descartes LIPADE, F-75006 Paris, FranceInst Polytech Paris, Telecom SudParis, F-91764 Palaiseau, FranceNatl Telecommun Inst NTI, Cairo 11768, Egypt	Capgemini EngnInst Polytech ParisNatl Telecommun Inst NTI			2022-06-08	WOS:000802060100001		
B	Huang, Mengzhe										Learning-Based Optimal Control of Connected and Autonomous Vehicles																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798662407688									New York University Tandon School of Engineering, Electrical and Computer Engineering, New York, United States	New York University Tandon School of Engineering				PQDT:67659564		
J	Manchella, K.; Umrawal, A.K.; Aggarwal, V.										FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers & Goods Transportation [arXiv]								arXiv								13 pp.	13 pp.											Journal Paper	27 July 2020	2020	The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries. On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching. The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems. This paper considers combining passenger transportation with goods delivery to improve vehicle-based transportation. Even though the problem has been studied with a defined dynamics model of the transportation system environment, this paper considers a model-free approach that has been demonstrated to be adaptable to new or erratic environment dynamics. We propose FlexPool, a distributed model-free deep reinforcement learning algorithm that jointly serves passengers & goods workloads by learning optimal dispatch policies from its interaction with the environment. The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method. These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods. Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers & goods. FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers & goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.									0	0	0	0	0	0	0														Purdue Univ., West Lafayette, IN, USA				2020-10-16	INSPEC:19965594		
J	Wang, Chengyu; Wang, Luhan; Lu, Zhaoming; Chu, Xinghe; Shi, Zhengrui; Deng, Jiayin; Su, Tianyang; Shou, Guochu; Wen, Xiangming				Wang, Chengyu/HNS-1603-2023	Wang, Chengyu/0000-0003-1910-6297; deng, jiayin/0000-0002-6772-9168; Su, Tianyang/0009-0006-0591-1331					SRL-TR<SUP>2</SUP>: A <i>S</i>afe <i>Re</i>inforcement <i>L</i>earning Based <i>TR</i>ajectory <i>TR</i>acker Framework								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	6			5765	5780				10.1109/TITS.2023.3250720					MAR 2023		Article; Early Access		2023	This paper aims to solve the trajectory tracking con-trol problem for an autonomous vehicle based on reinforcement learning methods. Existing reinforcement learning approaches have found limited successful applications on safety-critical tasks in the real world mainly due to two challenges: 1) sim-to-real transfer; 2) closed-loop stability and safety concern. In this paper, we propose an actor-critic-style framework SRL-TR2, in which the RL-based TRajectory TRackers are trained under the safety constraints and then deployed to a full-size vehicle as the lateral controller. To improve the generalization ability, we adopt a light-weight adapter State and Action Space Alignment (SASA) to establish mapping relations between the simulation and reality. To address the safety concern, we leverage an expert strategy to take over the control when the safety constraints are not satisfied. Hence, we conduct safe explorations during the training process and improve the stability of the policy. The experiments show that our agents can achieve one-shot transfer across simulation scenarios and unseen realistic scenarios, finishing the field tests with average running time less than 10 ms/step and average lateral error less than 0.1 m under the speed ranging from 12 km/h to 18 km/h. A video of the field tests is available at https://youtu.be/pjWcN_fV24g.									1	0	0	0	0	0	1			1524-9050	1558-0016										Beijing Univ Posts & Telecommun, Beijing Lab Adv Informat Networks, Beijing Key Lab Network Syst Architecture & Conver, Beijing 100876, Peoples R China				2023-03-27	WOS:000947810600001		
B	Li, J.; Abusharkh, M.; Xu, Y.										DeepRacer Model Training for autonomous vehicles on AWS EC2								2022 International Telecommunications Conference (ITC-Egypt)								5 pp.	5 pp.				10.1109/ITC-Egypt55520.2022.9855675							Conference Paper	2022	2022	Autonomous vehicle (AV) is the future of public transportation to reduce the road congestion and accidents. However, fully self-driving car is still a challenge for carmaker, and they must ensure the maximum driving security to avoid the ethical issue. To develop a mature model to AV, reinforcement learning becomes a solution which can explore many possibilities and choose the best possible action choice facing different road conditions. AWS DeepRacer is a comprehensive platform for researchers to start reinforcement learning for AV. With using DeepRacer Console, all the parameters including action spaces, reward functions and hyper parameters can be edited online and trained remotely. However, the price for training a converged model on DeepRacer Console is quite expensive for beginners. This paper present a DeepRacer simulation process built on EC2 instance which demand no hardware and cost significantly less than regular DeepRacer Console. Based on the default models from AWS, the authors experimentally adjusted hyper parameters and added reward functions to achieve higher speed and smoother driving actions. Even though the computing resources still limited the agent performance and stopped the model from convergence, there are 2-3m/s speed increases when adding low speed penalty and progress penalty on reward function.					2022 International Telecommunications Conference (ITC-Egypt)2022 International Telecommunications Conference (ITC-Egypt)	26-28 July 202226-28 July 2022	Commander-in-Chief of the Armed Forces, Minister of Defense and Military ProductionCommander-in-Chief of the Armed Forces, Minister of Defense and Military Production	Alexandria, EgyptAlexandria, Egypt	0	0	0	0	0	0	0					978-1-6654-8808-2									Inf. Syst., Northeastern Univ., Boston, MA, USADigital Media Software Eng., Ferris State Univ., Grand Rapids, MI, USA				2022-11-01	INSPEC:21975155		
C	Qiao, Zhiqian; Schneider, Jeff; Dolan, John M.			IEEE							Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning								2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2021)		IEEE International Conference on Robotics and Automation ICRA						2667	2673				10.1109/ICRA48506.2021.9561095							Proceedings Paper	2021	2021	For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure Ill allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 30-JUN 05, 2021MAY 30-JUN 05, 2021	IEEE; IEEE Robot & Automat Soc; So Univ Sci & Technol; Harbin Inst Technol; Xian Jiaotong Univ; Univ Toronto; Nat Sci & Engn Res Council Canada; Chinese Univ Hong Kong; DJI; Biomimet Intelligence & Robot; Toyota Res Inst; Baidu; Mech Mind Robot Technologies; Facebook AI; Nokov; Manycore; Franka Emika; Zhejiang Univ, Huzhou Inst; Jing Tian; ATI Ind Automat; Amber; Direct Drive Tech; Inceptio; Flexiv; Deeproute AI; Unitree; Waymo; Kuka; Pudu Robot; Deep Robot; Nvidia; MathWorksIEEE; IEEE Robot & Automat Soc; So Univ Sci & Technol; Harbin Inst Technol; Xian Jiaotong Univ; Univ Toronto; Nat Sci & Engn Res Council Canada; Chinese Univ Hong Kong; DJI; Biomimet Intelligence & Robot; Toyota Res Inst; Baidu; Mech Mind Robot Technologies; Facebook AI; Nokov; Manycore; Franka Emika; Zhejiang Univ, Huzhou Inst; Jing Tian; ATI Ind Automat; Amber; Direct Drive Tech; Inceptio; Flexiv; Deeproute AI; Unitree; Waymo; Kuka; Pudu Robot; Deep Robot; Nvidia; MathWorks	Xian, PEOPLES R CHINAXian, PEOPLES R CHINA	8	1	0	0	0	0	8			1050-4729	2577-087X	978-1-7281-9077-8									Carnegie Mellon Univ, Elect & Comp Engn, 5000 Forbes Ave, Pittsburgh, PA 15213 USACarnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA				2022-04-22	WOS:000765738802037		
J	Park, Jehyun; Choi, Jongeun; Nah, Sungjae; Kim, Dohee					Choi, Jongeun/0000-0002-7532-5315; PARK, JEHYUN/0000-0002-6991-229X					Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations								ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE				123						106465			10.1016/j.engappai.2023.106465					MAY 2023		Article	AUG 2023	2023	Reinforcement learning has shown remarkable success in various applications, and in some cases, even out-performs human performance. However, despite the potential of reinforcement learning, numerous challenges still exist. In this paper, we introduce a novel approach that exploits the synergies between hierarchical reinforcement learning and distributional reinforcement learning to address complex sparse-reward tasks, where noisy state observations or non-stationary exogenous perturbations are present. Our proposed method has a hierarchical policy structure, where random rewards are modeled as random variables that follow a value distribution. This approach enables the handling of complex tasks and increases robustness to uncertainties arising from measurement noise or exogenous perturbations, such as wind. To achieve this, we extend the distributional soft Bellman operator and temporal difference error to include the hierarchical structure, and we use quantile regression to approximate the reward distribution. We evaluate our method using a bipedal robot in the OpenAI Gym environment and an electric autonomous vehicle in the SUMO traffic simulator. The results demonstrate the effectiveness of our approach in solving complex tasks with the aforementioned uncertainties when compared to state-of-the-art methods. Our approach demonstrates promising results in handling uncertainties caused by noise and perturbations for challenging sparse-reward tasks, and could potentially pave the way for the development of more robust and effective reinforcement learning algorithms in real physical systems.									0	0	0	0	0	0	0			0952-1976	1873-6769										Yonsei Univ, Sch Mech Engn, Seoul 03722, South KoreaYonsei Univ, Dept Artificial Intelligence, Seoul 03722, South KoreaHyundai Motor Co, Electrified Syst Control Res Lab, Res & Dev Div, Hwaseong 18280, Gyeonggi Do, South Korea				2023-07-01	WOS:001011429700001		
J	D'alfonso, L.; Giannini, F.; Franze, G.; Fedele, G.; Pupo, F.; Fortino, G.				Fortino, Giancarlo/J-2950-2017; Fedele, Giuseppe/K-2950-2017	Fortino, Giancarlo/0000-0002-4039-891X; Fedele, Giuseppe/0000-0003-0273-780X; Franze, Giuseppe/0000-0002-6712-9066; D'Alfonso, Luigi/0000-0001-7251-1334					Autonomous vehicle platoons in urban road networks: a joint distributed reinforcement learning and model predictive control approach								IEEE/CAA Journal of Automatica Sinica								141	56				10.1109/JAS.2023.123705							Journal Paper	2024	2024	In this paper, platoons of autonomous vehicles operating in urban road networks are considered. From a methodological point of view, the problem of interest consists of formally characterizing vehicle state trajectory tubes by means of routing decisions complying with traffic congestion criteria. To this end, a novel distributed control architecture is conceived by taking advantage of two methodologies: deep reinforcement learning and model predictive control. On one hand, the routing decisions are obtained by using a distributed reinforcement learning algorithm that exploits available traffic data at each road junction. On the other hand, a bank of model predictive controllers is in charge of computing the more adequate control action for each involved vehicle. Such tasks are here combined into a single framework: the deep reinforcement learning output (action) is translated into a set-point to be tracked by the model predictive controller; conversely, the current vehicle position, resulting from the application of the control move, is exploited by the deep reinforcement learning unit for improving its reliability. The main novelty of the proposed solution lies in its hybrid nature: on one hand it fully exploits deep reinforcement learning capabilities for decision-making purposes; on the other hand, time-varying hard constraints are always satisfied during the dynamical platoon evolution imposed by the computed routing decisions. To efficiently evaluate the performance of the proposed control architecture, a co-design procedure, involving the SUMO and MATLAB platforms, is implemented so that complex operating environments can be used, and the information coming from road maps (links, junctions, obstacles, semaphores, etc.) and vehicle state trajectories can be shared and exchanged. Finally by considering as operating scenario a real entire city block and a platoon of eleven vehicles described by double-integrator models, several simulations have been performed with the aim to put in light the main features of the proposed approach. Moreover, it is important to underline that in different operating scenarios the proposed reinforcement learning scheme is capable of significantly reducing traffic congestion phenomena when compared with well-reputed competitors.									0	0	0	0	0	0	0			2329-9274											Dept. of Comput. Eng., Modeling, Electron. & Syst., Univ. della Calabria, Rende, ItalyDept. of Mech. Eng., Energy Eng. & Manage., Univ. della Calabria, Rende, Italy				2024-02-01	INSPEC:24433305		
J	Lee, Dongsu; Kwon, Minhae				lee, dongsu/HPD-2513-2023						Stability Analysis in Mixed-Autonomous Traffic With Deep Reinforcement Learning								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				72	3			2848	2862				10.1109/TVT.2022.3215505							Article	MAR 2023	2023	The emergence of autonomous driving vehicles on roads has increased the importance of research on autonomous driving in mixed-autonomous traffic. In mixed-autonomous traffic scenarios, it is necessary to comprehend the instability of autonomous vehicles and traffic flow corresponding to the uncertainty level in human-driven behaviors. However, studies of stability analysis in deep reinforcement learning are limited. This study focuses on the impact of deep reinforcement learning based autonomous vehicles in mixed-autonomous traffic from the stability perspective. We define the policy instability and traffic flow instabilities using the entropy of the velocity distributions to quantitatively measure the instability of an autonomous vehicle. Subsequently, we provide mathematical analyses to explain logarithmic growth patterns of instability. Moreover, we propose a novel deep reinforcement learning approach that jointly determines discrete and continuous actions under partial observation. To verify the proposed solution, we perform extensive simulations of various traffic scenarios (e.g., increasing traffic volumes, increasing the number of autonomous vehicles on the road, and setting the multiple uncertainty levels for human-driven behaviors) with ablation studies on reward function. Moreover, we analyze instabilities when human-driven vehicles are modeled using the human-like noisy controller and a policy that imitates actual human-driving data based on imitation learning. The simulation results support the theoretical analysis and confirm that the proposed method is stabler compared to a conventional control-theoretic approach.									1	0	0	0	0	0	1			0018-9545	1939-9359										Soongsil Univ, Sch Elect Engn, Seoul 06978, South Korea				2023-06-12	WOS:000966511200001		
C	Bogosyan, Seta; Gokasan, Metin; Vamvoudakis, Kyriakos G.			IEEE		Vamvoudakis, Kyriakos G./0000-0003-1978-4848					Zero-Sum Game (ZSG) based Integral Reinforcement Learning for Trajectory Tracking Control of Autonomous Smart Car								2022 IEEE 31ST INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS (ISIE)													10.1109/ISIE51582.2022.9948217							Proceedings Paper	2022	2022	The ultimate aim of our research study is the development, practical implementation, and benchmarking of continuous-time, online reinforcement learning (RL) schemes for the trajectory tracking control (TTC) of fully autonomous vehicles (AVs) in real-world scenarios. The adaptive optimality and model-free nature offered by RL has a stronger promise against its model-based counterparts, such as MPC, against uncertainties related to the vehicle, road, tire-terrain and environmental dynamics. The existing studies on RL based AV control are mostly theoretical, often dealing with high-level TTC, and perform evaluations in simulations considering simplified or linear models with no disturbance and slip effects. The literature also demonstrates the lack of practical implementations in overall RL based autonomous vehicle control. Our ultimate goal is to fill these theoretical and practical gaps by designing and practically evaluating novel RL strategies that will improve the performance of TTC against uncertainties at all levels. This paper presents the simulation results of our preliminary studies in the online, longitudinal tracking control of a realistic AV (with uncertain nonlinear dynamics, as well as disturbance, and slip effects), which we treat as a Zero-Sum Game (ZSG) problem using an Integral Reinforcement Learning (IRL) approach with synchronous actor and critic updates (SyncIRL). The results are promising and motivate the practical implementation of the approach for combined longitudinal and lateral control of AV.					IEEE 31st International Symposium on Industrial Electronics (ISIE)IEEE 31st International Symposium on Industrial Electronics (ISIE)	JUN 01-03, 2022JUN 01-03, 2022	IEEEIEEE	Anchorage, AKAnchorage, AK	2	0	0	0	0	0	2					978-1-6654-8240-0									Istanbul Tech Univ, Elect & Elect Engn, Istanbul, TurkeyGeorgia Inst Technol, Aerosp Engn, Atlanta, GA 30332 USA				2023-04-12	WOS:000946662000001		
J	Tian Kang; Yu Di; Li Qing; Zhang Hongchang; Wu Yingnian; Fan Lingling							田康; 于镝; 李擎; 张宏昌; 吴迎年; 范玲玲			Lane keeping decision-making method of autonomous vehicles based on improved TD3			基于改进TD3的自动驾驶车道保持决策方法				北京交通大学学报. 自然科学版	Journal of Beijing Jiaotong University				46	5			84	94	1673-0291(2022)46:5<84:JYGJTD>2.0.TX;2-J										Article	2022	2022	This paper proposes a new end-to-end decision-making scheme for lane keeping based on the improved TD3 algorithm. First, a multi-data fusion TD3 algorithm framework is constructed to perceive the kinematics data information and visual image information of autonomous vehicle to enhance the stability of the algorithm. Second, combined with the concept of attention mechanism, image features are refined so that the algorithm pays attention to the key road information, which enhances the interpretability of the algorithm. Then, a guidance-based reward function is designed with comprehensive consideration of driving safety, comfort and efficiency to guide the intelligent agent to learn a more human-like driving strategy. Finally, a classification and high-value prioritized experience replay method is applied to improve the sample utilization and accelerate the algorithm convergence. With the aid of TORCS simulation platform, multiple sets of comparative experiments are designed to verify the effectiveness and feasibility of the proposed method. Furthermore, through simulation tests in multiple scenarios, it is verified that the overall performance of the proposed improved TD3 algorithm is better than that of the TD3 algorithm.			本文提出基于改进TD3算法的车道保持端到端决策新方案.首先,构建多数据融合TD3算法框架,感知自主车辆运动学数据信息和视觉图像信息来提升算法的稳定性.并且结合注意力机制思想细化图像特征,使得算法关注重要道路信息,以此增强算法可解释性.其次,综合考虑驾驶的安全性、舒适性和效率性因素设计了指导型奖励函数,以引导智能体学到更加类人的驾驶策略.最后,采用分类与高价值优先级经验回放方法,以提高样本利用率和加快算法收敛速度.借助TORCS仿真平台,设计了多组对比实验,以验证所提方法的有效性和可行性;并且通过多个场景的仿真测试,验证了改进TD3算法的整体性能优于TD3算法.						0	0	0	0	0	0	0			1673-0291											北京信息科技大学自动化学院, 北京 100192, 中国清华大学自动化学院, 北京 100084, 中国School of Automation, Beijing Information Science and Technology University, Beijing 100192, ChinaSchool of Automation, Tsinghua University, Beijing 100084, China	北京信息科技大学自动化学院清华大学自动化学院School of Automation, Beijing Information Science and Technology UniversitySchool of Automation, Tsinghua University			2023-04-22	CSCD:7370753		
J	Holen, Martin; Knausgard, Kristian Muri; Goodwin, Morten					Knausgard, Kristian Muri/0000-0003-4088-1642; Holen, Martin/0000-0003-0967-221X					Development of a Simulator for Prototyping Reinforcement Learning-Based Autonomous Cars								INFORMATICS-BASEL				9	2					33			10.3390/informatics9020033							Article	JUN 2022	2022	Autonomous driving is a research field that has received attention in recent years, with increasing applications of reinforcement learning (RL) algorithms. It is impractical to train an autonomous vehicle thoroughly in the physical space, i.e., the so-called 'real world'; therefore, simulators are used in almost all training of autonomous driving algorithms. There are numerous autonomous driving simulators, very few of which are specifically targeted at RL. RL-based cars are challenging due to the variety of reward functions available. There is a lack of simulators addressing many central RL research tasks within autonomous driving, such as scene understanding, localization and mapping, planning and driving policies, and control, which have diverse requirements and goals. It is, therefore, challenging to prototype new RL projects with different simulators, especially when there is a need to examine several reward functions at once. This paper introduces a modified simulator based on the Udacity simulator, made for autonomous cars using RL. It creates reward functions, along with sensors to create a baseline implementation for RL-based vehicles. The modified simulator also resets the vehicle when it gets stuck or is in a non-terminating loop, making it more reliable. Overall, the paper seeks to make the prototyping of new systems simple, with the testing of different RL-based systems.									1	0	0	0	0	0	1				2227-9709										Univ Agder, Ctr Artificial Intelligence Res, N-4879 Grimstad, NorwayUniv Agder, Top Res Ctr Mechatron, N-4879 Grimstad, Norway				2022-07-03	WOS:000816368100001		
J	Zhang, Ethan; Zhang, Ruixuan; Masoud, Neda					Masoud, Neda/0000-0002-6526-3317					Predictive trajectory planning for autonomous vehicles at intersections using reinforcement learning								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				149						104063			10.1016/j.trc.2023.104063					FEB 2023		Article; Early Access		2023	In this work we put forward a predictive trajectory planning framework to help autonomous vehicles plan future trajectories. We develop a partially observable Markov decision process (POMDP) to model this sequential decision making problem, and a deep reinforcement learning solution methodology to learn high-quality policies. The POMDP model utilizes driving scenar-ios, condensed into graphs, as inputs. More specifically, an input graph contains information on the history trajectory of the subject vehicle, predicted trajectories of other agents in the scene (e.g., other vehicles, pedestrians, and cyclists), as well as predicted risk levels posed by surrounding vehicles to devise safe, comfortable, and energy-efficient trajectories for the subject vehicle to follow. In order to obtain sufficient driving scenarios to use as training data, we propose a simulation framework to generate socially acceptable driving scenarios using a real world autonomous vehicle dataset. The simulation framework utilizes Bayesian Gaussian mixture models to learn trajectory patterns of different agent types, and Gibbs sampling to ensure that the distribution of simulated scenarios matches that of the real-world dataset collected by an autonomous fleet. We evaluate the proposed work in two complex urban driving environments: a non-signalized T-junction and a non-signalized lane merge intersection. Both environments provide vastly more complex driving scenarios compared to a highway driving environment, which has been mostly the focus of previous studies. The framework demonstrates promising performance for planning horizons as long as five seconds. We compare safety, comfort, and energy efficiency of the planned trajectories against human-driven trajectories in both experimental driving environments, and demonstrate that it outperforms human-driven trajectories in a statistically significant fashion in all aspects.									1	0	0	0	0	0	1			0968-090X	1879-2359										Univ Michigan, Civil & Environm Engn, Ann Arbor, MI 48109 USA				2023-06-14	WOS:000997914300001		
J	Li, Zhuo; You, Keyou; Sun, Jian; Wang, Gang				You, Keyou/HGC-2361-2022; Wang, Gang/I-9061-2019	You, Keyou/0000-0003-4355-5340; Wang, Gang/0000-0002-7266-2412; Li, Zhuo/0000-0002-1958-024X					Informative Trajectory Planning Using Reinforcement Learning for Minimum-Time Exploration of Spatiotemporal Fields								IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS													10.1109/TNNLS.2023.3300926					AUG 2023		Article; Early Access		2023	This article studies the informative trajectory planning problem of an autonomous vehicle for field exploration. In contrast to existing works concerned with maximizing the amount of information about spatial fields, this work considers efficient exploration of spatiotemporal fields with unknown distributions and seeks minimum-time trajectories of the vehicle while respecting a cumulative information constraint. In this work, upon adopting the observability constant as an information measure for expressing the cumulative information constraint, the existence of a minimum-time trajectory is proven under mild conditions. Given the spatiotemporal nature, the problem is modeled as a Markov decision process (MDP), for which a reinforcement learning (RL) algorithm is proposed to learn a continuous planning policy. To accelerate the policy learning, we design a new reward function by leveraging field approximations, which is demonstrated to yield dense rewards. Simulations show that the learned policy can steer the vehicle to achieve an efficient exploration, and it outperforms the commonly-used coverage planning method in terms of exploration time for sufficient cumulative information.									0	0	0	0	0	0	0			2162-237X	2162-2388										Beijing Inst Technol, Sch Automat, Natl Key Lab Autonomous Intelligent Unmanned Syst, Beijing 100081, Peoples R ChinaBeijing Inst Technol, Chongqing Innovat Ctr, Chongqing 401120, Peoples R ChinaTsinghua Univ, Dept Automat, Beijing 100084, Peoples R ChinaTsinghua Univ, BNRist, Beijing 100084, Peoples R China				2023-08-30	WOS:001051249000001	37581975	
C	Tian, Zhaofeng; Shi, Weisong			IEEE	Shi, Weisong/D-2233-2016	Shi, Weisong/0000-0001-5864-4675					Design and Implement an Enhanced Simulator for Autonomous Delivery Robot								2022 FIFTH INTERNATIONAL CONFERENCE ON CONNECTED AND AUTONOMOUS DRIVING (METROCAD 2022)								21	29				10.1109/MetroCAD56305.2022.00009							Proceedings Paper	2022	2022	As autonomous driving technology is getting more and more mature today, autonomous delivery companies like Starship, Marble, and Nuro has been making progress in the tests of their autonomous delivery robots. While simulations and simulators are very important for the final product landing of the autonomous delivery robots since the autonomous delivery robots need to navigate on the sidewalk, campus, and other urban scenarios, where the simulations can avoid real damage to pedestrians and properties in the real world caused by any algorithm failures and programming errors and thus accelerate the whole developing procedure and cut down the cost. In this case, this study proposes an open-source simulator based on our autonomous delivery robot ZebraT to accelerate the research on autonomous delivery. The simulator developing procedure is illustrated step by step. What is more, the applications on the simulator that we are working on are also introduced, which includes autonomous navigation in the simulated urban environment, cooperation between an autonomous vehicle and an autonomous delivery robot, and reinforcement learning practice on the task training in the simulator. We have published the proposed simulator in Github.					5th International Conference on Connected and Autonomous Driving (MetroCAD)5th International Conference on Connected and Autonomous Driving (MetroCAD)	APR 28-29, 2022APR 28-29, 2022	Kettering Univ; Wayne State Univ, Connected & Autonomous Res LabKettering Univ; Wayne State Univ, Connected & Autonomous Res Lab	Detroit, MIDetroit, MI	0	0	0	0	0	0	0					978-1-6654-7112-1									Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA				2022-09-28	WOS:000855235300004		
C	Liu, Tong; Lei, Lei; Liu, Zhiming; Zheng, Kan			IEEE							Jointly Learning V2X Communication and Platoon Control with Deep Reinforcement Learning								2023 IEEE 34TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS, PIMRC		IEEE International Symposium on Personal Indoor and Mobile Radio Communications Workshops-PIMRC Workshops											10.1109/PIMRC56721.2023.10293991							Proceedings Paper	2023	2023	In autonomous vehicle platooning, Vehicle-to-Everything (V2X) communications are leveraged in cooperative adaptive cruise control (CACC) to improve control performance. Since exchanging information at all times incurs significant communication overhead in vehicular networks, it is important to determine when V2X communication is necessary. To solve this problem, we propose a Deep Reinforcement Learning (DRL)-based algorithm named Attention-DDPG, which learns platoon control with Deep Deterministic Policy Gradient (DDPG), and learns when to communicate with an attention network. Specifically, each preceding vehicle is equipped with a deep neural network (DNN), which takes as input its local state and platoon control action and determines whether to transmit its acceleration or not to the following vehicle at each time step. The attention network of a preceding vehicle is trained using the feedback from the following vehicle on the value of V2X information in the form of an advantage function. In order to evaluate Attention-DDPG, simulations are performed using real driving data, and performance is compared with those of two baselines that communicate and do not communicate at all times, respectively. The results demonstrate that Attention-DDPG strikes a competitive trade-off between control performance and communication overhead while ensuring platoon string stability.					IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)	SEP 05-08, 2023SEP 05-08, 2023	IEEE; IEEE Commun SocIEEE; IEEE Commun Soc	Toronto, CANADAToronto, CANADA	0	0	0	0	0	0	0			2166-9570		978-1-6654-6483-3									Beijing Univ Posts & Telecommun, Key Lab Univ Wireless Commun, Beijing, Peoples R ChinaUniv Guelph, Sch Engn, Guelph, ON, CanadaNingbo Univ, Coll Elect Engn & Comp Sci, Ningbo, Peoples R China				2024-01-25	WOS:001103214700242		
J	Syavasya, C. V. S. R.; Muddana, A. Lakshmi										Optimization of autonomous vehicle speed control mechanisms using hybrid DDPG-SHAP-DRL-stochastic algorithm								ADVANCES IN ENGINEERING SOFTWARE				173						103245			10.1016/j.advengsoft.2022.103245					SEP 2022		Article; Early Access		2022	Autonomous Vehicles (AV) are the future milestones of the automobile industry, which functions without the intervention of human being. Numerous researches have been stimulated by leading automobile sectors of the world, to address the anticipated challenges in implementing the autonomous vehicles in a practical scenario. The speed control mechanism is the predominant challenge which acts in the basis of Machine Learning mechanism is the major thrust area associated with autonomous vehicles. Reinforcement Learning (RL) is the effective algorithm to solve the challenges associated with the autonomous driving of vehicles and its decision on complex scenarios. A simulative environment is advantageous for training and validation of an RL algorithm because it reduces risk and saves resources. This research work introduces a novel hybrid algorithm composed of Deep Deterministic Policy Gradient (DDPG) -SHapley Additive exPlanations (SHAP) - Deep Reinforcement Learning (DRL)-stochastic algorithm. The primary objective of this research work is to introduce an RL envi-ronment for optimizing longitudinal control.									1	0	0	0	0	0	1			0965-9978	1873-5339										GITAM Univ, Dept Comp Sci & Engn, Hyderabad 502329, India				2022-09-28	WOS:000857299100008		
J	Ahadi, Ramin; Ketter, Wolfgang; Collins, John; Daina, Nicolo				Ketter, Wolfgang/W-4931-2017	Ketter, Wolfgang/0000-0001-9008-142X; Ahadi, Ramin/0000-0002-8447-5008; Daina, Nicolo/0000-0002-5902-4651					Cooperative Learning for Smart Charging of Shared Autonomous Vehicle Fleets								TRANSPORTATION SCIENCE				57	3			613	630				10.1287/trsc.2022.1187					DEC 2022		Article; Early Access		2023	We study the operational problem of shared autonomous electric vehicles that cooperate in providing on-demand mobility services while maximizing fleet profit and service quality. Therefore, we model the fleet operator and vehicles as interactive agents enriched with advanced decision-making aids. Our focus is on learning smart charging policies (when and where to charge vehicles) in anticipation of uncertain future demands to accommodate long charging times, restricted charging infrastructure, and time-varying electricity prices. We propose a distributed approach and formulate the problem as a semiMarkov decision process to capture its stochastic and dynamic nature. We use cooperative multiagent reinforcement learning with reshaped reward functions. The effectiveness and scalability of the proposed model are upgraded through deep learning. A mean-field approximation deals with environment instabilities, and hierarchical learning distinguishes high-level and low-level decisions. We evaluate our model using various numerical examples based on real data from ShareNow in Berlin, Germany. We show that the policies learned using our decentralized and dynamic approach outperform central static charging strategies. Finally, we conduct a sensitivity analysis for different fleet characteristics to demonstrate the proposed model's robustness and provide managerial insights into the impacts of strategic decisions on fleet performance and derived charging policies.									0	0	0	0	0	0	0			0041-1655											Univ Cologne, Fac Management Econ & Social Sci, D-50923 Cologne, GermanyErasmus Univ, Rotterdam Sch Management, NL-3062 PA Rotterdam, NetherlandsUniv Minnesota, Comp Sci & Engn, Minneapolis, MN 55455 USAColumbia Univ, Civil Engn & Engn Mech, New York, NY 10027 USAColumbia Univ, Ctr Global Energy Policy, New York, NY 10027 USA				2023-04-10	WOS:000955702300001		
J	Ji, Yuan; Zhang, Junzhi; Lv, Chen; He, Chengkun; Chen, Hao; Han, Jinheng; Hou, Xiaohui				Lv, Chen/N-7055-2018	Lv, Chen/0000-0001-6897-4512; Zhang, Junzhi/0000-0002-5055-2941					Optimal Path Tracking Control Based on Online Modeling for Autonomous Vehicle With Completely Unknown Parameters								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	12			15207	15218				10.1109/TITS.2023.3306040							Article	DEC 2023	2023	Reliable path tracking control (PTC) method is essential for autonomous driving. However, existing PTC methods count on prior vehicle parameters to achieve good performance. This paper presents an optimal PTC method without requiring any prior vehicle parameters based on online modeling with strict parameter convergence ability. First, we build a virtual optimal control problem using adaptive dynamic programming (ADP) scheme to guide the data collection and solve two characteristic matrices containing parameter information. Then, the model construction method is derived using the solved matrices and the optimal PTC method is constructed using the constructed model. Finally, a fault-tolerant control scheme is further designed using the constructed model and the online modeling ability of the proposed method. The effectiveness of the proposed method is validated through co-simulation between Matlab/Simulink and high-fidelity vehicle dynamic simulation software CarSim (R) under both fault-free and fault-tolerant situations.									0	0	0	0	0	0	0			1524-9050	1558-0016										Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R ChinaTsinghua Univ, State Key Lab Intelligent Green Vehicle & Mobil, Beijing 100084, Peoples R ChinaNanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore 639798, SingaporeBeijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China				2024-01-26	WOS:001140690100128		
J	Shi, Yanjun; Liu, Yuanzhuo; Qi, Yuhan; Han, Qiaomei				SHI, YAN/HNI-1042-2023	Han, Qiaomei/0000-0001-5535-8390					A Control Method with Reinforcement Learning for Urban Un-Signalized Intersection in Hybrid Traffic Environment								SENSORS				22	3					779			10.3390/s22030779							Article	FEB 2022	2022	To control autonomous vehicles (AVs) in urban unsignalized intersections is a challenging problem, especially in a hybrid traffic environment where self-driving vehicles coexist with human driving vehicles. In this study, a coordinated control method with proximal policy optimization (PPO) in Vehicle-Road-Cloud Integration System (VRCIS) is proposed, where this control problem is formulated as a reinforcement learning (RL) problem. In this system, vehicles and everything (V2X) was used to keep communication between vehicles, and vehicle wireless technology can detect vehicles that use vehicles and infrastructure (V2I) wireless communication, thereby achieving a cost-efficient method. Then, the connected and autonomous vehicle (CAV) defined in the VRCIS learned a policy to adapt to human driving vehicles (HDVs) across the intersection safely by reinforcement learning (RL). We have developed a valid, scalable RL framework, which can communicate topologies that may be dynamic traffic. Then, state, action and reward of RL are designed according to urban unsignalized intersection problem. Finally, how to deploy within the RL framework was described, and several experiments with this framework were undertaken to verify the effectiveness of the proposed method.									5	0	0	0	0	0	5				1424-8220										Dalian Univ Technol, Sch Mech Engn, Dalian 116024, Peoples R ChinaWestern Univ, Dept Elect & Comp Engn, London, ON N6A 5B9, Canada				2022-02-28	WOS:000759387900001	35161523	
C	Lu, Liping; Ning, Qinjian; Qiu, Yujie; Chu, Duanfeng				Chu, Duanfeng/GPS-8591-2022; LU, Li/JFJ-9011-2023	Chu, Duanfeng/0000-0001-5225-9143; 	Reformat, M; Zhang, D; Bourbakis, N				Vehicle Trajectory Prediction Model Based on Attention Mechanism and Inverse Reinforcement Learning								2022 IEEE 34TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI		Proceedings-International Conference on Tools With Artificial Intelligence						1160	1166				10.1109/ICTAI56018.2022.00177							Proceedings Paper	2022	2022	Predicting the future trajectory of a vehicle in a dynamic scene is not a simple problem because the future trajectory of a vehicle is not only influenced by its historical trajectory but also by other vehicles. To solve this problem, we propose a vehicle trajectory prediction model based on attention mechanism and inverse reinforcement learning. The model uses the LSTM encoder-decoder framework as an infrastructure to efficiently extract the temporal features of vehicle trajectories. A social attention module is proposed to model the degree of inter-vehicle influence based on the distance between vehicles. The module generates feature vectors and serves as the weight values of the attention mechanism, enabling the prediction network to focus more on the surrounding vehicles with a greater degree of influence. An inverse reinforcement learning framework is introduced to regularize the encoder network using a reward function. The reward function effectively evaluates the gap between the predicted and true positions of the encoder output and enables the predicted positions to be closer to the true positions by training the network parameters. Based on the experimental results of public datasets SDD and NGSIM, our model can predict the future trajectory of vehicles more accurately than other models.					34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)	OCT 31-NOV 02, 2022OCT 31-NOV 02, 2022	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			1082-3409		979-8-3503-9744-4									Wuhan Univ Technol, Sanya Sci & Educ Innovat Pk, Sanya, Peoples R ChinaWuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R ChinaWuhan Univ Technol, Sch Automot Engn, Wuhan, Peoples R China				2023-06-18	WOS:000992544400169		
J	Sun, Yipu; Chen, Xin; Wang, Wei; Fu, Hao; Wu, Min				chen, xin/AFS-7186-2022	sun, yipu/0000-0003-4868-1506; Chen, Xin/0000-0001-9924-6833; Wu, Min/0000-0002-0668-8315					Model-Free Output Consensus Control for Partially Observable Heterogeneous Multivehicle Systems								IEEE INTERNET OF THINGS JOURNAL				7	8			7135	7147				10.1109/JIOT.2020.2981654							Article	AUG 2020	2020	Internet of Vehicles (IoV) is a typical application of Internet-of-Things (IoT) technology in the field of intelligent transportation systems. In the actual IoV, such as the autonomous vehicle fleet, there exists the problem of heterogeneous multivehicle coordination based on IoT communication. How to ensure the synchronization of multiple vehicles is a hot issue. In particular, when the system can only obtain a partial state of the vehicle, and does not know the dynamic model, including the vehicle itself and the companion model. To overcome these deficiencies, this article deals with the model-free output consensus control problem for a class of partially observable heterogeneous multivehicle systems (MVSs). Using measurable input/output data without any system knowledge, this article develops a Q-function-based adaptive dynamic programming (ADP). First, an adaptive distributed observer is designed to estimate the output of the leader. The augmented state representation is built using historical measurable input/output data instead of the unmeasurable inner system state. Then, a Q-function-based ADP method using measurable input/output data was introduced. The method is used to solve this distributed tracking control problem without the requirement for the MVSs dynamics. The convergence analysis of the proposed method is also given. To facilitate the implementation of the proposed method, an actor-critic framework is adopted to approximate the optimal Q-functions and the optimal control policies. It shows that the approximated control policies achieve the distributed optimal tracking control. Finally, the simulation results verify the effectiveness of the developed method for solving multivehicle formation control problems.									6	0	0	0	0	0	6			2327-4662											China Univ Geosci, Hubei Key Lab Adv Control & Intelligent Automat C, Wuhan 430074, Peoples R ChinaChina Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China				2020-08-28	WOS:000559482800036		
J	Teng Liu; Bo Wang; Dongpu Cao; Xiaolin Tang; Yalian Yang										Integrated Longitudinal Speed Decision-Making and Energy Efficiency Control for Connected Electrified Vehicles [arXiv]								arXiv								11 pp.	11 pp.											Journal Paper	24 July 2020	2020	To improve the driving mobility and energy efficiency of connected autonomous electrified vehicles, this paper presents an integrated longitudinal speed decision-making and energy efficiency control strategy. The proposed approach is a hierarchical control architecture, which is assumed to consist of higher-level and lower-level controls. As the core of this study, model predictive control and reinforcement learning are combined to improve the powertrain mobility and fuel economy for a group of automated vehicles. The higher-level exploits the signal phase and timing and state information of connected autonomous vehicles via vehicle to infrastructure and vehicle to vehicle communication to reduce stopping at red lights. The higher-level outputs the optimal vehicle velocity using model predictive control technique and receives the power split control from the lower-level con-troller. These two levels communicate with each other via a controller area network in the real vehicle. The lower-level utilizes a model-free reinforcement learning method to improve the fuel economy for each connected autonomous vehicle. Numerical tests illustrate that vehicle mobility can be noticeably improved (traveling time reduced by 30%) by reducing red-light idling. The effectiveness and performance of the proposed method are validated via comparison analysis among different energy efficiency controls (fuel economy promoted by 13%).									0	0	0	0	0	0	0														Coll. of Automotive Eng., Chongqing Univ., Chongqing, ChinaBeijing Key Lab. on MCAACI, Beijing Inst. of Technol., Beijing, ChinaDept. of Mech. & Mechatron. Eng., Univ. of Waterloo, Waterloo, ON, CanadaState Key Lab. of Mech. Transmissions, Chongqing Univ., Chongqing, China				2021-04-18	INSPEC:20461502		
J	Ma, Haitong; Liu, Changliu; Li, Shengbo Eben; Zheng, Sifa; Sun, Wenchao; Chen, Jianyu				Li, Shengbo/J-7662-2013	Li, Shengbo/0000-0003-4923-3633; Zheng, Sifa/0000-0001-5160-1365					Learn Zero-Constraint-Violation Safe Policy in Model-Free Constrained Reinforcement Learning.								IEEE transactions on neural networks and learning systems				PP									10.1109/TNNLS.2023.3348422							Journal Article	2024-Jan-17	2024	We focus on learning the zero-constraint-violation safe policy in model-free reinforcement learning (RL). Existing model-free RL studies mostly use the posterior penalty to penalize dangerous actions, which means they must experience the danger to learn from the danger. Therefore, they cannot learn a zero-violation safe policy even after convergence. To handle this problem, we leverage the safety-oriented energy functions to learn zero-constraint-violation safe policies and propose the safe set actor-critic (SSAC) algorithm. The energy function is designed to increase rapidly for potentially dangerous actions, locating the safe set on the action space. Therefore, we can identify the dangerous actions prior to taking them and achieve zero-constraint violation. Our major contributions are twofold. First, we use the data-driven methods to learn the energy function, which releases the requirement of known dynamics. Second, we formulate a constrained RL problem to solve the zero-violation policies. We prove that our Lagrangian-based constrained RL solutions converge to the constrained optimal zero-violation policies theoretically. The proposed algorithm is evaluated on the complex simulation environments and a hardware-in-loop (HIL) experiment with a real autonomous vehicle controller. Experimental results suggest that the converged policies in all environments achieve zero-constraint violation and comparable performance with model-based baseline.									0	0	0	0	0	0	0				2162-2388														2024-01-19	MEDLINE:38231811	38231811	
C	Shoaraee, Hamid; Chen, Liang; Jiang, Fan			IEEE Computer Society							Decision-Making of an Autonomous Vehicle when Approached by an Emergency Vehicle using Deep Reinforcement Learning								2021 IEEE INTL CONF ON DEPENDABLE, AUTONOMIC AND SECURE COMPUTING, INTL CONF ON PERVASIVE INTELLIGENCE AND COMPUTING, INTL CONF ON CLOUD AND BIG DATA COMPUTING, INTL CONF ON CYBER SCIENCE AND TECHNOLOGY CONGRESS DASC/PICOM/CBDCOM/CYBERSCITECH 2021								185	191				10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00041							Proceedings Paper	2021	2021	Autonomous Vehicles (AVs) are the future of road transportation which can increase safety, efficiency, and productivity. Decision-making of AVs in a highway environment with different goals like overtaking, staying in a lane, and merging have been the focus of many studies. In this study, we want to address a new edge case in autonomous driving when the AV (ego) needs to make the best lateral and longitudinal decisions when approached by an emergency vehicle (emg). To achieve the desired behavior and learn the sequence decision process, we trained ego with the help of Deep Reinforcement Learning (DRL) algorithms and compared the results with rule-based algorithms. We proposed two neural networks as function approximators that help the ego to learn the optimum actions. The driving environment for this problem was developed by using Simulation Urban Mobility (SUMO) as an open-source traffic simulator. We will show our proposed solution based on the DRL outperforming the rule-based solution and demonstrate that it has a decent performance both in normal driving situations and when an emergency vehicle is approaching.					6th IEEE Cyber Science and Technology Congress (CyberSciTech)6th IEEE Cyber Science and Technology Congress (CyberSciTech)	OCT 25-28, 2021OCT 25-28, 2021		ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1					978-1-6654-2174-4									Univ Northern British Columbia, Dept Comp Sci, Prince George, BC, Canada				2021-01-01	WOS:000942753000015		
C	Liessner, Roman; Dohmen, Jan; Wiering, Marco					Wiering, Marco/0000-0003-4331-7537	Rocha, AP; Steels, L; VandenHerik, J				Explainable Reinforcement Learning for Longitudinal Control								ICAART: PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE - VOL 2								874	881				10.5220/0010256208740881							Proceedings Paper	2021	2021	Deep Reinforcement Learning (DRL) has the potential to surpass the existing state of the art in various practical applications. However, as long as learned strategies and performed decisions are difficult to interpret, DRL will not find its way into safety-relevant fields of application. SHAP values are an approach to overcome this problem. It is expected that the addition of these values to DRL provides an improved understanding of the learned action-selection policy. In this paper, the application of a SHAP method for DRL is demonstrated by means of the OpenAI Gym LongiControl Environment. In this problem, the agent drives an autonomous vehicle under consideration of speed limits in a single lane route. The controls learned with a DDPG algorithm are interpreted by a novel approach combining learned actions and SHAP values. The proposed RL-SHAP representation makes it possible to observe in every time step which features have a positive or negative effect on the selected action and which influences are negligible. The results show that RL-SHAP values are a suitable approach to interpret the decisions of the agent.					13th International Conference on Agents and Artificial Intelligence (ICAART)13th International Conference on Agents and Artificial Intelligence (ICAART)	FEB 04-06, 2021FEB 04-06, 2021		ELECTR NETWORKELECTR NETWORK	6	0	0	0	0	0	6					978-989-758-484-8									Tech Univ Dresden, Dresden Inst Automobile Engn, George Bahr Str 1, Dresden, GermanyUniv Groningen, Bernoulli Inst Math Comp Sci & Artificial Intelli, NL-9747 AG Groningen, Netherlands				2021-07-03	WOS:000661455800097		
J	; ; ; 							Yang Shun; Wu Jian; Zhang Sumin; Han Wei			Autonomous driving in the uncertain traffica deep reinforcement learning approach							中国邮电高校学报	The Journal of China Universities of Posts and Telecommunications				25	6			21	30,96	1005-8885(2018)25:6<21:ADITUT>2.0.TX;2-I										Article	2018	2018	Driving in the complex traffic safely and efficiently is a difficult task for autonomous vehicle because of the stochastic characteristics of engaged human drivers. Deep reinforcement learning (DRL),which combines the abstract representation capability of deep learning (DL) and the optimal decision making and control capability of reinforcement learning (RL),is a good approach to address this problem. Traffic environment is built up by combining intelligent driver model (IDM) and lane-change model as behavioral model for vehicles. To increase the stochastic of the established traffic environment,tricks such as defining a speed distribution with cutoff for traffic cars and using various politeness factors to represent distinguished lane-change style,are taken. For training an artificial agent to achieve successful strategies that lead to the greatest long-term rewards and sophisticated maneuver,deep deterministic policy gradient (DDPG) algorithm is deployed for learning. Reward function is designed to get a trade-off between the vehicle speed,stability and driving safety. Results show that the proposed approach can achieve good autonomous maneuvering in a scenario of complex traffic behavior through interaction with the environment.									1	0	0	0	0	0	1			1005-8885											Jilin University, State Key Laboratory of Automotive Simulation and Control, Changchun, Jilin 130022, ChinaInstitute of Microelectronics,Chinese Academy of Sciences, Beijing 100029, China130022100029	Jilin UniversityInstitute of Microelectronics,Chinese Academy of Sciences			2018-01-01	CSCD:6453187		
J	Tung, Tze-Yang; Kobus, Szymon; Roig, Joan Pujol; Gunduz, Deniz				Gunduz, Deniz/ABE-1855-2021	Gunduz, Deniz/0000-0002-7725-395X; Tung, Tze-Yang/0000-0003-2716-5211					Effective Communications: A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning Over Noisy Channels								IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS				39	8			2590	2603				10.1109/JSAC.2021.3087248							Article	AUG 2021	2021	We propose a novel formulation of the "effectiveness problem" in communications, put forth by Shannon and Weaver in their seminal work "The Mathematical Theory of Communication", by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework. Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment, can also communicate with each other over a noisy communication channel. The noisy communication channel is considered explicitly as part of the dynamics of the environment, and the message each agent sends is part of the action that the agent can take. As a result, the agents learn not only to collaborate with each other but also to communicate "effectively" over a noisy channel. This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the "learning to communicate" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free. We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP. This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems.									15	0	0	0	0	0	16			0733-8716	1558-0008										Imperial Coll London, Dept Elect & Elect Engn, Informat Proc & Commun Lab IPC Lab, London SW7 2AZ, EnglandSamsung Elect Res & Dev Inst UK, Staines Upon Thames TW18 4QE, England	Samsung Elect Res & Dev Inst UK			2021-07-29	WOS:000673624000024		
J	Barbu, Clara; Mocanu, Stefan Alexandru				Mocanu, Stefan/M-3370-2013						ON THE DEVELOPMENT OF AUTONOMOUS AGENTS USING DEEP REINFORCEMENT LEARNING								UNIVERSITY POLITEHNICA OF BUCHAREST SCIENTIFIC BULLETIN SERIES C-ELECTRICAL ENGINEERING AND COMPUTER SCIENCE				83	3			97	116											Article	2021	2021	This paper presents a study on the general concept of autonomous agents, with an accent on the development of such agents using deep reinforcement learning. This is combined with the domain of autonomous vehicles, as illustrated by a practical application: having a vehicle agent learn how to navigate and park by itself on a designated spot, in a virtual parking lot environment created in Unity. The reinforcement learning method Deep Q-Learning is implemented, with the addition of a few improvements such as Double Deep Q-Learning and Experience Replay.									0	0	0	0	0	0	0			2286-3540	2286-3559										Univ Politehn Bucuresti, Fac Automat Control & Comp Sci, Bucharest, RomaniaUniv Politehn Bucharesti, Fac Automat Control & Comp Sci, Bucharest, Romania				2021-09-18	WOS:000692196300008		
B	Jardine, P. Travis										A reinforcement learning approach to predictive control design: Autonomous vehicle applications																												Dissertation/Thesis	Jan 01 2018	2018										0	0	0	0	0	0	0					979-8-209-59018-7									Queen's University (Canada), Ontario, Canada	Queen's University (Canada)				PQDT:58900058		
J	Deng, Huifan; Zhao, Youqun; Wang, Qiuwei; Nguyen, Anh-Tu				Nguyen, Anh-Tu/J-9193-2017	Nguyen, Anh-Tu/0000-0002-9636-3927					Deep Reinforcement Learning Based Decision-Making Strategy of Autonomous Vehicle in Highway Uncertain Driving Environments								AUTOMOTIVE INNOVATION				6	3			438	452				10.1007/s42154-023-00231-6					AUG 2023		Article; Early Access		2023	Uncertain environment on multi-lane highway, e.g., the stochastic lane-change maneuver of surrounding vehicles, is a big challenge for achieving safe automated highway driving. To improve the driving safety, a heuristic reinforcement learning decision-making framework with integrated risk assessment is proposed. First, the framework includes a long short-term memory model to predict the trajectory of surrounding vehicles and a future integrated risk assessment model to estimate the possible driving risk. Second, a heuristic decaying state entropy deep reinforcement learning algorithm is introduced to address the exploration and exploitation dilemma of reinforcement learning. Finally, the framework also includes a rule-based vehicle decision model for interaction decision problems with surrounding vehicles. The proposed framework is validated in both low-density and high-density traffic scenarios. The results show that the traffic efficiency and vehicle safety are both improved compared to the common dueling double deep Q-Network method and rule-based method.									0	0	0	0	0	0	0			2096-4250	2522-8765										Nanjing Univ Aeronaut & Astronaut, Dept Vehicle Engn, Nanjing 210016, Peoples R ChinaUniv Polytech Hauts de France, Lab LAMIH, UMR 8201, CNRS, F-59300 Valenciennes, Hauts De France, FranceINSA Hauts de France, F-59300 Valenciennes, France				2023-09-21	WOS:001060053800001		
J	Chen, Baiming; Chen, Xiang; Wu, Qiong; Li, Liang				Li, Li/IAQ-0885-2023; Li, Donglin/JEF-4284-2023; li, li/GPX-3938-2022; li, li/HII-4157-2022; Chen, Xiang/HZL-7226-2023; Li, Li/AEM-3636-2022; LI, LI/GVS-5344-2022	Chen, Baiming/0000-0002-3782-0251; Li, Yuxing/0000-0002-3790-9088					Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	8			10333	10342				10.1109/TITS.2021.3091477					JUN 2021		Article; Early Access		2022	Autonomous vehicles must be comprehensively evaluated before deployed in cities and highways. However, most existing evaluation approaches for autonomous vehicles are static and lack adaptability, so they are usually inefficient in generating challenging scenarios for tested vehicles. In this paper, we propose an adaptive evaluation framework to efficiently evaluate autonomous vehicles in adversarial environments generated by deep reinforcement learning. Considering the multimodal nature of dangerous scenarios, we use ensemble models to represent different local optimums for diversity. We then utilize a nonparametric Bayesian method to cluster the adversarial policies. The proposed method is validated in a typical lane-change scenario that involves frequent interactions between the ego vehicle and the surrounding vehicles. Results show that the adversarial scenarios generated by our method significantly degrade the performance of the tested vehicles. We also illustrate different patterns of generated adversarial environments, which can be used to infer the weaknesses of the tested vehicles.									16	1	0	0	0	0	17			1524-9050	1558-0016										Tsinghua Univ, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R ChinaAnhui Jianghuai Automobile Co Ltd, Ctr Technol, Hefei 230601, Peoples R China	Anhui Jianghuai Automobile Co Ltd			2021-06-29	WOS:000732424900001		
J	Teruhiko, U.; Noriaki, S.										Distributed scheduling for autonomous vehicles by reinforcement learning								Transactions of the Institute of Electrical Engineers of Japan, Part C				117-C	10			1513	20											Journal Paper	Oct. 1997	1997	We propose an autonomous vehicle scheduling schema in large physical distribution terminals publicly used as the next generation wide area physical distribution bases. This schema uses learning automata for vehicle scheduling based on the Contract Net Protocol, in order to obtain useful emergent behaviors of agents in the system based on the local decision-making of each agent. The state of the automaton is updated at each instant on the basis of new information that includes the arrival estimation time of vehicles. Each agent estimates the arrival time of vehicles by using a Bayesian learning process. Using traffic simulation, we evaluate the schema in various simulated environments. The result shows the advantage of the schema over when each agent provides the same criteria from the top down, and each agent voluntarily generates criteria via interactions with the environment, playing an individual role in the system.									0	0	0	0	0	0	0			0385-4221											OKI Electr. Ind. Co. Ltd., Tokyo, Japan				1997-10-01	INSPEC:5793231		
C	Yen, Yi-Tung; Chou, Jyun-Jhe; Shih, Chi-Sheng; Chen, Chih-Wei; Tsung, Pei-Kuei			IEEE							Proactive Car-Following Using Deep-Reinforcement Learning								2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC											10.1109/itsc45102.2020.9294194							Proceedings Paper	2020	2020	Car-following is a fundamental operation for vehicle control for both ADAS on modern vehicles and vehilce control on autonomous vehicles. Most existing car following mechanisms react to the observations of nearby vehicles in real-time. Unfortunately, lack of capability of taking into account multiple constraints and objectives, these mechanisms lead to poor efficiency, discomfort, and unsafe operations. In this paper, we design and implement a proactive car-following model to take into account safety regulation, efficiency, and comfort using deep reinforcement learning. The evaluation results show that the proactive model not only reduces the number of inefficient and unsafe headway but also eliminates the traffic jerk, compared to human drivers. The model outperformed 79% human drivers in public data set and the road efficiency is only 2% less than the optimal bound. Compared to ACC model, the DDPG model allows 4.1% more vehicles to finish the simulation than ACC model does, and increases the average speed for 28.4%.					23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)23rd IEEE International Conference on Intelligent Transportation Systems (ITSC)	SEP 20-23, 2020SEP 20-23, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	12	0	0	0	0	0	13			2153-0009		978-1-7281-4149-7									Natl Taiwan Univ, Grad Inst Networking & Multimedia, Dept Comp Sci & Informat Engn, Embedded Syst & Wireless Networking Lab, Taipei, TaiwanMediaTek Inc, Hsinchu, Taiwan				2020-01-01	WOS:000682770700024		
J	Zhu, Zixuan; Teng, Chenglong; Cai, Yingfeng; Chen, Long; Lian, Yubo; Wang, Hai				Teng, Chenglong/IAM-8650-2023; long, chen/JVM-8568-2024; Yan, Jing/JFA-6705-2023; lu, yuan/JZD-0832-2024; long, chen/N-8723-2013	Wang, Hai/0000-0002-9136-8091					Vehicle Safety Planning Control Method Based on Variable Gauss Safety Field								WORLD ELECTRIC VEHICLE JOURNAL				13	11					203			10.3390/wevj13110203							Article	NOV 2022	2022	The existing intelligent vehicle trajectory-planning methods have limitations in terms of efficiency and safety. To overcome these limitations, this paper proposes an automatic driving trajectory-planning method based on a variable Gaussian safety field. Firstly, the time series bird's-eye view is used as the input state quantity of the network, which improves the effectiveness of the trajectory planning policy network in extracting the features of the surrounding traffic environment. Then, the policy gradient algorithm is used to generate the planned trajectory of the autonomous vehicle, which improves the planning efficiency. The variable Gaussian safety field is used as the reward function of the trajectory planning part and the evaluation index of the control part, which improves the safety of the reinforcement learning vehicle tracking algorithm. The proposed algorithm is verified using the simulator. The obtained results show that the proposed algorithm has excellent trajectory planning ability in the highway scene and can achieve high safety and high precision tracking control.									1	0	0	0	0	0	1			2032-6653											Jiangsu Univ, Automot Engn Res Inst, Zhenjiang 212013, Jiangsu, Peoples R ChinaBYD Auto Ind Co Ltd, Shenzhen 518118, Peoples R ChinaJiangsu Univ, Sch Automot & Traff Engn, Zhenjiang 212013, Jiangsu, Peoples R China				2022-12-10	WOS:000888559700001		
J	Prathiba, Sahaya Beni; Raja, Gunasekaran; Anbalagan, Sudha; Gurumoorthy, Sugeerthi; Kumar, Neeraj; Guizani, Mohsen				Guizani, Mohsen/AAX-4534-2021; Raja, Gunasekaran/AAD-8076-2022; Kumar, Neeraj/L-3500-2016; Anbalagan, Sudha/AAH-9214-2020	Guizani, Mohsen/0000-0002-8972-8094; Raja, Gunasekaran/0000-0002-2253-7648; Kumar, Neeraj/0000-0002-3020-3947; Anbalagan, Sudha/0000-0002-7886-2327; , Sahaya Beni Prathiba/0000-0002-1299-0465					Cybertwin-Driven Federated Learning Based Personalized Service Provision for 6G-V2X								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				71	5			4632	4641				10.1109/TVT.2021.3133291							Article	MAY 2022	2022	The rapid growth of Autonomous Vehicle (AV) technology and the integration of edge computing grasp new challenges along with the ever-increasing mobile internet traffic and services. Tackling such challenges through customized edge computing services is the critical research in 6G Vehicle-to-Everything (6G-V2X) communication. V2X contributes detailed information about the current navigation of vehicles, automatic payments for toll roads, parking fees and other services. With the countless, unique, and personalized service requirements of AVs over computation-intensive applications, exploring the edge resources for the excellent Quality of Service (QoS) provision is the greatest concern. This paper proposes a Federated Learning and edge Cache-assisted Cybertwin (FLCC) framework for personalized service provision in 6G-V2X. Integration of cybertwin in 6G enables the connectivity of the physical system to the digital realm, allowing for adequate instantaneous wireless access. The FLCC jointly considers the edge cooperation and optimizations through the proposed Federated Multi-agent Deep Reinforcement Learning based (FM-DRL) algorithm. The FM-DRL algorithm balances the FLCC's learning accuracy. It minimizes the time and cost by taking the factors such as cybertwin association, training data batch size, and bandwidth. Finally, caching is performed using the Federated Reinforcement Learning-based Edge Caching (FREC) algorithm to obtain the desired datasets required that train the model for providing personalized 6G-V2X services for the AVs. Numerical studies and simulation results reveal that the proposed system outperforms the baseline learning approaches by 17.6%.									12	0	0	0	0	0	12			0018-9545	1939-9359										Anna Univ, Dept Comp Technol, NGNLab, Chennai 600025, Tamil Nadu, IndiaVellore Inst Technol, Sch Comp Sci & Engn, Chennai 632014, Tamil Nadu, IndiaThapar Inst Engn & Technol, Patiala 147004, Punjab, IndiaAsia Univ, Dept Comp Sci & Informat Engn, Taichung 40704, TaiwanUniv Petr & Energy Studies, Sch Comp Sci, Dehra Dun 248007, Uttarakhand, IndiaMohamed Bin Zayed Univ Artificial Intelligence, Machine Learning Dept, Abu Dhabi, U Arab Emirates				2022-06-10	WOS:000799654900013		
J	He, Xiangkun; Lv, Chen				Lv, Chen/N-7055-2018; He, Xiangkun/ABA-5268-2021	Lv, Chen/0000-0001-6897-4512; He, Xiangkun/0000-0001-9818-0879					Towards Safe Autonomous Driving: Decision Making with Observation-Robust Reinforcement Learning								AUTOMOTIVE INNOVATION				6	4			509	520				10.1007/s42154-023-00256-x					NOV 2023		Article; Early Access		2023	Most real-world situations involve unavoidable measurement noises or perception errors which result in unsafe decision making or even casualty in autonomous driving. To address these issues and further improve safety, automated driving is required to be capable of handling perception uncertainties. Here, this paper presents an observation-robust reinforcement learning against observational uncertainties to realize safe decision making for autonomous vehicles. Specifically, an adversarial agent is trained online to generate optimal adversarial attacks on observations, which attempts to amplify the average variation distance on perturbed policies. In addition, an observation-robust actor-critic approach is developed to enable the agent to learn the optimal policies and ensure that the changes of the policies perturbed by optimal adversarial attacks remain within a certain bound. Lastly, the safe decision making scheme is evaluated on a lane change task under complex highway traffic scenarios. The results show that the developed approach can ensure autonomous driving performance, as well as the policy robustness against adversarial attacks on observations.									0	0	0	0	0	0	0			2096-4250	2522-8765										Jilin Univ, State Key Lab Automot Simulat & Control, Changchun, Peoples R ChinaNanyang Technol Univ, Sch Mech & Aerosp Engn, Nanyang 639798, Singapore				2023-11-20	WOS:001097193700001		
J	Zhai, Yuanzhao; Ding, Bo; Liu, Xuan; Jia, Hongda; Zhao, Yong; Luo, Jie					Zhai, Yuanzhao/0000-0003-1385-0074					Decentralized Multi-Robot Collision Avoidance in Complex Scenarios With Selective Communication								IEEE ROBOTICS AND AUTOMATION LETTERS				6	4			8379	8386				10.1109/LRA.2021.3102636							Article	OCT 2021	2021	Deep reinforcement learning has been demonstrated to be an effective solution to the multi-robot collision avoidance problem. However, with existing methods, robots typically generate actions only based on local observations, sometimes augmented with global communication. Their performance deteriorates in limited bandwidth environments and complex scenarios with various obstacles and high robot density. We propose SelComm, a selective communication framework to generate cooperative and collision-free actions for robots in multi-robot navigation tasks. Specifically, we develop a decentralized message selector, enabling each robot to calculate relations with other robots using both agent-level information and sensor-level information, and select the most valuable messages to meet the bandwidth limitation. Then we introduce the attentional communication channel for efficient communication. Our experimental evaluations based on various scenarios demonstrate that SelComm learns more cooperative behaviors and outperforms state-of-the-art methods in limited bandwidth environments and complex scenarios.									10	2	0	0	0	0	12			2377-3766											Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R ChinaHunan Univ, Coll Comp Sci & Elect Engn, Changsha 410000, Peoples R China				2021-09-19	WOS:000694697200005		
J	Wang, Zhe; Huang, Helai; Tang, Jinjun; Meng, Xianwei; Hu, Lipeng										Velocity control in car-following behavior with autonomous vehicles using reinforcement learning								ACCIDENT ANALYSIS AND PREVENTION				174						106729			10.1016/j.aap.2022.106729					JUN 2022		Article; Early Access		2022	Car-following behavior is a common driving behavior. It is necessary to consider the following vehicle in the car following model of autonomous vehicle (AV) under the background of the vehicle-to-vehicle transportation system. In this study, a safe velocity control method for AV based on reinforcement learning with considering the following vehicle is proposed. First, the mixed driving environment of AVs and human-driven vehicles is constructed, and the trajectories of the leading and following vehicles are extracted from the naturalistic High D driving dataset. Next, the soft actor-critic (SAC) algorithm is used as the velocity control algorithm, in which the agent is AV, the action is acceleration, and the state is the relative distance and relative speed between the AV and the leading and following vehicles. Then, a reward function based on state and corresponding action is designed to guide AV to choose acceleration without collision between the leading and following vehicles. Furthermore, AVs are gradually able to learn to avoid collisions between the leading and following vehicles after training the model. The test result of the trained model shows that the SAC agent can achieve complete collision avoidance, resulting in zero collision. Finally, the driving performance of the SAC agent and that of human driving are compared and analyzed for safety and efficiency. The results of this study are expected to improve the safety of the car-following process..									8	1	0	0	1	0	9			0001-4575	1879-2057										Cent South Univ, Sch Traff & Transportat Engn, Smart Transport Key Lab Hunan Prov, Changsha 410075, Peoples R China				2022-10-16	WOS:000864686600007	35700685	
C	Garzon, Mario; Spalanzani, Anne			IEEE	Oviedo, Mario Andrei Garzon/ABI-6479-2020	Oviedo, Mario Andrei Garzon/0000-0001-6672-4827					Game theoretic decision making based on real sensor data for autonomous vehicles' maneuvers in high traffic								2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						5378	5384											Proceedings Paper	2020	2020	This paper presents an approach for implementing game theoretic decision making in combination with realistic sensory data input so as to allow an autonomous vehicle to perform maneuvers, such as lane change or merge in high traffic scenarios. The main novelty of this work, is the use of realistic sensory data input to obtain the observations as input of an iterative multi-player game in a realistic simulator. The game model allows to anticipate reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication. Moreover, direct information from the simulator, such as position or speed of the vehicles is also avoided.The solution of the game is based on cognitive hierarchy reasoning and it uses Monte Carlo reinforcement learning in order to obtain a near-optimal policy towards a specific goal. Moreover, the game proposed is capable of solving different situations using a single policy. The system has been successfully tested and compared with previous techniques using a realistic hybrid simulator, where the ego-vehicle and its sensors are simulated on a 3D simulator and the additional vehicles' behavior is obtained from a traffic simulator.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 31-JUN 15, 2020MAY 31-JUN 15, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	3	0	0	0	0	0	3			1050-4729	2577-087X	978-1-7281-7395-5									Univ Grenoble Alpes, Grenoble INP, INRIA, F-38000 Grenoble, France				2021-12-10	WOS:000712319503105		
C	Wang, Zhitao; Zhuang, Yuzheng; Gu, Qiang; Chen, Dong; Zhang, Hongbo; Liu, Wulong			IEEE							Reinforcement Learning based Negotiation-aware Motion Planning of Autonomous Vehicles								2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						4532	4537				10.1109/IROS51168.2021.9635935							Proceedings Paper	2021	2021	For autonomous vehicles integrating onto road-ways with human traffic participants, it requires understanding and adapting to the participants' intention by responding in predictable ways. This paper proposes a reinforcement learning based negotiation-aware motion planning framework, which adopts RL to adjust the driving style of the planner by dynamically modifying the prediction horizon length of the motion planner in real time adaptively. The framework models the interaction between the autonomous vehicle and other traffic participants as a Markov Decision Process. A temporal sequence of occupancy grid maps are taken as inputs for RL module to embed an implicit intention reasoning. Curriculum learning is employed to enhance the training efficiency and the robustness of the algorithm. We applied our method to narrow lane navigation in both simulation and real world to demonstrate that the proposed method outperforms the common alternative due to its advantage in alleviating the social dilemma problem with proper negotiation skills.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	SEP 27-OCT 01, 2021SEP 27-OCT 01, 2021	IEEE; RSJIEEE; RSJ	ELECTR NETWORKELECTR NETWORK	4	1	0	0	0	0	6			2153-0858		978-1-6654-1714-3									Huawei Technol, Noahs Ark Lab, Beijing, Peoples R China				2022-03-16	WOS:000755125503084		
C	Zheng, Rui; Liu, Chunming; Guo, Qi			IEEE							A DECISION-MAKING METHOD FOR AUTONOMOUS VEHICLES BASED ON SIMULATION AND REINFORCEMENT LEARNING								PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS (ICMLC), VOLS 1-4		International Conference on Machine Learning and Cybernetics						362	369											Proceedings Paper	2013	2013	There are still some problems need to be solved though there are a lot of achievements in the field of automatic driving. One of those problems is the difficulty of designing a decision-making system for complex traffic conditions. In recent years, reinforcement learning (RL) shows the potential in solving sequential decision optimization problems, which can be modeled as Markov decision processes (MDPs). In this paper, we establish a 14-DOF dynamic model of an autonomous vehicle and use RL to build a decision-making system for autonomous driving based on simulation. The decision-making process of the vehicle is modeled as an MDP, and the performance of the MDP is improved using an approximate RL. At last, we show the efficiency of the proposed method by simulation in a highway environment.					International Conference on Machine Learning and Cybernetics (ICMLC)International Conference on Machine Learning and Cybernetics (ICMLC)	JUL 14-17, 2013JUL 14-17, 2013	IEEE; Hebei Univ; IEEE Syst, Man & Cybernet Soc; S China Univ Technol; Univ Macau; Chongqing Univ; Shenzhen Grad Sch, Harbin Inst Technol; Univ Ulster; City Univ Hong Kong; Univ Cagliari Dept Elect & Elect Engn; Hebei Univ Sci & TechnolIEEE; Hebei Univ; IEEE Syst, Man & Cybernet Soc; S China Univ Technol; Univ Macau; Chongqing Univ; Shenzhen Grad Sch, Harbin Inst Technol; Univ Ulster; City Univ Hong Kong; Univ Cagliari Dept Elect & Elect Engn; Hebei Univ Sci & Technol	Tianjin, PEOPLES R CHINATianjin, PEOPLES R CHINA	18	2	0	0	0	0	20			2160-133X		978-1-4799-0260-6									Natl Univ Def Technol, Coll Mechatron & Automat, Changsha 410073, Hunan, Peoples R China				2013-01-01	WOS:000366853800052		
J	Papini, Gastone Pietro Rosati; Plebe, Alice; Da Lio, Mauro; Dona, Riccardo					Dona, Riccardo/0000-0003-4066-2124; Plebe, Alice/0000-0001-8567-0553; Rosati Papini, Gastone Pietro/0000-0003-1075-9603					A Reinforcement Learning Approach for Enacting Cautious Behaviours in Autonomous Driving System: Safe Speed Choice in the Interaction With Distracted Pedestrians								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	7			8805	8822				10.1109/TITS.2021.3086397					JUN 2021		Article; Early Access		2022	Driving requires the ability to handle unpredictable situations. Since it is not always possible to predict an impending danger, a good driver should preventively assess whether a situation has risks and adopt a safe behavior. Considering, in particular, the possibility of a pedestrian suddenly crossing the road, a prudent driver should limit the traveling speed. We present a work exploiting reinforcement learning to learn a function that specifies the safe speed limit for a given artificial driver agent. The safe speed function acts as a behavioral directive for the agent, thus extending its cognitive abilities. We consider scenarios where the vehicle interacts with a distracted pedestrian that might cross the road in hard-to-predict ways and propose a neural network mapping the pedestrian's context onto the appropriate traveling speed so that the autonomous vehicle can successfully perform emergency braking maneuvers. We discuss the advantages of developing a specialized neural network extension on top of an already functioning autonomous driving system, removing the burden of learning to drive from scratch while focusing on learning safe behavior at a highlevel. We demonstrate how the safe speed function can be learned in simulation and then transferred into a real vehicle. We include a statistical analysis of the network's improvements compared to the original autonomous driving system. The code implementing the presented network is available at https://githuh.com/tonegas/safe-speed-neural-network with MIT license and at https://zenodo.org/communities/dreams4cars.									13	1	0	0	3	0	14			1524-9050	1558-0016										Univ Trento, Dept Ind Engn, I-38123 Trento, Italy				2021-12-25	WOS:000732076100001		
C	Zhang, Yi; Sun, Ping; Yin, Yuhan; Lin, Lin; Wang, Xuesong			IEEE							Human-like Autonomous Vehicle Speed Control by Deep Reinforcement Learning with Double Q-Learning								2018 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						1251	1256											Proceedings Paper	2018	2018	Autonomous driving has become a popular research project. How to control vehicle speed is a core problem in autonomous driving. Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to control the vehicle speed. However, the popular Q-learning algorithm is unstable in some games in the Atari 2600 domain. In this paper, a reinforcement learning approach called Double Q-learning is used to control a vehicle's speed based on the environment constructed by naturalistic driving data. Depending on the concept of the direct perception approach, we propose a new method called integrated perception approach to construct the environment. The input of the model is made up of high dimensional data including road information processed from the video data and the low dimensional data processed from the sensors. During experiment, compared with deep Q-learning algorithm, double deep Q-learning has improvements both in terms of value accuracy and policy quality. Our model's score is 271.73% times that of deep Q-learning.					IEEE Intelligent Vehicles Symposium (IV)IEEE Intelligent Vehicles Symposium (IV)	JUN 26-30, 2018JUN 26-30, 2018	IEEEIEEE	Changshu, PEOPLES R CHINAChangshu, PEOPLES R CHINA	72	6	0	0	1	0	84			1931-0587		978-1-5386-4452-2									Tongji Univ, Coll Software Engn, Shanghai, Peoples R ChinaTongji Univ, Minist Educ, Natl Engn Lab Integrated Optimizat Rd Traff & Saf, Key Lab Rd & Traff Engn, Shanghai, Peoples R China				2018-01-01	WOS:000719424500197		
J	Huang, Chuan; Hu, Ping; Lian, Jing				Huang, Chuan/H-6983-2019	Huang, Chuan/0000-0001-6052-0663					Online optimum velocity calculation under V2X for smart new energy vehicles								TRANSACTIONS OF THE INSTITUTE OF MEASUREMENT AND CONTROL				43	10			2368	2377				10.1177/0142331221997280							Article	JUN 2021	2021	In this paper, a vector data net solver is proposed, which can reduce bivariate discrete-time dynamic programming (DP) computation time by 98.0% without losing accuracy. Therefore, for the first time, bivariate discrete-time DP can operate under model predictive control rolling optimization to calculate future optimum vehicle velocity in real time considering future road altitude and instant traffic information. Simulation results indicate that with the solution presented in this paper, front vehicles and the proper windows to pass through front intersections can be constantly considered. Meanwhile, the calculated optimum vehicle velocity almost remains the same as the global optimum solutions. Simulation results are validated by real-car tests, and the test new energy vehicle (NEV) electricity consumption is reduced by up to 48.6%. A comparison experiment is performed between the solution presented in this paper and commonly used adaptive dynamic programming (ADP), and the results indicate that the former has better performance and stability. This paper describes a novel solution for online optimum velocity calculation under vehicle to everything (V2X) environment and can be used by all smart NEVs with autonomous driving or active cruise control functions for lower electricity consumption and better riding comfort.									1	1	0	0	0	0	2			0142-3312	1477-0369										Dalian Univ Technol, Fac Vehicle Engn & Mech, Sch Automot Engn, State Key Lab Struct Anal Ind Equipment, 2 Linggong Rd, Dalian 116024, Peoples R ChinaBMW Brilliance Automot, BBT Powertrain E52, R&D, Shenyang, Peoples R China				2021-05-28	WOS:000648995800019		
J	Hartmann, Gabriel; Shiller, Zvi; Azaria, Amos				Hartmann, Gabriel/GYQ-8642-2022	Hartmann, Gabriel/0000-0001-5386-3799; Shiller, Zvi/0000-0003-1303-0367					Model-Based Reinforcement Learning for Time-Optimal Velocity Control								IEEE ROBOTICS AND AUTOMATION LETTERS				5	4			6185	6192				10.1109/LRA.2020.3012128							Article	OCT 2020	2020	Autonomous navigation has recently gained great interest in the field of reinforcement learning. However, little attention was given to the time-optimal velocity control problem, i.e. controlling a vehicle such that it travels at the maximal speed without becoming dynamically unstable (roll-over or sliding). Time optimal velocity control can be solved numerically using existing methods that are based on optimal control and vehicle dynamics. In this letter, we develop a model-based deep reinforcement learning to generate the time-optimal velocity control. Moreover, we introduce a method that uses a numerical solution that predicts whether the vehicle may become unstable and intervenes if needed. We show that our combined model outperforms several baselines as it achieves higher velocities (with only one minute of training) and does not encounter any failures during the training process.									7	0	0	0	0	0	7			2377-3766											Ariel Univ, Dept Mech Engn & Mechatron, IL-40700 Ariel, IsraelAriel Univ, Dept Comp Sci, IL-40700 Ariel, Israel				2020-08-20	WOS:000556760000004		
J	Alagumuthukrishnan, S.; Deepajothi, S.; Vani, R.; Velliangiri, S.										Reliable and Efficient Lane Changing Behaviour for Connected Autonomous Vehicle through Deep Reinforcement Learning								Procedia Computer Science								1112	21				10.1016/j.procs.2023.01.090							Journal Paper	2023	2023	The establishment of future intelligent transport systems is dependable on the reliable and seamless function of Connected and Autonomous Vehicles (CAV). Reinforcement learning (RL), which allows autonomous vehicles (AVs) to learn an ideal driving strategy through constant contact with the environment, plays a significant part in the decision-making process of autonomous driving (AD). The networking of CAV is advantageous since it allows for the transmission of traffic-related data to vehicles via Vehicle-to-External (V2X) communication. Recognition and anticipation of driving behaviour are critical for avoiding collisions because they can provide useful information to other drivers and vehicles. The fundamental challenge in developing CAV is the construction of an autonomous controller that can effectively perform close real-time control selections, such as a fast acceleration while merging onto a highway and rapid speed adjustments in stop-and-go traffic congestion. CAV driving behaviours can be considerably improved by utilizing shared information, resulting in more accountable, intelligent, and efficient driving. In the present work, a deep reinforcement learning approach is proposed that integrates the information gathered through connectivity capabilities and sensing from neighbour automobiles in the vicinity of CAV. The fused information is used for providing safe and cooperative lane-changing behaviour. The deployment of an algorithm in CAV is expected to improve the transportation safety of CAV driving behaviours. All rights reserved Elsevier.									0	0	0	0	0	0	0			1877-0509											CMR Inst. of Technol., Fraunhofer USA Center for Sustainable Energy Syst., Boston, MA, USADept. of CSE, REVA Univ., Bengaluru, IndiaDept. of CSE, Kongu Eng. Coll., Erode, IndiaDept. of Comput. Intelligence, SRM Inst. of Sci. & Technol., Chennai, India				2023-07-28	INSPEC:23432428		
J	Li, Duowei; Zhu, Feng; Chen, Tianyi; Wong, Yiik Diew; Zhu, Chunli; Wu, Jianping				WONG, Y. D./A-3761-2011; Zhu, Feng/O-4219-2015	WONG, Y. D./0000-0001-7419-5777; Li, Duowei/0000-0002-1940-2435					COOR-PLT: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				146						103933			10.1016/j.trc.2022.103933					NOV 2022		Article; Early Access		2023	Platooning and coordination are two implementation strategies that are frequently proposed for traffic control of connected and autonomous vehicles (CAVs) at signal-free intersections instead of using conventional traffic signals. However, few studies have attempted to integrate both strategies to better facilitate the CAV control at signal-free intersections. To this end, this study proposes a hierarchical control model, named COOR-PLT, to coordinate adaptive CAV platoons at a signal-free intersection based on deep reinforcement learning (DRL). COOR-PLT has a two-layer framework. The first layer uses a centralized control strategy to form adaptive platoons. The optimal size of each platoon is determined by considering multiple objectives (i.e., efficiency, fairness and energy saving). The second layer employs a decentralized control strategy to coordinate multiple platoons passing through the intersection. Each platoon is labeled with coordinated status or independent status, upon which its passing priority is determined. As an efficient DRL algorithm, Deep Q-network (DQN) is adopted to determine platoon sizes and passing priorities respectively in the two layers. The model is validated and examined on the simulator Simulation of Urban Mobility (SUMO). The simulation results demonstrate that the model is able to: (1) achieve satisfactory convergence performances; (2) adaptively determine platoon size in response to varying traffic conditions; and (3) completely avoid deadlocks at the intersection. By comparison with other control methods, the model manifests its superiority of adopting adaptive platooning and DRL-based coordination strategies. Also, the model outperforms several state-of-the-art methods on reducing travel time and fuel consumption in different traffic conditions.									7	0	0	0	1	0	7			0968-090X	1879-2359										Tsinghua Univ, Dept Civil Engn, Beijing, Peoples R ChinaNanyang Technol Univ, Sch Civil & Environm Engn, Singapore, SingaporeBeijing Inst Technol, Sch Informat & Elect, Beijing, Peoples R ChinaBeijing Inst Technol, Adv Res Inst Multidisciplinary Sci, Beijing, Peoples R China				2023-03-10	WOS:000928209700001		
J	Jung, Chanyoung; Shim, David Hyunchul					Jung, chanyoung/0000-0001-8104-2461					Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning								IEEE ROBOTICS AND AUTOMATION LETTERS				6	2			1662	1669				10.1109/LRA.2021.3059628							Article	APR 2021	2021	Autonomous driving in an urban environment with surrounding agents remains challenging. One of the key challenges is to accurately predict the traversability map that probabilistically represents future trajectories considering multiple contexts: inertial, environmental, and social. To address this, various approaches have been proposed; however, they mainly focus on considering the individual context. In addition, most studies utilize expensive prior information (such as HD maps) of the driving environment, which is not a scalable approach. In this study, we extend a deep inverse reinforcement learning-based approach that can predict the traversability map while incorporating multiple contexts for autonomous driving in a dynamic environment. Instead of using expensive prior information of the driving scene, we propose a novel deep neural network to extract contextual cues from sensing data and effectively incorporate them in the output, i.e., the reward map. Based on the reward map, our method predicts the ego-centric traversability map that represents the probability distribution of the plausible and socially acceptable future trajectories. The proposed method is qualitatively and quantitatively evaluated in real-world traffic scenarios with various baselines. The experimental results show that our method improves the prediction accuracy compared to other baseline methods and can predict future trajectories similar to those followed by a human driver.									12	0	0	0	0	0	14			2377-3766											Korea Adv Inst Sci & Technol, Sch Elect Engn, KS015, Daejeon, South Korea				2021-04-08	WOS:000629028400008		
J	Young Joun Ha, P.; Sikai Chen; Jiqian Dong; Runjia Du; Yujie Li; Labi, S.										Leveraging the Capabilities of Connected and Autonomous Vehicles and Multi-Agent Reinforcement Learning to Mitigate Highway Bottleneck Congestion [arXiv]								arXiv								22 pp.	22 pp.											Journal Paper	11 Oct. 2020	2020	Active Traffic Management strategies are often adopted in real-time to address such sudden flow breakdowns. When queuing is imminent, Speed Harmonization (SH), which adjusts speeds in upstream traffic to mitigate traffic showckwaves downstream, can be applied. However, because SH depends on driver awareness and compliance, it may not always be effective in mitigating congestion. The use of multiagent reinforcement learning for collaborative learning, is a promising solution to this challenge. By incorporating this technique in the control algorithms of connected and autonomous vehicle (CAV), it may be possible to train the CAVs to make joint decisions that can mitigate highway bottleneck congestion without human driver compliance to altered speed limits. In this regard, we present an RL-based multi-agent CAV control model to operate in mixed traffic (both CAVs and human-driven vehicles (HDVs)). The results suggest that even at CAV percent share of corridor traffic as low as 10%, CAVs can significantly mitigate bottlenecks in highway traffic. Another objective was to assess the efficacy of the RL-based controller vis-\`a-vis that of the rule-based controller. In addressing this objective, we duly recognize that one of the main challenges of RL-based CAV controllers is the variety and complexity of inputs that exist in the real world, such as the information provided to the CAV by other connected entities and sensed information. These translate as dynamic length inputs which are difficult to process and learn from. For this reason, we propose the use of Graphical Convolution Networks (GCN), a specific RL technique, to preserve information network topology and corresponding dynamic length inputs. We then use this, combined with Deep Deterministic Policy Gradient (DDPG), to carry out multi-agent training for congestion mitigation using the CAV controllers.									0	0	0	0	0	0	0														Lyles Sch. of Civil Eng., Purdue Univ., West Lafayette, IN, USA				2021-01-29	INSPEC:20236515		
C	Hossain, Jumman; Faridee, Abu-Zaher; Roy, Nirmalya; Basak, Anjan; Asher, Derrik E.			IEEE	Hossain, Jumman/HLW-7909-2023	Hossain, Jumman/0009-0009-4461-7604					<i>CoverNav</i>: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning								2023 IEEE INTERNATIONAL CONFERENCE ON AUTONOMIC COMPUTING AND SELF-ORGANIZING SYSTEMS, ACSOS								127	132				10.1109/ACSOS58161.2023.00030							Proceedings Paper	2023	2023	Autonomous navigation in off-road environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an under-explored area. In this paper, we propose CoverNav, a novel Deep Reinforcement Learning (DRL) based algorithm, for identifying covert and navigable trajectories with minimal cost in off-road terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low-cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL-based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state-of-the-art (SOTA) methods without compromising accuracy.					4th IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)4th IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)	SEP 25-29, 2023SEP 25-29, 2023	IEEE; IEEE Comp Soc Tech Comm Software Engn; York Univ, Lassonde Sch Engn; Telus; OntarioTech Univ; Google; HuaweiIEEE; IEEE Comp Soc Tech Comm Software Engn; York Univ, Lassonde Sch Engn; Telus; OntarioTech Univ; Google; Huawei	York Univ, Toronto, CANADAYork Univ, Toronto, CANADA	0	0	0	0	0	0	0					979-8-3503-3744-0									Univ Maryland Baltimore Cty, Dept Informat Syst, Baltimore, MD 21228 USADEVCOM Army Res Lab, Adelphi, MD USA	DEVCOM Army Res Lab			2024-01-12	WOS:001122711700014		
B	Wei-Lun Chen; Kwan-Hung Lee; Pao-Ann Hsiung										Intersection crossing for autonomous vehicles based on deep reinforcement learning								2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)								2 pp.	2 pp.				10.1109/ICCE-TW46550.2019.8991738							Conference Paper	2019	2019	Future intersection crossings for autonomous vehicles will not be controlled by traffic signals, rather a controller will be used for communication among vehicles that need to cross an intersection. In this work, we propose an innovative management system called Deep Reinforcement Learning-based Autonomous Intersection Management (DRLAIM) system, which is the first system to use deep reinforcement learning. We train the system to learn a good intersection control policy by interacting with traffic environment through reinforcement learning. The brake-safe control model is used to ensure the safety of each autonomous vehicle while crossing. Experiment results show that after training using reinforcement learning, the throughput of intersection control model increased by 83%. In comparison with the Fast First Service (FFS) policy, the average waiting time of DRLAIM reduced by about 1.2% to 11.4%.					2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)	20-22 May 201920-22 May 2019		Yilan, TaiwanYilan, Taiwan	0	0	0	0	0	0	0					978-1-7281-3279-2									Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Cheng Univ., Chiayi, Taiwan				2020-03-12	INSPEC:19357713		
J	Peng, Bile; Keskin, Musa Furkan; Kulcsar, Balazs; Wymeersch, Henk				; Wymeersch, Henk/G-7125-2011	Peng, Bile/0000-0001-5511-4396; Wymeersch, Henk/0000-0002-1298-6159					Connected autonomous vehicles for improving mixed traffic efficiency in unsignalized intersections with deep reinforcement learning								COMMUNICATIONS IN TRANSPORTATION RESEARCH				1						100017			10.1016/j.commtr.2021.100017							Article	DEC 2021	2021	Human driven vehicles (HDVs) with selfish objectives cause low traffic efficiency in an un-signalized intersection. On the other hand, autonomous vehicles can overcome this inefficiency through perfect coordination. In this paper, we propose an intermediate solution, where we use vehicular communication and a small number of autonomous vehicles to improve the transportation system efficiency in such intersections. In our solution, two connected autonomous vehicles (CAVs) lead multiple HDVs in a double-lane intersection in order to avoid congestion in front of the intersection. The CAVs are able to communicate and coordinate their behavior, which is controlled by a deep reinforcement learning (DRL) agent. We design an altruistic reward function which enables CAVs to adjust their velocities flexibly in order to avoid queuing in front of the intersection. The proximal policy optimization (PPO) algorithm is applied to train the policy and the generalized advantage estimation (GAE) is used to estimate state values. Training results show that two CAVs are able to achieve significantly better traffic efficiency compared to similar scenarios without and with one altruistic autonomous vehicle.									51	2	0	0	0	0	52			2772-4247											TU Braunschweig, Inst Commun Technol, D-38106 Braunschweig, GermanyChalmers Univ Technol, Dept Elect Engn, S-41296 Gothenburg, Sweden				2021-12-01	WOS:001058776100009		
J	Cho, Youngwan; Lee, Jongseok; Lee, Kwangyup										CNN based Reinforcement Learning for Driving Behavior of Simulated Self-Driving Car			시뮬레이티드 자율주행 자동차의 주행행동 학습을 위한 기반 강화학습						전기학회논문지			69	11			1740	1749											research-article	2020	2020	This paper proposes a self-learning method for autonomous vehicle driving behavior using reinforcement learning without considering the dynamic model of the vehicle. In order to make decision needed for determine the optimal driving behavior (steering, throttle, brake) to achieve a given driving purpose in each state by using state information of the vehicle, such as vehicle movement speed, direction, degree of deviation from the center of the track, and distance to the edge of the track, we propose a method of applying the reinforcement learning by the DDPG structure and further using the driving image to improve driving performance. In this paper, we propose structures of an action decision network(Actor) and an action value evaluation network(Critic) to implement the DDPG learning model. We also propose a prediction model for predict the next state driving image based on the current driving image to improve driving performance in the corner path and a corner classifier for classifying the driving track type. The method proposed in this paper was implemented in a TORCS simulator environment, and the performance of the target driving behavior was evaluated through applying the learning model to driving agent.									0	0	0	0	0	0	0			1975-8359															2021-06-22	KJD:ART002643566		
B	Li, Qianwen										Trajectory Optimization for Connected and Autonomous Vehicle Platooning and Split Operations: Modeling and Experiments																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					979-8-209-95621-1									University of South Florida, Civil and Environmental Engineering, Florida, United States	University of South Florida				PQDT:49541346		
C	Krödel, N; Kuhnert, KD			INRIA; INRIA							Pattern matching as the nucleus for either autonomous driving or driver assistance systems								IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS								135	140											Proceedings Paper	2002	2002	Researchers in the area of autonomous vehicle driving are mainly focused on two different directions: neural networks (e.g. [ 1], [2], [3], [4]) or explicit modelling (e.g. [5],[6],[7]). This research starts a new direction called Driving by Pattern Matching combined with a Reinforcement Learning algorithm.In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicles respectively obstacles on the road but with the strict requirement to self-improve driving behaviour based on delayed, self-created rewards or punishments.A single connected camera is being used as the basis to quickly (real-time-requirement) build an abstract complete situation description (ACSD) of the situation the vehicle is currently in. Such conversion process is a reduction process and in addition to the requirement of the reduction to the relevant value-containing information, the calculation of such ACSD's has to be very quick. Therefore, this part combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an Approximate Nearest Neighbour Pattern Matching Algorithm with a O(n log m) characteristic compared to O(n a m) for the conventional nearest neighbour calculation.In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.					IEEE Intelligent Vehicle SymposiumIEEE Intelligent Vehicle Symposium	JUN 17-21, 2002JUN 17-21, 2002	IEEE, ITSC; INRIAIEEE, ITSC; INRIA	VERSAILLES, FRANCEVERSAILLES, FRANCE	2	0	0	0	0	0	2					0-7803-7346-4									Univ Siegen, Inst Real Time Syst, D-57068 Siegen, Germany				2002-01-01	WOS:000181392800022		
C	Ko, Eisaku; Chen, Kwang-Cheng			IEEE							Wireless Communications Meets Artificial Intelligence: An Illustration by Autonomous Vehicles on Manhattan Streets								2018 IEEE GLOBAL COMMUNICATIONS CONFERENCE (GLOBECOM)		IEEE Global Communications Conference																		Proceedings Paper	2018	2018	Interactions of multiple smart agents serve a fundamental aspect of Internet of Things (IoT), or known as social IoT. Such smart agents are equipped with sophisticated machine learning for mobile operation, such as autonomous vehicles and robots. Although wireless networking is intuitively important in such scenarios, there lacks investigations to provide a holistic and in-depth understanding on wireless networked multi-agent systems. In this paper, we disruptively use reinforcement learning to model each agent of artificial intelligence, and explore the interplay between wireless communications and multi-agent systems. Autonomous vehicles navigating over Manhattan streets serve the illustrating system. The first new finding is the need to modify reinforcement learning of policy exchange due to getting information from other agents through wireless communication. The advantage of applying wireless communication is clearly observed. We also demonstrate the impacts of communication errors to result in penalty in system performance. Multi-agent systems equipped with direct vehicleto-vehicle communication and vehicle-to-infrastructure communication are compared to initially conclude favorable using infrastructure of small cells. Finally, we explore multiple access communication over multi-agent systems by employing realtime ALOHA. Different from traditional thinking on reliable delivery of packets using re-transmit after collisions, real-time ALOHA discards re-transmission mechanism to ensure in-time contributions from wireless communication on the learning algorithm of a multi-agent system with satisfactory performance.					IEEE Global Conference on Communications (GLOBECOM) / Workshop on Wireless Networking and Control for UAVIEEE Global Conference on Communications (GLOBECOM) / Workshop on Wireless Networking and Control for UAV	DEC 09-13, 2018DEC 09-13, 2018	IEEEIEEE	Abu Dhabi, U ARAB EMIRATESAbu Dhabi, U ARAB EMIRATES	6	0	0	0	0	0	6			2334-0983		978-1-5386-4727-1									Natl Taiwan Univ, Taipei, TaiwanUniv S Florida, Tampa, FL 33620 USA				2018-01-01	WOS:000465774301077		
J	Ye, Qiming; Feng, Yuxiang; Candela, Eduardo; Escribano Macias, Jose; Stettler, Marc; Angeloudis, Panagiotis					Angeloudis, Panagiotis/0000-0002-6778-8264; Escribano Macias, Jose/0000-0002-8472-7498; YE, Qiming/0000-0003-4732-2042; Candela, Eduardo/0000-0002-8190-185X; Stettler, Marc/0000-0002-2066-9380; Feng, Yuxiang (Felix)/0000-0002-5530-2503					Spatial-Temporal Flows-Adaptive Street Layout Control Using Reinforcement Learning								SUSTAINABILITY				14	1					107			10.3390/su14010107							Article	JAN 2022	2022	Complete streets scheme makes seminal contributions to securing the basic public right-of-way (ROW), improving road safety, and maintaining high traffic efficiency for all modes of commute. However, such a popular street design paradigm also faces endogenous pressures like the appeal to a more balanced ROW for non-vehicular users. In addition, the deployment of Autonomous Vehicle (AV) mobility is likely to challenge the conventional use of the street space as well as this scheme. Previous studies have invented automated control techniques for specific road management issues, such as traffic light control and lane management. Whereas models and algorithms that dynamically calibrate the ROW of road space corresponding to travel demands and place-making requirements still represent a research gap. This study proposes a novel optimal control method that decides the ROW of road space assigned to driveways and sidewalks in real-time. To solve this optimal control task, a reinforcement learning method is introduced that employs a microscopic traffic simulator, namely SUMO, as its environment. The model was trained for 150 episodes using a four-legged intersection and joint AVs-pedestrian travel demands of a day. Results evidenced the effectiveness of the model in both symmetric and asymmetric road settings. After being trained by 150 episodes, our proposed model significantly increased its comprehensive reward of both pedestrians and vehicular traffic efficiency and sidewalk ratio by 10.39%. Decisions on the balanced ROW are optimised as 90.16% of the edges decrease the driveways supply and raise sidewalk shares by approximately 9%. Moreover, during 18.22% of the tested time slots, a lane-width equivalent space is shifted from driveways to sidewalks, minimising the travel costs for both an AV fleet and pedestrians. Our study primarily contributes to the modelling architecture and algorithms concerning centralised and real-time ROW management. Prospective applications out of this method are likely to facilitate AV mobility-oriented road management and pedestrian-friendly street space design in the near future.									2	0	0	0	0	0	2				2071-1050										Imperial Coll London, Dept Civil & Environm Engn, London SW7 2AZ, England				2022-03-06	WOS:000758580300001		
C	Leal Cota, Jamir; Tavares Rodriguez, Jose A.; Garcia Alonso, Brandon; Vazquez Hurtado, Carlos						Kallel, I; Kammoun, HM; Akkari, A; Hsairi, L				Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle								PROCEEDINGS OF THE 2022 IEEE GLOBAL ENGINEERING EDUCATION CONFERENCE (EDUCON 2022)		IEEE Global Engineering Education Conference						1355	1364				10.1109/EDUCON52537.2022.9766659							Proceedings Paper	2022	2022	Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.					13th IEEE Global Engineering Education Conference (IEEE EDUCON)13th IEEE Global Engineering Education Conference (IEEE EDUCON)	MAR 28-31, 2022MAR 28-31, 2022	IEEE; IEEE Educ Soc; IEEE Tunisia Sect Educ Chapter; IEEE Reg 8IEEE; IEEE Educ Soc; IEEE Tunisia Sect Educ Chapter; IEEE Reg 8	Gammarth, TUNISIAGammarth, TUNISIA	2	0	0	0	0	0	2			2165-9567		978-1-6654-4434-7									Tecnol Monterrey, Sch Engn & Sci, Monterrey, Mexico				2022-08-27	WOS:000836390500198		
J	Karalakou, Athanasia; Troullinos, Dimitrios; Chalkiadakis, Georgios; Papageorgiou, Markos					Troullinos, Dimitrios/0000-0003-3228-3888; Chalkiadakis, Georgios/0000-0002-0716-2972					Deep Reinforcement Learning Reward Function Design for Autonomous Driving in Lane-Free Traffic								SYSTEMS				11	3					134			10.3390/systems11030134							Article	MAR 2023	2023	Lane-free traffic is a novel research domain, in which vehicles no longer adhere to the notion of lanes, and consider the whole lateral space within the road boundaries. This constitutes an entirely different problem domain for autonomous driving compared to lane-based traffic, as there is no leader vehicle or lane-changing operation. Therefore, the observations of the vehicles need to properly accommodate the lane-free environment without carrying over bias from lane-based approaches. The recent successes of deep reinforcement learning (DRL) for lane-based approaches, along with emerging work for lane-free traffic environments, render DRL for lane-free traffic an interesting endeavor to investigate. In this paper, we provide an extensive look at the DRL formulation, focusing on the reward function of a lane-free autonomous driving agent. Our main interest is designing an effective reward function, as the reward model is crucial in determining the overall efficiency of the resulting policy. Specifically, we construct different components of reward functions tied to the environment at various levels of information. Then, we combine and collate the aforementioned components, and focus on attaining a reward function that results in a policy that manages to both reduce the collisions among vehicles and address their requirement of maintaining a desired speed. Additionally, we employ two popular DRL algorithms-namely, deep Q-networks (enhanced with some commonly used extensions), and deep deterministic policy gradient (DDPG), which results in better policies. Our experiments provide a thorough investigative study on the effectiveness of different combinations among the various reward components we propose, and confirm that our DRL-employing autonomous vehicle is able to gradually learn effective policies in environments with varying levels of difficulty, especially when all of the proposed rewards components are properly combined.									1	0	0	0	0	0	1				2079-8954										Tech Univ Crete, Sch Elect & Comp Engn, Khania 73100, GreeceTech Univ Crete, Sch Prod Engn & Management, Khania 73100, Greece				2023-04-15	WOS:000960229200001		
J	Kim, Chan; Cho, Jae-Kyung; Yoon, Hyung-Suk; Seo, Seung-Woo; Kim, Seong-Woo				Yoon, Alex/ADQ-5328-2022	Yoon, Alex/0000-0001-6368-2684; Cho, Jae-Kyung/0000-0002-1835-534X; Kim, Chan/0000-0003-1642-6789					UNICON: Uncertainty-Conditioned Policy for Robust Behavior in Unfamiliar Scenarios								IEEE ROBOTICS AND AUTOMATION LETTERS				7	4			9099	9106				10.1109/LRA.2022.3189447							Article	OCT 2022	2022	Deep reinforcement learning has been used to solve complex tasks in various fields, particularly in robotics control. However, agents trained using deep reinforcement learning have a problem of taking overconfident actions, even when the input state is far from the learned state distribution. This restricts deep reinforcement learning from being applied to real-world environments as overconfident actions in unlearned situations can result in catastrophic events; such as the collision of an autonomous vehicle. To address this, the agents should know "what they do not know" and choose an action by considering not only the state but also its uncertainty. In this study, we propose a novel uncertainty-conditioned policy (UNICON) inspired by the human behavior of changing policies according to uncertainty, e.g., slowing a car on a narrow road that has never been visited before. Our experimental results demonstrate that the proposed method is robust to unfamiliar scenarios that are not seen during training.									0	0	0	0	0	0	0			2377-3766											Seoul Natl Univ, Seoul 08826, South Korea				2022-08-22	WOS:000838567100068		
C	Wu, Yang; Zhang, Zhiyong; Yuan, Jianhua; Ma, Qing; Gao, Lifen			IEEE	WU, Yang/G-6227-2019	WU, Yang/0000-0002-0258-4842					Sequential game solution for lane-merging conflict between autonomous vehicles <i>A multi-agent reinforcement learning approach</i>								2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)								1482	1488											Proceedings Paper	2016	2016	Lane-merging conflict between Autonomous Vehicles (AV) calls for coordinated solution to allocate right-of-way. Related studies resort to centralized decision-making optimization models such as "right-of-way reservation/auction", which are suitable only for the scenarios with a centralized intersection agent (acting as arbiter or auctioneer); and involved fiat currency spent on bidding may trigger controversial issues concerning law or taxation. This paper: (i) establishes a prototype of 2-player complete-information 3-stage sequential game architecture within distributed decision-making paradigm, to formalize the interactions between 2 AVs trapped in the lane-merging conflict, so as to suit for scenarios with or without a centralized decision-maker, i.e. intersection or road segment; (ii) designs the dynamic rewards of AV's game-playing (velocity-adjusting) actions which result from the space-time status of AV; (iii) based on the proposed rewards, uses Multi-agent Reinforcement Learning to obtain the optimal (in Nash equilibrium sense) strategies of action-sequence for both AVs after 3-stage game-theoretic negotiations, promisingly avoiding the potential right-of-way deadlock in a lane-merging conflict.					19th IEEE International Conference on Intelligent Transportation Systems (ITSC)19th IEEE International Conference on Intelligent Transportation Systems (ITSC)	NOV 01-04, 2016NOV 01-04, 2016	IEEEIEEE	Rio de Janeiro, BRAZILRio de Janeiro, BRAZIL	4	0	0	0	0	0	5					978-1-5090-1889-5									Minist Publ Secur China, Traff Management Res Inst, Wuxi, Peoples R ChinaCollaborat Innovat Ctr Modern Urban Traff Technol, Wuxi, Peoples R ChinaWuxi WinTrans IT Co Ltd, Wuxi, Peoples R China	Collaborat Innovat Ctr Modern Urban Traff TechnolWuxi WinTrans IT Co Ltd			2017-02-15	WOS:000392215500232		
J	Hu, Hui; Wang, Yuge; Tong, Wenjie; Zhao, Jiao; Gu, Yulei										Path Planning for Autonomous Vehicles in Unknown Dynamic Environment Based on Deep Reinforcement Learning								APPLIED SCIENCES-BASEL				13	18					10056			10.3390/app131810056							Article	SEP 2023	2023	Autonomous vehicles can reduce labor power during cargo transportation, and then improve transportation efficiency, for example, the automated guided vehicle (AGV) in the warehouse can improve the operation efficiency. To overcome the limitations of traditional path planning algorithms in unknown environments, such as reliance on high-precision maps, lack of generalization ability, and obstacle avoidance capability, this study focuses on investigating the Deep Q-Network and its derivative algorithm to enhance network and algorithm structures. A new algorithm named APF-D3QNPER is proposed, which combines the action output method of artificial potential field (APF) with the Dueling Double Deep Q Network algorithm, and experience sample rewards are considered in the experience playback portion of the traditional Deep Reinforcement Learning (DRL) algorithm, which enhances the convergence ability of the traditional DRL algorithm. A long short-term memory (LSTM) network is added to the state feature extraction network part to improve its adaptability in unknown environments and enhance its spatiotemporal sensitivity to the environment. The APF-D3QNPER algorithm is compared with mainstream deep reinforcement learning algorithms and traditional path planning algorithms using a robot operating system and the Gazebo simulation platform by conducting experiments. The results demonstrate that the APF-D3QNPER algorithm exhibits excellent generalization abilities in the simulation environment, and the convergence speed, the loss value, the path planning time, and the path planning length of the APF-D3QNPER algorithm are all less than for other algorithms in diverse scenarios.									0	0	0	0	0	0	0				2076-3417										Changan Univ, Coll Transportat Engn, Xian 710064, Peoples R ChinaChangan Univ, Coll Automobile, Xian 710064, Peoples R China				2023-10-06	WOS:001072544800001		
B	Koren, M.; Kochenderfer, M.J.										Adaptive Stress Testing without Domain Heuristics using Go-Explore								2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)								6 pp.	6 pp.				10.1109/ITSC45102.2020.9294729							Conference Paper	2020	2020	Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domains-pecific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more likely actions, in order to to find more likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.					2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)	20-23 Sept. 202020-23 Sept. 2020		Rhodes, GreeceRhodes, Greece	0	0	0	0	0	0	0					978-1-7281-4149-7									Aeronaut. & Astronaut., Stanford Univ., Stanford, CA, USA				2021-02-19	INSPEC:20303244		
C	Han, Songyang; Wang, He; Su, Sanbao; Shi, Yuanyuan; Miao, Fei			IEEE							Stable and Efficient Shapley Value-Based Reward Reallocation for Multi-Agent Reinforcement Learning of Autonomous Vehicles								2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022)								8765	8771				10.1109/ICRA46639.2022.9811626							Proceedings Paper	2022	2022	With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs). However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability. When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process. In this work, we propose to reallocate the system's total reward efficiently to motivate stable cooperation among autonomous vehicles. We formally define and quantify how to reallocate the system's total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system's total reward. We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game. Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group. We then propose a cooperative policy learning algorithm with Shapley value reward reallocation. In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 23-27, 2022MAY 23-27, 2022	IEEE; IEEE Robot & Automat SocIEEE; IEEE Robot & Automat Soc	Philadelphia, PAPhiladelphia, PA	3	0	0	0	0	0	3					978-1-7281-9681-7									Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT USAShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai, Peoples R ChinaUniv Calif San Diego, Elect & Comp Engn Dept, San Diego, CA USA				2022-01-01	WOS:000941277601100		
C	Salvi, Ameya; Coleman, John; Buzhardt, Jake; Krovi, Venkat; Tallapragada, Phanindra				Krovi, Venkat N/B-3269-2009; Tallapragada, Phanindra/W-3035-2019						Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning								IFAC PAPERSONLINE				55	37			276	281				10.1016/jifacol.2022.11.197							Proceedings Paper	2022	2022	Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort. The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle. The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled. The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios. Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance. This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action. In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity. Copyright (c) 2022 The Authors. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)					2nd Modeling, Estimation and Control Conference (MECC)2nd Modeling, Estimation and Control Conference (MECC)	OCT 02-05, 2022OCT 02-05, 2022	Int Federat Automat Control, Tech Comm 4 2 Mechatron Syst; Int Federat Automat Control, Tech Comm 9 4 Control Educ; Amer Automat Control Council; SIEMENS Corp; Rutgers Univ, Dept Mech & Aerosp Engn; Univ Washington, Coll Engn; Univ Washington, Dept Mech Engn; ProcessesInt Federat Automat Control, Tech Comm 4 2 Mechatron Syst; Int Federat Automat Control, Tech Comm 9 4 Control Educ; Amer Automat Control Council; SIEMENS Corp; Rutgers Univ, Dept Mech & Aerosp Engn; Univ Washington, Coll Engn; Univ Washington, Dept Mech Engn; Processes	Jersey City, NJJersey City, NJ	0	0	0	0	0	0	0			2405-8963											Clemson Univ, Dept Automot Engn, Clemson, SC 29634 USAClemson Univ, Dept Mech Engn, Clemson, SC 29634 USA				2023-02-01	WOS:000904629000046		
J	ElSamadisy, Omar; Shi, Tianyu; Smirnov, Ilia; Abdulhai, Baher					Abdulhai, Baher/0000-0002-8787-2578; ElSamadisy, Omar/0000-0002-7678-7319					Safe, Efficient, and Comfortable Reinforcement-Learning-Based Car-Following for AVs with an Analytic Safety Guarantee and Dynamic Target Speed								TRANSPORTATION RESEARCH RECORD				2678	1			643	661				10.1177/03611981231171899					JUN 2023		Article; Early Access		2024	Over the last decade, there has been rising interest in automated driving systems and adaptive cruise control (ACC). Controllers based on reinforcement learning (RL) are particularly promising for autonomous driving, being able to optimize a combination of criteria such as efficiency, stability, and comfort. However, RL-based controllers typically offer no safety guarantees. In this paper, we propose SECRM (the Safe, Efficient, and Comfortable RL-based car-following Model) for autonomous car-following that balances traffic efficiency maximization and jerk minimization, subject to a hard analytic safety constraint on acceleration. The acceleration constraint is derived from the criterion that the follower vehicle must have sufficient headway to be able to avoid a crash if the leader vehicle brakes suddenly. We critique safety criteria based on the time-to-collision (TTC) threshold (commonly used for RL controllers), and confirm in simulator experiments that a representative previous TTC-threshold-based RL autonomous-vehicle controller may crash (in both training and testing). In contrast, we verify that our controller SECRM is safe, in training scenarios with a wide range of leader behaviors, and in both regular-driving and emergency-braking test scenarios. We find that SECRM compares favorably in efficiency, comfort, and speed-following to both classical (non-learned) car-following controllers (intelligent driver model, Shladover, Gipps) and a representative RL-based car-following controller.									0	0	0	0	0	0	0			0361-1981	2169-4052										Univ Toronto, Dept Civil & Mineral Engn, Toronto, ON, CanadaArab Acad Sci Technol & Maritime Transport, Dept Elect & Commun Engn, Alexandria, Egypt				2023-07-09	WOS:001018067400001		
J	Liu, Qi; Li, Xueyuan; Tang, Yujie; Gao, Xin; Yang, Fan; Li, Zirui				li, zr/JTT-8305-2023	Liu, Qi/0000-0001-5172-0989					Graph Reinforcement Learning-Based Decision-Making Technology for Connected and Autonomous Vehicles: Framework, Review, and Future Trends								SENSORS				23	19					8229			10.3390/s23198229							Review	OCT 2023	2023	The proper functioning of connected and autonomous vehicles (CAVs) is crucial for thesafety and efficiency of future intelligent transport systems. Meanwhile, transitioning to fully autonomousdriving requires a long period of mixed autonomy traffic, including both CAVs andhuman-driven vehicles. Thus, collaborative decision-making technology for CAVs is essential togenerate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomytraffic. In recent years, deep reinforcement learning (DRL) methods have become an efficient way insolving decision-making problems. However, with the development of computing technology, graphreinforcement learning (GRL) methods have gradually demonstrated the large potential to furtherimprove the decision-making performance of CAVs, especially in the area of accurately representingthe mutual effects of vehicles and modeling dynamic traffic environments. To facilitate the developmentof GRL-based methods for autonomous driving, this paper proposes a review of GRL-basedmethods for the decision-making technologies of CAVs. Firstly, a generic GRL framework is proposedin the beginning to gain an overall understanding of the decision-making technology. Then, theGRL-based decision-making technologies are reviewed from the perspective of the constructionmethods of mixed autonomy traffic, methods for graph representation of the driving environment,and related works about graph neural networks (GNN) and DRL in the field of decision-makingfor autonomous driving. Moreover, validation methods are summarized to provide an efficientway to verify the performance of decision-making methods. Finally, challenges and future researchdirections of GRL-based decision-making methods are summarized.									0	0	0	0	0	0	0				1424-8220										Beijing Inst Technol, Sch Mech Engn, Beijing 100811, Peoples R ChinaDalhousie Univ, Fac Comp Sci, Halifax, NS B3H 4R2, Canada				2023-10-28	WOS:001083301500001	37837063	
J	Qiang, Yuchuan; Wang, Xiaolan; Liu, Xintian; Wang, Yansong; Zhang, Weiwei										Edge-enhanced Graph Attention Network for driving decision-making of autonomous vehicles via Deep Reinforcement Learning								PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING													10.1177/09544070231217762					JAN 2024		Article; Early Access		2024	Despite the rapid advancement in the field of autonomous driving vehicles, developing a safe and sensible decision-making system remains a challenging problem. The driving decision-making module is one of the most essential sections of the entire autonomous driving system, and the decision generated from it can significantly impinge the lives and property of passengers. Complicated interactions among traffic participants have the most profound impact on the decision-making process, yet the interactions are often simplified or overlooked due to their complexity and implicit nature. To address this issue, this work proposes an Edge-Enhanced Graph Attention Reinforcement Learning (EGARL) framework that aims to make rational driving decisions by comprehensively modeling the interactions among agents. EGARL comprises three core components: a graphical representation of the traffic scenario that covers both topological and interactive information; an Edge-enhanced Graph Attention Network (E-GAT) that utilizes the graphical representation to extract interactive features by comprehensively considering nodes and edges of the graph; and a deep reinforcement learning method that generates driving decisions based on the current state and features extracted from E-GAT. Experimental results demonstrate the satisfying performance of EGARL. Our proposed framework can contribute to the development of intelligent transportation systems, enhancing the safety and efficiency of driving.									0	0	0	0	0	0	0			0954-4070	2041-2991										Shanghai Univ Engn Sci, Sch Mech & Automot Engn, Shanghai, Peoples R ChinaShanghai Smart Vehicle Cooperating Innovat Ctr Co, Shanghai, Peoples R ChinaShanghai Univ Engn Sci, Sch Mech & Automot Engn, 333,Longteng Rd,Songjiang Dist, Shanghai 201620, Peoples R China	Shanghai Smart Vehicle Cooperating Innovat Ctr Co			2024-01-19	WOS:001136801100001		
J	Chen Yue; Jiao Pengpeng; Bai Ruyu; Li Rujian							陈越; 焦朋朋; 白如玉; 李汝鉴			Modeling Car Following Behavior of Autonomous Driving Vehicles Based on Deep Reinforcement Learning			基于深度强化学习的自动驾驶车辆跟驰行为建模				交通信息与安全	Journal of Transport Information and Safety				41	2			67	75,102	1674-4861(2023)41:2<67:JYSDQH>2.0.TX;2-M										Article	2023	2023	In order to enhance the performance of car following behavior of autonomous vehicles and mitigate the negative effects of traffic oscillations, a deep reinforcement learning-based car following model for automated driving is investigated. The existing reward function is improved by incorporating energy consumption, and the related terms for representing energy consumption are established based on the VT-Micro model. In addition, the method of using the time gap between vehicles to establish the reward function related to driving efficiency is improved by adding virtual speed to the time gap, in order to avoid computation overflow and unrealistic short following distance in the traffic oscillation scenario. To overcome the limitations of training on closed-loop simulated roads and simulated vehicle trajectories, human driver behavior extracted from the NGSIM trajectory data during traffic oscillation are used to develop the training environment. By applying the twin delayed deep deterministic policy gradient algorithm (TD3), a multi-objective car following model is then developed. A system for evaluating model performance is established to compare the performance of the TD3 model with traditional models in car following and traffic oscillations scenarios. Study results of car following scenarios show that the TD3 model and the traditional adaptive cruise control (ACC) model perform similarly in terms of comfort and driving efficiency, but both outperform the human drivers. In terms of safety, the TD3 model reduces safety hazards by 53.65% compared to the traditional ACC model, and 36.24% compared to the human drivers. Regarding energy consumption, the TD3 model reduces the energy consumption of the conventional ACC model and human drivers by 6.73% and 15.65%, respectively. Study results show that the TD3 model can reduce the negative impacts of traffic oscillations. In the scenario with a 100% TD3 model penetration rate, driving discomfort decreases by 55.95%, driving efficiency increases by 8.82%, crash risks reduce by 73.21%, and fuel consumption drops by 5.97%, compared to a 100% human-driven environment.			为提高自动驾驶车辆的跟驰性能,减轻交通震荡干扰的负面影响,研究了1种基于深度强化学习的自动驾驶跟驰模型。在现有奖励函数设计基础上融入对能源消耗的考虑,基于VT-Micro模型构建能耗相关项;同时对使用跟车时距构建行驶效率因素相关项的方法进行优化,添加虚拟速度来避免在交通震荡场景中出现计算溢出和车间距过近的问题。为克服过往抑制震荡研究中仅用闭合环状模拟道路和仿真车辆轨迹开展训练的局限性,选用NGSIM轨迹数据中交通震荡阶段的驾驶员行为特征搭建训练环境,应用双延迟深度确定性策略梯度算法(Twin Delayed Deep Deterministic Policy Gradient Algorithm,TD3)训练形成多目标优化的跟驰模型。进一步构建模型性能测试评价体系,对比分析TD3模型与其他传统模型在跟车与交通震荡2类测试场景中的表现。跟车测试场景实验结果表明:在舒适度与行驶效率上,TD3模型和传统自适应巡航控制(Adaptive Cruise Control, ACC)模型表现相近,二者均优于人类驾驶员;在安全性上,TD3模型相较于传统ACC模型安全隐患降低53.65%,相较于人类驾驶员降低36.24%;在能源消耗上,TD3模型相较于传统ACC模型和人类驾驶员分别降低6.73%和15.65%。交通震荡场景实验结果表明:TD3模型可以有效减少交通振荡的负面影响;当TD3模型渗透率为100%时,相较于纯人类驾驶环境,行驶过程中的不适性降低55.95%,行驶效率提高8.82%,安全隐患降低73.21%,油耗减少5.97%。						0	0	0	0	0	0	0			1674-4861											北京建筑大学, 通用航空技术北京实验室, 北京 100044, 中国Beijing University of Civil Engineering and Architecture, Beijing Key Laboratory of General Aviation Technology, Beijing 100044, China	北京建筑大学Beijing University of Civil Engineering and Architecture			2023-09-28	CSCD:7497624		
J	Ning Qiang; Liu Yuansheng; Xie Longyang										Application of SAC-Based Autonomous Vehicle Control Method								Computer Engineering and Applications								306	14				10.3778/j.issn.1002-8331.2112-0084							Journal Paper	2023	2023	In order to improve the problem of slow network convergence and unstable training process caused by equal probability sampling of SAC (soft actor critic) algorithm samples and random initialization of the network, an improved algorithm PE-SAC (priority playback soft actor) is proposed that combines priority playback and expert data. The algorithm classifies the sample pool according to the sample value, uses expert data to pre-train the network, reduces the invalid exploration space of unmanned vehicles, reduces the number of trials and errors, and effectively improves the learning efficiency of the algorithm. At the same time, a reward function for multiple obstacles is designed to enhance the applicability of the algorithm. Simulation experiments are carried out on the CARLA platform, and the results show that the proposed method can better control the safe driving of unmanned vehicles in the environment, and the reward value and convergence speed obtained under the same training times are better than TD3 (twin delayed deep deterministic policy gradient algorithm) and SAC algorithm. Finally, combined with the radar point cloud map and the PID (proportional integral derivative) control method, the difference between the simulation environment and the real scene is reduced, and the training model is transplanted to the low-speed unmanned vehicle in the park to verify the generality of the algorithm.									0	0	0	0	0	0	0			1002-8331											Coll. of Smart City, Beijing Union Univ., Beijing, ChinaBeijing Eng. Res. Center of Smart Mech. Innovation Design Service, Beijing, ChinaBeijing Key Lab. of Inf. Service Eng., Beijing Union Univ., Beijing, China				2023-08-10	INSPEC:23507987		
J	Liang, Jing; Weerakoon, Kasun; Guan, Tianrui; Karapetyan, Nare; Manocha, Dinesh					Manocha, Dinesh/0000-0001-7047-9801; Karapetyan, Nare/0000-0002-0947-3408					AdaptiveON: Adaptive Outdoor Local Navigation Method for Stable and Reliable Actions								IEEE ROBOTICS AND AUTOMATION LETTERS				8	2			648	655				10.1109/LRA.2022.3229907							Article	FEB 2023	2023	We present a novel outdoor navigation algorithm to generate stable and efficient actions to navigate a robot to reach a goal. We use a multi-stage training pipeline and show that our approach produces policies that result in stable and reliable robot navigation on complex terrains. Based on the Proximal Policy Optimization (PPO) algorithm, we developed a novel method to achieve multiple capabilities for outdoor local navigation tasks, namely alleviating the robot's drifting, keeping the robot stable on bumpy terrains, avoiding climbing on hills with steep elevation changes, and avoiding collisions. Our training process mitigates the reality (sim-to-real) gap by introducing generalized environmental and robotic parameters and training with rich features captured from light detection and ranging (Lidar) sensor in a high-fidelity Unity simulator. We evaluate our method in both simulation and real-world environments using Clearpath Husky and Jackal robots. Further, we compare our method against the state-of-the-art approaches and observe that, in the real world, our method improves stability by at least 30.7% on uneven terrains, reduces drifting by 8.08%, and decreases the elevation changes by 14.75%.									0	0	0	0	0	0	0			2377-3766											Univ Maryland, College Pk, MD 20742 USA				2023-01-20	WOS:000903580800004		
J	Han, Songyang; Zhou, Shanglin; Wang, Jiangwei; Pepin, Lynn; Ding, Caiwen; Fu, Jie; Miao, Fei				Han, Songyang/HNI-9860-2023	Han, Songyang/0000-0001-8314-4832					A Multi-Agent Reinforcement Learning Approach for Safe and Efficient Behavior Planning of Connected Autonomous Vehicles								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS													10.1109/TITS.2023.3336670					DEC 2023		Article; Early Access		2023	The recent advancements in wireless technology enable connected autonomous vehicles (CAVs) to gather information about their environment by vehicle-to-vehicle (V2V) communication. In this work, we design an information-sharing based multi-agent reinforcement learning (MARL) framework for CAVs, to take advantage of the extra information when making decisions to improve traffic efficiency and safety. The safe actor-critic algorithm we propose has two new techniques: the truncated Q-function and safe action mapping. The truncated Q-function utilizes the shared information from neighboring CAVs such that the joint state and action spaces of the Q-function do not grow in our algorithm for a large-scale CAV system. We prove the bound of the approximation error between the truncated -Q and global Q-functions. The safe action mapping provides a provable safety guarantee for both the training and execution based on control barrier functions. Using the CARLA simulator for experiments, we show that our approach improves the CAV system's efficiency in terms of average velocity and comfort under different CAV ratios and different traffic densities. We also show that our approach avoids the execution of unsafe actions and always maintains a safe distance from other vehicles. We construct an obstacle-at-corner scenario to show that the shared vision can help CAVs to observe obstacles earlier and take action to avoid traffic jams. The experiment video is on https://songyanghan.github.io/cavmarl/.									0	0	0	0	0	0	0			1524-9050	1558-0016										Univ Connecticut, Dept Comp Sci & Engn, , Mansfield, Storrs, CT 06268 USASony Al, New York, NY 10010 USAUniv Connecticut, Dept Comp Sci & Engn, Storrs, CT 06268 USAUniv Connecticut, Dept Elect & Comp Engn, Storrs, CT 06268 USAUniv Florida, Dept Elect & Comp Engn, Gainesville, FL 32605 USA	Sony Al			2024-01-09	WOS:001129779800001		
C	Manjanna, Sandeep; van Hoof, Herke; Dudek, Gregory			IEEE	van Hoof, Herke/N-7775-2017	van Hoof, Herke/0000-0002-1583-3692					Reinforcement Learning with Non-uniform State Representations for Adaptive Search								2018 IEEE INTERNATIONAL SYMPOSIUM ON SAFETY, SECURITY, AND RESCUE ROBOTICS (SSRR)		IEEE International Symposium on Safety Security and Rescue Robotics																		Proceedings Paper	2018	2018	Efficient spatial exploration is a key aspect of search and rescue. In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher. This should allow an autonomous vehicle find one or more lost targets as rapidly as possible. We do this by performing non-uniform sampling of the search region. The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning. We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target. Key features of our search algorithm are the ability to employ a very general nondeterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions. One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics. We compare the paths generated by our algorithm with other accepted spatial coverage techniques such as distribution independent boustrophedonic coverage and model dependent spiral search.We present a proof showing that rewarding for clearing probability mass instead of locating the target does not bias the objective function. The experiments show that the learned policy outperforms several well-known baselines even in scenarios different from the one it has been trained on.					IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	AUG 06-08, 2018AUG 06-08, 2018	IEEEIEEE	Philadelphia, PAPhiladelphia, PA	2	0	0	0	0	0	2			2374-3247		978-1-5386-5572-6									McGill Univ, Ctr Intelligent Machines, Mobile Robot Lab MRL, Montreal, PQ, CanadaUniv Amsterdam, Amsterdam Machine Learning Lab AMLAB, Amsterdam, Netherlands				2018-01-01	WOS:000853885200032		
J	Ye, Q.; Feng, Y.; Macias, J.J.E.; Stettler, M.; Angeloudis, P.										Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions using Reinforcement Learning [arXiv]								arXiv																				Journal Paper	21 March 2023	2023	The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure. In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed. Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand. Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions. We implement a centralised paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations. Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians. Furthermore, the distributive learning algorithm outperforms its centralised counterpart regarding computational cost (49.55\%), benchmark rewards (25.35\%), best cumulative rewards (24.58\%), optimal actions (13.49\%) and rate of convergence. This novel road management technique could potentially contribute to the flow-adaptive and active mobility-friendly streets in the AVs era.									0	0	0	0	0	0	0																		2023-07-01	INSPEC:23284287		
J	Feng, Shuo; Haykin, Simon										Cognitive Risk Control for Anti-Jamming V2V Communications in Autonomous Vehicle Networks								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				68	10			9920	9934				10.1109/TVT.2019.2935999							Article	OCT 2019	2019	The future of intelligent transportation system (ITS) is expected to be composed of connected and autonomous vehicles (CAVs), the development of which will have great impact on peoples everyday life. Unfortunately, this progress will be accompanied by all kinds of potential threats and attacks rising in CAV network. As a legacy from traditional wireless networks, jamming attack is still one of the major and serious threats to vehicle-to-vehicle (V2V) communications. In this paper, we investigate the anti-jamming V2V communication in CAV networks through power control in conjunction with channel selection. Bringing into play a brain-inspired research tool called cognitive dynamic system (CDS), the general structure of cognitive risk control (CRC) is well-tailored to analyze and address the jamming problem. Specifically, power control is carried out first using reinforcement learning, the result of which is then examined by a module called task-switch control. Based on the risk assessment, a multi-armed bandit (MAB) problem is formulated to perform the channel-selection process when necessary. Through continuous perception-action cycles (PACs), the feature of predictive adaptation is realized for the legitimate vehicle in its behavioral interactions with the jammer. Simulation results have shown that the proposed method has desirable performance in terms of several evaluation metrics.									37	0	0	0	0	0	40			0018-9545	1939-9359										McMaster Univ, Cognit Syst Lab, Hamilton, ON L8S 4L8, CanadaArmy Engn Univ PLA, Coll Commun Engn, Nanjing 210007, Jiangsu, Peoples R ChinaShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China				2019-12-23	WOS:000501349900047		
J	Al Younes, Younes; Barczyk, Martin				Al Younes, Younes/AEX-2581-2022	Al Younes, Younes/0000-0002-8295-8356					Adaptive Nonlinear Model Predictive Horizon Using Deep Reinforcement Learning for Optimal Trajectory Planning								DRONES				6	11					323			10.3390/drones6110323							Article	NOV 2022	2022	This paper presents an adaptive trajectory planning approach for nonlinear dynamical systems based on deep reinforcement learning (DRL). This methodology is applied to the authors' recently published optimization-based trajectory planning approach named nonlinear model predictive horizon (NMPH). The resulting design, which we call 'adaptive NMPH', generates optimal trajectories for an autonomous vehicle based on the system's states and its environment. This is done by tuning the NMPH's parameters online using two different actor-critic DRL-based algorithms, deep deterministic policy gradient (DDPG) and soft actor-critic (SAC). Both adaptive NMPH variants are trained and evaluated on an aerial drone inside a high-fidelity simulation environment. The results demonstrate the learning curves, sample complexity, and stability of the DRL-based adaptation scheme and show the superior performance of adaptive NMPH relative to our earlier designs.									1	0	0	0	0	0	1				2504-446X										Univ Alberta, Dept Mech Engn, Edmonton, AB T6G 1H9, Canada				2022-11-21	WOS:000880868700001		
J	Chen, Yuying; Liu, Congcong; Shi, Bertram E.; Liu, Ming				Alidadi, Mehdi/HJZ-0235-2023; Liu, Haibo/JWP-8549-2024; Liu, Ming/AAC-9891-2020; chen, yuying/JNS-9778-2023	Alidadi, Mehdi/0000-0001-5183-7829; Liu, Haibo/0000-0002-4213-2883; Liu, Ming/0000-0002-4500-238X; Liu, Congcong/0000-0002-1749-1075; Chen, Yuying/0000-0003-4954-8192; Shi, Bertram/0000-0001-9167-7495					Robot Navigation in Crowds by Graph Convolutional Networks With Attention Learned From Human Gaze								IEEE ROBOTICS AND AUTOMATION LETTERS				5	2			2754	2761				10.1109/LRA.2020.2972868							Article	APR 2020	2020	Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies. However, their performance deteriorates when the crowd size grows. We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation. We propose a novel network utilizing a graph representation to learn the policy. We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd as they perform a navigation task based on a top down view of the environment. We incorporate the learned attention into a graph-based reinforcement learning architecture. The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability. Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods, increasing task completion rate by 18.4% and decreasing navigation time by 16.4%.									68	4	0	0	1	0	77			2377-3766											Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China				2020-04-01	WOS:000526702500018		
J	Zhang Jian; Li Qingyang; Li Dan; Jiang Xia; Lei Yanhong; Ji Yaping							张健; 李青扬; 李丹; 姜夏; 雷艳红; 季亚平			Merging guidance of exclusive lanes for connected and autonomous vehicles based on deep reinforcement learning			基于深度强化学习的自动驾驶车辆专用道汇入引导				吉林大学学报. 工学版	Journal of Jilin University. Engineering and Technology Edition				53	9			2508	2518	1671-5497(2023)53:9<2508:JYSDQH>2.0.TX;2-U										Article	2023	2023	Exclusive lanes for connected and autonomous vehicles(CAVs)will emerge in order to ensure the safety and efficiency requirements in the process of traffic flow mixed with human-driving vehicles and CAVs. When the inner lane of the expressway is set as the exclusive lane for CAVs, it has important theoretical significance and practical value to study the strategy of guiding CAVs to merge from the ordinary lane to the exclusive lane. Firstly, the entrance area of exclusive lane was designed and vehicle control rules were proposed. Secondly, with the goal of making more CAVs change lanes to the exclusive lane, the strategy of selecting lane-changing signal actions was proposed based on deep reinforcement learning. Finally, the numerical simulation was carried out with Python language compilation. The results show that the proposed algorithm can converge very quickly under 9 scenarios constructed by different factors, such as the CAV penetration rates and the proportion of CAVs arriving at the exclusive lane; it can effectively guide CAVs to merge into exclusive lanes and ensure traffic efficiency; congestion in the second lane can be significantly reduced compared to the unsignalized control when the penetration rate changes from 20% to 40%; the proportion of CAVs changing to the exclusive lane is significantly higher under the two exclusive lane entrances scenario than that under the one entrance scenario. It shows that the proposed strategy has good applicability and can provide reference for engineering construction.			为满足自动驾驶车辆(CAV)与人工驾驶车辆混行过程中安全和效率的需求,自动驾驶车辆专用道应运而生。当高速公路内侧车道设为自动驾驶车辆专用道时,引导自动驾驶车辆从普通车道汇入至专用道的策略研究具有重要的理论意义和实际价值。首先,设计专用道入口并提出车辆控制规则;其次,以使更多自动驾驶车辆换道至专用道为目标,基于深度强化学习,选择换道信号动作引导车辆换道;最后,通过Python语言编译进行数值仿真验证。结果表明:在自动驾驶车辆渗透率、到达专用道自动驾驶车辆比例等不同因素构建的9种场景下,本文算法能够快速收敛;能够有效引导自动驾驶车辆汇入专用道,保证通行效率;相较无信号控制情况,渗透率为20%~40%时,第2车道交通拥堵显著减少;在两段式专用道入口场景下, CAV换道至专用道的比例比单入口场景明显提高。所提出的策略具有较好的适用性,能为工程建设提供参考借鉴。						0	0	0	0	0	0	0			1671-5497											东南大学;;西藏大学工学院;;东南大学交通学院, 江苏省城市智能交通重点实验室;;;;, 南京;;拉萨;;南京, ;;;; 211189;;850013;;211189东南大学;;东南大学交通学院, 江苏省城市智能交通重点实验室;;, 南京;;南京, ;; 211189;;211189西藏大学工学院, 拉萨, 西藏 850013, 中国Southeast University;;Institute of Technology, Tibet University;;School of Transportation, Southeast University, Jiangsu Key Laboratory of Urban ITS;;;;, Nanjing;;Lhasa;;Nanjing, ;;;; 211189;;850013;;211189Southeast University;;School of Transportation, Southeast University, Jiangsu Key Laboratory of Urban ITS;;, Nanjing;;Nanjing, ;; 211189;;211189Institute of Technology, Tibet University, Lhasa, Tibet 850013, China	东南大学;;西藏大学工学院;;东南大学交通学院东南大学;;东南大学交通学院西藏大学工学院Southeast University;;Institute of Technology, Tibet University;;School of Transportation, Southeast UniversitySoutheast University;;School of Transportation, Southeast UniversityInstitute of Technology, Tibet University			2024-01-12	CSCD:7585719		
J	Koren, M.; Kochenderfer, M.J.										Adaptive stress testing without domain heuristics using go-explore [arXiv]								arXiv								7 pp.	7 pp.											Journal Paper	8 April 2020	2020	Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more-likely actions, because we want to find more-likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most-likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.									0	0	0	0	0	0	0														Aeronaut. & Astronaut., Stanford Univ., Stanford, CA, USA				2020-08-28	INSPEC:19824673		
J	Yoon, Hyung-Jin; Jafarnejadsani, Hamidreza; Voulgaris, Petros					YOON, HYUNG JIN/0000-0001-6828-5303; Jafarnejadsani, Hamidreza/0000-0002-8489-005X; voulgaris, petros/0000-0002-2965-3246					Learning When to Use Adaptive Adversarial Image Perturbations Against Autonomous Vehicles								IEEE ROBOTICS AND AUTOMATION LETTERS				8	7			4179	4186				10.1109/LRA.2023.3280813							Article	JUL 2023	2023	Deep neural network (DNN) models are widely used in autonomous vehicles for object detection using camera images. However, these models are vulnerable to adversarial image perturbations. Existing methods for generating these perturbations use the image frame as the decision variable, resulting in a computationally expensive optimization process that starts over for each new image. Few approaches have been developed for attacking online image streams while considering the physical dynamics of autonomous vehicles, their mission, and the environment. To address these challenges, we propose a multi-level stochastic optimization framework that monitors the attacker's capability to generate adversarial perturbations. Our framework introduces a binary decision attack/not attack based on the attacker's capability level to enhance its effectiveness. We evaluate our proposed framework using simulations for vision-guided autonomous vehicles and actual tests with a small indoor drone in an office environment. Our results demonstrate that our method is capable of generating real-time image attacks while monitoring the attacker's proficiency given state estimates.									0	0	0	0	0	0	0			2377-3766											Univ Nevada, Dept Mech Engn, Reno, NV 89557 USAStevens Inst Technol, Dept Mech Engn, Hoboken, NJ 07030 USA				2023-07-08	WOS:001004297800016		
B	Vantsevich, V.; Gorsich, D.; Lozynskyy, A.; Demkiv, L.; Borovets, T.; Klos, S.				Lozynskyy, Andriy/R-9730-2017	Lozynskyy, Andriy/0000-0003-1351-7183; Borovets, Taras/0009-0003-7808-6588; Vantsevich, Vladimir/0000-0002-8176-9414	Palestini, C.				Agile Tyre Mobility: Observation and Control in Severe Terrain Environments								Advanced Technologies for Security Applications. Proceedings of the NATO Science for Peace and Security Cluster Workshop on Advanced Technologies'. NATO Science for Peace and Security Series B: Physics and Biophysics (NAPSB)								247	58				10.1007/978-94-024-2021-0_22							Conference Paper	2020	2020	This research study develops fundamentals for a new ground vehicle technology to radically improve and protect off-road vehicle mobility by providing agile (fast, exact and pre-emptive) responses and advanced mobility controls in severe terrain conditions. The current framework of terrain vehicle mobility that estimates a vehicle capability to go through or not to go through the given terrain conditions cannot provide an analytical basis for novel system design solutions. Indeed, modern traction control and other mobility related electronic control systems possess control response time within the range of 100-120 milliseconds and greater. With this response time, the actual control occurs after the vehicle has reached a critical motion situation, e.g., a wheel(s) is/are spinning and the vehicle is already losing its mobility. In this study, the developed methods allowed for estimating tyre mobility and controlling tyre motion before the tyre starts spinning. As shown in the conducted analysis, the response time, which occurs within the longitudinal tyre relaxation time constant of 40-60 ms, is sufficient for a tyre to avoid spinning and to maintain its required mobility. Most common traditional approaches to observation of data supplied by virtual sensors were simulated and improved by means of machine learning algorithms. Computational simulations of an one-wheel-locomotion module driven by an electric driveline system demonstrated a sufficient performance of the proposed observation method to estimate mobility margins of the module in real time.A hybrid intelligent control algorithm was designed, in which reinforcement learning was used to fine-tune the parameters of a fuzzy logic controller. A new wheel mobility index was utilized as a cost function to guarantee a designed behavior of the locomotion module. A fuzzy corrector was additionally designed to take into account both the dynamic state of the system and the dynamics of the tyre-terrain interaction. The fuzzy corrector supports upper level controls of autonomous vehicle dynamics by decreasing tyre slippage on severe terrains.Computer simulations testified both stability of the controller (due to utilization of fuzzy logic polynomial control) and its desired performance (due to application of reinforcement learning). The fine-tuned controller requires minimal online computations.This paper provides an extended summary of the above-listed research studies. Further details can be found in publications referenced in the paper.					NATO Science for Peace and Security 'Cluster Workshop on Advanced Technologies'NATO Science for Peace and Security 'Cluster Workshop on Advanced Technologies'	17-18 Sept. 201917-18 Sept. 2019		Leuven, BelgiumLeuven, Belgium	2	0	0	0	0	0	2					978-94-024-2020-3									Univ. of Alabama at Birmingham, Birmingham, AL, USAU.S. Army CCDC Ground Vehicle Syst. Center, Warren, MI, USALviv Polytech. Nat. Univ., Lviv, UkraineNorth Atlantic Treaty Organ., Brussels, Belgium				2020-01-01	INSPEC:19750723		
J	Zhao, Ning; Wu, Hao; Yu, F. Richard; Wang, Lifu; Zhang, Weiting; Leung, Victor C. M.				Leung, Victor C. M./AGU-2462-2022	Leung, Victor C. M./0000-0003-3529-2640; Zhao, Ning/0000-0003-1389-2469; Zhang, Weiting/0000-0002-7473-2234; Wu, Hao/0000-0002-4733-0125					Deep-Reinforcement-Learning-Based Latency Minimization in Edge Intelligence Over Vehicular Networks								IEEE INTERNET OF THINGS JOURNAL				9	2			1300	1312				10.1109/JIOT.2021.3078480							Article	JAN 15 2022	2022	A novel paradigm that combines federated learning with blockchain to empower edge intelligence over vehicular networks (FBVN) can enable latency-sensitive deep neural network-based applications to be executed in a distributed pattern. However, the complex environments in FBVN make the system latency much harder to minimize by traditional methods. In this article, we model the training and transmission latency of each autonomous vehicle (AV) and consensus latency of the blockchain in-edge side in FBVN. Considering the dynamic and time-varying wireless channel conditions, unpredictable packet error rate, and unstable data sets quality, we adopt duel deep Q-learning (DDQL) as the solving approach. We propose a federated DDQL algorithm, in which the learning agent is deployed on each AV side, and the sensing states on each AV do not need to be shared so that it increases scalability and flexibility for practical implementation. Simulation results show that the proposed algorithm has better performance in reducing system latency compared with the other schemes.									7	3	0	0	0	0	8			2327-4662											Beijing Jiaotong Univ, State Key Lab Rail Traff Control & Safety, Beijing 100044, Peoples R ChinaCarleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, CanadaBeijing Jiaotong Univ, Dept Elect & Informat Engn, Beijing 100044, Peoples R ChinaUniv British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada				2022-01-15	WOS:000740006200038		
J	Zhang, Ruiqi; Hou, Jing; Chen, Guang; Li, Zhijun; Chen, Jianxiao; Knoll, Alois				Knoll, Alois/AAN-8417-2021; Ruiqi, Zhang/JAC-1601-2023; Zhang, Ruiqi/GVT-0199-2022	Knoll, Alois/0000-0003-4840-076X; Ruiqi, Zhang/0000-0001-8789-5609; Hou, Jing/0000-0003-4778-137X					Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing								IEEE ROBOTICS AND AUTOMATION LETTERS				7	4			11625	11632				10.1109/LRA.2022.3192770							Article	OCT 2022	2022	Motion planning for autonomous racing is a challenging task due to the safety requirement while driving aggressively. Most previous solutions utilize the prior information or depend on complex dynamics modeling. Classical model-free reinforcement learning methods are based on random sampling, which severely increases the training consumption and undermines the exploration efficiency. In this letter, we propose an efficient residual policy learning method for high-speed autonomous racing named ResRace, which leverages only the real-time raw observation of LiDAR and IMU for low-latency obstacle avoiding and navigation. We first design a controller based on the modified artificial potential field (MAPF) to generate a policy for navigation. Besides, we utilize the deep reinforcement learning (DRL) algorithm to generate a residual policy as a supplement to obtain the optimal policy. Concurrently, the MAPF policy effectively guides the exploration and increases the update efficiency. This complementary property contributes to the fast convergence and few required resources of our method. We also provide extensive experiments to illustrate our method outperforms the leading algorithms and reaches the comparable level of professional human players on the five F1Tenth tracks.									5	0	0	0	0	0	5			2377-3766											Tongji Univ, Sch Automot Studies, Shanghai 201804, Peoples R ChinaTech Univ Munich, Dept Informat, Munich, GermanyUniv Sci & Technol China, Wearable Robot & Autonomous Syst Lab, Hefei 230022, Peoples R China				2022-09-23	WOS:000853835700005		
B	Salvato, E.; Ghosh, A.; Fenu, G.; Parisini, T.										Control of a Mixed Autonomy Signalised Urban Intersection: An Action-Delayed Reinforcement Learning Approach								2021 IEEE International Intelligent Transportation Systems Conference (ITSC)								2042	7				10.1109/ITSC48978.2021.9564983							Conference Paper	2021	2021	We consider a mixed autonomy scenario where the traffic intersection controller decides whether the traffic light will be green or red at each lane for multiple traffic-light blocks. The objective of the traffic intersection controller is to minimize the queue length at each lane and maximize the outflow of vehicles over each block. We consider that the traffic intersection controller informs the autonomous vehicle (AV) whether the traffic light will be green or red for the future traffic-light block. Thus, the AV can adapt its dynamics by solving an optimal control problem. We model the decision process of the traffic intersection controller as a deterministic delayed Markov decision process owing to the delayed action by the traffic controller. We propose Reinforcement Learning based model-free algorithm to obtain the optimal policy. We show - by extensive simulations - that our algorithm converges and drastically reduces the energy costs of AVs as the traffic controller communicates with the AVs.					2021 IEEE International Intelligent Transportation Systems Conference (ITSC)2021 IEEE International Intelligent Transportation Systems Conference (ITSC)	19-22 Sept. 202119-22 Sept. 2021		Indianapolis, IN, USAIndianapolis, IN, USA	0	0	0	0	0	0	0					978-1-7281-9142-3									Dept. of Eng. & Archit., Univ. of Trieste, Trieste, ItalyDept. of Electr. & Electron. Eng., Imperial Coll. London, London, UKDept. of Electr. & Electron. Eng., Imperial Coll. London, London, UKDept. of Eng. & Archit., Univ. of Trieste, Trieste, ItalyKIOS Res. & Innovation Center of Excellence, Univ. of Cyprus, Nicosia, Cyprus				2022-01-07	INSPEC:21259511		
B	Franklin, D.M.; Martin, D.						Arai, K.; Bhatia, R.				eSense 2.0: modeling multi-agent biomimetic predation with multi-layered reinforcement learning								Advances in Information and Communication. Proceedings of the 2019 Future of Information and Communication Conference (FICC). Lecture Notes in Networks and Systems (LNNS 70)								466	78				10.1007/978-3-030-12385-7_35							Conference Paper	2020	2020	Learning in multi-agent systems, especially with adversarial behavior being exhibited, is difficult and challenging. The learning within these complicated environments is often muddied by the multitudinous conflicting or poorly correlated data coming from the multiple agents and their diverse goals. This should not be compared against well-known flocking-type behaviors where each agent has the same policy; rather, in our scenario each agent may have their own policy, sets of behaviors, or overall group strategy. Most learning algorithms will observe the actions of the agents and inform their algorithm which seeks to form the models. When these actions are consistent a reasonable model can be formed; however, eSense was designed to work even when observing complicated and highly-interactive must-agent behavior. eSense provides a powerful yet simplistic reinforcement learning algorithm that employs model-based behavior across multiple learning layers. These independent layers split the learning objectives across multiple layers, avoiding the learning-confusion common in many multi-agent systems. We examine a multi-agent predator-prey biomimetic sensing environment that simulates such coordinated and adversarial behaviors across multiple goals. This work could also be applied to theater wide autonomous vehicle coordination, such as that of the hierarchical command and control of autonomous drones and ground vehicles.					2019 Future of Information and Communication Conference (FICC)2019 Future of Information and Communication Conference (FICC)	14-15 March 201914-15 March 2019		San Francisco, CA, USASan Francisco, CA, USA	0	0	0	0	0	0	0					978-3-030-12384-0									Kennesaw State Univ., Marietta, GA, USAFac. of Sci. & Eng., Saga Univ., Saga, JapanSci. & Inf. (SAI) Organ., Bradford, UK				2020-08-24	INSPEC:19814053		
J	He, Xiangkun; Chen, Hao; Lv, Chen				He, Xiangkun/ABA-5268-2021	He, Xiangkun/0000-0001-9818-0879					Robust Multiagent Reinforcement Learning toward Coordinated Decision-Making of Automated Vehicles								SAE INTERNATIONAL JOURNAL OF VEHICLE DYNAMICS STABILITY AND NVH				7	4			475	488				10.4271/10-07-04-0031							Article	2023	2023	Automated driving is essential for developing and deploying intelligent transportation systems. However, unavoidable sensor noises or perception errors may cause an automated vehicle to adopt suboptimal driving policies or even lead to catastrophic failures. Additionally, the automated driving longitudinal and lateral decision-making behaviors (e.g., driving speed and lane changing decisions) are coupled, that is, when one of them is perturbed by unknown external disturbances, it causes changes or even performance degradation in the other. The presence of both challenges significantly curtails the potential of automated driving. Here, to coordinate the longitudinal and lateral driving decisions of an automated vehicle while ensuring policy robustness against observational uncertain-ties, we propose a novel robust coordinated decision-making technique via robust multiagent reinforcement learning. Specifically, the automated driving longitudinal and lateral decisions under observational perturbations are modeled as a constrained robust multiagent Markov decision process. Meanwhile, a nonlinear constraint setting with Kullback-Leibler divergence is developed to keep variation of the driving policy perturbed by stochastic perturbations within bounds. Additionally, robust multiagent policy optimization approach is proposed to approximate the optimal robust coordinated driving policy. Finally, we evaluate the proposed robust coordinated decision-making method in three highway scenarios with different traffic densities. Quantitatively, in the absence noises, the proposed method achieves an approximate average enhancement of 25.58% in traffic efficiency and 91.31% in safety compared to all baselines across the three scenarios. In the presence of noises, our technique improves traffic efficiency and safety by an approximate average of 30.81% and 81.02% compared to all baselines in the three scenarios, respectively. The results demonstrate that the proposed approach is capable of improving automated driving performance and ensuring policy robustness against observational uncertainties.									1	0	0	0	0	0	1			2380-2162	2380-2170										Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore, Singapore				2024-01-01	WOS:001121223500004		
C	Gonzalez, David Sierra; Dibangoye, Jilles Steeve; Laugier, Christian			IEEE							High-Speed Highway Scene Prediction Based on Driver Models Learned From Demonstrations								2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)								149	155											Proceedings Paper	2016	2016	One of the key factors to ensure the safe operation of autonomous and semi-autonomous vehicles in dynamic environments is the ability to accurately predict the motion of the dynamic obstacles in the scene. In this work, we show how to use a realistic driver model learned from demonstrations via Inverse Reinforcement Learning to predict the long-term evolution of highway traffic scenes. We model each traffic participant as a Markov Decision Process in which the cost function is a linear combination of static and dynamic features. In particular, the static features capture the preferences of the driver while the dynamic features, which change over time depending on the actions of the other traffic participants, capture the driver's risk-aversive behavior. Using such a model for prediction enables us to explicitly consider the interactions between traffic participants while keeping the computational complexity quadratic in the number of vehicles in the scene. Preliminary experiments in simulated and real scenarios show the capability of our approach to produce reliable, human-like scene predictions.					19th IEEE International Conference on Intelligent Transportation Systems (ITSC)19th IEEE International Conference on Intelligent Transportation Systems (ITSC)	NOV 01-04, 2016NOV 01-04, 2016	IEEEIEEE	Rio de Janeiro, BRAZILRio de Janeiro, BRAZIL	14	0	0	0	0	0	16					978-1-5090-1889-5									INRIA Grenoble Rhone Alpes, F-38330 Montbonnot St Martin, France	INRIA Grenoble Rhone Alpes			2017-02-15	WOS:000392215500025		
J	Everett, M.; Lutjens, B.; How, J.P.										Certified Adversarial Robustness for Deep Reinforcement Learning [arXiv]								arXiv								14 pp.	14 pp.											Journal Paper	11 April 2020	2020	Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certified defense for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends our previous paper with new performance guarantees, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.									0	0	0	0	0	0	0														Aerosp. Controls Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA				2020-06-26	INSPEC:19651426		
J	Manjanna, S.; Jiahao, T.Z.; Hsieh, M.A.										Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal Fluid Processes [arXiv]								arXiv																				Journal Paper	03 April 2023	2023	Persistent monitoring of a spatiotemporal fluid process requires data sampling and predictive modeling of the process being monitored. In this paper we present PASST algorithm: Predictive-model based Adaptive Sampling of a Spatio-Temporal process. PASST is an adaptive robotic sampling algorithm that leverages predictive models to efficiently and persistently monitor a fluid process in a given region of interest. Our algorithm makes use of the predictions from a learned prediction model to plan a path for an autonomous vehicle to adaptively and efficiently survey the region of interest. In turn, the sampled data is used to obtain better predictions by giving an updated initial state to the predictive model. For predictive model, we use Knowledged-based Neural Ordinary Differential Equations to train models of fluid processes. These models are orders of magnitude smaller in size and run much faster than fluid data obtained from direct numerical simulations of the partial differential equations that describe the fluid processes or other comparable computational fluids models. For path planning, we use reinforcement learning based planning algorithms that use the field predictions as reward functions. We evaluate our adaptive sampling path planning algorithm on both numerically simulated fluid data and real-world nowcast ocean flow data to show that we can sample the spatiotemporal field in the given region of interest for long time horizons. We also evaluate PASST algorithm's generalization ability to sample from fluid processes that are not in the training repertoire of the learned models.									0	0	0	0	0	0	0																		2023-07-07	INSPEC:23326018		
C	Yang, Tao; Nan, Zhixiong; Zhang, He; Chen, Shitao; Zheng, Nanning			IEEE							Traffic Agent Trajectory Prediction Using Social Convolution and Attention Mechanism								2020 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						278	283											Proceedings Paper	2020	2020	The trajectory prediction is significant for the decision-making of autonomous driving vehicles. In this paper, we propose a model to predict the trajectories of target agents around an autonomous vehicle. The main idea of our method is considering the history trajectories of the target agent and the influence of surrounding agents on the target agent. To this end, we encode the target agent history trajectories as an attention mask and construct a social map to encode the interactive relationship between the target agent and its surrounding agents. Given a trajectory sequence, the LSTM networks are firstly utilized to extract the features for all agents, based on which the attention mask and social map are formed. Then, the attention mask and social map are fused to get the fusion feature map, which is processed by the social convolution to obtain a fusion feature representation. Finally, this fusion feature is taken as the input of a variable-length LSTM to predict the trajectory of the target agent. We note that the variable-length LSTM enables our model to handle the case that the number of agents in the sensing scope is highly dynamic in traffic scenes. To verify the effectiveness of our method, we widely compare with several methods on a public dataset, achieving a 20% error decrease. In addition, the model satisfies the real-time requirement with the 32 fps.					31st IEEE Intelligent Vehicles Symposium (IV)31st IEEE Intelligent Vehicles Symposium (IV)	JUN 23-26, 2020JUN 23-26, 2020	IEEEIEEE	ELECTR NETWORKELECTR NETWORK	5	1	0	0	0	0	6			1931-0587		978-1-7281-6673-5									Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China				2020-01-01	WOS:000653124200045		
J	Guo, Wenfeng; Cao, Haotian; Zhao, Song; Wang, Jianqiang; Yi, Binlin; Song, Xiaolin				Cao, Haotian/J-5051-2019; Wang, Jianqiang/F-2806-2017	Cao, Haotian/0000-0001-5905-4116; 					Optimal design of a driver assistance controller based on surrounding vehicle?s social behavior game model								APPLIED MATHEMATICAL MODELLING				114				646	670				10.1016/j.apm.2022.10.010					OCT 2022		Article; Early Access		2023	Driver assistance control in the cut-in scenarios is challenging, since the controller needs to ensure driving safety and avoid unnecessary intervention, while considering the inter-action with surrounding vehicles. This paper proposes an optimal driver assistance con-troller considering the social behaviors of the surrounding vehicles to assist the drivers in the cut-in scenarios. To model the social behavior of the surrounding vehicle, we first for-mulate the interaction between the semi-autonomous vehicle and the surrounding vehicle as a misinformation game, which is achieved by assuming the surrounding vehicle is in-teracting with a hypothetical vehicle under the framework of non-cooperative game. Then, adaptive dynamic programming theory is utilized to find the Nash equilibrium, which is represented by deep neural networks and solved iteratively. Based on the established so-cial behavior model and the nonlinear driver-vehicle dynamic model, an affine input non-linear system model is obtained for the design of driver assistance controller, and the op-timal assistance control strategy is also derived under the structure of adaptive dynamic programming. Several numerical simulations and driver-in-the-loop simulator experiments are conducted for validation. Results show that the proposed strategy can assist the driver in the cut-in scenario while addressing different social interactions with the surround-ing vehicles. Importantly, by taking into account the surrounding vehicle's social behavior through our established social behavior model, our proposed strategy significantly outper-forms the no-interaction strategy both in terms of driving safety and intervention degree, validating its effectiveness and superiority.(c) 2022 Published by Elsevier Inc.									3	0	0	0	0	0	3			0307-904X	1872-8480										Hunan Univ, State Key Lab Adv Design & Mfg Vehicle Body, Changsha 410082, Peoples R ChinaUniv Waterloo, Dept Mech & Mechatron Engn, 200 Univ Ave W, Waterloo, ON N2L 3G1, CanadaTsinghua Univ, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R China				2022-11-13	WOS:000877521900004		
J	Chen, Long; Hu, Xuemin; Tian, Wei; Wang, Hong; Cao, Dongpu; Wang, Fei-Yue				liu, bing/JJD-5566-2023; Tian, Wei/T-8872-2017	Tian, Wei/0000-0002-5085-7219; Cao, Dongpu/0000-0003-2541-5272					Parallel Planning: A New Motion Planning Framework for Autonomous Driving								IEEE-CAA JOURNAL OF AUTOMATICA SINICA				6	1			236	246				10.1109/JAS.2018.7511186							Article	JAN 2019	2019	Motion planning is one of the most significant technologies for autonomous driving. To make motion planning models able to learn from the environment and to deal with emergency situations, a new motion planning framework called as "parallel planning" is proposed in this paper. In order to generate sufficient and various training samples, artificial traffic scenes are firstly constructed based on the knowledge from the reality. A deep planning model which combines a convolutional neural network (CNN) with the Long Short-Term Memory module (LSTM) is developed to make planning decisions in an end-toend mode. This model can learn from both real and artificial traffic scenes and imitate the driving style of human drivers. Moreover, a parallel deep reinforcement learning approach is also presented to improve the robustness of planning model and reduce the error rate. To handle emergency situations, a hybrid generative model including a variational auto-encoder (VAE) and a generative adversarial network (GAN) is utilized to learn from virtual emergencies generated in artificial traffic scenes. While an autonomous vehicle is moving, the hybrid generative model generates multiple video clips in parallel, which correspond to different potential emergency scenarios. Simultaneously, the deep planning model makes planning decisions for both virtual and current real scenes. The final planning decision is determined by analysis of real observations. Leveraging the parallel planning approach, the planner is able to make rational decisions without heavy calculation burden when an emergency occurs.									88	18	0	0	0	0	93			2329-9266	2329-9274										Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510275, Guangdong, Peoples R ChinaHubei Univ, Sch Comp Sci & Informat Engn, Wuhan 430062, Hubei, Peoples R ChinaKarlsruhe Inst Technol, Inst Measurement & Control Syst, D-76131 Karlsruhe, GermanyUniv Waterloo, Dept Mech & Mechatron Engn, 200 Univ Ave West, Waterloo, ON N2L 3G1, CanadaChinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing, Peoples R ChinaNatl Univ Def Technol, Mil Computat Expt & Parallel Syst Technol, Res Ctr, Changsha 410073, Hunan, Peoples R China				2019-01-25	WOS:000455705900022		
J	Ren, Yangang; Zhan, Guojian; Tang, Liye; Li, Shengbo Eben; Jiang, Jianhua; Li, Keqiang; Duan, Jingliang				Duan, Jingliang/JTV-0943-2023; Jiang, Jianhua/Z-1848-2018; Li, Shengbo/J-7662-2013	Jiang, Jianhua/0000-0002-9149-2922; Duan, Jingliang/0000-0002-3697-1576; Li, Shengbo/0000-0003-4923-3633; Ren, Yangang/0000-0002-1173-7230					Improve generalization of driving policy at signalized intersections with adversarial learning								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				152						104161			10.1016/j.trc.2023.104161					MAY 2023		Article; Early Access		2023	Intersections are quite challenging among various driving scenes wherein the interaction of signal lights and distinct traffic actors poses great difficulty to learn a wise and robust driving policy. Current research rarely considers the diversity of intersections and stochastic behaviors of traffic participants. For practical applications, the randomness usually leads to some devastating events, which should be the focus of autonomous driving. This paper introduces an adversarial learning paradigm to boost the intelligence and robustness of driving policy for signalized intersections with dense traffic flow. Firstly, we design a static path planner which is capable of generating trackable candidate paths for multiple intersections with diversified topology. Next, a constrained optimal control problem (COCP) is built based on these candidate paths wherein the bounded uncertainty of dynamic models is considered to capture the randomness of driving environment. We propose adversarial policy gradient (APG) to solve the COCP wherein the adversarial policy is introduced to provide disturbances by seeking the most severe uncertainty while the driving policy learns to handle this situation by competition. Finally, a comprehensive system is established to conduct training and testing wherein the perception module is introduced and the human experience is incorporated to solve the yellow light dilemma. Simulation results indicate that the trained policy can handle the signal lights flexibly meanwhile realizing smooth and efficient passing with a humanoid paradigm. Besides, APG enables a large-margin improvement of the resistance to the abnormal behaviors and thus ensures a high safety level for the autonomous vehicle.									0	0	0	0	0	0	0			0968-090X	1879-2359										Tsinghua Univ, Sch Vehicle & Mobil, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R ChinaUniv Sci & Technol Beijing, Sch Mech Engn, Beijing 100083, Peoples R ChinaChina Agr Univ, Coll Engn, Beijing 100083, Peoples R China				2023-09-02	WOS:001053548000001		
J	Graves, D.; Nguyen, N.M.; Hassanzadeh, K.; Jun Jin										Learning predictive representations in autonomous driving to improve deep reinforcement learning [arXiv]								arXiv								31 pp.	31 pp.											Journal Paper	26 June 2020	2020	Reinforcement learning using a novel predictive representation is applied to autonomous driving to accomplish the task of driving between lane markings where substantial benefits in performance and generalization are observed on unseen test roads in both simulation and on a real Jackal robot. The novel predictive representation is learned by general value functions (GVFs) to provide out-of-policy, or counter-factual, predictions of future lane centeredness and road angle that form a compact representation of the state of the agent improving learning in both online and offline reinforcement learning to learn to drive an autonomous vehicle with methods that generalizes well to roads not in the training data. Experiments in both simulation and the real-world demonstrate that predictive representations in reinforcement learning improve learning efficiency, smoothness of control and generalization to roads that the agent was never shown during training, including damaged lane markings. It was found that learning a predictive representation that consists of several predictions over different time scales, or discount factors, improves the performance and smoothness of the control substantially. The Jackal robot was trained in a two step process where the predictive representation is learned first followed by a batch reinforcement learning algorithm (BCQ) from data collected through both automated and human-guided exploration in the environment. We conclude that out-of-policy predictive representations with GVFs offer reinforcement learning many benefits in real-world problems.									0	0	0	0	0	0	0														Noahs Ark Lab., Huawei Technol. Canada, Ltd., Edmonton, AB, Canada				2020-11-20	INSPEC:20059325		
J	Wu, Zhenyu; Qiu, Kai; Gao, Hongbo										Driving policies of V2X autonomous vehicles based on reinforcement learning methods								IET INTELLIGENT TRANSPORT SYSTEMS				14	5			331	337				10.1049/iet-its.2019.0457							Article	MAY 2020	2020	Autonomous driving has been achieving great progress since last several years. However, the autonomous vehicles always ignore the important traffic information on the road because of the uncertainties of driving environment and the limitations of onboard sensors. This might cause serious safety problem in autonomous driving. This study argues that the connected vehicles could share much more environmental information with each other. Therefore, a decision-making method based on reinforcement learning is proposed for V2X autonomous vehicles. First, the V2X autonomous driving architecture with three subsystems is designed. By V2V communication, an autonomous vehicle could obtain much more environmental information. Second, a reinforcement learning based model is applied to learn from the V2V observation data. A simulation environment is setup based on OpenAI reinforcement learning framework. The experimental results demonstrate the effectiveness of the V2X in autonomous driving.									12	0	0	0	0	0	12			1751-956X	1751-9578										Nanjing Univ Posts & Telecommun, Sch Internet Things, 66 XinMoFan Rd, Nanjing, Peoples R ChinaUniv Sci & Technol China, Sch Informat Sci & Technol, 96 JinZhai Rd, Hefei, Peoples R China				2020-05-15	WOS:000530480700009		
C	Fischer, Johannes; Eyberg, Christoph; Werling, Moritz; Lauer, Martin			IEEE		Lauer, Martin/0000-0003-4414-5722					Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints								2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						791	798				10.1109/IROS51168.2021.9636672							Proceedings Paper	2021	2021	Planning for robotic systems is frequently formulated as an optimization problem. Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL). Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective. The consideration of safety constraints is of paramount importance for human-robot collaboration. For this reason, our work addresses maximum entropy IRL in constrained environments. Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs). (2) We transfer maximum entropy IRL to CMDPs based on CSRL. (3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching. In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	SEP 27-OCT 01, 2021SEP 27-OCT 01, 2021	IEEE; RSJIEEE; RSJ	ELECTR NETWORKELECTR NETWORK	4	0	0	0	0	0	4			2153-0858		978-1-6654-1714-3									Karlsruhe Inst Technol KIT, Inst Measurement & Control Syst, Karlsruhe, GermanyBMWGrp, Unterschleissheim, Germany	BMWGrp			2022-03-16	WOS:000755125500079		
J	Vijayakumar, V. A.; Shanthini, J.; Karthik, S.; Srihari, K.				Sampath, Shanthini/C-8428-2014						Route Planning for Autonomous Transmission of Large Sport Utility Vehicle								COMPUTER SYSTEMS SCIENCE AND ENGINEERING				45	1			659	669				10.32604/csse.2023.028400							Article	2023	2023	The autonomous driving aims at ensuring the vehicle to effectively sense the environment and use proper strategies to navigate the vehicle without the interventions of humans. Hence, there exist a prediction of the background scenes and that leads to discontinuity between the predicted and planned outputs. An optimal prediction engine is required that suitably reads the background objects and make optimal decisions. In this paper, the author(s) develop an autonomous model for vehicle driving using ensemble model for large Sport Utility Vehicles (SUVs) that uses three different modules involving (a) recognition mod-el, (b) planning model and (c) prediction model. The study develops a direct realization method for an autonomous vehicle driving. The direct realization method is designed as a behavioral model that incorporates three different modules to ensure optimal autonomous driving. The behavioral model includes recognition, planning and prediction modules that regulates the input trajectory processing of input video datasets. A deep learning algorithm is used in the proposed approach that helps in the classification of known or unknown objects along the line of sight. This model is compared with conventional deep learning classifiers in terms of recall rate and root mean square error (RMSE) to estimate its efficacy. Simulation results on different traffic environment shows that the Ensemble Convolutional Network Reinforcement Learning (E-CNN-RL) offers increased accuracy of 95.45%, reduced RMSE and increased recall rate than existing Ensemble Convolutional Neural Networks (CNN) and Ensemble Stacked CNN.									1	0	0	0	0	0	1			0267-6192											SNS Coll Technol, Coimbatore 641035, Tamil Nadu, India				2022-10-08	WOS:000860818300013		
J											HAZARD DETECTION AI SYSTEM FOR AUTONOMOUS VEHICLE																												Awarded Grant	Sep 30 2020	2020	Perform literature review.Develop a Perception AI system based on deep learning using publicly available data (e.g. KAIST Multi-Spectral Day/Night Data Set (Choi et al., 2018)) to: (a) Predict the intent for different road users e.g. pedestrians, cyclists etc. (b) Detect static road features e.g. road works, road signs etc. and (c) Detect weather condition e.g. fog, rain etc.Investigate the deep learning framework e.g. TensorFlow or PyTorch.Build, train, test and validate the neural network.Determine the performance of the AI in an unconstrained environment.Investigate reinforcement learning for continuous learning of the developed AI system.Develop a Hazard Detection AI system for AV using deep learning based on the output from the Perception AI system.Build, train, test and validate the neural network.Determine the performance of the Hazard Detection AI system system in an unconstrained environment.Investigate reinforcement learning for continuous learning of the developed Hazard Detection AI system.									0	0	0	0	0	0	0						2437901								Brunel University				2023-12-08	GRANTS:15347190		
J	JUNG,, Sangchul; Lee, Seungjae; Kim, Jooyoung										The Real-Time Signal Control System Using Reinforcement Learning Considering Priority Signaling for Emergency Vehicle			긴급차량 우선신호 부여를 고려한 강화학습 기반 지체 시간 최소화 실시간 신호 제어 체계 개발					Korean Society of Transportation	대한교통학회지			39	3			329	344											research-article	2021	2021	Recently, with the development of autonomous vehicles and V2X (Vehicle-to-Everyting) technology, researches are being conducted to reduce vehicle delay time and unnecessary stop-and-go phenomenon at the urban signalized intersection. In particular, current urban networks, in which the fixed-time traffic signal control system is dominant, can not reflect the real-time traffic situation in the signaling system. Even if the autonomous vehicle is commercialized, it could not exhibit its performance. At this time, if the artificial intelligence traffic signal control system based on accurate real-time vehicle information obtained through V2I technology is developed, more efficient traffic signal control will be possible and it will affect not only traffic flow but also the performance of the V2X technology-equipped vehicles. Especially, when there is unnecessary waiting time for the emergency vehicle in the urban intersection, it may affect the travel time to reach the destination. In case of driving while leaving the lane for the purpose of shortening the travel time, it can threaten the safety of a patient and surrounding vehicles. At this point, if the traffic signal control system that detects the entrance of the emergency vehicle to an intersection through the V2I technology and gives the priority signal to the emergency vehicle is developed, the safer emergency vehicle operation could be guaranteed. In this paper, the change of traffic flow according to the change of traffic signal control system is analyzed based on micro traffic information at single signalized intersection with V2I technology commercialization. A reinforcement learning model that expresses the optimum signal in real-time by learning the traffic information at the single signalized intersection was constructed based on the deep learning through the tensor flow. The performance of the developed traffic signal control system was verified through Vissim. The developed reinforcement learning model expresses a specific signal phase through real-time traffic information and expresses appropriate signal display in the changed network situation. The traffic signal control system developed in this paper is expected to contribute to achieve system optimization in a complex road network and to contribute to the analysis of traffic flow. In addition, it will contribute to building a smart city when autonomous vehicles are operated in the future V2X environment.				최근, 자율주행차량 및 V2X 기술의 개발에 따라 도심부 신호 교차로에서 차량의 지체시간과 불필요한 stop-and-go 현상을 줄이기 위한 연구가 시도되고 있다. 특히, 정주기식 신호 제어 체계가 주를 이루는 현재의 도시 네트워크에서는 실시간 교통상황을 신호체계에 반영할 수 없어 자율주행차량이 등장하여 상용화된다고 하더라도 그 성능을 모두 발휘할 수 없다. 이 때, V2I 기술을 통해 얻어지는 정확한 실시간 차량 정보를 바탕으로 한 인공지능 신호 제어 체계를 개발한다면 정주기식 신호 제어 체계에 비해 불필요한 신호 대기 시간을 감소시킬 수 있어 더욱 효율적인 신호제어가 가능하며 교통류 흐름뿐만 아니라 V2X 기술 탑재 차량의 성능 향상에도 영향을 미칠 것이다. 특히, 도심부 교차로에서 긴급 차량이 목적지까지 도달하는 데에는 불필요한 신호 대기 시간이 존재할 경우 현장까지의 통행 시간에 영향을 끼칠 수 있으며, 통행 시간 단축을 위해 차선을 이탈해가면서 운행할 경우 오히려 긴급 차량에 탑승한 환자 및 주변 차량의 안전을 위협할 수 있다. 이 때, V2I 기술을 통해 긴급 차량의 교차로 진입을 감지하고, 긴급 차량에 우선 신호를 부여하는 신호 제어 체계를 개발한다면 보다 안전한 긴급 차량의 운행을 보장할 수 있을 것이다. 본 논문에서는 V2I 기술 상용화를 전제로 단일 신호교차로에서 미시적 교통 정보를 바탕으로 신호 제어 체계의 변화에 따른 교통류 변화를 분석하였다. 단일 신호 교차로에서 이동류별 실시간 교통 정보를 학습하여 실시간으로 최적 신호를 표출하는 강화학습 모형을 구축하였으며, 개발한 신호 제어 체계의 성능은 미시적 교통 시뮬레이터인 Vissim을 통해 측정하였다. 개발한 강화학습 모형은 이동류별 실시간 교통 정보를 통해 특정한 신호를 표출하고, 변화된 네트워크 상황에 적절한 신호 현시를 표출한다. 본 논문에서 개발한 신호 제어 체계를 통해 복잡한 도로 네트워크에서 체계 최적을 달성할 수 있는 신호 제어 체계를 개발하는 바탕이 되고자 하며, 교통류 흐름을 분석하는 데 기여할 것으로 기대된다. 또한, 미래 V2X 환경에서 자율주행차량이 운영될 때 스마트시티를 구축하는 데 기여할 것이다.					0	0	0	0	0	0	0			1229-1366															2021-07-25	KJD:ART002726357		
J	Brunnbauer, A.; Berducci, L.; Brandstatter, A.; Lechner, M.; Hasani, R.; Rus, D.; Grosu, R.										Model-based versus Model-free Deep Reinforcement Learning for Autonomous Racing Cars [arXiv]								arXiv								12 pp.	12 pp.											Journal Paper	8 March 2021	2021	Despite the rich theoretical foundation of model-based deep reinforcement learning (RL) agents, their effectiveness in real-world robotics-applications is less studied and understood. In this paper, we, therefore, investigate how such agents generalize to real-world autonomous-vehicle control-tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional LiDAR sensors, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination, substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the observation-model choice. Finally, we provide extensive empirical evidence for the effectiveness of model-based agents provided with long enough memory horizons in sim2real tasks.									0	0	0	0	0	0	0														CPS, Tech. Univ. Wien (TU Wien), Vienna, AustriaInst. of Sci. & Technol. Austria, Klosterneuburg, AustriaCSAIL, Massachusetts Inst. of Technol., Cambridge, MA, USA				2021-05-22	INSPEC:20570157		
J	Chen, Sikai; Dong, Jiqian; Ha, Paul (Young Joun); Li, Yujie; Labi, Samuel				chen, zhuo/JXX-1337-2024; Li, YuJie/HGT-8657-2022; yujie, lei/JQW-7495-2023; Li, yu/HHZ-5236-2022; zhong, jing/KBP-7800-2024; Li, YuJie/JAC-4451-2023	Chen, Sikai/0000-0002-5931-5619; Ha, Young Joun/0000-0002-8511-8010					Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles								COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING				36	7	SI		838	857				10.1111/mice.12702							Article	JUL 2021	2021	A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment. The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements. Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations. Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control. The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques. Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources. Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor. In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor. We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp. For purposes of comparison, the proposed model is evaluated vis-a-vis traditional rule-based and long short-term memory-based fusion models. The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility. That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams. The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations. The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities.									80	3	0	0	0	0	84			1093-9687	1467-8667										Purdue Univ, Ctr Connected & Automated Transportat CCAT, W Lafayette, IN 47907 USAPurdue Univ, Lyles Sch Civil Engn, W Lafayette, IN 47907 USACarnegie Mellon Univ, Sch Comp Sci, Robot Inst, Pittsburgh, PA 15213 USA				2021-07-01	WOS:000659731700003		
J	Liu, Boyi; Wang, Lujia; Liu, Ming				Liu, Ming/AAC-9891-2020	Liu, Ming/0000-0002-4500-238X; Wang, Lujia/0000-0002-6710-4897					Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems								IEEE ROBOTICS AND AUTOMATION LETTERS				4	4			4555	4562				10.1109/LRA.2019.2931179							Article	OCT 2019	2019	This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments. To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL). In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud. Then, effective transfer learning methods in LFRL are introduced. LFRL is consistent with human cognitive science and fits well in cloud robotic systems. Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation. The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge. In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.									81	3	0	0	0	0	88			2377-3766											Chinese Acad Sci, Shenzhen Inst Adv Technol, Cloud Comp Lab, Shenzhen 518055, Peoples R ChinaUniv Chinese Acad Sci, Beijing 100190, Peoples R ChinaHong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China				2019-11-19	WOS:000494827600020		
J	Wu, Junta; Li, Huiyun										Deep Ensemble Reinforcement Learning with Multiple Deep Deterministic Policy Gradient Algorithm								MATHEMATICAL PROBLEMS IN ENGINEERING				2020						4275623			10.1155/2020/4275623							Article	JAN 22 2020	2020	Deep deterministic policy gradient algorithm operating over continuous space of actions has attracted great attention for reinforcement learning. However, the exploration strategy through dynamic programming within the Bayesian belief state space is rather inefficient even for simple systems. Another problem is the sequential and iterative training data with autonomous vehicles subject to the law of causality, which is against the i.i.d. (independent identically distributed) data assumption of the training samples. This usually results in failure of the standard bootstrap when learning an optimal policy. In this paper, we propose a framework of m-out-of-n bootstrapped and aggregated multiple deep deterministic policy gradient to accelerate the training process and increase the performance. Experiment results on the 2D robot arm game show that the reward gained by the aggregated policy is 10%-50% better than those gained by subpolicies. Experiment results on the open racing car simulator (TORCS) demonstrate that the new algorithm can learn successful control policies with less training time by 56.7%. Analysis on convergence is also given from the perspective of probability and statistics. These results verify that the proposed method outperforms the existing algorithms in both efficiency and performance.									8	1	0	0	0	0	10			1024-123X	1563-5147										Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518071, Peoples R China				2020-02-18	WOS:000510885700015		
B	Manchella, Kaushik										Flexpool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers & Goods Transportation																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798379831530									Purdue University, Indiana, United States	Purdue University				PQDT:84589011		
J	Bezerra, Carlos Daniel de Sousa; Vieira, Flavio Henrique Teles; Soares, Anderson da Silva					Vieira, Flavio/0000-0003-3572-4036					Deep-Q-Network hybridization with extended Kalman filter for accelerate learning in autonomous navigation with auxiliary security module								TRANSACTIONS ON EMERGING TELECOMMUNICATIONS TECHNOLOGIES				35	2					e4946			10.1002/ett.4946							Article	FEB 2024	2024	This article proposes an algorithm for autonomous navigation of mobile robots that mixes reinforcement learning with extended Kalman filter (EKF) as a localization technique, namely EKF-DQN, aiming to accelerate the maximization of the learning curve and improve the reward values obtained in the learning process. More specifically, Deep-Q-Networks (DQN) are used to control the trajectory of an autonomous robot in an environment with many obstacles. To improve navigation capability in this environment, we also propose a fusion of visual and nonvisual sensors. Due to the ability of EKF to predict states, this algorithm is used as a learning accelerator for the DQN network, predicting future states and inserting this information into the memory replay. Aiming to increase the safety of the navigation process, a visual safety system is also proposed to avoid collisions between the mobile robot and people circulating in the environment. The efficiency of the proposed control system is verified through computational simulations using the CoppeliaSIM simulator with code insertion in Python. The simulation results show that the EKF-DQN algorithm accelerates the maximization of rewards obtained and provides a higher success rate in fulfilling the mission assigned to the robot when compared to other value-based and policy-based algorithms. A demo video of the navigation system can be seen at: .This article proposes an algorithm for autonomous navigation of mobile robots that merges reinforcement learning with extended Kalman filter (EKF) as a localization technique, namely, EKF-DQN, aiming to accelerate learning and improve the reward values obtained in the process of apprenticeship. More specifically, deep neural networks (DQN-Deep-Q-Networks) are used to control the trajectory of an autonomous vehicle in an indoor environment. Due to the ability of EKF to predict states, this algorithm is proposed to be used as a learning accelerator of the DQN network, predicting states ahead and inserting this information in the memory replay. Aiming at the enhancing safety of the navigation process, it is also proposed a visual safety system that avoids collisions of the mobile vehicle with people moving in the environment. image									0	0	0	0	0	0	0			2161-3915											Fed Univ Goias UFG, Inst Informat INF, Goiania, GO, BrazilUniv Fed Goias UFG, Escola Engn Eletr Mecan & Comp EMC, Goiania, GO, BrazilUniv Fed Goias, Inst Informat, Campus Samambaia Alameda Palmeiras s-n, BR-74690900 Goiania, GO, Brazil				2024-02-10	WOS:001155169600001		
J	Pokhrel, Shiva Raj; Choi, Jinho				Pokhrel, Shiva/L-1160-2019; Choi, Jinho/X-4157-2018	Pokhrel, Shiva/0000-0001-5819-765X; Choi, Jinho/0000-0002-4895-6680					Understand-Before-Talk (UBT): A Semantic Communication Approach to 6G Networks								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				72	3			3544	3556				10.1109/TVT.2022.3219363							Article	MAR 2023	2023	In Shannon theory, semantic aspects of communication were identified but considered irrelevant to the technical communication problems. Semantic communication (SC) techniques have recently attracted renewed research interests in 6(th) generation (6G) wireless, because they have the capability to support an efficient interpretation of the significance and meaning intended by a sender (or accomplishment of the goal) when dealing with multi-modal data such as videos, images, audio, text messages, and so on, which would be the case for various applications such as intelligent transportation systems where each autonomous vehicle needs to deal with real-time videos and data from a number of sensors including radars. To this end, most of the emerging SC works focus on specific data types and employ sophisticated machine learning models including deep learning and neural networks. However, they could be impractical for multi-modal data possibly within a real-time constraint, relative to the purpose of the communication. A notable difficulty of existing SC frameworks lies in handling the discrete constraints imposed on the pursued semantic coding and its interaction with the independent knowledge-base, which makes reliable semantic extraction extremely challenging. Therefore, we develop a new hashing-based semantic extraction approach to SC framework, where our learning objective is to generate one time signatures (hash codes) using supervised learning for low latency, secure and efficient management of the SC dynamics. We first evaluate the proposed semantic extraction framework over large image data sets, extend it with domain adaptive hashing and then demonstrate the effectiveness of "semantics signature" in bulk transmission and multi-modal data.									3	0	0	0	0	0	3			0018-9545	1939-9359										Deakin Univ, Geelong, Vic 3220, Australia				2023-06-12	WOS:000966966300001		
J	Valiente, Rodolfo; Razzaghpour, Mahdi; Toghi, Behrad; Shah, Ghayoor; Fallah, Yaser P.				Razzaghpour, Mahdi/ABE-5064-2021	Razzaghpour, Mahdi/0000-0001-8970-4394; , Ghayoor/0000-0002-4791-0496					Prediction-Aware and Reinforcement Learning-Based Altruistic Cooperative Driving								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS													10.1109/TITS.2023.3323440					OCT 2023		Article; Early Access		2023	Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents' behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV's decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.									0	0	0	0	0	0	0			1524-9050	1558-0016										Univ Cent Florida, Connected & Autonomous Vehicle Res Lab CAVREL, Orlando, FL 32816 USA				2023-11-17	WOS:001096156200001		
J	Afifi, Haitham; Ramaswamy, Arunselvan; Karl, Holger					Afifi, Haitham/0000-0002-2602-5535					Reinforcement learning for autonomous vehicle movements in wireless multimedia applications								PERVASIVE AND MOBILE COMPUTING				92						101799			10.1016/j.pmcj.2023.101799					MAY 2023		Article	MAY 2023	2023	We develop a Deep Reinforcement Learning (DeepRL)-based, multi-agent algorithm to efficiently control autonomous vehicles that are typically used within the context of Wireless Sensor Networks (WSNs), in order to boost application performance. As an application example, we consider wireless acoustic sensor networks where a group of speakers move inside a room. In a traditional setup, microphones cannot move autonomously and are, e.g., located at fixed positions. We claim that autonomously moving microphones improve the application performance. To control these movements, we compare simple greedy heuristics against a DeepRL solution and show that the latter achieves best application performance. As the range of audio applications is broad and each has its own (subjective) per-formance metric, we replace those application metrics by two immediately observable ones: First, quality of information (QoI), which is used to measure the quality of sensed data (e.g., audio signal strength). Second, quality of service (QoS), which is used to measure the network's performance when forwarding data (e.g., delay). In this context, we propose two multi-agent solutions (where one agent controls one microphone) and show that they perform similarly to a single-agent solution (where one agent controls all microphones and has a global knowledge). Moreover, we show via simulations and theoretical analysis how other parameters such as the number of microphones and their speed impacts performance.(c) 2023 Elsevier B.V. All rights reserved.									0	0	0	0	0	0	0			1574-1192	1873-1589										Hasso Platter Inst, Internet Technol & Softwarizat, D-14482 Potsdam, GermanyKarlstad Univ, Dept Math & Comp Sci, S-65188 Karlstad, Sweden	Hasso Platter Inst			2023-06-29	WOS:001010863600001		
B	Ogbebor, JoshuaOnyeka										Distributed Control and Learning of Connected and Autonomous Vehicles Approaching and Departing Signalized Intersections																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798841531074									Louisiana State University and Agricultural & Mechanical College, Louisiana, United States	Louisiana State University and Agricultural & Mechanical College				PQDT:68663859		
J	Wu, Yukun; Wu, Xuncheng; Qiu, Siyuan; Xiang, Wenbin										A Method for High-Value Driving Demonstration Data Generation Based on One-Dimensional Deep Convolutional Generative Adversarial Networks								ELECTRONICS				11	21					3553			10.3390/electronics11213553							Article	NOV 2022	2022	As a promising sequential decision-making algorithm, deep reinforcement learning (RL) has been applied in many fields. However, the related methods often demand a large amount of time before they can achieve acceptable performance. While learning from demonstration has greatly improved reinforcement learning efficiency, it poses some challenges. In the past, it has required collecting demonstration data from controllers (either human or controller). However, demonstration data are not always available in some sparse reward tasks. Most importantly, there exist unknown differences between agents and human experts in observing the environment. This means that not all of the human expert's demonstration data conform to a Markov decision process (MDP). In this paper, a method of reinforcement learning from generated data (RLfGD) is presented, and consists of a generative model and a learning model. The generative model introduces a method to generate the demonstration data with a one-dimensional deep convolutional generative adversarial network. The learning model applies the demonstration data to the reinforcement learning process to greatly improve the effectiveness of training. Two complex traffic scenarios were tested to evaluate the proposed algorithm. The experimental results demonstrate that RLfGD is capable of obtaining higher scores more quickly than DDQN in both of two complex traffic scenarios. The performance of reinforcement learning algorithms can be greatly improved with this approach to sparse reward problems.									1	0	0	0	0	0	1				2079-9292										Shanghai Univ Engn Sci, Sch Mech & Automot Engn, Shanghai 201620, Peoples R China				2022-11-21	WOS:000881041700001		
J	Agarwal, T.; Arora, H.; Schneider, J.										Affordance-based Reinforcement Learning for Urban Driving [arXiv]								arXiv								15 pp.	15 pp.											Journal Paper	15 Jan. 2021	2021	Traditional autonomous vehicle pipelines that follow a modular approach have been very successful in the past both in academia and industry, which has led to autonomy deployed on road. Though this approach provides ease of interpretation, its generalizability to unseen environments is limited and hand-engineering of numerous parameters is required, especially in the prediction and planning systems. Recently, deep reinforcement learning has been shown to learn complex strategic games and perform challenging robotic tasks, which provides an appealing framework for learning to drive. In this work, we propose a deep reinforcement learning framework to learn optimal control policy using waypoints and low-dimensional visual representations, also known as affordances. We demonstrate that our agents when trained from scratch learn the tasks of lane-following, driving around inter-sections as well as stopping in front of other actors or traffic lights even in the dense traffic setting. We note that our method achieves comparable or better performance than the baseline methods on the original and NoCrash benchmarks on the CARLA simulator.									0	0	0	0	0	0	0														Robot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA				2021-04-18	INSPEC:20471901		
J									Ramasubramanian, Bhaskar		CRII: CPS: RUI: Cognizant Learning for Autonomous Cyber-Physical Systems																												Awarded Grant	Feb 15 2022	2022	This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).  &lt;br/&gt;&lt;br/&gt;The objective of this Computer and Information Science and Engineering (CISE) Research Initiation Initiative (CRII) proposal is to develop a cognizant learning framework for cyber-physical systems (CPS) that incorporates risk-sensitive and irrational decision making. The necessity for such a framework is exemplified by two observations. First, CPS such as self-driving cars will share an environment with other CPS and human users. Human drivers demonstrate a heightened sensitivity to changes in speed and can easily adapt to changes in the environment and road conditions, which makes it essential for a CPS to have an ability to recognize non-rational behaviors. Second, large amounts of data generated during their operation and limited access to models of their environments can make a CPS reliant on machine learning algorithms for decision making to meet performance requirements such as reachability and safety. Our research will be grounded on improving behaviors of autonomous vehicles in realistic traffic situations. Outcomes from this effort will contribute to the development of a research paradigm unifying control, learning, and behavioral economics. Students at a Primarily Undergraduate Institution will benefit by being directly involved in all aspects of the research process. Research tasks will involve a team of undergraduate students in a vertically integrated manner where more experienced students will mentor newer team members. &lt;br/&gt;&lt;br/&gt;The proposed effort comprises two thrusts. Thrust 1 will construct utilities to encode CPS performance objectives consistent with practical models of risk-sensitive and irrational decision making. Strategies will be learned by formulating and solving a reinforcement learning problem to maximize this utility. Methods to enable learned strategies to adequately consider delays between evaluation and execution of actions arising from the physical components of the CPS will be developed. Thrust 2 will design algorithms to learn decentralized cognizant strategies when multiple CPS operate in the same environment. To improve reliability in uncertain environments, or when feedback is sparse, techniques to identify contributions of each CPS to a shared utility will be identified. Solution methodologies will be evaluated empirically through extensive experiments and theoretically by determining probabilistic performance guarantees. The PI will develop a research agenda and new undergraduate curriculum in CPS and machine learning at Western Washington University (WWU). Research and educational goals of the project will be integrated through the CARLA simulator for autonomous vehicle research and the F1/10 Autonomous Vehicle platform. The multidisciplinary scope of the project will be emphasized in outreach efforts through Student Outreach Services and STEM Clubs at WWU to encourage and broaden participation from traditionally underrepresented student groups.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.									0	0	0	0	0	0	0						2153136								Western Washington University				2023-12-08	GRANTS:15517603		
B	Farid, Alec Jacob										Provably Safe Learning-Based Robot Control Via Anomaly Detection and Generalization Theory																												Dissertation/Thesis	Jan 01 2023	2023										0	0	0	0	0	0	0					9798379403836									Princeton University, Mechanical and Aerospace Engineering, New Jersey, United States	Princeton University				PQDT:82216683		
C	Lecerf, Ugo U. L.; Yemdji-Tchassi, Christelle C. Y.; Aubert, Sebastien S. A.; Michiardi, Pietro P. M.			ACM							Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios								PROCEEDINGS OF 2022 7TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING TECHNOLOGIES, ICMLT 2022								209	215				10.1145/3529399.3529432							Proceedings Paper	2022	2022	When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment. Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety. Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.In this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment. We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy. We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent. Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.We apply this method to an autonomous driving scenario and show that we are able to learn useful policies that would have otherwise been missed out on during training, and would have been unavailable to the agent when executing the control algorithm.					7th International Conference on Machine Learning Technologies (ICMLT)7th International Conference on Machine Learning Technologies (ICMLT)	MAR 11-12, 2022MAR 11-12, 2022		ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0					978-1-4503-9574-8									EURECOM, Renault Software Labs, Biot, FranceRenault Software Labs, Toulouse, FranceEURECOM, Biot, France	Renault Software Labs			2023-10-22	WOS:001053939400033		
J	Lee, Keuntaek; Isele, David; Theodorou, Evangelos A.; Bae, Sangjae				Lee, Keuntaek/GRE-7972-2022	Bae, Sangjae/0000-0001-7974-8203; Lee, Keuntaek/0000-0001-5955-5965					Spatiotemporal Costmap Inference for MPC Via Deep Inverse Reinforcement Learning								IEEE ROBOTICS AND AUTOMATION LETTERS				7	2			3194	3201				10.1109/LRA.2022.3146635							Article	APR 2022	2022	It can he difficult to autonomously produce driver behavior so that it appears natural to other traffic participants. Through Inverse Reinforcement Learning (IRL), we can automate this process by learning the underlying reward function from human demonstrations. We propose a new IRL algorithm that learns a goal-conditioned spatio-temporal reward function. The resulting costmap is used by Model Predictive Controllers (MPCs) to perform a task without any hand-designing or hand-tuning of the cost function. We evaluate our proposed Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL framework together with MPC in the CARLA simulator for autonomous driving, lane keeping, and lane changing tasks in a challenging dense traffic highway scenario. Our proposed methods show higher success rates compared to other baseline methods including behavior cloning, state-of-the-art RL policies, and MPC with a learning-based behavior prediction model.									6	1	0	0	0	0	6			2377-3766											Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USAHonda Res Inst USA Inc, Div Res, San Jose, CA 95110 USAGeorgia Inst Technol, Sch Aerosp Engn, Atlanta, GA 30318 USA				2022-02-28	WOS:000753553400005		
B	Hoel, Carl-Johan E.										Decision-Making in Autonomous Driving Using Reinforcement Learning																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798762121019									Chalmers Tekniska Hogskola (Sweden), Sweden	Chalmers Tekniska Hogskola (Sweden)				PQDT:50814519		
J	Liu, Jiahang; Huang, Zhenhua; Xu, Xin; Zhang, Xinglong; Sun, Shiliang; Li, Dazi				徐, 昕/JNS-1298-2023	徐, 昕/0000-0003-3238-745X; Sun, Shiliang/0000-0001-7069-3752; Li, Dazi/0000-0003-1610-6558; ZHANG, XINGLONG/0000-0002-0587-2487					Multi-Kernel Online Reinforcement Learning for Path Tracking Control of Intelligent Vehicles								IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS				51	11			6962	6975				10.1109/TSMC.2020.2966631							Article	NOV 2021	2021	Path tracking control of intelligent vehicles has to deal with the difficulties of model uncertainties and nonlinearities. As a class of adaptive optimal control methods, reinforcement learning (RL) has received increasing attention in solving difficult control problems. However, feature representation and online learning ability are two major problems to be solved for learning control of uncertain dynamic systems. In this article, we propose a multi-kernel online RL approach for path tracking control of intelligent vehicles. In the proposed approach, a multiple kernel feature learning framework is designed for online learning control based on dual heuristic programming (DHP) and the new online learning control algorithm is called multi-kernel DHP (MKDHP). In MKDHP, instead of the expert knowledge for selecting and fine-tuning of a suitable kernel function, only a set of basic kernel functions is required to be predefined and the multi-kernel features can be learned for value function approximation in the critic. The simulation studies on path tracking control for intelligent vehicles have been conducted under S-curve and urban road conditions. The results demonstrated that compared with other typical path tracking controllers for intelligent vehicles, such as the linear quadratic regulator (LQR), the pure pursuit controller and the ribbon-based controller, the proposed multi-kernel learning controller can achieve better performance in terms of tracking precision and smoothness.									19	1	0	0	0	0	20			2168-2216	2168-2232										Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R ChinaRocket Engn Design & Res Inst, Beijing 100011, Peoples R ChinaEast China Normal Univ, Dept Comp Sci & Technol, Shanghai 200241, Peoples R ChinaBeijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China	Rocket Engn Design & Res Inst			2021-10-27	WOS:000707443800040		
J	Ma Yining; Jiang Wei; Wu Jingyu; Chen Junyi; Li Nan; Xu Zhigang; Xiong Lu							马依宁; 姜为; 吴靖宇; 陈君毅; 李南; 徐志刚; 熊璐			Self-evolution Scenarios for Simulation Tests of Autonomous Vehicles Based on Different Models of Driving Styles			基于不同风格行驶模型的自动驾驶仿真测试自演绎场景研究				中国公路学报	China Journal of Highway and Transport				36	2			216	228	1001-7372(2023)36:2<216:JYBTFG>2.0.TX;2-2										Article	2023	2023	To verify the safety of the decision-making results of autonomous vehicles (AVs), a method for generating driving models with autonomous decision-making and interaction capabilities was proposed, and the driving models were as background vehicles (BVs) and used to build a self-evolution simulation scenario to test the continuous decision-making capability of AVs. First, based on reinforcement learning and a combination of inheritance and evolution ideas, different driving styles with autonomous decision-making and interaction capabilities were designed in this study. Second, in the model-building stage, three styles of driving models, namely, conservative, general, and aggressive, were generated and trained. The simulation training parameters for the general-style driving model were derived from the parameter distribution of a naturalistic driving dataset named highD to ensure fidelity. Finally, based on this, an aggressive-style driving model with significant aggressive features was designed and trained to enhance the complexity and testing effect of the self-evolution scenario. The results show that the distributions of parameters such as the car-following speed, distance headway, and lane-change moment time-to-collision obtained by using the highD dataset are in agreement with real data. An average similarity of 88% is observed between the general-style driving model generated and the corresponding real data, which is an improvement of 20.3% on the results obtained from the rule-based intelligent driver model (IDM). The proposed self-evolution scenario is seven times more testable than the baseline scenario composed of IDMs for the different driving models generated, as confirmed by the number of collisions in which the system under test is primarily responsible. Thus, self-evolution scenarios composed of the driving models designed and generated in this study can effectively support simulation tests for aiding the decision-making system in AVs.			为了验证自动驾驶汽车决策结果的安全性,提出一种具有自主决策和交互能力的行驶模型生成方法,该行驶模型作为背景车被用于构建自演绎仿真场景来测试自动驾驶汽车的连续决策能力。首先,以强化学习为基础、结合遗传与进化思想,创新地设计并生成了具有自主决策和交互能力的不同风格行驶模型;然后,在模型构建阶段分别训练生成了保守、普通和激进3种风格的行驶模型,其中普通风格行驶模型的训练参数来源于自然驾驶数据集highD的车辆参数分布,保证了该行驶模型的真实性;最后,在普通风格行驶模型的基础上设计并训练出了具有显著激进特征的激进风格行驶模型,以增强自演绎场景的复杂性和测试效果。结果表明:在模型真实性方面,以highD数据集中的跟车速度、车头间距、换道时刻下碰撞时间等参数的分布为真值,研究所生成的普通风格行驶模型的参数分布与真值的平均相似程度为88%,相较于基于规则的智能驾驶人模型(IDM)提升了20.3%;在场景测试性方面,以被测系统为主要责任方的碰撞次数为评估指标,研究生成的不同风格行驶模型所构成的自演绎场景的测试性约是由IDM构成的基线场景的7倍。因此,设计和生成的行驶模型所构成的自演绎场景可以有效支撑面向自动驾驶决策系统的仿真测试。						0	0	0	0	0	0	0			1001-7372											同济大学汽车学院, 上海 201804, 中国密歇根大学安娜堡分校航空航天工程学院, 安娜堡, MI48109长安大学信息工程学院, 西安, 陕西 710064, 中国School of Automotive Studies, Tongji University, Shanghai 201804, ChinaDepartment of Aerospace Engineering, University of Michigan, Ann Arbor, MI48109, USASchool of Information Engineering, Chang'an University, Xi'an, Shaanxi 710064, China	同济大学汽车学院密歇根大学安娜堡分校航空航天工程学院长安大学信息工程学院School of Automotive Studies, Tongji UniversityDepartment of Aerospace Engineering, University of MichiganSchool of Information Engineering, Chang'an University			2023-07-01	CSCD:7430318		
J	Lambert, Reeve; Li, Jianwen; Wu, Li-Fan; Mahmoudian, Nina										Robust ASV Navigation Through Ground to Water Cross-Domain Deep Reinforcement Learning								FRONTIERS IN ROBOTICS AND AI				8						739023			10.3389/frobt.2021.739023							Article	SEP 20 2021	2021	This paper presents a framework to alleviate the Deep Reinforcement Learning (DRL) training data sparsity problem that is present in challenging domains by creating a DRL agent training and vehicle integration methodology. The methodology leverages accessible domains to train an agent to solve navigational problems such as obstacle avoidance and allows the agent to generalize to challenging and inaccessible domains such as those present in marine environments with minimal further training. This is done by integrating a DRL agent at a high level of vehicle control and leveraging existing path planning and proven low-level control methodologies that are utilized in multiple domains. An autonomy package with a tertiary multilevel controller is developed to enable the DRL agent to interface at the prescribed high control level and thus be separated from vehicle dynamics and environmental constraints. An example Deep Q Network (DQN) employing this methodology for obstacle avoidance is trained in a simulated ground environment, and then its ability to generalize across domains is experimentally validated. Experimental validation utilized a simulated water surface environment and real-world deployment of ground and water robotic platforms. This methodology, when used, shows that it is possible to leverage accessible and data rich domains, such as ground, to effectively develop marine DRL agents for use on Autonomous Surface Vehicle (ASV) navigation. This will allow rapid and iterative agent development without the risk of ASV loss, the cost and logistic overhead of marine deployment, and allow landlocked institutions to develop agents for marine applications.									1	0	0	0	0	0	1			2296-9144											Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA				2021-11-29	WOS:000721614900001	34616776	
J	Lombard, Alexandre; Noubli, Ahmed; Abbas-Turki, Abdeljalil; Gaud, Nicolas; Galland, Stephane				Gaud, Nicolas/C-6923-2011; Galland, Stephane/F-3789-2011	Gaud, Nicolas/0000-0001-8151-8650; Galland, Stephane/0000-0002-1559-7861					Deep Reinforcement Learning Approach for V2X Managed Intersections of Connected Vehicles								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	7			7178	7189				10.1109/TITS.2023.3253867					MAR 2023		Article; Early Access		2023	Intersections are major bottlenecks for road traffic, as well as the origin of many accidents. Efficient management of traffic at intersections is required to ensure both safety and efficiency. Yet, the traditional solutions (static signs, traffic lights) are limited in their efficiency as they consider the flow of vehicles and not the vehicles at the microscopic level. By using inter-vehicular communication of connected vehicles, recent works have shown the possibility to have a great increase in the number of evacuated vehicles thanks to the possibility to give an individual right-of-way directly to each vehicle. In this context of intersections of cooperative vehicles, the scheduling of this right-of-way in order to maximize the throughput of the intersection is still a challenging task, with regard to the hybrid and dynamic aspects of the problem. In this paper, we propose an approach based on Deep Reinforcement Learning (DRL) to efficiently distribute the right-of-way to each vehicle. A Markov Decision Process model of intersections of cooperative vehicles, enabling the application of DRL, is proposed. The performance of the DRL-based scheduling is then compared with classic traffic lights, and with two state-of-the-art cooperative scheduling policies, showing the benefits of the approach (increase of the flow, reduction of CO2 emissions).									1	0	0	0	0	0	1			1524-9050	1558-0016										Univ Technol Belfort Montbeliard UTBM, Lab Connaissance & Intelligence Artificielle Dist, Belfort F-90010, France				2023-04-05	WOS:000953462000001		
J	Feng, Wenhao; Ding, Liang; Zhou, Ruyi; Xu, Chongfu; Yang, Huaiguang; Gao, Haibo; Liu, Guangjun; Deng, Zongquan				wang, xiaoqiang/JMT-2783-2023; wei, xiao/ISB-6027-2023	Zhou, Ruyi/0000-0002-1854-9451					Learning-Based End-to-End Navigation for Planetary Rovers Considering Non-Geometric Hazards								IEEE ROBOTICS AND AUTOMATION LETTERS				8	7			4084	4091				10.1109/LRA.2023.3281261							Article	JUL 2023	2023	Autonomous navigation plays an increasingly crucial role in rover-based planetary missions. End-to-end navigation approaches developed upon deep reinforcement learning have enabled great adaptability in complex environments. However, most existing works focus on geometric obstacle avoidance thus have limited capability to cope with ubiquitous non-geometric hazards, such as sinkage and slippage. Autonomous navigation in unstructured harsh environments remains a great challenge requiring further in-depth study. In this letter, a DRL-based navigation method is proposed to autonomously guide a planetary rover towards goals via hazard-free paths with low wheel slip ratios. We introduce an end-to-end network architecture, in which the visual perception and the wheel-terrain interaction are fused to learn the representation of terrain mechanical properties implicitly and further facilitate policy learning for non-geometric hazard avoidance. Our approach outperforms baseline methods in simulation evaluation with superior avoidance capabilities against geometric obstacles and non-geometric hazards. Experiments conducted at a Mars emulation site suggest the successful deployment of our approach on a planetary rover prototype and the capacity of dealing with locomotion risks in real-world navigation tasks.									0	0	0	0	0	0	0			2377-3766											Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150080, Peoples R ChinaToronto Metropolitan Univ, Dept Aerosp Engn, Toronto, ON M5B 2K3, Canada				2023-07-08	WOS:001004297800004		
J	Tze-Yang Tung; Pujol, J.R.; Kobus, S.; Gunduz, D.										A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning over Noisy Channels [arXiv]								arXiv								30 pp.	30 pp.											Journal Paper	2 Jan. 2021	2021	We propose a novel formulation of the "effectiveness problem" in communications, put forth by Shannon and Weaver in their seminal work [2], by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework. Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment can also communicate with each other over a noisy communication channel. The noisy communication channel is considered explicitly as part of the dynamics of the environment and the message each agent sends is part of the action that the agent can take. As a result, the agents learn not only to collaborate with each other but also to communicate "effectively" over a noisy channel. This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the "learning to communicate" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free. We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP. This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems.									0	0	0	0	0	0	0														Dept. of Electr. & Electron. Eng., Imperial Coll. London, London, UK				2021-04-03	INSPEC:20415094		
C	Koren, Mark; Nassar, Ahmed; Kochenderfer, Mykel J.			IEEE	Kochenderfer, Mykel/E-7069-2010	Kochenderfer, Mykel/0000-0002-7238-9663					Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm								2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)		IEEE International Conference on Intelligent Robots and Systems						5944	5949				10.1109/IROS51168.2021.9636072							Proceedings Paper	2021	2021	Validating the safety of autonomous systems generally requires the use of high-fidelity simulators that adequately capture the variability of real-world scenarios. However, it is generally not feasible to exhaustively search the space of simulation scenarios for failures. Adaptive stress testing (AST) is a method that uses reinforcement learning to find the most likely failure of a system. AST with a deep reinforcement learning solver has been shown to be effective in finding failures across a range of different systems. This approach generally involves running many simulations, which can be very expensive when using a high-fidelity simulator. To improve efficiency, we present a method that first finds failures in a low-fidelity simulator. It then uses the backward algorithm, which trains a deep neural network policy using a single expert demonstration, to adapt the low-fidelity failures to high-fidelity. We have created a series of autonomous vehicle validation case studies that represent some of the ways low-fidelity and high-fidelity simulators can differ, such as time discretization. We demonstrate in a variety of case studies that this new AST approach is able to find failures with significantly fewer high-fidelity simulation steps than are needed when just running AST directly in high-fidelity. As a proof of concept, we also demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art high-fidelity simulator for finding failures in autonomous vehicles.					IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	SEP 27-OCT 01, 2021SEP 27-OCT 01, 2021	IEEE; RSJIEEE; RSJ	ELECTR NETWORKELECTR NETWORK	7	0	0	0	0	0	7			2153-0858		978-1-6654-1714-3									Stanford Univ, Aeronaut & Astronaut, Stanford, CA 94305 USANVIDIA, Santa Clara, CA 95051 USA				2022-03-16	WOS:000755125504110		
C	Ozkan, Mehmet Fatih; Ma, Yao			IEEE							Personalized Adaptive Cruise Control and Impacts on Mixed Traffic								2021 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						412	417											Proceedings Paper	2021	2021	This paper presents a personalized adaptive cruise control (PACC) design that can learn human driver behavior and adaptively control the semi-autonomous vehicle (SAV) in the car-following scenario, and investigates its impacts on mixed traffic. In mixed traffic where the SAV and human-driven vehicles share the road, the SAV's driver can choose a PACC tuning that better fits the driver's preferred driving strategies. The individual driver's preferences are learned through the inverse reinforcement learning (IRL) approach by recovering a unique cost function from the driver's demonstrated driving data that best explains the observed driving style. The proposed PACC design plans the motion of the SAV by minimizing the learned unique cost function considering the short preview information of the preceding human-driven vehicle. The results reveal that the learned driver model can identify and replicate the personalized driving behaviors accurately and consistently when following the preceding vehicle in a variety of traffic conditions. Furthermore, we investigated the impacts of the PACC with different drivers on mixed traffic by considering time headway, gap distance, and fuel economy assessments. A statistical investigation shows that the impacts of the PACC on mixed traffic vary among tested drivers due to their intrinsic driving preferences.					American Control Conference (ACC)American Control Conference (ACC)	MAY 25-28, 2021MAY 25-28, 2021	Amer Automat Control Council; Mitsubishi Elect Res Lab; Halliburton; MathWorks; Wiley; GE Global Res; Soc Ind Appl Math; dSPACE; Tangibles That Teach; Elsevier; GMAmer Automat Control Council; Mitsubishi Elect Res Lab; Halliburton; MathWorks; Wiley; GE Global Res; Soc Ind Appl Math; dSPACE; Tangibles That Teach; Elsevier; GM	ELECTR NETWORKELECTR NETWORK	6	0	0	0	0	0	6			0743-1619	2378-5861	978-1-6654-4197-1									Texas Tech Univ, Dept Mech Engn, Lubbock, TX 79409 USA				2021-10-29	WOS:000702263300066		
J	Hu, Chaofang; Zhao, Lingxue; Qu, Ge										Event-Triggered Model Predictive Adaptive Dynamic Programming for Road Intersection Path Planning of Unmanned Ground Vehicle								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				70	11			11228	11243				10.1109/TVT.2021.3111692							Article	NOV 2021	2021	Autonomous driving of unmanned ground vehicle (UGV) at road intersection is a challenging task due to the complicated traffic conditions. In this paper, an event-triggered model predictive adaptive dynamic programming (MPADP) algorithm is proposed for path planning of UGV at road intersection. Following the critic-actor scheme of adaptive dynamic programming (ADP), cost function approximation and control policy generation are combined to formulate MPADP. The infinite horizon cost function of ADP is stacked over predictive horizon of model predictive control (MPC), and then the infinite horizon cost function is converted to the finite horizon-stacked cost function in MPADP. By minimizing the approximation error within predictive horizon, the approximation accuracy is enhanced. Considering the limitation of energy consumption, the event-triggered mechanism is designed based on the mismatch of cost function approximation. Three triggering conditions are designed, and the corresponding boundedness of approximation error is proved. Simulation results illustrate the effectiveness, efficiency and feasibility in application of the event-triggered MPADP method for path planning at road intersection.									11	0	0	0	0	0	11			0018-9545	1939-9359										Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China				2021-12-03	WOS:000720520400012		
J	Yang, Haodong; Yao, Chenpeng; Liu, Chengju; Chen, Qijun					Yang, Haodong/0009-0007-9104-9430					RMRL: Robot Navigation in Crowd Environments With Risk Map-Based Deep Reinforcement Learning								IEEE ROBOTICS AND AUTOMATION LETTERS				8	12			7930	7937				10.1109/LRA.2023.3322093							Article	DEC 2023	2023	Achieving safe and effective navigation in crowds is a crucial yet challenging problem. Recent work has mainly encoded the pedestrian-robot state pairs, which cannot fully capture the interactions among humans. Besides, existing work attempts to achieve "hard" collision avoidance, which may leave no feasible path to the robot in human-rich scenarios. We suppose that this can be addressed by introducing the local risk map and thus incorporate the risk map into the deep reinforcement learning architecture. The proposed map structure contains the crowd interaction states and geometric information. Meanwhile, a "soft" risk mapping of pedestrians is proposed to promote the robot to generate more humanlike motion patterns, and the riskaware dynamic window is designed to enhance the robot's obstacle avoidance ability. Experiments show that our method outperforms the baseline in terms of navigation performance and social attributes. Furthermore, we successfully validate the proposed policy through real-world environments.									0	0	0	0	0	0	0			2377-3766											Tongji Univ, Dept Elect & Informat Engn, Shanghai 201804, Peoples R China				2023-11-09	WOS:001087723700005		
J	Huang, Mengzhe; Gao, Weinan; Wang, Yebin; Jiang, Zhong-Ping				Gao, Weinan/ABA-9794-2021; Wang, Ye/GWQ-7482-2022; Jiang, Zhong-Ping/AAH-4981-2019; Huang, Mengzhe/AAB-9921-2019	Gao, Weinan/0000-0001-7921-018X; Jiang, Zhong-Ping/0000-0002-4868-9359; Huang, Mengzhe/0000-0002-5670-8193; Wang, Yebin/0000-0001-7209-7866					Data-Driven Shared Steering Control of Semi-Autonomous Vehicles								IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS				49	4			350	361				10.1109/THMS.2019.2900409							Article	AUG 2019	2019	This paper presents a cooperative/shared framework of the driver and his/her semi-autonomous vehicle in order to achieve desired steering performance. In particular, a copilot controller and the driver together operate and control the vehicle. Exploiting the classical small-gain theory, our proposed shared steering controller is developed independent of the unmeasurable internal states of the human driver, and only relies on his/her steering torque. Furthermore, by adopting data-driven adaptive dynamic programming and an iterative learning scheme, the shared steering controller is studied from the measurable data of the driver and the vehicle. Meanwhile, the accurate knowledge of the driver and the vehicle dynamics is unnecessary, which settles the problem of their potential parametric variations/uncertainties in practice. The effectiveness of the proposed method is validated by rigorous analysis and demonstrated by numerical simulations.									35	6	0	0	0	0	39			2168-2291	2168-2305										NYU, Control & Networks Lab, Dept Elect & Comp Engn, Tandon Sch Engn, Brooklyn, NY 11201 USAGeorgia Southern Univ, Dept Elect & Comp Engn, Allen E Paulson Coll Engn & Comp, Statesboro, GA 30460 USAMitsubishi Elect Res Labs, Cambridge, MA 02139 USA	Mitsubishi Elect Res Labs			2019-08-08	WOS:000476755000006		
J	Dong, Jiqian; Chen, Sikai; Li, Yujie; Du, Runjia; Steinfeld, Aaron; Labi, Samuel				chen, zhuo/JXX-1337-2024; Li, YuJie/HGT-8657-2022; Steinfeld, Aldo/B-8869-2008; Dong, Jiqian/JDM-3216-2023; yujie, lei/JQW-7495-2023; zhong, jing/KBP-7800-2024; Li, YuJie/JAC-4451-2023; Li, yu/HHZ-5236-2022	Steinfeld, Aldo/0000-0001-7797-686X; Dong, Jiqian/0000-0002-2924-5728; Chen, Sikai/0000-0002-5931-5619					Space-weighted information fusion using deep reinforcement learning: The context of tactical control of lane-changing autonomous vehicles and connectivity range assessment								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				128						103192			10.1016/j.trc.2021.103192					MAY 2021		Article; Early Access		2021	The connectivity aspect of connected autonomous vehicles (CAV) is beneficial because it facilitates dissemination of traffic-related information to vehicles through Vehicle-to-External (V2X) communication. Onboard sensing equipment including LiDAR and camera can reasonably characterize the traffic environment in the immediate locality of the CAV. However, their performance is limited by their sensor range (SR). On the other hand, longer-range information is helpful for characterizing imminent conditions downstream. By contemporaneously coalescing the shortand long-range information, the CAV can construct comprehensively its surrounding environment and thereby facilitate informed, safe, and effective movement planning in the shortterm (local decisions including lane change) and long-term (route choice). Current literature provides useful information on CAV control approaches that use only local information sensed from the proximate traffic environment but relatively little guidance on how to fuse this information with that obtained from downstream sources and from different time stamps, and how to use the fused information to enhance CAV movements. In this paper, we describe a Deep Reinforcement Learning based approach that integrates the data collected through sensing and connectivity capabilities from other vehicles located in the proximity of the CAV and from those located further downstream, and we use the fused data to guide lane changing, a specific context of CAV operations. In addition, recognizing the importance of the connectivity range (CR) to the performance of not only the algorithm but also of the vehicle in the actual driving environment, the study carried out a case study. The case study demonstrates the application of the proposed algorithm and duly identifies the appropriate CR for each level of prevailing traffic density. It is expected that implementation of the algorithm in CAVs can enhance the safety and mobility associated with CAV driving operations. From a general perspective, its implementation can provide guidance to connectivity equipment manufacturers and CAV operators, regarding the default CR settings for CAVs or the recommended CR setting in a given traffic environment.									23	1	0	0	1	0	24			0968-090X	1879-2359										Purdue Univ, Ctr Connected & Automated Transportat CCAT, W Lafayette, IN 47907 USAPurdue Univ, Lyles Sch Civil Engn, W Lafayette, IN 47907 USACarnegie Mellon Univ, Inst Robot, Sch Comp Sci, Pittsburgh, PA 15213 USA				2021-07-09	WOS:000664746600004		
J											Adversarial scenario curriculums for navigation and scene understanding																												Awarded Grant	Sep 30 2020	2020	Brief description of the context of the research including potential impact:Autonomous vehicles are increasingly reliant on deep-learned solutions for many tasks within scene understanding and navigation. While broadly improving effectiveness, these solutions often cannot guarantee robustness, whichcontrasts with the safety-critical nature of these applications. Many rare challenging scenarios are likely to be absent from training sets but could lead to unexpected behaviours and disastrous outcomes when encountered in practice.The ability to generate synthetic examples of such rare and adversarial scenarios for a given system during learning will help assess its robustness and improve it.Aims and Objectives:This work will create a framework to automatically discover challenging scenarios that have not been observed in an autonomous vehicle system's training set. The framework will be able to understand how to synthesise an autonomous vehicle scenario along several &quot;axes&quot; of difficulty - which we propose to include the appearance of the surroundings and the configuration of a scene.Moreover, we propose a curriculum-based method to incorporate these synthesised scenarios into the training of the studied systems to improve their performance and robustness before deployment as compared to standard neural network training techniques.Novelty of the research methodology:While many research areas relevant to our project are well explored, they often consider specific components of a challenging scenario (e.g. adversarial attacks for image classification).However, the framework we envision will contain many interacting aspects, such as modelling distributions, synthesising data, efficiently searching for adversarial scenarios, or augmenting training.To develop it, we will combine ideas from generative modelling, active learning, simulation, reinforcement learning, adversarial attacks, and curriculum learning. Our task will include devising how existing methods can interact and combining ideas from different fields into a new framework. Alignment to EPSRC's strategies and research areas:This project relates to the EPSRC's research areas: robotics, artificial intelligence technologies,engineering design, software engineering, and verification and correctness.Companies or collaborators involved:We will conduct the project in collaboration with industry-partner Oxbotica									0	0	0	0	0	0	0						2420376								University of Oxford				2023-12-08	GRANTS:15340612		
J	Cheng, Yanqiu; Hu, Xianbiao; Chen, Kuanmin; Yu, Xinlian; Luo, Yulong					Yu, Xinlian/0000-0001-8418-4377					Online longitudinal trajectory planning for connected and autonomous vehicles in mixed traffic flow with deep reinforcement learning approach								JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS				27	3			396	410				10.1080/15472450.2022.2046472					FEB 2022		Article; Early Access		2023	This manuscript presents an Adam optimization-based Deep Reinforcement Learning model for Mixed Traffic Flow control (ADRL-MTF), so as to guide Connected and Autonomous vehicle's (CAV) longitudinal trajectory on a typical urban roadway with signal-controlled intersections. Two improvements are made when compared with prior literatures. First, the common assumptions to simplify the problem solving, such as dividing a vehicle trajectory into several segments with constant acceleration/deceleration, are avoided, to improve the modeling realism. Second, built on the efficient Adam Optimization and Deep Q-Learning, the proposed mod& avoids the enumeration of states and actions, and is computational efficient and suitable for real time applications. The mixed traffic flow dynamic is firstly formulated as a finite Markov decision process (MDP) model. Due to the discretization of time, space and speed, this MDP model becomes high-dimensional in state, and is very challenging to solve. We then propose a temporal difference-based deep reinforcement learning approach, with E-greedy for exploration-exploitation balance. Two neural networks are developed to replace the traditional Q function and generate the targets in the Q-learning update. These two neural networks are trained by the Adam optimization algorithm, which extends stochastic gradient descent and considers the second moments of the gradients, and is thus highly computational efficient and has lower memory requirements. The proposed model is shown to reduce fuel consumption by 7.8%, which outperforms a prior benchmark model based on Monte Carlo Tree Search. The model's runtime efficiency and stability are tested, and the sensitivity analysis is also performed.									29	3	0	0	3	0	32			1547-2450	1547-2442										Changan Univ, Coll Transportat Engn, Dept Traff Engn, Xian, Shaanxi, Peoples R ChinaMissouri Univ Sci & Technol, Dept Civil Architectural & Environm Engn, Rolla, MO 65409 USAPenn State Univ, Dept Civil & Environm Engn, University Pk, PA 16802 USASoutheast Univ, Sch Transportat Engn, Nanjing, Peoples R ChinaGuangdong Univ Technol, Sch Architecture & Urban Planning, Guangzhou, Guangdong, Peoples R China				2022-03-18	WOS:000763228900001		
C	Masmitja, Ivan; Martin, Mario; Katija, Kakani; Gomariz, Spartacus; Navarro, Joan			IEEE	Masmitja, Ivan/AFQ-6426-2022; Navarro, Joan/C-2119-2009	Masmitja, Ivan/0000-0001-6355-7955; Katija, Kakani/0000-0002-7249-0147; Martin, Mario/0000-0002-4125-6630; Navarro, Joan/0000-0002-5756-9543					A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles								2022 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING (CASE)		IEEE International Conference on Automation Science and Engineering						675	682				10.1109/CASE49997.2022.9926499							Proceedings Paper	2022	2022	Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems. Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position. Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption. To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms. Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies. The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g. the median predicted error at the beginning of the target's localisation is 17% less. These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles. This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios.					IEEE 18th International Conference on Automation Science and Engineering (IEEE CASE)IEEE 18th International Conference on Automation Science and Engineering (IEEE CASE)	AUG 20-24, 2022AUG 20-24, 2022	IEEEIEEE	Mexico City, MEXICOMexico City, MEXICO	3	0	0	0	0	0	3			2161-8070		978-1-6654-9042-9									Mbari, Bioinspirat Lab, Moss Landing, CA 95062 USACSIC, Inst Ciencies Mar, Barcelona 08003, SpainUniv Politecn Cataluna, Knowledge Engn & Machine Learning Grp, Barcelona Tech, Barcelona 08034, SpainUniv Politecn Cataluna, SARTI Res Grp, Barcelona Tech, Elect Dept, Barcelona 080934, Spain				2023-03-04	WOS:000927622400075		
C	Jiang, Shenghao; Chen, Jiying; Shen, Macheng			IEEE	Chen, Jiying/AAV-8256-2020	Chen, Jiying/0000-0002-0874-5192					An Interactive Lane Change Decision Making Model With Deep Reinforcement Learning								2019 IEEE 7TH INTERNATIONAL CONFERENCE ON CONTROL, MECHATRONICS AND AUTOMATION (ICCMA 2019)								370	376				10.1109/iccma46720.2019.8988750							Proceedings Paper	2019	2019	By considering lane change maneuver as primarily a Partial Observed Markov Decision Process (POMDP) and motion planning problem, this paper presents an interactive model with a Recurrent Neural Network (RNN) approach to determine the adversarial or cooperative intention probability of following vehicle in target lane. To make proper and efficient lane change decision, Deep Q-value network (DQN) is applied to solve POMDP with expected global maximum reward. Then quintic polynomials-based motion planning algorithm is used to obtain both optimal lateral and longitudinal trajectory for autonomous vehicle to pursuit. Experimental results demonstrate the capability of the proposed model to execute lane change maneuver with comfortable and safety reference trajectory at an appropriate time instance and traffic gap in various highway traffic scenarios.					7th IEEE International Conference on Control, Mechatronics and Automation(ICCMA)7th IEEE International Conference on Control, Mechatronics and Automation(ICCMA)	NOV 06-08, 2019NOV 06-08, 2019	IEEEIEEE	Delft Univ Technol, Delft, NETHERLANDSDelft Univ Technol, Delft, NETHERLANDS	13	0	0	0	0	0	14					978-1-7281-3787-2									Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USAChina Univ Petr, Coll New Energy, Qingdao, Peoples R ChinaMIT, Dept Mech Engn, Cambridge, MA 02139 USA				2020-07-14	WOS:000543726100064		
J	Zhang, Han; Liang, Hongbin; Wang, Lei; Yao, Yiting; Lin, Bin; Zhao, Dongmei					zhang, han/0009-0009-4622-5065					Joint resource allocation and security redundancy for autonomous driving based on deep reinforcement learning algorithm								IET INTELLIGENT TRANSPORT SYSTEMS													10.1049/itr2.12489					FEB 2024		Article; Early Access		2024	Autonomous vehicles navigating urban roads require technology that combines low latency with high computing power. The limited resources of the vehicle itself compel it to offload task requirements to edge server (ES) for processing assistance. However, as the number of vehicles continues to increase, how edge servers reasonably allocate limited resources to autonomous vehicles becomes critical to the success of urban intelligent transportation services. This paper establishes an urban road scenario with multiple autonomous vehicles and an edge computing server and considers two main driving behaviour transition resource requests, namely car-following behaviour requests and lane-changing behaviour requests. Simultaneously, acknowledging that vehicles may encounter unforeseen traffic hazards when switching driving behaviours, a safety redundancy setting strategy is employed to allocate additional resources to the vehicle to ensure safety and model the vehicle resource allocation problem in the autonomous driving system. Double-deep Q-network (DDQN) is then used to solve this model and maximize the total system utility by comprehensively considering resource costs, system revenue, and autonomous vehicle safety. Finally, results from the simulation experiment indicate that the proposed dynamic resource allocation scheme, based on deep reinforcement learning for autonomous vehicles under edge computing, not only greatly improves the system's benefits and reduces processing delays compared to traditional greedy algorithms and value iteration, but also effectively ensures security.A dynamic resource allocation scheme is proposed for vehicular edge computing networks in autonomous driving systems using deep reinforcement learning. This approach, leveraging double-deep Q-network, aims to maximize system revenue while balancing resource costs, system income, and security considerations. Simulation results demonstrate significant improvements in system rewards and reduced processing delay, guaranteeing system security compared to traditional greedy algorithms and value iteration methods. image									0	0	0	0	0	0	0			1751-956X	1751-9578										Southwest Jiaotong Univ, Sch Transportat & Logist, Natl United Engn Lab Integrated & Intelligent Tran, Natl Engn Lab Integrated Transportat Big Data Appl, Chengdu 611756, Peoples R ChinaDalian Maritime Univ, Dept Civil Engn, Dalian, Peoples R ChinaPeng Cheng Lab, Network Commun Res Ctr, Shenzhen, Peoples R ChinaMcMaster Univ, Dept Elect & Comp Engn, Hamilton, ON, Canada				2024-02-14	WOS:001157889600001		
J	Al-Turki, Mohammed; Ratrout, Nedal T.; Rahman, Syed Masiur; Assi, Khaled J.				Rahman, Syed/D-4611-2011	Rahman, Syed/0000-0003-3624-0519; ALTurki, Mohammed/0000-0002-5527-4549					Signalized Intersection Control in Mixed Autonomous and Regular Vehicles Traffic Environment-A Critical Review Focusing on Future Control								IEEE ACCESS				10				16942	16951				10.1109/ACCESS.2022.3148706							Review	2022	2022	The recent advancement in industrial technology has offered new opportunities to overcome different problems of stochastic driving behavior of humans through effective implementation of autonomous vehicles (AVs). Optimum utilization of driving behavior and advanced capabilities of the AVs has enabled researchers to propose autonomous cooperative-based methods for signalized intersection control under an AV traffic environment. In the future, AVs will share road networks with regular vehicles (RVs), representing a dynamic mixed traffic environment of two groups of vehicles with different characteristics. Without compromising the safety and level of service, traffic operation and control of such a complex environment is a challenging task. The current study includes a comprehensive review focused on the signalized intersection control methods under a mixed traffic environment. The different proposed methods in the literature are based on certain assumptions, requirements, and constraints mainly associated with traffic composition, connectivity, road infrastructures, intersection, and functional network design. Therefore, these methods should be evaluated with appropriate consideration of the underlying assumptions and limitations. This study concludes that the application of adaptive traffic signal control can effectively optimize traffic signal plans for variations of AV traffic environments. However, artificial intelligence approaches primarily focusing on reinforcement learning should be considered to better utilization of the improved AV characteristics.									5	0	0	0	0	0	5			2169-3536											King Fahd Univ Petr & Minerals, Dept Civil & Environm Engn, Dhahran 31261, Saudi ArabiaKing Fahd Univ Petr & Minerals, Interdisciplinary Res Ctr Smart Mobil & Logist, Dhahran 31261, Saudi ArabiaKing Fahd Univ Petr & Minerals, Res Inst, Dhahran 31261, Saudi Arabia				2022-03-03	WOS:000756559700001		
J	Cao, Zhong; Yang, Diange; Xu, Shaobing; Peng, Huei; Li, Boqi; Feng, Shuo; Zhao, Ding					Zhao, Ding/0000-0002-9400-8446; Cao, Zhong/0000-0002-2243-5705; Feng, Shuo/0000-0002-2117-4427; Peng, Huei/0000-0002-7684-1696; Li, Boqi/0000-0001-8959-1406					Highway Exiting Planner for Automated Vehicles Using Reinforcement Learning								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				22	2			990	1000				10.1109/TITS.2019.2961739							Article	FEB 2021	2021	Exiting from highways in crowded dynamic traffic is an important path planning task for autonomous vehicles (AVs). This task can be challenging because of the uncertain motion of surrounding vehicles and limited sensing/observing window. Conventional path planning methods usually compute a mandatory lane change (MLC) command, but the lane change behavior (e.g., vehicle speed and gap acceptance) should also adapt to traffic conditions and the urgency for exiting. In this paper, we propose a reinforcement learning-enhanced highway-exit planner. The learning-based strategy learns from past failures and adjusts the vehicle motion when the AV fails to exit. The reinforcement learning is based on the Monte Carlo tree search (MCTS) approach. The proposed learning-enhanced highway-exit planner is tested 6000 times in stochastic simulations. The results indicate that the proposed planner achieves a higher probability of successful highway exiting than a benchmark MLC planner.									34	1	0	0	0	0	34			1524-9050	1558-0016										Tsinghua Univ, Dept Automot Engn, Beijing 100084, Peoples R ChinaUniv Michigan, Dept Mech Engn, Ann Arbor, MI 48105 USACarnegie Mellon Univ, Dept Mech Engn, Pittsburgh, PA 15213 USA				2021-03-14	WOS:000615045000023		
B	Albilani, M.; Bouzeghoub, A.										Guided Hierarchical Reinforcement Learning for Safe Urban Driving								2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)								746	53				10.1109/ICTAI59109.2023.00115							Conference Paper	2023	2023	Designing a safe decision-making system for end-to-end urban driving is still challenging. Numerous contributions based on Deep Reinforcement Learning (DRL) were developed. However, they all suffer from the cold start issue and require extensive convergence training. Recent solutions for urban driving have emerged based on both Hierarchical Reinforcement Learning (HRL) and imitation learning to overcome these limitations. Nevertheless, they do not guarantee a safe exploration for an autonomous vehicle. In the literature, rule-based systems played a pivotal role in ensuring the safety of self-driving cars, but they require manual rule encoding. This paper introduces GHRL, a decision-making framework for vision-based urban driving that benefits from HRL, and a rule-based system for safe urban driving. The HRL algorithm learns the high-level policies, whereas the low-level policies are guided by the expert demonstration rules modeled with the Answer Set Programming (ASP) formalism. When a critical situation occurs, the system will shift to rely on ASP rules. The state of each policy includes visual features extracted by a convolutional neural network from a monocular camera, information on localization, and waypoints. GHRL is evaluated on the Carla NoCrash benchmark. The results show that by incorporating logical rules, GHRL achieved better performance over state-of-the-art HRL algorithms.					2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)	20232023	IEEE Computer Society; BAIFIEEE Computer Society; BAIF	Atlanta, GA, USAAtlanta, GA, USA	0	0	0	0	0	0	0					979-8-3503-4273-4									Telecom SudParis, Inst. Polytech. de Paris, Palaiseau, FranceAvisto Telecom, France				2024-01-18	INSPEC:24311206		
J	Li, Guofa; Yang, Yifan; Li, Shen; Qu, Xingda; Lyu, Nengchao; Li, Shengbo Eben				Lyu, Nengchao/AFR-1620-2022; Li, Shengbo E/J-7662-2013; Li, Guofa/Q-1176-2016	Lyu, Nengchao/0000-0002-0926-9140; 					Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				134						103452			10.1016/j.trc.2021.103452							Article	JAN 2022	2022	Driving safety is the most important element that needs to be considered for autonomous vehicles (AVs). To ensure driving safety, we proposed a lane change decision-making framework based on deep reinforcement learning to find a risk-aware driving decision strategy with the minimum expected risk for autonomous driving. Firstly, a probabilistic-model based risk assessment method was proposed to assess the driving risk using position uncertainty and distance-based safety metrics. Then, a risk aware decision making algorithm was proposed to find a strategy with the minimum expected risk using deep reinforcement learning. Finally, our proposed methods were evaluated in CARLA in two scenarios (one with static obstacles and one with dynamically moving vehicles). The results show that our proposed methods can generate robust safe driving strategies and achieve better driving performances than previous methods.									66	6	0	0	2	0	71			0968-090X	1879-2359										Shenzhen Univ, Inst Human Factors & Ergon, Coll Mechatron & Control Engn, Shenzhen 518060, Peoples R ChinaTsinghua Univ, Dept Civil Engn, Beijing 100084, Peoples R ChinaWuhan Univ Technol, Intelligent Transportat Syst Res Ctr, Wuhan 430063, Peoples R ChinaTsinghua Univ, Sch Vehicle & Mobil, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R China				2022-04-19	WOS:000778616200002		
J	Arslan, Bartu; Ekren, Banu Yetkin					Arslan, Bartu/0000-0003-2114-767X; Yetkin Ekren, Banu/0009-0009-4228-7795					Transaction selection policy in tier-to-tier SBSRS by using Deep <i>Q</i>-Learning								INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH				61	21			7353	7366				10.1080/00207543.2022.2148767					DEC 2022		Article; Early Access		2023	This paper studies a Deep Q-Learning (DQL) method for transaction sequencing problems in an automated warehousing system, Shuttle-based Storage and Retrieval System (SBSRS), in which shuttles can move between tiers flexibly. Here, the system is referred to as tier-to-tier SBSRS (t-SBSRS), developed as an alternative design to tier-captive SBSRS (c-SBSRS). By the flexible travel of shuttles between tiers in t-SBSRS, the number of shuttles in the system may be reduced compared to its simulant c-SBSRS design. The flexible travel of shuttles makes the operation decisions more complex in that system, motivating us to explore whether integration of a machine learning approach would help to improve the system performance. We apply the DQL method for the transaction selection of shuttles in the system to attain process time advantage. The outcomes of the DQN are confronted with the well-applied heuristic approaches: first-come-first-serve (FIFO) and shortest process time (SPT) rules under different racking and numbers of shuttles scenarios. The results show that DQL outperforms the FIFO and SPT rules promising for the future of smart industry applications. Especially, compared to the well-applied SPT rule in industries, DQL improves the average cycle time per transaction by roughly 43% on average.									5	0	0	0	0	0	5			0020-7543	1366-588X										Eindhoven Univ Technol, Dept Ind Engn & Innovat Sci, Eindhoven, NetherlandsYasar Univ, Dept Ind Engn, Izmir, TurkeyCranfield Univ, Sch Management, Cranfield, Beds, England				2023-01-30	WOS:000912215500001		
J	Gu, Ziqing; Gao, Lingping; Ma, Haitong; Li, Shengbo Eben; Zheng, Sifa; Jing, Wei; Chen, Junbo				; Li, Shengbo/J-7662-2013	Zheng, Sifa/0000-0001-5160-1365; Li, Shengbo/0000-0003-4923-3633; Ma, Haitong/0000-0002-9943-0638					Safe-State Enhancement Method for Autonomous Driving via Direct Hierarchical Reinforcement Learning								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	9			9966	9983				10.1109/TITS.2023.3271642					MAY 2023		Article; Early Access		2023	Reinforcement learning (RL) has shown excellent performance in the sequential decision-making problem, where safety in the form of state constraints is of great significance in the design and application of RL. Simple constrained end-to-end RL methods might lead to significant failure in a complex system like autonomous vehicles. In contrast, some hierarchical RL (HRL) methods generate driving goals directly, which could be closely combined with motion planning. With safety requirements, some safe-enhanced RL methods add post-processing modules to avoid unsafe goals or achieve expectation-based safety, which accepts the existence of unsafe states and allows some violations of safe constraints. However, ensuring state safety is vital for autonomous vehicles. Therefore, this paper proposes a state-based safety enhancement method for autonomous driving via direct hierarchical reinforcement learning. Finally, we design a constrained reinforcement learner based on the State-based Constrained Markov Decision Process (SCMDP), where a learnable safety module could adjust the constraint strength adaptively. We integrate a dynamic module in the policy training and generate future goals considering safety, temporal-spatial continuity, and dynamic feasibility, which could eliminate dependence on the prior model. Simulations in the typical highway scenes with uncertainties show that the proposed method has better training performance, higher driving safety in interactive scenes, more decision intelligence in traffic congestions, and better economic driving ability on roads with changing slopes.									1	0	0	0	0	0	1			1524-9050	1558-0016										Tsinghua Univ, Sch Vehicle & Mobil, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R ChinaAlibaba Grp, Hangzhou 310000, Peoples R ChinaHarvard John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard John A Paulson Sch Engn & Appl Sci			2023-05-28	WOS:000988371100001		
J	Antonio, Guillen-Perez; Maria-Dolores, Cano				Cano, Maria-Dolores/F-3130-2016; Guillen-Perez, Antonio/AAE-5463-2021	Cano, Maria-Dolores/0000-0003-4952-0325; Guillen-Perez, Antonio/0000-0003-3067-6469					Multi-Agent Deep Reinforcement Learning to Manage Connected Autonomous Vehicles at Tomorrow's Intersections								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				71	7			7033	7043				10.1109/TVT.2022.3169907							Article	JUL 2022	2022	In recent years, the growing development of Connected Autonomous Vehicles (CAV), Intelligent Transport Systems (ITS), and 5G communication networks have led to the advent of Autonomous Intersection Management (AIM) systems. AIMs present a new paradigm for CAV control in future cities, taking control of CAVs in scenarios where cooperation is necessary and allowing safe and efficient traffic flows, eliminating traffic signals. So far, the development of AIM algorithms has been based on basic control algorithms, without the ability to adapt or keep learning new situations. To solve this, in this paper we present a new advanced AIM approach based on end-to-end Multi-Agent Deep Reinforcement Learning (MADRL) and trained using Curriculum through Self-Play, called advanced Reinforced AIM (adv.RAIM). adv.RAIM enables the control of CAVs at intersections in a collaborative way, autonomously learning complex real-life traffic dynamics. In addition, adv.RAIM provides a new way to build smarter AIMs capable of proactively controlling CAVs in other highly complex scenarios. Results show remarkable improvements when compared to traffic light control techniques (reducing travel time by 59% or reducing time lost due to congestion by 95%), as well as outperforming other recently proposed AIMs (reducing waiting time by 56%), highlighting the advantages of using MADRL.									22	0	0	0	2	0	25			0018-9545	1939-9359										Univ Politecn Cartagena, Informat Technol & Commun Dept, Murcia 30203, Spain				2022-11-18	WOS:000876768200017		
J	Hieu Trong Nguyen; Phuong Minh Chu; Park, Jisun; Sung, Yunsick; Cho, Kyungeun				Nguyen, Hieu/AAG-6594-2021	Nguyen, Hieu/0000-0003-1667-1135; cho, kyungeun/0000-0003-2219-0848					Intelligent motivation framework based on Q-network for multiple agents in Internet of Things simulations								INTERNATIONAL JOURNAL OF DISTRIBUTED SENSOR NETWORKS				15	7					1550147719866385			10.1177/1550147719866385							Article	JUL 2019	2019	Internet of Things simulations play significant roles in the diverse kinds of activities in our daily lives and have been extensively researched. Creating and controlling virtual agents in three-dimensional Internet of Things simulations is a key technology for achieving realism in three-dimensional simulations. Given that traditional virtual agent-based approaches have limitations for realism, it is necessary to improve the realism of three-dimensional Internet of Things simulations. This article proposes a Q-Network-based motivation framework that applies a Q-Network to select motivations from desires and hierarchical task network planning to execute actions based on goals of the selected motivations. The desires are to be identified and calculated based on states. Selected motivations will be chosen to determine the goals that agents must achieve. In the experiments, the proposed framework achieved an average accuracy of up to 85.5% when the Q-Network-based motivation model was trained. To verify the Q-Network-based motivation framework, a traditional Q-learning is also applied in the three-dimensional virtual environment. Comparing the results of the two frameworks, the Q-Network-based motivation framework shows better results than those of traditional Q-learning, as the accuracy of the Q-Network-based motivation is higher by 15.58%. The proposed framework can be applied to the diverse kinds of Internet of Things systems such as a training autonomous vehicle. Moreover, the proposed framework can generate big data on animal behaviors for other training systems.									1	0	0	0	0	0	1			1550-1329	1550-1477										Dongguk Univ, Dept Multimedia Engn, 30,Pildong Ro 1 Gil, Seoul 04620, South Korea				2019-08-13	WOS:000478037500001		
B	Selvi, S.S.; Dhananjaya, K.M.V.; Lohith, N.V.; Bhardwaj, M.P.V.; Shetty, K.S.										Deep reinforcement learning algorithms for multi-agent systems - a solution for modeling epidemics								2021 IEEE Mysore Sub Section International Conference (MysuruCon)								322	7				10.1109/MysuruCon52639.2021.9641663							Conference Paper	2021	2021	Multi-agent reinforcement learning (MARL) consists of large number of artificial intelligence-based agents interacting with each other in the same environment, often collaborating towards a common end goal. In single-agent reinforcement learning system the change in the environment is only due to the actions of a particular agent. In contrast, a multi-agent environment is subject to the actions of all the agents involved. Multiagent systems can be deployed in various applications like stock trading to maximize profits in stock market, control and coordination of a swarm of robots, modeling of epidemics, autonomous vehicle and traffic control, smart grids and self-healing networks. It is not possible to solve these complex tasks with a pre-programmed single agent. Instead, the many agents should be trained to automatically search for a solution through reinforcement learning (RL) based algorithms. In general, arriving at a decision in a multi-agent system is almost close to impossible due to exponential increase of problem size with an increase in the number of agents. In this paper, multi-agent systems using Deep Reinforcement Learning (DRL) is explored with a possible application in modeling of epidemics. Different stochastic environments are considered, and various multi-agent policies are implemented using DRL. The performance of various MARL algorithms was evaluated against single agent RL algorithms under different environments. MARL agents were able to learn much faster compared to single RL agents with a more stable training phase. Mean Field Q-Learning was able to scale and perform much better even in the situation of hundreds of agents in the environment and is a sure candidate to model and predict the epidemics, in the existing frightening dangerous situation of corona pandemic.					2021 IEEE Mysore Sub Section International Conference (MysuruCon)2021 IEEE Mysore Sub Section International Conference (MysuruCon)	24-25 Oct. 202124-25 Oct. 2021		Hassan, IndiaHassan, India	0	0	0	0	0	0	0					978-1-6654-3888-9									Dept. of Electron. & Commun., Ramaiah Inst. of Technol., Bangalore, India				2022-02-25	INSPEC:21429876		
J	Wang, Huanjie; Yuan, Shihua; Guo, Mengyu; Li, Xueyuan; Lan, Wei				Wang, Huanjie/AAP-5858-2020; Li, Xueyuan/HZM-1127-2023	Li, Xueyuan/0000-0003-1112-5610					A deep reinforcement learning-based approach for autonomous driving in highway on-ramp merge								PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING				235	10-11			2726	2739				10.1177/0954407021999480							Article	SEP 2021	2021	In this paper, we focus on the problem of highway merge via parallel-type on-ramp for autonomous vehicles (AVs) in a decentralized non-cooperative way. This problem is challenging because of the highly dynamic and complex road environments. A deep reinforcement learning-based approach is proposed. The kernel of this approach is a Deep Q-Network (DQN) that takes dynamic traffic state as input and outputs actions including longitudinal acceleration (or deceleration) and lane merge. The total reward for this on-ramp merge problem consists of three parts, which are the merge success reward, the merge safety reward, and the merge efficiency reward. For model training and testing, we construct a highway on-ramp merging simulation experiments with realistic driving parameters. The experimental results show that the proposed approach can make reasonable merging decisions based on the observation of the traffic environment. We also compare our approach with a state-of-the-art approach and the superior performance of our approach is demonstrated by making challenging merging decisions in complex highway parallel-type on-ramp merging scenarios.									12	0	0	0	0	0	12			0954-4070	2041-2991										Beijing Inst Technol, Sch Mech Engn, 5 South Zhongguancun St, Beijing 100081, Peoples R ChinaUniv Calif Berkeley, ITS, Calif PATH Program, Berkeley, CA 94720 USATsinghua Univ, Dept Ind Engn, Room 1109,Bldg 4,Shuangqing Rd, Beijing 100085, Peoples R ChinaChina North Vehicle Res Inst, Beijing, Peoples R China	China North Vehicle Res Inst			2021-08-09	WOS:000679410100006		
J	Park, Shinkyu; Cap, Michal; Alonso-Mora, Javier; Ratti, Carlo; Rus, Daniela					Rus, Daniela/0000-0001-5473-3566; Alonso-Mora, Javier/0000-0003-0058-570X					Social Trajectory Planning for Urban Autonomous Surface Vessels								IEEE TRANSACTIONS ON ROBOTICS				37	2			452	465				10.1109/TRO.2020.3031250							Article	APR 2021	2021	In this article, we propose a trajectory planning algorithm that enables autonomous surface vessels to perform socially compliant navigation in a city's canal. The key idea behind the proposed algorithm is to adopt an optimal control formulation in which the deviation of movements of the autonomous vessel from nominal movements of human-operated vessels is penalized. Consequently, given a pair of origin and destination points, it finds vessel trajectories that resemble those of human-operated vessels. To formulate this, we adopt kernel density estimation (KDE) to build a nominal movement model of human-operated vessels from a prerecorded trajectory dataset, and use a Kullback-Leibler control cost to measure the deviation of the autonomous vessel's movements from the model. We establish an analogy between our trajectory planning approach and the maximum entropy inverse reinforcement learning (MaxEntIRL) approach to explain how our approach can learn the navigation behavior of human-operated vessels. On the other hand, we distinguish our approach from the MaxEntIRL approach in that it does not require well-defined bases, often referred to as features, to construct its cost function as required in many of inverse reinforcement learning approaches in the trajectory planning context. Through experiments using a dataset of vessel trajectories collected from the automatic identification system, we demonstrate that the trajectories generated by our approach resemble those of human-operated vessels and that using them for canal navigation is beneficial in reducing head-on encounters between vessels and improving navigation safety.									1	0	0	0	0	0	1			1552-3098	1941-0468										MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USADelft Univ Technol, Dept Cognit Robot, NL-2628 Delft, NetherlandsMIT, Senseable City Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA				2021-05-05	WOS:000637533900010		
J	Hieu, Nguyen Quang; Hoang, Dinh Thai; Luong, Nguyen Cong; Niyato, Dusit				Nguyen, Hieu/AAA-7363-2022; Hoang, Dinh Thai/K-5135-2012; Dinh, Hoang Thai/I-1910-2018; Niyato, Dusit/Y-2769-2019	Hoang, Dinh Thai/0000-0002-9528-0863; Dinh, Hoang Thai/0000-0002-9528-0863; Nguyen, Hieu/0000-0003-1517-8285; Niyato, Dusit/0000-0002-7442-7416					iRDRC: An Intelligent Real-Time Dual-Functional Radar-Communication System for Automotive Vehicles								IEEE WIRELESS COMMUNICATIONS LETTERS				9	12			2140	2143				10.1109/LWC.2020.3014972							Article	DEC 2020	2020	This letter introduces an intelligent Real-time Dual-functional Radar-Communication (iRDRC) system for autonomous vehicles (AVs). This system enables an AV to perform both radar and data communications functions to maximize bandwidth utilization as well as significantly enhance safety. In particular, the data communications function allows the AV to transmit data, e.g., of current traffic, to edge computing systems and the radar function is used to enhance the reliability and reduce the collision risks of the AV, e.g., under bad weather conditions. The problem of the iRDRC is to decide when to use the communication mode or the radar mode to maximize the data throughput while minimizing the miss detection probability of unexpected events given the uncertainty of surrounding environment. To solve the problem, we develop a deep reinforcement learning algorithm that allows the AV to quickly obtain the optimal policy without requiring any prior information about the environment. Simulation results show that the proposed scheme outperforms baseline schemes in terms of data throughput, miss detection probability, and convergence rate.									12	0	0	0	0	0	12			2162-2337	2162-2345										Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, SingaporeUniv Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, AustraliaPHENIKAA Univ, Fac Comp Sci, Hanoi 12116, Vietnam	PHENIKAA Univ			2020-12-24	WOS:000597146100028		
B	Song, Li										Impacts of Connected and Autonomous Vehicles on Deep Reinforcement Learning Controlled Intersection Systems																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798837536915									The University of North Carolina at Charlotte, Civil Engineering, North Carolina, United States	The University of North Carolina at Charlotte				PQDT:68474422		
J	Chen, Dongliang; Huang, Hongyong; Zheng, Yuchao; Gawkowski, Piotr; Lv, Haibin; Lv, Zhihan				Lyu, Zhihan/I-3187-2014; Lv, Zhihan/GLR-6000-2022; Lv, Zhihan/AHD-0875-2022	Lyu, Zhihan/0000-0003-2525-3074; Lv, Zhihan/0000-0003-2525-3074; Zheng, Yuchao/0000-0002-2048-1222; Dongliang, Chen/0000-0001-7700-4729					The Scanner of Heterogeneous Traffic Flow in Smart Cities by an Updating Model of Connected and Automated Vehicles								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	12			25361	25370				10.1109/TITS.2022.3165155					APR 2022		Article; Early Access		2022	The problems of traditional traffic flow detection and calculation methods include limited traffic scenes, high system costs, and lower efficiency over detecting and calculating. Therefore, in this paper, we presented the updating Connected and Automated Vehicles (CAVs) model as the scanner of heterogeneous traffic flow, which uses various sensors to detect the characteristics of traffic flow in several traffic scenes on the roads. The model contains the hardware platform, software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project, where the driving of vehicles is mainly controlled by Reinforcement Learning (RL). Finally, the effectiveness of the proposed model and the corresponding swarm intelligence strategy is evaluated through simulation experiments. The results showed that the traffic flow scanning, tracking, and data recording performed continuously by CAVs are effective. The increase in the penetration rate of CAVs in the overall traffic flow has a significant effect on vehicle detection and identification. In addition, the vehicle occlusion rate is independent of the CAV lane position in all cases. The complete street scanner is a new technology that realizes the perception of the human settlement environment with the help of the Internet of Vehicles based on 5G communications and sensors. Although there are some shortcomings in the experiment, it still provides an experimental reference for the development of smart vehicles.									1	0	0	0	0	0	1			1524-9050	1558-0016										Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R ChinaTongji Univ, Minist Educ, Dept Traff Engn, Key Lab Rd & Traff Engn, Shanghai 200070, Peoples R ChinaKyushu Inst Technol, Dept Mech & Control Engn, Fukuoka 8048550, JapanWarsaw Univ Technol, Inst Comp Sci, PL-00661 Warsaw, PolandMinist Nat Resources North Sea Bur, North China Sea Offshore Engn Survey Inst, Qingdao 266000, Peoples R ChinaUppsala Univ, Fac Arts, Dept Game Design, S-75105 Uppsala, Sweden	Minist Nat Resources North Sea Bur			2022-04-23	WOS:000782825400001		
J	Jang, Jihye; Tulkinbekov, Khikmatullo; Kim, Deok-Hwan					KIM, DEOK-HWAN/0000-0002-6048-9392					Task Offloading of Deep Learning Services for Autonomous Driving in Mobile Edge Computing								ELECTRONICS				12	15					3223			10.3390/electronics12153223							Article	AUG 2023	2023	As the utilization of complex and heavy applications increases in autonomous driving, research on using mobile edge computing and task offloading for autonomous driving is being actively conducted. Recently, researchers have been studying task offloading algorithms using artificial intelligence, such as reinforcement learning or partial offloading. However, these methods require a lot of training data and critical deadlines and are weakly adaptive to complex and dynamically changing environments. To overcome this weakness, in this paper, we propose a novel task offloading algorithm based on Lyapunov optimization to maintain the system stability and minimize task processing delay. First, a real-time monitoring system is built to utilize distributed computing resources in an autonomous driving environment efficiently. Second, the computational complexity and memory access rate are analyzed to reflect the characteristics of the deep learning applications to the task offloading algorithm. Third, Lyapunov and Lagrange optimization solves the trade-off issues between system stability and user requirements. The experimental results show that the system queue backlog remains stable, and the tasks are completed within an average of 0.4231 s, 0.7095 s, and 0.9017 s for object detection, driver profiling, and image recognition, respectively. Therefore, we ensure that the proposed task offloading algorithm enables the deep learning application to be processed within the deadline and keeps the system stable.									2	0	0	0	0	0	2				2079-9292										Inha Univ, Dept Elect & Comp Engn, Incheon 22212, South Korea				2023-08-19	WOS:001046091100001		
J	Turan, B.; Pedarsani, R.; Alizadeh, M.										Dynamic Pricing and Management for Electric Autonomous Mobility on Demand Systems Using Reinforcement Learning [arXiv]								arXiv								14 pp.	14 pp.											Journal Paper	15 Sept. 2019	2019	The proliferation of ride sharing systems is a major drive in the advancement of autonomous and electric vehicle technologies. This paper considers the joint routing, battery charging, and pricing problem faced by a profit-maximizing transportation service provider that operates a fleet of autonomous electric vehicles. We define the dynamic system model that captures the time dependent and stochastic features of an electric autonomous-mobility-on-demand system. To accommodate for the time-varying nature of trip demands, renewable energy availability, and electricity prices and to further optimally manage the autonomous fleet, a dynamic policy is required. In order to develop a dynamic control policy, we first formulate the dynamic progression of the system as a Markov decision process. We argue that it is intractable to exactly solve for the optimal policy using exact dynamic programming methods and therefore apply deep reinforcement learning to develop a near-optimal control policy. Furthermore, we establish the static planning problem by considering time-invariant system parameters. We define the capacity region and determine the optimal static policy to serve as a baseline for comparison with our dynamic policy. While the static policy provides important insights on optimal pricing and fleet management, we show that in a real dynamic setting, it is inefficient to utilize a static policy. The two case studies we conducted in Manhattan and San Francisco demonstrate the efficacy of our dynamic policy in terms of network stability and profits, while keeping the queue lengths up to 200 times less than the static policy.									0	0	0	0	0	0	0														Dept. of Electr. & Comput. Eng., Univ. of California, Santa Barbara, Santa Barbara, CA, USA				2020-02-21	INSPEC:19301261		
J	Li, Guofa; Lin, Siyan; Li, Shen; Qu, Xingda				Li, Guofa/Q-1176-2016	Li, Guofa/0000-0002-7889-4695; Li, Shen/0000-0002-7111-8861; Lin, Siyan/0000-0001-7411-4869; Qu, Xingda/0000-0003-1764-0357					Learning Automated Driving in Complex Intersection Scenarios Based on Camera Sensors: A Deep Reinforcement Learning Approach								IEEE SENSORS JOURNAL				22	5			4687	4696				10.1109/JSEN.2022.3146307							Article	MAR 1 2022	2022	Making proper decisions at intersections that are one of the most dangerous and sophisticated driving scenarios is full of challenges, especially for autonomous vehicles (AVs). The existing decision-making approaches for AVs at intersections are limited as they only consider driving safety in simple intersection scenarios while sacrificing travel efficiency and driving comfort. To solve this issue, a decision-making structure motivated by deep reinforcement learning was proposed for autonomous driving at complex intersection scenarios based on long short-term memory (LSTM). The mapping relationship between traffic images collected from camera sensors and AVs' actions was established by constructing convolutional-recurrent neural networks in a decision-making framework. Traffic images collected from camera sensors at two different timesteps were used to understand the relative motion information between AVs and other vehicles. To model the interaction between the AV and other vehicles, Markov decision process was used. The deep Q-network (DQN) algorithm was applied to generate the optimal driving policy that could comprehensively consider driving safety, travel efficiency and driving comfort. Three crash-prone complex intersection scenarios were reconstructed in CARLA (car learning to act) to evaluate the performance of our proposed method. The results indicate that our method can make AV drive through intersections safely and efficiently with desirable driving comfort in all the examined scenarios.									7	2	0	0	0	0	10			1530-437X	1558-1748										Shenzhen Univ, Inst Human Factors & Ergon, Coll Mechatron & Control Engn, Shenzhen 518060, Guangdong, Peoples R ChinaTsinghua Univ, Dept Civil Engn, Beijing 100084, Peoples R China				2022-03-25	WOS:000766276000090		
J	Sivashangaran, S.; Eskandarian, A.										XTENTH-CAR: A Proportionally Scaled Experimental Vehicle Platform for Connected Autonomy and All-Terrain Research [arXiv]								arXiv																				Journal Paper	03 Dec. 2022	2022	Connected Autonomous Vehicles (CAVs) are key components of the Intelligent Transportation System (ITS), and all-terrain Autonomous Ground Vehicles (AGVs) are indispensable tools for a wide range of applications such as disaster response, automated mining, agriculture, military operations, search and rescue missions, and planetary exploration. Experimental validation is a requisite for CAV and AGV research, but requires a large, safe experimental environment when using full-size vehicles which is time-consuming and expensive. To address these challenges, we developed XTENTH-CAR (eXperimental one-TENTH scaled vehicle platform for Connected autonomy and All-terrain Research), an open-source, cost-effective proportionally one-tenth scaled experimental vehicle platform governed by the same physics as a full-size on-road vehicle. XTENTH-CAR is equipped with the best-in-class NVIDIA Jetson AGX Orin System on Module (SOM), stereo camera, 2D LiDAR and open-source Electronic Speed Controller (ESC) with drivers written in the new Robot Operating System (ROS 2) to facilitate experimental CAV and AGV perception, motion planning and control research, that incorporate state-of-the-art computationally expensive algorithms such as Deep Reinforcement Learning (DRL). XTENTH-CAR is designed for compact experimental environments, and aims to increase the accessibility of experimental CAV and AGV research with low upfront costs, and complete Autonomous Vehicle (AV) hardware and software architectures similar to the full-sized X-CAR experimental vehicle platform, enabling efficient cross-platform development between small-scale and full-scale vehicles.									0	0	0	0	0	0	0																		2023-05-12	INSPEC:23018105		
J	Zakaria, N.J.; Shapiai, M.I.; Wahid, N.										A study of multiple reward function performances for vehicle collision avoidance systems applying the DQN algorithm in reinforcement learning								IOP Conference Series: Materials Science and Engineering								012033 (13 pp.)	012033 (13 pp.)				10.1088/1757-899X/1176/1/012033							Conference Proceedings	Aug. 2021	2021	Reinforcement Learning (RL) is an area of Machine Learning (ML) that intends to improve the acts of agents learning from environmental interconnection. The significant concern in RL is to achieve the promising potential of the training process in the model. However, network convergence speed is often sluggish in RL and converges quickly to local optimal solutions. Reward function has been used to deal with these problems as a useful tool to speed up the agent's learning speed. Even though RL convergence properties have been comprehensively explored, there are no specific rules for choosing the reward function. Therefore, searching for efficient potential reward function is still an exciting field of study. This paper discusses the reward function, execute some analysis, and provides the learning agent with the extracted information to increase the speed of learning for collision avoidance task. We provide an experimental study for selecting one reward function in a simulated collision-avoidance environment of an autonomous vehicle by applying the DQN algorithm. It has been conducted on online environments, which is using the CARLA simulator. This experimental study consists of three cases with a various exploration of reward values. Case 1 consists of the range of the penalty value larger than the reward function by 200 times. Case 2 is similar, but with the small range number of the penalty is applied, case 3, which is the reward function and penalty value, is in the same range value. The result shows that case 3 performances outperform case 1 and case 2 with 94% average accuracy; meanwhile, case 1 obtains 70%, and case 2 achieves 85% accuracy. It is may due to the monumental size of the collision penalty in comparison to all else. Hence, the findings obtained show the efficacy of the exploration of the reward function.					International Conference of Emerging Challenges in Engineering and Current Technology (ICECTIII 2021)International Conference of Emerging Challenges in Engineering and Current Technology (ICECTIII 2021)	30-31 March 202130-31 March 2021		Terengganu, MalaysiaTerengganu, Malaysia	2	0	0	0	0	0	2			1757-899X											Malaysia-Japan Int. Inst. of Technol., Univ. Teknologi Malaysia, Kuala Lumpur, JapanFac. of Electr. Eng., Univ. Teknologi MARA, Shah Alam, Malaysia				2021-08-01	INSPEC:23025131		
J	Xie, Jiaohong; Liu, Yang; Chen, Nan					Chen, Nan/0000-0003-2495-5234					Two-Sided Deep Reinforcement Learning for Dynamic Mobility-on-Demand Management with Mixed Autonomy								TRANSPORTATION SCIENCE				57	4			1019	1046	1188			10.1287/trsc.2022.1188					JAN 2023		Article; Early Access		2023	Autonomous vehicles (AVs) are expected to operate on mobility-on-demand (MoD) platforms because AV technology enables flexible self-relocation and systemoptimal coordination. Unlike the existing studies, which focus on MoD with pure AV fleet or conventional vehicles (CVs) fleet, we aim to optimize the real-time fleet management of an MoD system with a mixed autonomy of CVs and AVs. We consider a realistic case that heterogeneous boundedly rational drivers may determine and learn their relocation strategies to improve their own compensation. In contrast, AVs are fully compliant with the platform's operational decisions. To achieve a high level of service provided by a mixed fleet, we propose that the platform prioritizes human drivers in the matching decisions when on-demand requests arrive and dynamically determines the AV relocation tasks and the optimal commission fee to influence drivers' behavior. However, it is challenging to make efficient real-time fleet management decisions when spatiotemporal uncertainty in demand and complex interactions among human drivers and operators are anticipated and considered in the operator's decision making. To tackle the challenges, we develop a two-sided multiagent deep reinforcement learning (DRL) approach in which the operator acts as a supervisor agent on one side and makes centralized decisions on the mixed fleet, and each CV driver acts as an individual agent on the other side and learns to make decentralized decisions noncooperatively. We establish a two-sided multiagent advantage actor-critic algorithm to simultaneously train different agents on the two sides. For the first time, a scalable algorithm is developed here for mixed fleet management. Furthermore, we formulate a two-head policy network to enable the supervisor agent to efficiently make multitask decisions based on one policy network, which greatly reduces the computational time. The two-sided multiagent DRL approach is demonstrated using a case study in New York City using real taxi trip data. Results show that our algorithm can make high-quality decisions quickly and outperform benchmark policies. The efficiency of the two-head policy network is demonstrated by comparing it with the case using two separate policy networks. Our fleet management strategy makes both the platform and the drivers better off, especially in scenarios with high demand volume.									5	0	0	0	0	0	5			0041-1655											Natl Univ Singapore, Dept Ind Syst Engn & Management, Singapore 117576, SingaporeNatl Univ Singapore, Dept Civil & Environm Engn, Singapore 117576, Singapore				2023-04-12	WOS:000951823200001		
J	Hu, Yifan; Fu, Junjie; Wen, Guanghui				Fu, Junjie/P-5035-2015	Fu, Junjie/0000-0002-1528-8727; Hu, Yifan/0000-0003-0756-6157; Wen, Guanghui/0000-0003-0070-8597					Safe Reinforcement Learning for Model-Reference Trajectory Tracking of Uncertain Autonomous Vehicles With Model-Based Acceleration								IEEE TRANSACTIONS ON INTELLIGENT VEHICLES				8	3			2332	2344				10.1109/TIV.2022.3233592							Article	MAR 2023	2023	Applying reinforcement learning (RL) algorithms to control systems design remains a challenging task due to the potential unsafe exploration and the low sample efficiency. In this paper, we propose a novel safe model-based RL algorithm to solve the collision-free model-reference trajectory tracking problem of uncertain autonomous vehicles (AVs). Firstly, a new type of robust control barrier function (CBF) condition for collision-avoidance is derived for the uncertain AVs by incorporating the estimation of the system uncertainty with Gaussian process (GP) regression. Then, a robust CBF-based RL control structure is proposed, where the nominal control input is composed of the RL policy and a model-based reference control policy. The actual control input obtained from the quadratic programming problem can satisfy the constraints of collision-avoidance, input saturation and velocity boundedness simultaneously with a relatively high probability. Finally, within this control structure, a Dyna-style safe model-based RL algorithm is proposed, where the safe exploration is achieved through executing the robust CBF-based actions and the sample efficiency is improved by leveraging the GP models. The superior learning performance of the proposed RL control structure is demonstrated through simulation experiments.									2	0	0	0	0	0	2			2379-8858	2379-8904										Southeast Univ, Sch Math, Nanjing 210096, Peoples R ChinaPurple Mt Labs, Nanjing 211111, Peoples R China	Purple Mt Labs			2023-06-12	WOS:000981348100029		
R	Masmitja, Ivan; Martin, Mario; O'Reilly, Tom; Kieft, Brian; Palomeras, Narcis; Navarro, Joan; Katija, Kakani										Dynamic robotic tracking of underwater targets using reinforcement learning								Dryad													https://doi.org/10.5061/DRYAD.CZ8W9GJ7Z							Data set	2024-01-16	2023	To realize the potential of autonomous underwater robots that scale up our observational capacity in the ocean, new approaches and techniques are needed. Fleets of autonomous robots could be used to study complex marine systems and animals with either new imaging configurations or by tracking tagged animals to study their behavior. These activities can then inform and create new policies for community conservation. The role of animal connectivity via active movement of animals represents a major knowledge gap related to the distribution of deep ocean populations. Tracking underwater targets represents a major challenge for observing biological processes in situ, and methods to robustly respond to a changing environment during monitoring missions are needed. Analytical techniques for optimal sensor placement and path planning to locate underwater targets are not straightforward in such cases. The aim of this study is to investigate the use of deep reinforcement learning as a tool for range-only underwater target tracking optimization, whose promising capabilities have been demonstrated in terrestrial scenarios. To evaluate its usefulness, a reinforcement learning method was implemented as a path planning system for an autonomous surface vehicle while tracking an underwater mobile target. A complete description of an open-source model, performance metrics in simulated environments, and evaluated algorithms based on more than 15 hours of at-sea field experiments are presented. These efforts demonstrate that deep reinforcement learning is a powerful approach that enhances the abilities of autonomous robots in the ocean and encourages the deployment of algorithms like these for monitoring marine biological systems in the future. Deep Reinforcement Learning methods for Underwater Target Tracking This is a set of tools developed to train an agent (and multiple agents) to find the optimal path to localize and track a target (and multiple targets). The deep Reinforcement Learning (RL) algorithms implemented are: Deep Deterministic Policy Gradient (DDPG) Twin-Delayed DDPG (TD3) Soft Actor-Critic (SAC) The environment to train the agents is based on theOpenAI Particle. The main objective is to find the optimal path that an autonomous vehicle (e.g. autonomous underwater vehicles (AUV) or autonomous surface vehicles (ASV)) should follow in order to localize and track an underwater target usingrange-only and single-beacon algorithms. The target estimation algorithms implemented are based on: Least Squares (LS) Particle Filter (PF) More information at this Github repository:https://github.com/imasmitja/RLforUTracking Copyright: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication									0	0	0	0	0	0	0														Institut de Ciencies del Mar (ICM-CMIMA), CSIC, Barcelona, SpainUniversitat Politecnica de Catalunya (UPC), SpainMonterey Bay Aquarium Research Institute, USAMonterey Bay Aquarium Research Institute, USA	Institut de Ciencies del Mar (ICM-CMIMA), CSIC, BarcelonaUniversitat Politecnica de Catalunya (UPC)Monterey Bay Aquarium Research InstituteMonterey Bay Aquarium Research Institute			2023-08-17	DRCI:DATA2023137027547757		
J	Ngai, Daniel Chi Kit; Yung, Nelson Hon Ching				Yung, Nelson Hon Ching/C-1873-2009						A Multiple-Goal Reinforcement Learning Method for Complex Vehicle Overtaking Maneuvers								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				12	2	SI		509	522				10.1109/TITS.2011.2106158							Article	JUN 2011	2011	In this paper, we present a learning method to solve the vehicle overtaking problem, which demands a multitude of abilities from the agent to tackle multiple criteria. To handle this problem, we propose to adopt a multiple-goal reinforcement learning (MGRL) framework as the basis of our solution. By considering seven different goals, either Q-learning (QL) or double-action QL is employed to determine action decisions based on whether the other vehicles interact with the agent for that particular goal. Furthermore, a fusion function is proposed according to the importance of each goal before arriving to an overall but consistent action decision. This offers a powerful approach for dealing with demanding situations such as overtaking, particularly when a number of other vehicles are within the proximity of the agent and are traveling at different and varying speeds. A large number of overtaking cases have been simulated to demonstrate its effectiveness. From the results, it can be concluded that the proposed method is capable of the following: 1) making correct action decisions for overtaking; 2) avoiding collisions with other vehicles; 3) reaching the target at reasonable time; 4) keeping almost steady speed; and 5) maintaining almost steady heading angle. In addition, it should also be noted that the proposed method performs lane keeping well when not overtaking and lane changing effectively when overtaking is in progress.									85	6	0	0	0	1	95			1524-9050	1558-0016										ASM Assembly Automat Ltd, Kwai Chung, Hong Kong, Peoples R ChinaUniv Hong Kong, Dept Elect & Elect Engn, Pokfulam, Hong Kong, Peoples R China				2011-06-18	WOS:000291315100020		
J	Ahmic, Kenan; Ultsch, Johannes; Brembeck, Jonathan; Winter, Christoph				Ultsch, Johannes/AEN-2571-2022	Ultsch, Johannes/0000-0001-6483-8468; Winter, Christoph/0000-0002-6949-303X					Reinforcement Learning-Based Path following Control with Dynamics Randomization for Parametric Uncertainties in Autonomous Driving								APPLIED SCIENCES-BASEL				13	6					3456			10.3390/app13063456							Article	MAR 2023	2023	Reinforcement learning-based controllers for safety-critical applications, such as autonomous driving, are typically trained in simulation, where a vehicle model is provided during the learning process. However, an inaccurate parameterization of the vehicle model used for training heavily influences the performance of the reinforcement learning agent during execution. This inaccuracy is either caused by changes due to environmental influences or by falsely estimated vehicle parameters. In this work, we present our approach of combining dynamics randomization with reinforcement learning to overcome this issue for a path-following control task of an autonomous and over-actuated robotic vehicle. We train three independent agents, where each agent experiences randomization for a different vehicle dynamics parameter, i.e., the mass, the yaw inertia, and the road-tire friction. We randomize the parameters uniformly within predefined ranges to enable the agents to learn an equally robust control behavior for all possible parameter values. Finally, in a simulation study, we compare the performance of the agents trained with dynamics randomization to the performance of an agent trained with the nominal parameter values. Simulation results demonstrate that the former agents obtain a higher level of robustness against model uncertainties and varying environmental conditions than the latter agent trained with nominal vehicle parameter values.									0	0	0	0	0	0	0				2076-3417										German Aerosp Ctr DLR, Inst Syst Dynam & Control, Robot & Mechatron Ctr, D-82234 Wessling, Germany				2023-04-06	WOS:000953881500001		
J	Bautista-Montesano, Rolando; Bustamante-Bello, Rogelio; Ramirez-Mendoza, Ricardo A.				Ramirez-Mendoza, Ricardo A./I-4584-2015; Montesano, Rolando Bautista/AAX-8203-2020; ramirez, ricardo/HJA-2728-2022; Bustamante-Bello, Rogelio/AAW-7668-2021	Ramirez-Mendoza, Ricardo A./0000-0002-5122-507X; Montesano, Rolando Bautista/0000-0002-0723-5799; 					Explainable navigation system using fuzzy reinforcement learning								INTERNATIONAL JOURNAL OF INTERACTIVE DESIGN AND MANUFACTURING - IJIDEM				14	4			1411	1428				10.1007/s12008-020-00717-1					OCT 2020		Article; Early Access		2020	Explainable outcomes in autonomous navigation have become crucial for drivers, other vehicles, as well as for pedestrians. Creating trustworthy strategies is mandatory for the integration of self-driving cars into quotidian environments. This paper presents the successful implementation of an explainable Fuzzy Deep Reinforcement Learning approach for autonomous vehicles based on the AWS DeepRacer (TM) platform. A model of the environment is created by transforming crisp values into linguistic variables. A fuzzy inference system is used to define the reward of the vehicle depending on its current state. Guidelines to define the actions and to improve performance of the reinforcement learning agent are given based on the characteristics of the existing hardware. The performance of the models is tested on tracks with distinctive properties using agents with different policies and action spaces, and shows explainable and successful navigation of the agent on diverse scenarios.[GRAPHICS].									5	0	0	0	0	0	5			1955-2513	1955-2505										Tecnol Monterrey, Campus Ciudad Mexico,Calle Puente 222, Tlalpan, Cdmx, Mexico				2020-10-20	WOS:000575711700001		
B	Ma, Xiaobai										Deep Reinforcement Learning Methods for Autonomous Driving Safety and Interactivity																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798494454171									Stanford University, California, United States	Stanford University				PQDT:64459528		
J	Le, M.; Huynh-The, T.; Do-Duy, T.; Vu, T.-H.; Hwang, W.-J.; Pham, Q.-V.										Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey [arXiv]								Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey [arXiv]																				Preprint	2023	2023	The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.									0	0	0	0	0	0	0																		2023-11-02	INSPEC:23901669		
J	Fayyazi, Mojgan; Abdoos, Monireh; Phan, Duong; Golafrouz, Mohsen; Jalili, Mahdi; Jazar, Reza N.; Langari, Reza; Khayyam, Hamid				; Langari, Reza/M-8420-2018	Khayyam, Hamid/0000-0001-9784-1452; Langari, Reza/0000-0001-7900-5186					Real-time self-adaptive Q-learning controller for energy management of conventional autonomous vehicles								EXPERT SYSTEMS WITH APPLICATIONS				222						119770			10.1016/j.eswa.2023.119770					MAR 2023		Article; Early Access		2023	Reducing emissions and energy consumption of autonomous vehicles is critical in the modern era. This paper presents an intelligent energy management system based on Reinforcement Learning (RL) for conventional autonomous vehicles. Furthermore, in order to improve the efficiency, a new exploration strategy is proposed to replace the traditional decayed epsilon-greedy strategy in the Q-learning algorithm associated with RL. Unlike tradi-tional Q-learning algorithms, the proposed self-adaptive Q-learning (SAQ-learning) can be applied in real-time. The learning capability of the controllers can help the vehicle deal with unknown situations in real-time. Nu-merical simulations show that compared to other controllers, Q-learning and SAQ-learning controllers can generate the desired engine torque based on the vehicle road power demand and control the air/fuel ratio by changing the throttle angle efficiently in real-time. Also, the proposed real-time SAQ-learning is shown to improve the operational time by 23% compared to standard Q-learning. Our simulations reveal the effectiveness of the proposed control system compared to other methods, namely dynamic programming and fuzzy logic methods.									5	0	0	0	0	0	5			0957-4174	1873-6793										RMIT Univ, Sch Engn, Melbourne, Vic 3083, AustraliaShahid Beheshti Univ SBU, Fac Comp Sci & Engn, Tehran 1983969411, IranVietnam Maritime Univ, Mech Engn Inst, Haiphong 180000, VietnamTexas A&M Univ, Engn Technol & Ind Distribut, 3367 TAMU, College Stn, TX USA				2023-04-07	WOS:000957512100001		
B	Gilbert, Thomas J.										Modes of Deliberation in Machine Ethics																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798381710731									University of California, Berkeley, Interdisciplinary Studies, California, United States	University of California, Berkeley				PQDT:87748101		
B	Wang, Yu										Trajectory Based Traffic Analysis and Control Utilizing Connected Autonomous Vehicles																												Dissertation/Thesis	Jan 01 2019	2019										0	0	0	0	0	0	0					9781392435021									University of South Florida, Civil and Environmental Engineering, Florida, United States	University of South Florida				PQDT:68687856		
J	Rafiq, Ghazala; Rafiq, Muhammad; Choi, Gyu Sang					Rafiq, Muhammad/0000-0001-6713-8766					Video description: A comprehensive survey of deep learning approaches								ARTIFICIAL INTELLIGENCE REVIEW				56	11			13293	13372				10.1007/s10462-023-10414-6					APR 2023		Article; Early Access		2023	Video description refers to understanding visual content and transforming that acquired understanding into automatic textual narration. It bridges the key AI fields of computer vision and natural language processing in conjunction with real-time and practical applications. Deep learning-based approaches employed for video description have demonstrated enhanced results compared to conventional approaches. The current literature lacks a thorough interpretation of the recently developed and employed sequence to sequence techniques for video description. This paper fills that gap by focusing mainly on deep learning-enabled approaches to automatic caption generation. Sequence to sequence models follow an Encoder-Decoder architecture employing a specific composition of CNN, RNN, or the variants LSTM or GRU as an encoder and decoder block. This standard-architecture can be fused with an attention mechanism to focus on a specific distinctiveness, achieving high quality results. Reinforcement learning employed within the Encoder-Decoder structure can progressively deliver state-of-the-art captions by following exploration and exploitation strategies. The transformer mechanism is a modern and efficient transductive architecture for robust output. Free from recurrence, and solely based on self-attention, it allows parallelization along with training on a massive amount of data. It can fully utilize the available GPUs for most NLP tasks. Recently, with the emergence of several versions of transformers, long term dependency handling is not an issue anymore for researchers engaged in video processing for summarization and description, or for autonomous-vehicle, surveillance, and instructional purposes. They can get auspicious directions from this research.									4	0	0	0	0	0	4			0269-2821	1573-7462										Yeungnam Univ, Dept Informat & Commun Engn, Gyongsan 38541, South KoreaKeimyung Univ, Dept Game & Mobile Engn, 1095 Dalgubeol Daero, Daegu 42601, South Korea				2023-04-30	WOS:000968013400001		
C	Musau, Patrick; Hamilton, Nathaniel; Lopez, Diego Manzanas; Robinette, Preston; Johnson, Taylor T.			IEEE COMP SOC	Hamilton, Nathaniel/JED-6262-2023; johnson, taylor/GRX-4598-2022						On Using Real-Time Reachability for the Safety Assurance of Machine Learning Controllers								2022 IEEE INTERNATIONAL CONFERENCE ON ASSURED AUTONOMY (ICAA 2022)								1	10				10.1109/ICAA52185.2022.00010							Proceedings Paper	2022	2022	Over the last decade, advances in machine learning and sensing technology have paved the way for the belief that safe, accessible, and convenient autonomous vehicles may be realized in the near future. Despite the prolific competencies of machine learning models for learning the nuances of sensing, actuation, and control, they are notoriously difficult to assure. The challenge here is that some models, such as neural networks, are "black box" in nature, making verification and validation difficult, and sometimes infeasible. Moreover, these models are often tasked with operating in uncertain and dynamic environments where design time assurance may only be partially transferable. Thus, it is critical to monitor these components at runtime. One approach for providing runtime assurance of systems with unverified components is the simplex architecture, where an unverified component is wrapped with a safety controller and a switching logic designed to prevent dangerous behavior. In this paper, we propose the use of a real-time reachability algorithm for the implementation of such an architecture for the safety assurance of a 1/10 scale open source autonomous vehicle platform known as F1/10. The reachability algorithm (a) provides provable guarantees of safety, and (b) is used to detect potentially unsafe scenarios. In our approach, the need to analyze the underlying controller is abstracted away, instead focusing on the effects of the controller's decisions on the system's future states. We demonstrate the efficacy of our architecture through experiments conducted both in simulation and on an embedded hardware platform.					IEEE International Conference on Assured Autonomy (ICAA)IEEE International Conference on Assured Autonomy (ICAA)	MAR 22-24, 2022MAR 22-24, 2022	IEEE; IEEE Comp Soc; Whiting Sch Engn, Appl Phys Lab; Johns Hopkins Univ Inst Assured Auton; IEEE Syst Man & Cybernet SocIEEE; IEEE Comp Soc; Whiting Sch Engn, Appl Phys Lab; Johns Hopkins Univ Inst Assured Auton; IEEE Syst Man & Cybernet Soc	ELECTR NETWORKELECTR NETWORK	3	0	0	0	0	0	5					978-1-6654-8539-5									Vanderbilt Univ, Dept Elect & Comp Engn, Nashville, TN 37235 USAVanderbilt Univ, Dept Comp Sci, Nashville, TN 37235 USA				2022-09-03	WOS:000843747200001		
B	Luo, Mulong										Hardware-Level Vulnerabilities and Support for Secure and Safe Cyber-Physical Systems																												Dissertation/Thesis	Jan 01 2023	2023										0	0	0	0	0	0	0					9798381413052									Cornell University, Electrical and Computer Engineering, New York, United States	Cornell University				PQDT:87245635		
J	He, Xiangkun; Lv, Chen				He, Xiangkun/ABA-5268-2021; Lv, Chen/N-7055-2018	He, Xiangkun/0000-0001-9818-0879; Lv, Chen/0000-0001-6897-4512					Toward personalized decision making for autonomous vehicles: A constrained multi-objective reinforcement learning technique								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				156						104352			10.1016/j.trc.2023.104352					SEP 2023		Article; Early Access		2023	Reinforcement learning promises to provide a state-of-the-art solution to the decision making problem of autonomous driving. Nonetheless, numerous real-world decision making problems involve balancing multiple conflicting or competing objectives. In addition, passengers may typically prefer to explore diversified driving modes through their specific preferences (i.e., relative importance of different objectives). Taking into account these demands, traditional reinforcement learning algorithms with applications in personalized self-driving vehicles remain challenging. Consequently, here we present a novel constrained multi-objective reinforcement learning technique for personalized decision making in autonomous driving, with the goal of learning a single model for Pareto optimal policies across the space of all possible user preferences. Specifically, a nonlinear constraint incorporating a user-specified preference and a vectorized action-value function is introduced to ensure both diversity in learned decision behaviors and efficient alignment between the user-specified preference and the corresponding optimal policy. Additionally, a constrained multi-objective actor-critic approach is advanced to approximate the Pareto optimal policies for any user-specified preferences while adhering to the nonlinear constraint. Finally, the proposed personalized decision making scheme for autonomous driving is assessed in a highway on-ramp merging scenario with dynamic traffic flows. The results demonstrate the effectiveness of our method by comparing it with classical and state-of-the-art baselines.									1	0	0	0	0	0	1			0968-090X	1879-2359										Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore, Singapore				2023-11-01	WOS:001083556700001		
J	Liu, Xiongqing; Jin, Yan					Jin, Yan/0000-0002-6502-5837					Reinforcement learning-based collision avoidance: impact of reward function and knowledge transfer								AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING				34	2	SI		207	222	PII S0890060420000141			10.1017/S0890060420000141							Article	MAY 2020	2020	Collision avoidance for robots and vehicles in unpredictable environments is a challenging task. Various control strategies have been developed for the agent (i.e., robots or vehicles) to sense the environment, assess the situation, and select the optimal actions to avoid collision and accomplish its mission. In our research on autonomous ships, we take a machine learning approach to collision avoidance. The lack of available ship steering data of human ship masters has made it necessary to acquire collision avoidance knowledge through reinforcement learning (RL). Given that the learned neural network tends to be a black box, it is desirable that a method is available which can be used to design an agent's behavior so that the desired knowledge can be captured. Furthermore, RL with complex tasks can be either time consuming or unfeasible. A multi-stage learning method is needed in which agents can learn from simple tasks and then transfer their learned knowledge to closely related but more complex tasks. In this paper, we explore the ways of designing agent behaviors through tuning reward functions and devise a transfer RL method for multi-stage knowledge acquisition. The computer simulation-based agent training results have shown that it is important to understand the roles of each component in a reward function and the various design parameters in transfer RL. The settings of these parameters are all dependent on the complexity of the tasks and the similarities between them.									10	0	0	0	0	0	10			0890-0604	1469-1760										Univ Southern Calif, Dept Aerosp & Mech Engn, 3650 McClintock Ave,OHE-430, Los Angeles, CA 90089 USA				2020-07-16	WOS:000544963500007		
J									Stone,  Peter		RI: Small: Efficient Reinforcement Learning for Generic Large-Scale Tasks																												Awarded Grant	Sep 01 2009	2009	Recent advances in autonomous agents research are pushing our society closer to the brink of the widespread adoption of autonomous agents in everyday life. Applications that incorporate agents already exist or are quickly emerging, such as domestic robots, autonomous vehicles, and financial management agents. Reinforcement learning (RL) of sequential decision making is an important paradigm for enabling the widespread deployment of autonomous agents. However, a few notable successes notwithstanding, state-of-the-art reinforcement learning algorithms are not yet fully capable of addressing generic large-scale applications. <br/><br/>This project is advancing in four directions to scale-up application of RL systems. Specifically, the project is (1) developing algorithms to automatically structure the input, output, and policy representations for learning; (2) introducing parallelizable reinforcement learning algorithms so as to exploit modern parallel architectures; (3) unifying abstraction and hierarchical reasoning with model-based learning for the purpose of enabling intelligent exploration of large-scale environments; and (4) enabling reinforcement learning algorithms to benefit from low-bandwidth interactions with human users. Finally, we intend to unify the four research thrusts above into a single algorithm and conduct empirical evaluation on real-world/large-scale applications, to include biped robot balancing and walking, robot soccer in simulation and with real robots, and a full-size autonomous vehicle capable of planning paths in an urban environment.<br/><br/>In addition to research advances and implications for improving national infrastructure, the project will contribute to undergraduate and graduate curriculum development.									0	0	0	0	0	0	0						0917122								University of Texas at Austin				2023-12-08	GRANTS:13853486		
J	Zhu, Wei; Hayashibe, Mitsuhiro				Hayashibe, Mitsuhiro/B-8170-2009	Hayashibe, Mitsuhiro/0000-0001-6179-5706; Zhu, Wei/0000-0003-4251-1741					Autonomous Navigation System in Pedestrian Scenarios Using a Dreamer-Based Motion Planner								IEEE ROBOTICS AND AUTOMATION LETTERS				8	6			3835	3842				10.1109/LRA.2023.3273514							Article	JUN 2023	2023	Navigation among pedestrians is a crucial capability of service robots; however, it is a challenge to manage time-varying environments stably. Recent deep reinforcement learning (DRL)-based approaches to crowd navigation have yielded numerous promising applications. However, they rely heavily on initial imitation learning and colossal positive datasets. Moreover, the difficulties in accurately localizing robots, detecting and tracking humans, representing and generalizing reciprocal human relationships restrict their deployment in real-world problems. We propose a Dreamer-based motion planner for collision-free navigation in diverse pedestrian scenarios. Our RL framework can completely learn from zero experience via a model-based DRL. The robot and humans are first projected onto a map, which is subsequently decoded into low-dimensional latent state. A predictive dynamic model in the latent space is jointly created to efficiently optimize the navigation policy. Additionally, we leverage the techniques of system identification, domain randomization, clustering and LiDAR SLAM for practical deployment. Simulation ablations and real implementations demonstrate that our motion planner outperforms state-of-the-art methods, and that the navigation system can be physically implemented in the real world.									2	0	0	0	0	0	2			2377-3766											Tohoku Univ, Grad Sch Engn, Dept Robot, Sendai 9808579, Japan				2023-06-07	WOS:000991665400003		
J	Kovari, Balint; Hegedus, Ferenc; Becsi, Tamas				Bécsi, Tamás/H-9818-2012	Bécsi, Tamás/0000-0002-1487-9672; Kovari, Balint/0000-0003-2178-2921; Hegedus, Ferenc/0000-0002-8063-6054					Design of a Reinforcement Learning-Based Lane Keeping Planning Agent for Automated Vehicles								APPLIED SCIENCES-BASEL				10	20					7171			10.3390/app10207171							Article	OCT 2020	2020	Featured ApplicationThe presented method can be used as a real-time trajectory following algorithm for autonomous vehicles using prediction based on lookahead information.Reinforcement learning-based approaches are widely studied in the literature for solving different control tasks for Connected and Autonomous Vehicles, from which this paper deals with the problem of lateral control of a dynamic nonlinear vehicle model, performing the task of lane-keeping. In this area, the appropriate formulation of the goals and environment information is crucial, for which the research outlines the importance of lookahead information, enabling to accomplish maneuvers with complex trajectories. Another critical part is the real-time manner of the problem. On the one hand, optimization or search based methods, such as the presented Monte Carlo Tree Search method, can solve the problem with the trade-off of high numerical complexity. On the other hand, single Reinforcement Learning agents struggle to learn these tasks with high performance, though they have the advantage that after the training process, they can operate in a real-time manner. Two planning agent structures are proposed in the paper to resolve this duality, where the machine learning agents aid the tree search algorithm. As a result, the combined solution provides high performance and low computational needs.									19	0	0	0	0	0	19				2076-3417										Budapest Univ Technol & Econ, Dept Control Transportat & Vehicle Syst, H-1111 Budapest, Hungary				2020-11-17	WOS:000585202000001		
J	Sovrano, Francesco; Raymond, Alex; Prorok, Amanda				Raymond, Alex/HPE-7653-2023; Sovrano, Francesco/HZI-4282-2023	Sovrano, Francesco/0000-0002-6285-1041					Explanation-Aware Experience Replay in Rule-Dense Environments								IEEE ROBOTICS AND AUTOMATION LETTERS				7	2			898	905				10.1109/LRA.2021.3135927							Article	APR 2022	2022	Human environments are often regulated by explicit and complex rulesets. Integrating Reinforcement Learning (RL) agents into such environments motivates the development of learning mechanisms that perform well in rule-dense and exception-ridden environments such as autonomous driving on regulated roads. In this letter, we propose a method for organising experience by means of partitioning the experience buffer into clusters labelled on a per-explanation basis. We present discrete and continuous navigation environments compatible with modular rulesets and 9 learning tasks. For environments with explainable rulesets, we convert rule-based explanations into case-based explanations by allocating state-transitions into clusters labelled with explanations. This allows us to sample experiences in a curricular and task-oriented manner, focusing on the rarity, importance, and meaning of events. We label this concept Explanation-Awareness (XA). We perform XA experience replay (XAER) with intra and inter-cluster prioritisation, and introduce XA-compatible versions of DQN, TD3, and SAC. Performance is consistently superior with XA versions of those algorithms, compared to traditional Prioritised Experience Replay baselines, indicating that explanation engineering can he used in lieu of reward engineering for environments with explainable features.									4	0	0	0	0	0	4			2377-3766											Univ Bologna, Dept Comp Sci, I-40126 Bologna, ItalyUniv Cambridge, Dept Comp Sci & Technol, Cambridge CB3 0FD, England				2021-12-31	WOS:000733943800003		
J									Nikiforov,  Vladimir		US-German Collaboration: Strategy Change in Cognitive Biological and Technical Systems																												Awarded Grant	Oct 01 2013	2013	The ability for strategy change, i.e., the change in action selection and action planning while an overarching goal is maintained is a fundamental, but still barely understood capability of cognitive systems. Sudden transitions are well documented in neurophysiological and cognitive experimental data, but application of the underlying theory of the spatio-temporal neurodynamics is yet to be done.  Current physiological and theoretical frameworks of learning focus on incremental learning (as exemplified the reinforcement learning).  This project aims at improved understanding of the nature and functional role of abrupt, large-scale state transitions in complex neuronal systems as the basis of cognitive strategy change. We exploit our experimental and theoretical understanding of a particular rodent learning model to simulate the neuronal mechanisms of instantaneous strategy change.  The investigators will develop an algorithmic formulation of the neurocomputational principles, and apply it in the engineering example of autonomous vehicle control.  This interdisciplinary project is based on the complementary and synergistic expertise of the team members in optimization theory and both theoretical and experimental neuroscience.<br/><br/>This project arises from our deep understanding of the rapid biological and cognitive processes displayed by strategy changes in coping with changing environments.  This research on decision making in human and animal brains provides a platform for developing robust decision support systems that operate in dynamically changing scenarios in the style of brains.  Detailed analysis of the mechanisms underlying rapid strategy change in brains will allow both this research team and other groups to equip various man-made systems with the fundamental property of insightful cognition.  This work addresses important societal needs by creating the foundations of cognitive engineering systems supporting emergency response to natural disasters and cyber security threats by adversaries, as well as optimized control of autonomous vehicles under complex operating conditions.<br/><br/>This award is being co-funded by NSF's Office of International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF).									0	0	0	0	0	0	0						1311165								University of Memphis				2023-12-08	GRANTS:14025608		
B	Vinitsky, Eugene										From Sim-to-Real: Learning and Deploying Autonomous Vehicle Controllers That Improve Transportation Metrics																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798380368254									University of California, Berkeley, Mechanical Engineering, California, United States	University of California, Berkeley				PQDT:85115530		
B	Saha, Olimpiya										Fast, Real-Time Robot Navigation in Initially Unknown Environments via Cross-Domain Transfer Learning of Options																												Dissertation/Thesis	Jan 01 2018	2018										0	0	0	0	0	0	0					978-0-438-26064-1									University of Nebraska at Omaha, Information Technology, Nebraska, United States	University of Nebraska at Omaha				PQDT:58912528		
C	Garzon, Mario; Spalanzani, Anne			IEEE	Oviedo, Mario Andrei Garzon/ABI-6479-2020	Oviedo, Mario Andrei Garzon/0000-0001-6672-4827					Game theoretic decision making for autonomous vehicles' merge manoeuvre in high traffic scenarios								2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						3448	3453											Proceedings Paper	2019	2019	This paper presents a game theoretic decision making process for autonomous vehicles. Its goal is to provide a solution for a very challenging task: the merge manoeuvre in high traffic scenarios. Unlike previous approaches, the proposed solution does not rely on vehicle-to-vehicle communication or any specific coordination, moreover, it is capable of anticipating both the actions of other players and their reactions to the autonomous vehicle's movements. \The game used is an iterative, multi-player level-k model, which uses cognitive hierarchy reasoning for decision making and has been proved to correctly model human decisions in uncertain situations. This model uses reinforcement learning to obtain a near-optimal policy, and since it is an iterative model, it is possible to define a goal state so that the policy tries to reach it.To test the decision making process, a kinematic simulation was implemented. The resulting policy was compared with a rule-based approach. The experiments show that the decision making system is capable of correctly performing the merge manoeuvre, by taking actions that require reactions of the other players to be successfully completed.					IEEE Intelligent Transportation Systems Conference (IEEE-ITSC)IEEE Intelligent Transportation Systems Conference (IEEE-ITSC)	OCT 27-30, 2019OCT 27-30, 2019	IEEE; Intel; Griffith Univ; Auckland Tourism Events & Econ DevIEEE; Intel; Griffith Univ; Auckland Tourism Events & Econ Dev	Auckland, NEW ZEALANDAuckland, NEW ZEALAND	10	0	0	0	0	0	11			2153-0009		978-1-5386-7024-8									Univ Grenoble Alpes, INRIA, Grenoble INP, F-38000 Grenoble, France				2020-04-22	WOS:000521238103080		
B	Bhadani, Rahul Kumar										Design and Synthesis of Controllers for Societal-Scale Cyber-Physical Systems																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798426829091									The University of Arizona, Electrical & Computer Engineering, Arizona, United States	The University of Arizona				PQDT:49498792		
J	Lee, Dongsu; Kwon, Minhae				lee, dongsu/HPD-2513-2023						ADAS-RL: Safety learning approach for stable autonomous driving								ICT EXPRESS				8	3			479	483				10.1016/j.icte.2022.05.004					SEP 2022		Article; Early Access		2022	Stability is the most significant component of an autonomous driving system, affecting both the lives of drivers and pedestrians and traffic flow. Reinforcement learning (RL) is a representative technology used in autonomous driving, but it has challenges because it is based on trial and error. In this letter, we propose an efficient learning approach for stable autonomous driving. The proposed deep reinforcement learning based approach can address the partially observable scenario in mixed traffic which includes both autonomous vehicles and human-driven vehicles. Simulation results show that the proposed model outperforms the control-theoretic and vanilla RL approaches. Furthermore, we confirm the effect of the sync-penalty, which teaches the agent about unsafe decisions without experiencing the accidents. (C) 2022 The Author(s). Published by Elsevier B.V. on behalf of The Korean Institute of Communications and Information Sciences.									2	0	0	0	0	0	2			2405-9595											Soongsil Univ, 369 Sangdo Ro, Seoul 06978, South Korea				2022-10-17	WOS:000864151000025		
C	Chung, Seung-Hwan; Kong, Seung-Hyun; Cho, Sangjae; Nahrendra, I. Made Aswin			IEEE							Segmented Encoding for Sim2Real of RL-based End-to-End Autonomous Driving								2022 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						1290	1296				10.1109/IV51971.2022.9827374							Proceedings Paper	2022	2022	Among the challenges in the recent research of end-to-end (E2E) driving, interpretability and distribution shift in the simulation-to-real (Sim2Real) have drawn considerable attention. Because of low interpretability, we cannot clearly explain the causal relationship between the input image and the control actions by the network. Moreover, the distribution shift problem in Sim2Real degrades the driving performance of the policy in the realworld deployment. In this paper, we propose a segmentation-based classwise disentangled latent encoding algorithm to cope with the two challenges. In the proposed algorithm, multi-class segmentation transfers RGB images in both simulation and real environments to the same domain, while preserving the necessary information of objects of primary classes, such as pedestrian, road, and cars, for driving decisions. Besides, in the class-wise disentangled latent encoding, segmented images are encoded to a latent vector, which improves the interpretability significantly, since the state input has a structured format. The interpretability improvement is testified by the t-stochastic neighbor embedding, image reconstruction and the causal relationship between the real images and the control actions. We deploy the driving policy trained in the simulation directly to an autonomous vehicle platform and show, to the best of our knowledge, the first demonstration of the RL-based E2E autonomous in various real environments.					33rd IEEE Intelligent Vehicles Symposium (IEEE IV)33rd IEEE Intelligent Vehicles Symposium (IEEE IV)	JUN 05-09, 2022JUN 05-09, 2022	IEEE; Bosch; IEEE Intelligent Transport Syst Soc; RWTH Aachen Univ, IKA; Mercedes BenzIEEE; Bosch; IEEE Intelligent Transport Syst Soc; RWTH Aachen Univ, IKA; Mercedes Benz	Aachen, GERMANYAachen, GERMANY	2	0	0	0	0	0	2			1931-0587		978-1-6654-8821-1									Korea Adv Inst Sci & Technol, Grad Sch Mobil, 193 Munji Ro, Daejeon, South KoreaNaver Labs, Gyeonggi, South KoreaKorea Adv Inst Sci & Technol, Robot Program, 291 Daehak Ro, Daejeon, South Korea				2022-10-13	WOS:000854106700181		
C	Xiong, Lu; Cao, Lei; Leng, Bo; Liu, Ming			IEEE	Liu, Ming/A-1489-2013						Noncooperative-Game-Based Intelligent Vehicle Decision ethod fo Lane-Changing Interactive Behavior								2022 34TH CHINESE CONTROL AND DECISION CONFERENCE, CCDC		Chinese Control and Decision Conference						28	34				10.1109/CCDC55256.2022.10033451							Proceedings Paper	2022	2022	In the future transportation, it is a foreseeable trend that human-driven vehicles and autonomous vehicles will coexist and interact on roads for a long time. how to integrate autonomous vehicles into human drivers' traffic ecology and make behavioral decisions and predictions in the complex environment considering vehicle-vehicle interaction is worthy of deep consideration. Therefore, the proposed game theoretic method is applied to automatic lane-changing scenarios. Then, the safety risk, ride comfort and traffic efficiency of vehicles in game are quantitatively evaluated as cost functions. Specifically, the driving maneuvers of interactive agents could be predicted by reasoning, and the optimal strategy is obtained by calculating Nash equilibrium. Finally, simulations are conducted using Commonroad under typical driving conditions. The results illustrate that our model could help autonomous vehicles make reasonable and explainable lane-changing decision when interacting with heterogeneous agents, demonstrating the feasibility and reliability of our proposed approach.					34th Chinese Control and Decision Conference (CCDC)34th Chinese Control and Decision Conference (CCDC)	AUG 15-17, 2022AUG 15-17, 2022		Hefei, PEOPLES R CHINAHefei, PEOPLES R CHINA	0	0	0	0	0	0	0			1948-9439		978-1-6654-7896-0									Tongji Univ, Sch Automot Studies, Shanghai 201804, Peoples R ChinaTongji Univ, Postdoctoral Stn Mech Engn, Shanghai 201804, Peoples R China				2023-05-31	WOS:000972039300004		
J	Wiberg, Viktor; Wallin, Erik; Nordfjell, Tomas; Servin, Martin					Wiberg, Viktor/0000-0001-6565-3123; Servin, Martin/0000-0002-0787-4988					Control of Rough Terrain Vehicles Using Deep Reinforcement Learning								IEEE ROBOTICS AND AUTOMATION LETTERS				7	1			390	397				10.1109/LRA.2021.3126904							Article	JAN 2022	2022	We explore the potential to control terrain vehicles using deep reinforcement in scenarios where human operators and traditional control methods are inadequate. This letter presents a controller that perceives, plans, and successfully controls a 16-tonne forestry vehicle with two frame articulation joints, six wheels, and their actively articulated suspensions to traverse rough terrain. The carefully shaped reward signal promotes safe, environmental, and efficient driving, which leads to the emergence of unprecedented driving skills. We test learned skills in a virtual environment, including terrains reconstructed from high-density laser scans of forest sites. The controller displays the ability to handle obstructing obstacles, slopes up to 27 degrees, and a variety of natural terrains, all with limited wheel slip, smooth, and upright traversal with intelligent use of the active suspensions. The results confirm that deep reinforcement learning has the potential to enhance control of vehicles with complex dynamics and high-dimensional observation data compared to human operators or traditional control methods, especially in rough terrain.									7	1	0	0	0	0	8			2377-3766											Umea Univ, Dept Phys, S-90338 Umea, SwedenSwedish Univ Agr Sci, S-75007 Uppsala, Sweden				2021-11-30	WOS:000721999500008		
B	Zhang, Ethan										A Predictive-Prescriptive Safety Framework at Intersections in a Connected Vehicle Environment																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798845458193									University of Michigan, Civil Engineering, Michigan, United States	University of Michigan				PQDT:68569486		
J	You, Changxi; Lu, Jianbo; Filev, Dimitar; Tsiotras, Panagiotis				You, Changxi/AAQ-8713-2020; Lu, Jianbo/Z-1316-2018; You, Changxi/Y-6598-2018	Lu, Jianbo/0000-0001-9088-5663; You, Changxi/0000-0003-0359-7917					Autonomous Planning and Control for Intelligent Vehicles in Traffic								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				21	6			2339	2349				10.1109/TITS.2019.2918071							Article	JUN 2020	2020	This paper addresses the trajectory planning problem for autonomous vehicles in traffic. We build a stochastic Markov decision process (MDP) model to represent the behaviors of the vehicles. This MDP model takes into account the road geometry and is able to reproduce more diverse driving styles. We introduce a new concept, namely, the "dynamic cell," to dynamically modify the state of the traffic according to different vehicle velocities, driver intents (signals), and the sizes of the surrounding vehicles (i.e., truck, sedan, and so on). We then use Bezier curves to plan smooth paths for lane switching. The maximum curvature of the path is enforced via certain design parameters. By designing suitable reward functions, different desired driving styles of the intelligent vehicle can be achieved by solving a reinforcement learning problem. The desired driving behaviors (i.e., autonomous highway overtaking) are demonstrated with an in-house developed traffic simulator.									35	3	0	0	0	0	39			1524-9050	1558-0016										Tencent Technol Co, Beijing 100084, Peoples R ChinaFord Motor Co, Res & Adv Engn, Dearborn, MI 48121 USAGeorgia Inst Technol, Sch Aerosp Engn, Atlanta, GA 30332 USAGeorgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA	Tencent Technol Co			2020-07-16	WOS:000545427200010		
J	Zhang, Z.; Huang, D.; Huang, C.; Hu, L.; Du, R.										TD3 Algorithm Improving and Lane-merging Strategy Learning for Autonomous Vehicles								Journal of Mechanical Engineering								224	34				10.3901/JME.2023.08.224							Journal Paper	2023	2023	To enhance the comprehensive performance of automotive lane-merging, the Q-value estimation method of twin delayed deep deterministic policy gradient(TD3) algorithm and the reward function are improved. The automotive lane-merging model is formalized as the Markov decision process, and the influences of Q-value underestimated by TD3 algorithm on lane-merging strategy are analyzed. A Q-value estimation method based on weighted average of sample variance is proposed to enhance the Q-value estimation accuracy, when two Q-value estimation samples are obtained by performing Monte Carlo dropout on the dual target critic network. With giving priority to the completion of the lane-merging, a more perfect reward function is designed considering the safety, comfort and traffic efficiency. Based on the improved TD3 algorithm and the reward function, a lane-merging strategy of autonomous vehicles is learned and verified with BARK simulator. The results show that the improved TD3 algorithm significantly enhances the accuracy of Q-value estimation. Combined with the established reward function, the safety and ride comfort of lane-merging are improved while ensuring traffic efficiency. © 2023 Journal of Mechanical Engineering.									0	0	0	0	0	0	0			0577-6686											Hunan Province Key Lab. of Intelligent Manuf. Technol. for High-performance Mech. Equip., Changsha Univ. of Sci. & Technol., Changsha, ChinaColl. of Automobile & Mech. Eng., Changsha Univ. of Sci. & Technol., Changsha, ChinaHunan Provincial Key Lab. of Automotive Power & Transmission Syst., Hunan Inst. of Technol., Hengyang, China				2023-09-29	INSPEC:23720794		
J	Zhang, Meng; Abbas-Turki, Abdeljalil; Mualla, Yazan; Koukam, Abderrafiaa; Tu, Xiaowei				Mualla, Yazan/AAE-3632-2019	Mualla, Yazan/0000-0002-6772-6135; ZHANG, Meng/0000-0002-7976-3152					Coordination Between Connected Automated Vehicles and Pedestrians to Improve Traffic Safety and Efficiency at Industrial Sites								IEEE ACCESS				10				68029	68041				10.1109/ACCESS.2022.3185734							Article	2022	2022	Transportation in controlled industrial sites provides a conducive environment for technologies of Connected Automated Vehicles (CAV). Recent studies show that safe and efficient road sharing between CAVs and pedestrians is challenging. Besides safety issues, a significant loss of time occurs when pedestrians cross a stream of CAVs. Currently, many techniques have been employed to improve the coordination between CAVs and pedestrians. They focus on pedestrian detection, display of the intention of CAVs, and cooperative collision avoidance. However, one of the most significant sources of information that the pedestrian uses for her/his decision-making is the speed profile of CAVs. This paper aims to provide a safe and efficient pedestrian crossing at industrial sites through communicative crossing behavior. To this end, a suitable speed profile of the CAV is designed by assuming that pedestrians and CAV play a cooperative game to move as close as possible to their desired speed. First, a system analysis is proposed to derive the optimal decision and trajectory for each agent. Then, Deep Reinforcement Learning (DRL) is used to control the longitudinal speed of the CAVs. Compared with Model Predictive Control approach, DRL allows coping with unforeseeable pedestrian behaviors (e.g. long reaction time, varying ideal speed, stop in the middle, etc.). Simulations and experiments with real human testers based on immersive hamlet are performed. Results show that the proposed speed profile outperforms significantly the collision avoidance approach.									1	0	0	0	0	0	1			2169-3536											Univ Bourgogne Franche Comte, UTBM, CIAD, F-90010 Belfort, FranceShanghai Univ, Inst Mechatron, Intelligent Sensor & Control Technol Lab, Shanghai 200444, Peoples R China				2022-07-08	WOS:000819816500001		
J	Hou, Xiaohui; Zhang, Junzhi; He, Chengkun; Ji, Yuan; Zhang, Junfeng; Han, Jinheng				zhang, junfeng/JHT-7871-2023						Autonomous driving at the handling limit using residual reinforcement learning								ADVANCED ENGINEERING INFORMATICS				54						101754			10.1016/j.aei.2022.101754					SEP 2022		Article; Early Access		2022	While driving a vehicle safely at its handling limit is essential in autonomous vehicles in Level 5 autonomy, it is a very challenging task for current conventional methods. Therefore, this study proposes a novel controller of trajectory planning and motion control for autonomous driving through manifold corners at the handling limit to improve the speed and shorten the lap time of the vehicle. The proposed controller innovatively combines the advantages of conventional model-based control algorithm, model-free reinforcement learning algorithm, and prior expert knowledge, to improve the training efficiency for autonomous driving in extreme conditions. The reward shaping of this algorithm refers to the procedure and experience of race training of professional drivers in real time. After training on track maps that exhibit different levels of difficulty, the proposed controller implemented a superior strategy compared to the original reference trajectory, and can to other tougher maps based on the basic driving knowledge learned from the simpler map, which verifies its superiority and exten-sibility. We believe this technology can be further applied to daily life to expand the application scenarios and maneuvering envelopes of autonomous vehicles.									3	0	0	0	0	0	3			1474-0346	1873-5320										Tsinghua Univ, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R ChinaTsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R China				2022-11-02	WOS:000870556000002		
J	Mavrogiannis, Angelos; Chandra, Rohan; Manocha, Dinesh					Manocha, Dinesh/0000-0001-7047-9801; Chandra, Rohan/0000-0003-4843-6375					B-GAP: Behavior-Rich Simulation and Navigation for Autonomous Driving								IEEE ROBOTICS AND AUTOMATION LETTERS				7	2			4718	4725				10.1109/LRA.2022.3152594							Article	APR 2022	2022	We address the problem of ego-vehicle navigation in dense simulated traffic environments populated by road agents with varying driver behaviors. Navigation in such environments is challenging due to unpredictability in agents' actions caused by their heterogeneous behaviors. We present a new simulation technique consisting of enriching existing traffic simulators with behavior-rich trajectories corresponding to varying levels of aggressiveness. We generate these trajectories with the help of a driver behavior modeling algorithm. We then use the enriched simulator to train a deep reinforcement learning (DRL) policy that consists of a set of high-level vehicle control commands and use this policy at test time to perform local navigation in dense traffic. Our policy implicitly models the interactions between traffic agents and computes safe trajectories for the ego-vehicle accounting for aggressive driver maneuvers such as overtaking, over-speeding, weaving, a nd sudden lane changes. Our enhanced behavior-rich simulator can be used for generating datasets that consist of trajectories corresponding to diverse driver behaviors and traffic densities, and our behavior-based navigation scheme can be combined with state-of-the-art navigation algorithms.									7	0	0	0	0	0	7			2377-3766											Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA				2022-03-21	WOS:000766269000030		
J	Siebinga, Olger; Zgonnikov, Arkady; Abbink, David				Zgonnikov, Arkady/B-2035-2012; Abbink, David/G-2650-2015	Zgonnikov, Arkady/0000-0002-6593-6948; Abbink, David/0000-0001-7778-0090; Siebinga, Olger/0000-0002-5614-1262					A Human Factors Approach to Validating Driver Models for Interaction-aware Automated Vehicles								ACM TRANSACTIONS ON HUMAN-ROBOT INTERACTION				11	4					47			10.1145/3538705							Article	DEC 2022	2022	A major challenge for autonomous vehicles is interacting with other traffic participants safely and smoothly. A promising approach to handle such traffic interactions is equipping autonomous vehicles with interaction-aware controllers (IACs). These controllers predict how surrounding human drivers will respond to the autonomous vehicle's actions, based on a driver model. However, the predictive validity of driver models used in IACs is rarely validated, which can limit the interactive capabilities of IACs outside the simple simulated environments in which they are demonstrated. In this article, we argue that besides evaluating the interactive capabilities of IACs, their underlying driver models should be validated on natural human driving behavior. We propose a workflow for this validation that includes scenario-based data extraction and a two-stage (tactical/operational) evaluation procedure based on human factors literature. We demonstrate this workflow in a case study on an inverse-reinforcement-learning-based driver model replicated from an existing IAC. This model only showed the correct tactical behavior in 40% of the predictions. The model's operational behavior was inconsistent with observed human behavior. The case study illustrates that a principled evaluation workflow is useful and needed. We believe that our workflow will support the development of appropriate driver models for future automated vehicles.									9	0	0	0	1	0	9				2573-9522										Delft Univ Technol, Fac Mech Maritime & Mat Engn 3mE, Mekelweg 2, NL-2628 CD Delft, Netherlands				2022-10-02	WOS:000859358400013		
J	Mazouchi, Majid; Nageshrao, Subramanya P. P.; Modares, Hamidreza				Mazouchi, Majid/AAS-1294-2020	Mazouchi, Majid/0000-0003-2069-8760					A Risk-Averse Preview-Based <i>Q</i> -Learning Algorithm: Application to Highway Driving of Autonomous Vehicles								IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY				31	4			1803	1818				10.1109/TCST.2023.3245824					FEB 2023		Article; Early Access		2023	A risk-averse preview-based Q-learning planner is presented for navigation of autonomous vehicles (AVs). To this end, the multilane road ahead of a vehicle is represented by a finite-state nonstationary Markov decision process (MDP). A risk assessment unit module is then presented, which leverages the preview information provided by sensors along with a stochastic reachability module to assign reward values to the MDP states and update them as scenarios develop. A sampling-based risk averse preview-based Q-learning algorithm is finally developed, which generates samples using the preview information and reward function to learn risk-averse optimal planning strategies without actual interaction with the environment. The risk factor is imposed on the objective function to avoid fluctuation of the Q values, which can jeopardize the vehicle's safety and/or performance. The overall hybrid automaton model of the system is leveraged to develop a feasibility check unit module that detects unfeasible plans and enables the planner system to react proactively to the changes of the environment. Finally, to verify the efficiency of the presented algorithm, its implementation on two highway driving scenarios of an AV in a varying traffic density is considered.									0	0	0	0	0	0	0			1063-6536	1558-0865										Michigan State Univ, Dept Mech Engn, E Lansing, MI 48863 USAFord Motor Co, Ford Res & Innovat Ctr, Palo Alto, CA 94304 USA				2023-03-25	WOS:000943487100001		
B	Wright, Matthew Abbott										Studies on Complex and Connected Vehicle Traffic Networks																												Dissertation/Thesis	Jan 01 2019	2019										0	0	0	0	0	0	0					9781392546062									University of California, Berkeley, Mechanical Engineering, California, United States	University of California, Berkeley				PQDT:68614958		
J	Cao, Zhong; Xu, Shaobing; Jiao, Xinyu; Peng, Huei; Yang, Diange										Trustworthy safety improvement for autonomous driving using reinforcement learning								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				138						103656			10.1016/j.trc.2022.103656					MAR 2022		Article; Early Access		2022	Reinforcement learning (RL) can learn from past failures and has the potential to provide selfimprovement ability and higher-level intelligence. However, the current RL algorithms still suffer from challenges in reliability, especially compared to the rule/model-based algorithms that are pre-engineered, human-input intensive, but widely used in autonomous vehicles. To take advantages of both the RL and rule-based algorithms, this work aims to design a decision-making framework that leverages RL and use an existing rule-based policy as its performance lower bound. In this way, the final policy remains the potential of self-learning, while guaranteeing a better system performance compared with the integrated rule-based policy. Such a decision making framework is called trustworthy improvement RL (TiRL). The basic idea is to make the RL policy iteration process synchronously estimate the given rule-based policy's value function. AV will then use the RL policy to drive only in the cases where the RL has learned a better policy, i.e., a higher policy value. This work takes highway safe driving as the case study. The results are obtained through more than 42,000 km driving in stochastic simulated traffic, and calibrated by naturalistic driving data. The TiRL planner is given two typical rule-based highway-driving policies for comparison. The results show that the TiRL can outperform the given arbitrary rule based driving policy. In summary, the proposed TiRL can leverage the learning-based method in stochastic and emergent scenarios, while having a trustworthy safety improvement from the existing rule-based policies.									11	0	0	0	0	0	11			0968-090X	1879-2359										Tsinghua Univ, Sch Vehicle & Mobil, Beijing 100084, Peoples R ChinaUniv Michigan, Dept Mech Engn, Ann Arbor, MI 48109 USA				2022-05-14	WOS:000790897500002		
J	Pan Feng; Bao Hong							潘峰; 鲍泓			Research progress of automatic driving control technology based on reinforcement learning			强化学习的自动驾驶控制技术研究进展				中国图象图形学报	Journal of Image and Graphics				26	1			28	35	1006-8961(2021)26:1<28:QHXXDZ>2.0.TX;2-I										Review	2021	2021	Research on fully automatic driving has been largely spurred by some important international challenges and competitions,such as the well-known Defense Advanced Research Projects Agency Grand Challenge held in 2005. Selfdriving cars and autonomous vehicles have migrated from laboratory development and testing conditions to driving on public roads. Self-driving cars are autonomous decision-making systems that process streams of observations coming from different on-board sources,such as cameras,radars,lidars,ultrasonic sensors,global positioning system units,and/or inertial sensors. The development of autonomous vehicles offers a decrease in road accidents and traffic congestions. Most driving scenarios can be simply solved with classical perception,path planning,and motion control methods. However,the remaining unsolved scenarios are corner cases where traditional methods fail. In the past decade,advances in the field of artificial intelligence (AI) and machine learning (ML) have greatly promoted the development of autonomous driving. Autonomous driving is a challenging application domain for ML . ML methods can be divided into supervised learning,unsupervised learning,and reinforcement learning (RL) . RL is a family of algorithms that allow agents to learn how to act in different situations. In other words,a map or a policy is established from situations (states) to actions to maximize a numerical reward signal. Most autonomous vehicles have a modular hierarchical structure and can be divided into four components or four layers,namely,perception,decision making,control,and actuator. RL is suitable for decision making and control in complex traffic scenarios to improve the safety and comfort of autonomous driving. Traditional controllers utilize an a priori model composed of fixed parameters. When robots or other autonomous systems are used in complex environments,such as driving,traditional controllers cannot foresee every possible situation that the system has to cope with. An RL controller is a learning controller and uses training information to learn their models over time. With every gathered batch of training data,the approximation of the true system model becomes accurate. Deep neural networks have been applied as function approximators for RL agents,thereby allowing agents to generalize knowledge to new unseen situations,along with new algorithms for problems with continuous state and action spaces. This paper mainly introduces the current status and progress of the application of RL methods in autonomous driving control. This paper consists of five sections. The first section introduces the background of autonomous driving and some basic knowledge about ML and RL. The second section briefly describes the architecture of autonomous driving framework. The control layer is an important part of an autonomous vehicle and has always been a key area of autonomous driving technology research. The control system of autonomous driving mainly includes lateral control and longitudinal control,namely,steering control and velocity control. Lateral control deals with the path tracking problem,and longitudinal control deals with the problem of tracking the reference speed and keeping a safe distance from the preceding vehicle. The third section introduces the basic principles of RL methods and focuses on the current research status of RL in autonomous driving control. RL algorithms are based on Markov decision process and aim to learn mapping from situations to actions to maximize a scalar reward or reinforcement signal. RL is a new and extremely old topic in AI. It gradually became an active and identifiable area of ML in 1980 s. Q-learning is a widely used RL algorithm.			自动驾驶车辆的本质是轮式移动机器人,是一个集模式识别、环境感知、规划决策和智能控制等功能于一体的综合系统。人工智能和机器学习领域的进步极大推动了自动驾驶技术的发展。当前主流的机器学习方法分为:监督学习、非监督学习和强化学习3种。强化学习方法更适用于复杂交通场景下自动驾驶系统决策和控制的智能处理,有利于提高自动驾驶的舒适性和安全性。深度学习和强化学习相结合产生的深度强化学习方法成为机器学习领域中的热门研究方向。首先对自动驾驶技术、强化学习方法以及自动驾驶控制架构进行简要介绍,并阐述了强化学习方法的基本原理和研究现状。随后重点阐述了强化学习方法在自动驾驶控制领域的研究历史和现状,并结合北京联合大学智能车研究团队的研究和测试工作介绍了典型的基于强化学习的自动驾驶控制技术应用,讨论了深度强化学习的潜力。最后提出了强化学习方法在自动驾驶控制领域研究和应用时遇到的困难和挑战,包括真实环境下自动驾驶安全性、多智能体强化学习和符合人类驾驶特性的奖励函数设计等。研究有助于深入了解强化学习方法在自动驾驶控制方面的优势和局限性,在应用中也可作为自动驾驶控制系统的设计参考。						2	2	0	0	1	0	4			1006-8961											北京化工大学;;北京联合大学, ;;, ;;, 北京;;北京 100029;;100101, 中国北京联合大学, 北京 100101, 中国Beijing University of Chemical Technology;;Beijing Union University, ;;, ;;, Beijing;;Beijing 100029;;100101Beijing Union University, Beijing 100101, China	北京化工大学;;北京联合大学北京联合大学Beijing University of Chemical Technology;;Beijing Union UniversityBeijing Union University			2021-05-21	CSCD:6912186		
J	Lin, Jie; Huang, Siqi; Zhang, Hanlin; Yang, Xinyu; Zhao, Peng				Huang, Siqi/JUF-3769-2023	Zhang, Hanlin/0000-0001-8869-6863					A Deep-Reinforcement-Learning-Based Computation Offloading With Mobile Vehicles in Vehicular Edge Computing								IEEE INTERNET OF THINGS JOURNAL				10	17			15501	15514				10.1109/JIOT.2023.3264281							Article	SEPT 1 2023	2023	Vehicular edge networks involve edge servers that are close to mobile devices to provide extra computation resource to complete the computation tasks of mobile devices with low latency and high reliability. Considerable efforts on computation offloading in vehicular edge networks have been developed to reduce the energy consumption and computation latency, in which roadside units (RSUs) are usually considered as the fixed edge servers (FESs). Nonetheless, the computation offloading with considering mobile vehicles as mobile edge servers (MESs) in vehicular edge networks still needs to be further investigated. To this end, in this article, we propose a Deep-Reinforcement-Learning-based computation offloading with mobile vehicles in vehicular edge computing, namely, Deep-Reinforcement-Learning-based computation offloading scheme (DRL-COMV), in which some vehicles (such as autonomous vehicle) are deployed and considered as the MESs that move in vehicular edge networks and cooperate with FESs to provide extra computation resource for mobile devices, in order to assist in completing the computation tasks of these mobile devices with great Quality of Experience (QoE) (i.e., low latency) for mobile devices. Particularly, the computation offloading model with considering both mobile and FESs is conducted to achieve the computation tasks offloading through vehicle-to-vehicle (V2V) communications, and a collaborative route planning is considered for these MESs to move in vehicular edge networks with objective of improving efficiency of computation offloading. Then, a Deep-Reinforcement-Learning approach with designing rational reward function is proposed to determine the effective computation offloading strategies for multiple mobile devices and multiple edge servers with objective of maximizing both QoE (i.e., low latency) for mobile devices. Through performance evaluations, our results show that our proposed DRL-COMV scheme can achieve a great convergence and stability. Additionally, our results also demonstrate that our DRL-COMV scheme also can achieve better both QoE and task offloading requests hit ratio for mobile devices in comparison with existing approaches (i.e., DDPG, IMOPSOQ, and GABDOS).									1	0	0	0	0	0	1			2327-4662											Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R ChinaXi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R ChinaQingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China				2023-10-28	WOS:001075378800040		
J	Qian, Yubin; Feng, Song; Hu, Wenhao; Wang, Wanqiu					Feng, Song/0000-0002-8397-2440					Obstacle avoidance planning of autonomous vehicles using deep reinforcement learning								ADVANCES IN MECHANICAL ENGINEERING				14	12					16878132221139661			10.1177/16878132221139661							Article	DEC 2022	2022	Obstacle avoidance path planning in a dynamic circumstance is one of the fundamental problems of autonomous vehicles, counting optional maneuvers: emergency braking and active steering. This paper proposes emergency obstacle avoidance planning based on deep reinforcement learning (DRL), considering safety and comfort. Firstly, the vehicle emergency braking and lane change processes are analyzed in detail. A graded hazard index is defined to indicate the degree of the potential risk of the current vehicle movement. The longitudinal distance and lateral waypoint models are established, including the comfort deceleration and stability coefficient considerations. Simultaneously, a fuzzy PID controller is installed to track to satisfy the stability and feasibility of the path. Then, this paper proposes a DRL process to determine the obstacle avoidance plan. Mainly, multi-reward functions are designed for different collisions, corresponding penalties for longitudinal rear-end collisions, and lane-changing side collisions based on the safety distance, comfort reward, and safety reward. Apply a special DRL method-DQN to release the planning program. The difference is that the long and short-term memory (LSTM) layer is utilized to solve incomplete observations and improve the efficiency and stability of the algorithm in a dynamic environment. Once the policy is practiced, the vehicle can automatically perform the best obstacle avoidance maneuver in an emergency, improving driving safety. Finally, this paper builds a simulated environment in CARLA and is trained to evaluate the effectiveness of the proposed algorithm. The collision rate, safety distance difference, and total reward index indicate that the collision avoidance path is generated safely, and the lateral acceleration and longitudinal velocity satisfy the comfort requirements. Besides, the method proposed in this paper is compared with traditional DRL, which proves the beneficial performance in safety and efficiency.									0	0	0	0	0	0	0			1687-8132	1687-8140										Shanghai Univ Engn Sci, Sch Mech & Automot Engn, Songjiang Campus LongTeng Rd 333, Shanghai 201620, Peoples R ChinaDefect Prod Adm Ctr SAMR, Beijing, Peoples R China	Defect Prod Adm Ctr SAMR			2023-02-23	WOS:000928072100001		
J	Masmoudi, Mehdi; Friji, Hamdi; Ghazzai, Hakim; Massoud, Yehia				FRIJI, Hamdi/ABB-9977-2021; Ghazzai, Hakim/K-2518-2019	Ghazzai, Hakim/0000-0002-8636-4264; Massoud, Yehia/0000-0002-6701-0639; Friji, Hamdi/0000-0001-7381-6164					A Reinforcement Learning Framework for Video Frame-Based Autonomous Car-Following								IEEE OPEN JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS				2				111	127				10.1109/OJITS.2021.3083201							Article	2021	2021	Car-following theory has received considerable attention as a core component of Intelligent Transportation Systems. However, its application to the emerging autonomous vehicles (AVs) remains an unexplored research area. AVs are designed to provide convenient and safe driving by avoiding accidents caused by human errors. They require advanced levels of recognition of other drivers' driving-style. With car-following models, AVs can use their built-in technology to understand the environment surrounding them and make real-time decisions to follow other vehicles. In this paper, we design an end-to-end car-following framework for AVs using automated object detection and navigation decision modules. The objective is to allow an AV to follow another vehicle based on Red Green Blue Depth (RGB-D) frames. We propose to employ a joint solution involving the You Look Once version 3 (YOLOv3) object detector to identify the leader vehicle and other obstacles and a reinforcement learning (RL) algorithm to navigate the self-driving vehicle. Two RL algorithms, namely Q-learning and Deep Q-learning have been investigated. Simulation results show the convergence of the developed models and investigate their efficiency in following the leader. It is shown that, with video frames only, promising results are achieved and that AVs can adopt a reasonable car-following behavior.									17	0	0	0	0	0	17				2687-7813										Stevens Inst Technol, Sch Syst & Enterprises, Hoboken, NJ 07030 USA				2021-06-23	WOS:000660860800001		
J	Wang, Mei; Ma, Chen; Li, Zhanli; Zhang, Siming; Li, Yuancheng				Chen, Chao/JHS-6563-2023; Li, yuancheng/IUO-3866-2023; zhang, wen/JXN-0191-2024	Wang, Mei/0000-0001-7834-5517; yuancheng, Li/0000-0002-9185-9974					Alertness Estimation Using Connection Parameters of the Brain Network								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	12			25448	25457				10.1109/TITS.2021.3124372					NOV 2021		Article; Early Access		2022	Alertness mechanism of unmanned monitoring vehicles to environment is important. Especially, the vigilance modeling of underground security robots has a particularly significance because the underground is a dangerous environment. However, there is no a mature methodology for the alertness computation. In this work, four parts of the alertness estimation are focused. First, an autonomous robot alertness mechanism framework is proposed by using the deep reinforcement learning model of the human alertness mechanism. Second, a fast K-T filtering algorithm is developed to eliminate the multiple noises of the electroencephalograph (EEG) signals by the blind source separation and the adjustable Q factor wavelet transform. Third, the description problem of the directed interaction stability of the cortical EEG signals is solved by the ensemble empirical mode decomposition and the directional transfer function. Fourth, the human alertness estimation is explored by using the support vector regression of the dynamically spatial-temporal brain network connection parameters. Experiments show that, the mean square error and the determination coefficient of the explored alertness estimation are respectively 0.115 and 0.8337. Compared with the scalp EEG alertness estimation, it has a better performance because the mean square error is decreased by 0.0684, and the determination coefficient is increased by 0.023.									6	0	0	0	2	0	6			1524-9050	1558-0016										Xian Univ Sci & Technol, Dept Comp Sci & Technol, Xian 710054, Peoples R ChinaHollysys, Xian 710000, Peoples R China	Hollysys			2021-12-25	WOS:000732186800001		
J											Effective and Semantic Communication in Multi-Agent Reinforcement Learning																												Awarded Grant	Sep 30 2021	2021	My project focuses on designing goal-oriented communication frameworks and systems using machine learning optimisation techniques.The communication problem can be divided into 3 levels:1. Technical problem: How accurately can the symbols of communication be transmitted?2. Semantic problem: How precisely do the transmitted symbols convey the desired meaning?3. Effectiveness problem: How effectively does the received meaning affect conduct in the desired way?The leading prevailing paradigm is the technical problem perspective. Consider streaming visual media or guiding a remote-controlled rover. Current approaches to communications consider a layered strategy. First, the message (for instance video frame or a rover command) is mapped to a bit pattern to remove redundancy. Next, the compressed bit pattern is protected against channel distortion with an error-correcting code. Finally, the protected bits are mapped to channel symbols for transmission. After the transmission, the process is reversed at the receiver. Each step of this process has been extensively researched in the last 80 years. This method has a significant advantage - it allows for much simpler analysis at each stage. The problem is broken into subproblems that are more manageable to tackle.This approach is optimal for specific sources and long enough block codes - it cannot be beaten asymptotically. However, in general, this approach is not flawless. Communication is rarely the goal in itself. Instead, it is used to achieve some other end. Thus, the success of communication should be measured in the context of the overall objective. That is the main focus of my project - semantic and effective communication problems. The modular system fails to exploit the interactions, dependencies, and correlations between the steps. However, each of these stages is complex by itself. Thus, doing away with the modular approach is not feasible in a straightforward analytic manner. That is why the current advancements in statistical methods such as machine learning are especially promising. The ability to learn inductively from examples allows for joining the modules of communication into a single system. However, what constitutes the desired behaviour is not always apparent. Thus, another framework is introduced. Reinforcement learning is the set of algorithms that allows for learning complex behaviours through interactions with an environment. By introducing realistic communication channels, we can extend those methods to allow for learning of the communication schemes themselves jointly with the desired conduct. This framework can be extended to include multiple agents interacting and learning. This study is relevant to remote control problems, drone swarm navigation, coordinated autonomous vehicle driving, distributed learning, or industrial internet of things where the number of independent actors need to coordinate to achieve a common goal.Relevant EPSRC research areas: Artificial intelligence technologies, ICT networks and distributed systems, Digital signal processing, Statistics and applied probability.									0	0	0	0	0	0	0						2619796								Imperial College London				2023-12-08	GRANTS:15513718		
B	Leu, Jessica En Shiuan										Designing Integrated Strategies for Modularized Robotic Systems in Uncertain Environments																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798351475950									University of California, Berkeley, Mechanical Engineering, California, United States	University of California, Berkeley				PQDT:68348198		
B	Maia, Francisco Alexandre Lourenço										Hybrid Machine Learning/Simulation Approaches for Logistic Systems Optimization																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798835534807									Universidade do Porto (Portugal), Portugal	Universidade do Porto (Portugal)				PQDT:68599791		
J	Du, Yuchuan; Chen, Jing; Zhao, Cong; Liu, Chenglong; Liao, Feixiong; Chan, Ching-Yao				Zhao, Cong/F-8645-2019; Liao, Feixiong/AFM-1430-2022; Liu, Chenglong/S-3352-2017	Zhao, Cong/0000-0002-1017-9118; Liao, Feixiong/0000-0002-8911-0788; Liu, Chenglong/0000-0002-8421-7017					Comfortable and energy-efficient speed control of autonomous vehicles on rough pavements using deep reinforcement learning								TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				134						103489			10.1016/j.trc.2021.103489							Article	JAN 2022	2022	Rough pavements cause ride discomfort and energy inefficiency for road vehicles. Existing methods to address these problems are time-consuming and not adaptive to changing driving conditions on rough pavements. With the development of sensor and communication technologies, crowdsourced road and dynamic traffic information become available for enhancing driving performance, particularly addressing the discomfort and inefficiency issues by controlling driving speeds. This study proposes a speed control framework on rough pavements, envisioning the operation of autonomous vehicles based on the crowdsourced data. We suggest the concept of 'maximum comfortable speed' for representing the vertical ride comfort of oncoming roads. A deep reinforcement learning (DRL) algorithm is designed to learn comfortable and energyefficient speed control strategies. The DRL-based speed control model is trained using realworld rough pavement data in Shanghai, China. The experimental results show that the vertical ride comfort, energy efficiency, and computation efficiency increase by 8.22%, 24.37%, and 94.38%, respectively, compared to an optimization-based speed control model. The results indicate that the proposed framework is effective for real-time speed controls of autonomous vehicles on rough pavements.									42	3	0	0	0	0	45			0968-090X	1879-2359										Tongji Univ, Key Lab Rd & Traff Engn, Minist Educ, Shanghai 201804, Peoples R ChinaShanghai Engn Res Ctr Urban Infrastruct Renewal, Shanghai 200032, Peoples R ChinaEindhoven Univ Technol, Urban Planning & Transportat Grp, Eindhoven, NetherlandsUniv Calif Berkeley, Inst Transportat Studies, Calif PATH, Richmond, CA 94804 USA	Shanghai Engn Res Ctr Urban Infrastruct Renewal			2022-04-16	WOS:000779694700002		
B	Alqahtani, Mohammed										Integrated Energy Scheduling and Routing for a Network of Mobile Prosumers																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798534686869									University of Illinois at Chicago, Mechanical & Industrial Engr., Illinois, United States	University of Illinois at Chicago				PQDT:64680139		
B	Sousa, Bruno António Rodrigues										Parallel, Angular and Perpendicular Parking for Autonomously Driven Vehicles																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798381271928									Universidade do Minho (Portugal), Portugal	Universidade do Minho (Portugal)				PQDT:87270147		
C	Fandango, Armando; Gutierrez, Alexander; Hoayun, Clive; Hurter, Jonathan; Reed, Dean						Dijk, J				ARORA & NavSim: a simulator system for training autonomous agents with geospecific data								ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING IN DEFENSE APPLICATIONS IV		Proceedings of SPIE		12276						122760C			10.1117/12.2647733							Proceedings Paper	2022	2022	This study accompanies the initial public release of the software for ARORA, or A Realistic Open environment for Rapid Agent training, and marks a high point of several years of work for the mature and completely open ARORA simulator. The purpose of ARORA is to support the training of an autonomous agent for tasks associated with a large-scale and geospecific outdoor urban environment, including the task of navigation as a car. The study elaborates on the simulator's architecture, agent, and environment. For the environment, ARORA provides an improvement on similar simulators through an unconstrained geospecific environment with detailed semantic annotation. The agent is represented as a car available with four different options of physics fidelity. The agent also has sensors available: a pose sensor, a camera sensor, and a set of three proximity sensors. Future use cases from training extend to both civilians and militaries (including human training and wargaming), in terms of training autonomous agents in outdoor urban environments. The study also presents a brief description of NavSim, a Python-based companion tool. The purpose of NavSim is to connect to ARORA (or any other similar simulator) and train an agent using reinforcement-learning algorithms. The study also provides challenges in development and subsequent work-arounds and solutions. The goal of the ARORA & NavSim system is to provide communities with a high-fidelity, publicly available, free, and open-source system for training an autonomous agent as a car.					Conference on Artificial Intelligence and Machine Learning in Defense Applications IVConference on Artificial Intelligence and Machine Learning in Defense Applications IV	SEP 06-07, 2022SEP 06-07, 2022	SPIESPIE	Berlin, GERMANYBerlin, GERMANY	0	0	0	0	0	0	0			0277-786X	1996-756X	978-1-5106-5556-0									Inst Simulat & Training, 3100 Technol Pkwy, Orlando, FL 32826 USA	Inst Simulat & Training			2023-02-10	WOS:000906230300010		
J	Ekren, Banu Y.; Arslan, Bartu					Yetkin Ekren, Banu/0009-0009-4228-7795; Arslan, Bartu/0000-0003-2114-767X					A reinforcement learning approach for transaction scheduling in a shuttle-based storage and retrieval system								INTERNATIONAL TRANSACTIONS IN OPERATIONAL RESEARCH				31	1			274	295				10.1111/itor.13135					MAR 2022		Article; Early Access		2024	With recent Industry 4.0 developments, companies tend to automate their industries. Warehousing companies also take part in this trend. A shuttle-based storage and retrieval system (SBS/RS) is an automated storage and retrieval system technology experiencing recent drastic market growth. This technology is mostly utilized in large distribution centers processing mini-loads. With the recent increase in e-commerce practices, fast delivery requirements with low volume orders have increased. SBS/RS provides ultrahigh-speed load handling due to having an excess amount of shuttles in the system. However, not only the physical design of an automated warehousing technology but also the design of operational system policies would help with fast handling targets. In this work, in an effort to increase the performance of an SBS/RS, we apply a machine learning (ML) (i.e., Q-learning) approach on a newly proposed tier-to-tier SBS/RS design, redesigned from a traditional tier-captive SBS/RS. The novelty of this paper is twofold: First, we propose a novel SBS/RS design where shuttles can travel between tiers in the system; second, due to the complexity of operation of shuttles in that newly proposed design, we implement an ML-based algorithm for transaction selection in that system. The ML-based solution is compared with traditional scheduling approaches: first-in-first-out and shortest process time (i.e., travel) scheduling rules. The results indicate that in most cases, the Q-learning approach performs better than the two static scheduling approaches.									7	0	0	0	0	0	7			0969-6016	1475-3995										Yasar Univ, Dept Ind Engn, 37-39 Bornova, Izmir, TurkeyCranfield Univ, Sch Management, Cranfield, Beds, EnglandEindhoven Univ Technol, Dept Ind Engn & Innovat Sci, Eindhoven, Netherlands				2022-04-04	WOS:000774061400001		
J	Li, Duowei; Zhu, Feng; Wu, Jianping; Wong, Yiik Diew; Chen, Tianyi				Zhu, Feng/O-4219-2015; WONG, Y. D./A-3761-2011	WONG, Y. D./0000-0001-7419-5777; Li, Duowei/0000-0002-1940-2435; Chen, Tianyi/0000-0003-1744-9579					Managing mixed traffic at signalized intersections: An adaptive signal control and CAV coordination system based on deep reinforcement learning								EXPERT SYSTEMS WITH APPLICATIONS				238						121959			10.1016/j.eswa.2023.121959					OCT 2023		Article; Early Access		2024	Managing the mixed traffic involving connected and autonomous vehicles (CAVs) and human-driven vehicles (HVs) at a signalized intersection has become a concern of researchers. However, the performances of most existing control methods are limited, especially when CAV penetration rate is low, since they fail to make a better trade-off between safety and operational efficiency for both CAVs and HVs. To this end, this study proposes a deep reinforcement learning (DRL) powered control system for the mixed traffic at signalized intersections, which aims to optimize operational efficiency of both CAVs and HVs while assuring safety and reducing interference on HVs' driving habits. The system adopts an adaptive traffic signal control strategy and an efficient CAV control policy with a passing rule proposed as a link in between. The traffic signal control strategy allows traffic light to adaptively adjust its phase and duration based on real-time traffic information, while the CAV control policy permits the CAVs meeting certain safety constraints to form platoons and pass through the intersection in a coordinated manner regardless of traffic signals. As an efficient DRL algorithm, Deep Q-Network (DQN) is adopted to adaptively control the signals and implement CAV coordination. The proposed system is examined on Simulation of Urban Mobility (SUMO), given different CAV penetration rates and traffic conditions. It is found that the proposed system not only outperforms the state-of-the-art control methods on reducing travel time and fuel consumption under low CAV penetration rate, but also enlarges its advantages with the increase of CAV penetration rate. In certain traffic scenarios, the proposed system can even achieve a maximum reduction of travel time by 37.33% and fuel consumption by 15.95%, in comparison to the existing method with the best performance. Besides, to some extent, the comparisons between the performances of CAVs and HVs demonstrate certain benefits of introducing CAVs into the mixed traffic.									0	0	0	0	0	0	0			0957-4174	1873-6793										Tsinghua Univ, Tsinghua Univ Univ Cambridge Massachusetts Inst Te, Dept Civil Engn, Beijing, Peoples R ChinaNanyang Technol Univ, Sch Civil & Environm Engn, Singapore, SingaporeNatl Univ Singapore, Dept Civil & Environm Engn, Singapore, Singapore				2023-11-22	WOS:001098752400001		
J									Jain,  Rahul	Nuzzo,  Pierluigi	EAGER: Real-Time: Formal Reinforcement Learning Methods for the Design of Safety-critical Autonomous Systems																												Awarded Grant	Apr 01 2019	2019	This EArly-Concept Grant for Exploratory Research (EAGER) project takes a clean-slate first-principles approach to the design of safety-critical autonomous systems by integrating formal methods and reinforcement learning from data. Several recent high-profile traffic incidents involving semi-autonomous vehicles have raised questions about whether current artificial intelligence (AI)-centered methods can ever lead us to Level 4 or 5 autonomy, i.e., to the realization of fully-autonomous vehicles with performance equivalent to a human driver in all driving scenarios. On the other hand, approaches rooted in formal methods for verification and synthesis can provide safety guarantees but have difficulty in efficiently reasoning about uncertainty and the correctness of data-driven models. This project will combine these two, seemingly incompatible, paradigms for designing autonomous systems. It will use model-free reinforcement learning algorithms to learn from semi-autonomous vehicle driving data. It will adopt model-based methods for system design, verification, and synthesis to offer provably safe operation in highly uncertain scenarios. An AutoDrive testbed will be set up where human driving data from scaled vehicular models will be leveraged to infer safe control policies using imitation and inverse reinforcement learning algorithms. The research is relevant to the science of intelligent autonomous transportation systems with significant societal implications. The experimental testbed will be used to provide hands-on research experience to undergraduate students and for K-12 outreach efforts.<br/><br/>In particular, the project will develop a framework for optimal control synthesis for safety and performance specification expressed in signal temporal logic. It will then incorporate vehicular and pedestrian kinematics in non-deterministic/probabilistic  transition models specified via probabilistic computation tree logic. Finally, it will develop formal reinforcement learning methods for partially observed dynamic models subject to safety specifications and complex temporal goals by learning from traces of safe human drivers. One key technical contribution of the project will be development of new formal reinforcement learning methods that may be useful in a broad array of applications wherein we must synthesize optimal controllers that satisfy certain safety specifications by learning from data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.									0	0	0	0	0	0	0						1839842								University of Southern California				2023-12-08	GRANTS:13532756		
B	Lin, Theresa										Modeling of Human Decision Making via Direct and Optimization-based Methods for Semi-Autonomous Systems																												Dissertation/Thesis	Jan 01 2015	2015										0	0	0	0	0	0	0					978-1-369-05532-0									University of California, Berkeley, Mechanical Engineering, California, United States	University of California, Berkeley				PQDT:48828218		
J	Wang, Shuo; Fujii, Hideki; Yoshimura, Shinobu										Generating merging strategies for connected autonomous vehicles based on spatiotemporal information extraction module and deep reinforcement learning								PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS				607						128172			10.1016/j.physa.2022.128172					SEP 2022		Article; Early Access		2022	A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs. Deep reinforcement learning (DRL) is a promising approach to address this challenge. It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need compliance of HDVs. However, the fusion of CAV sensing information is non-trivial. Traditional DRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment. Aimed at tackling these issues, this study proposes a DRL framework named spatiotemporal deep Q network (STDQN), by integrating a double deep Q network (DDQN) and a spatiotemporal information extraction module. A long-short term memory neural network with an attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information. In addition, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as the connectivity of multiple CAVs in a global range. Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems. Experimental results prove that as compared to baseline DRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort. The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving.(c) 2022 Elsevier B.V. All rights reserved.									0	0	0	0	0	0	0			0378-4371	1873-2119										Univ Tokyo, Sch Engn, 7-3-1 Hongo, Bunkyo, Tokyo 1138656, Japan				2022-11-13	WOS:000876442800006		
J									Wang,  Zhaohui		CAREER: Online Learning-based Underwater Acoustic Communications and Networking																												Awarded Grant	Feb 15 2017	2017	Underwater acoustic communication networks are the enabling technologies for unmanned, in situ, and real-time aquatic monitoring in a wide range of applications, such as scientific studies, pollution detection, offshore exploration, and tactical surveillance. The lifespan of underwater systems varies from a few years to decades, while the spatiotemporal dynamics of underwater acoustic environments at multiple scales pose grand challenges to efficient and reliable acoustic data transmission. The objective of this project is to develop a fundamental and systematic online-learning-based framework for underwater acoustic communications and networking, where the underwater acoustic system 1) models and predicts the long-term dynamics of the acoustic environment, and 2) proactively adapts its communication and networking strategy to the dynamics of the environment, thereby maximizing the long-term system performance in the aspects of energy efficiency, spectrum efficiency, and transmission reliability. Through explicit learning about its environment, the proposed framework will allow harmonious co-existence with other acoustic systems, including marine animals, to achieve eco-friendly operation. This project's research will be integrated with education through summer youth K-12 outreach, curriculum development, undergraduate and graduate student training that will be particularly tailored to females and underrepresented minorities, and collaboration with an underwater robotics team from the local Dollar Bay High School. These activities are designed to motivate and better train rural, female, minority, and economically disadvantaged students to pursue STEM careers.<br/><br/><br/>This project tackles fundamental challenges in online-learning-based underwater acoustic communications and networking by innovating across three interrelated domains. First, novel signal processing and sparse learning techniques will be developed to model and predict the large-scale dynamics and the statistical distribution of small-scale fading of underwater acoustic environments, including the acoustic transmission loss, ambient soundscape, and statistical characterization of external (anthropogenic and marine animal) acoustic sources. Second, an optimization framework will be developed, based on the acoustic environment prediction, for joint transmission power control, link scheduling, node-cooperative routing, and autonomous vehicle mobility control to achieve high network utility and harmonious coexistence with other acoustic systems. Third, the acoustic environment exploration-exploitation tradeoff will be tackled in the Bayesian reinforcement learning framework, which will provide a principled approach to weighing the immediate reward of a communication and networking strategy and its associated long-term benefit of revealing the environment's dynamics. Leveraging the geographic advantage of Michigan Tech and the state-of-the-art facilities of Michigan Tech's Great Lakes Research Center, extensive field experiments will be conducted for acoustic measurement collection and for offline and online algorithm evaluation in a software-defined networking architecture. The methodologies and crosscutting techniques developed in this project can be applied to the design of intelligent radio-frequency communication networks.									0	0	0	0	0	0	0						1651135								Michigan Technological University				2023-12-08	GRANTS:13547152		
J	Ma, Jichang; Xie, Hui; Song, Kang; Liu, Hao					Ma, Ji Chang/0000-0001-6565-6770; Song, Kang/0000-0002-7548-2214					Self-Optimizing Path Tracking Controller for Intelligent Vehicles Based on Reinforcement Learning								SYMMETRY-BASEL				14	1					31			10.3390/sym14010031							Article	JAN 2022	2022	The path tracking control system is a crucial component for autonomous vehicles; it is challenging to realize accurate tracking control when approaching a wide range of uncertain situations and dynamic environments, particularly when such control must perform as well as, or better than, human drivers. While many methods provide state-of-the-art tracking performance, they tend to emphasize constant PID control parameters, calibrated by human experience, to improve tracking accuracy. A detailed analysis shows that PID controllers inefficiently reduce the lateral error under various conditions, such as complex trajectories and variable speed. In addition, intelligent driving vehicles are highly non-linear objects, and high-fidelity models are unavailable in most autonomous systems. As for the model-based controller (MPC or LQR), the complex modeling process may increase the computational burden. With that in mind, a self-optimizing, path tracking controller structure, based on reinforcement learning, is proposed. For the lateral control of the vehicle, a steering method based on the fusion of the reinforcement learning and traditional PID controllers is designed to adapt to various tracking scenarios. According to the pre-defined path geometry and the real-time status of the vehicle, the interactive learning mechanism, based on an RL framework (actor-critic-a symmetric network structure), can realize the online optimization of PID control parameters in order to better deal with the tracking error under complex trajectories and dynamic changes of vehicle model parameters. The adaptive performance of velocity changes was also considered in the tracking process. The proposed controlling approach was tested in different path tracking scenarios, both the driving simulator platforms and on-site vehicle experiments have verified the effects of our proposed self-optimizing controller. The results show that the approach can adaptively change the weights of PID to maintain a tracking error (simulation: within +/- 0.071 m; realistic vehicle: within +/- 0.272 m) and steering wheel vibration standard deviations (simulation: within +/- 0.04 degrees; realistic vehicle: within +/- 80.69 degrees); additionally, it can adapt to high-speed simulation scenarios (the maximum speed is above 100 km/h and the average speed through curves is 63-76 km/h).									3	1	0	0	0	0	4				2073-8994										Tianjin Univ, State Key Lab Engines, Tianjin 300072, Peoples R China				2022-03-02	WOS:000759097400001		
J	Liu, Yongyang; Zhou, Anye; Wang, Yu; Peeta, Srinivas				Liu, Yongyang/AAN-1552-2021; Wang, Yu/JBS-1645-2023	Peeta, Srinivas/0000-0002-4146-6793; Liu, Yongyang/0000-0002-5619-7098					Proactive longitudinal control to preclude disruptive lane changes of human-driven vehicles in mixed-flow traffic								CONTROL ENGINEERING PRACTICE				136						105522			10.1016/j.conengprac.2023.105522					APR 2023		Article; Early Access		2023	Connected and autonomous vehicles (CAVs) can be leveraged to enable cooperative platooning control to alleviate traffic oscillations. However, prior to a pure CAV environment, CAVs and human-driven vehicles (HDVs) will coexist on roads, creating a mixed-flow traffic environment. Mixed-flow traffic introduces key challenges for CAV operations due to potential lane changes by HDVs in adjacent lanes, which can cause stop-and-go waves and traffic oscillations. An understanding of the interactions between CAVs and HDVs in the lane-change process can be leveraged to use CAVs to proactively preclude disruptive lane changes by HDVs. This study proposes a deep reinforcement learning-based proactive longitudinal control strategy (PLCS) for CAVs to counteract disruptive HDV lane-change behaviors that can induce disturbances, and to preserve the smoothness of traffic flow in the CAV platooning control process. In it, a Transformer-based lane-change traffic condition predictor is constructed to predict whether an HDV will likely perform a disruptive lane change under the ambient traffic conditions. If no disruptive lane change is predicted, an extended intelligent driver model is activated for the CAV to perform smooth car-following behavior under cooperative CAV platooning control. If a disruptive lane change is predicted, a rainbow deep Q-network (RDQN)-based lane-change preclusion model is proposed through which the CAV can alter the lane-change traffic condition to preclude the HDV's lane change. Results from numerical experiments suggest that a CAV controlled by the PLCS is effective in reducing disruptive lane-change maneuvers by an HDV in its vicinity, and can improve string stability performance in mixed-flow traffic. Further, the effectiveness of the PLCS is illustrated under different lane-change scenarios, CAV control setups, and HDV driver types.									1	0	0	0	0	0	1			0967-0661	1873-6939										Georgia Inst Technol, Sch Civil & Environm Engn, Atlanta, GA 30332 USAGeorgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA				2023-05-24	WOS:000986656400001		
J	Shi, Haotian; Zhou, Yang; Wu, Keshu; Chen, Sikai; Ran, Bin; Nie, Qinghui				chen, zhuo/JXX-1337-2024; zhong, jing/KBP-7800-2024; Zhou, Yang/ABB-6033-2021	Zhou, Yang/0000-0001-5366-5389; Chen, Sikai/0000-0002-5931-5619					Physics-informed deep reinforcement learning-based integrated two-dimensional car-following control strategy for connected automated vehicles								KNOWLEDGE-BASED SYSTEMS				269						110485			10.1016/j.knosys.2023.110485					MAR 2023		Article; Early Access		2023	Connected automated vehicles (CAVs) are broadly recognized as next-generation transformative transportation technologies having great potential to improve traffic safety, efficiency, and stability. Efficiently controlling CAVs on two-dimensional curvilinear roadways to follow preceding vehicles is denoted as the two-dimensional car-following process, which is highly critical; this process is challenging to implement owing to the complexity and varied nature of driving environments. This study proposes an innovative integrated two-dimensional control strategy for CAVs based on deep reinforcement learning (DRL), which efficiently regulates the two-dimensional car-following process of CAVs in terms of both stability-wise longitudinal control performance and accurate lateral path-tracking performance. Within the control framework, each CAV can receive the surrounding information from downstream vehicles and roadway geometry based on vehicle-to-everything (V2X) communication. To better utilize this information, we designed a physics-informed DRL state fusion approach and reward function, which efficiently embeds prior physics knowledge and borrows the merits of the equilibrium and consensus concepts from the control theory. Given the physics-informed information, the DRL-based controller outputs the integrated control instructions for both longitudinal and lateral control. For training, we constructed a roadway with a set of varying curvatures and em-bedded the ground-truth vehicle trajectory datasets to more effectively capture the realistic variations in the roadway geometry and driving environment. To facilitate value function approximation and enhance the policy iteration process in training, the distributed proximal policy optimization (DPPO) algorithm was applied, owing to its balanced performance. A series of simulated experiments were conducted to validate the controller's lateral control accuracy and stability-wise oscillation dampening performance in diverse traffic scenarios, including extreme ones.(c) 2023 Elsevier B.V. All rights reserved.									4	0	0	0	0	0	4			0950-7051	1872-7409										Univ Wisconsin, Dept Civil & Environm Engn, 1217 Engn Hall 1415 Engn Dr, Madison, WI 53706 USATexas A&M Univ, Dept Civil & Environm Engn, College Stn, TX 77843 USAUniv Wisconsin, Dept Civil & Environm Engn, 2266 Engn Hall 1415 Engn Dr, Madison, WI 53706 USAUniv Wisconsin, Dept Civil & Environm Engn, 2312 Engn Hall 1415 Engn Dr, Madison, WI 53706 USAYangzhou Univ, Coll Civil Sci & Engn, 88 Daxue South Rd, Yangzhou, Jiangsu, Peoples R China				2023-04-30	WOS:000971034300001		
B	Lu, Yantao										Human Activity Recognition from Egocentric Videos and Robustness Analysis of Deep Neural Networks																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798662432017									Syracuse University, Electrical Engineering and Computer Science, New York, United States	Syracuse University				PQDT:67627209		
J									Sargolzaei, Arman		CAREER: Systematic Approach for Extensively (SAfEly) Testing and Verifying the Security of Connected and Autonomous Vehicle																												Awarded Grant	Feb 15 2022	2022	This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).&lt;br/&gt;&lt;br/&gt;The potential benefits of connected autonomous vehicles (CAV) are numerous, and society is expecting that this technology will increase the quality of everyday life and follow through on its promises. However, to be effective, they must be tested to demonstrate a standard level of safety and security. The complex and interconnected nature of the transportation system makes the task of testing and verification exceedingly difficult, raising serious concerns regarding their safety and security. It, thus, calls for new problem formulation and a novel systematic approach for the task of CAV testing and verification. The existing testing solutions use ad-hoc methods, such as miles driven, to demonstrate some indication of safety, often assuming that the CAV's perception of the surrounding environment is comprehensive and ideal. However, no fundamental structure has been developed to demonstrate the security of CAV products. This CAREER proposal models the transportation system as a networked control system providing a novel resiliency metric enabling the testing resiliency of CAVs. In addition, it utilizes the prior developed verification framework to formulate the testing and verification process as a centralized feedback control system enabling the development of a novel attack generator. The expected outcomes of this project would pave the way towards safely testing CAVs, directly impacting the future of this technology and related standards, ultimately eliminating crash-related fatalities and saving lives. The research findings can be further implemented for all networked control systems, such as high-assurance military systems and autonomous systems ranging from unmanned aerial vehicles to power systems. The educational purpose of the project is to expand students', particularly underrepresented and women minorities, awareness of CAV security by designing fully integrated educational modules and demonstrations. We plan to include the following activities to serve the need for rural and largely economically distressed regions: (i) develop after school online STEM curriculum adjusted for primary, High-school, and college students; (ii) provide workshops for educators and industrial partners as their professional development activities; (iii) involve underrepresented undergraduate and college students through research for undergraduate experience and internship program; (iv) develop an undergraduate and an advanced graduate courses.&lt;br/&gt;&lt;br/&gt;This CAREER project addresses the problem of testing and verification for the security of CAVs. The importance of the security of CAVs has been recognized in the existing literature and has motivated the development of several detection and compensation algorithms to ensure safety under faults, failures, and attacks. However, not much effort is invested in the task of CAVs testing and verification. This CAREER project illustrates that the current approaches are insufficient to safely verify the security of CAVs in a realistic environment, suffering from the lack of a metric that is dynamic-dependent to measure the system resiliency. We describe a research plan where a transportation system is modeled as a networked control system where roads, pedestrians, vehicles, and traffic signs (due to their dynamic behavior) are modeled as agents, interacting with each other using sensors and communication networks. The new perspective allows us to propose a novel resiliency metric to be used alongside the safety metric to develop reinforcement learning-based controllers for testing CAVs' security. As there are infinite types of faults and attacks, the proposed controller formulates the effects of attacks rather than focusing on specific types, easing the process of fault and attack generation. This project is expected to advance the area of testing and verification of CAVs by (i) Introducing a novel perspective using the concept of networked control systems enabling the development of a unique data stream generator utilizing reinforcement learning to generate attacks by modeling the testing process as a feedback control system where minimizing safety and security is the desired objective and (ii) Developing a unique experimental platform enriched with the power of mixed reality (MR) and vehicle-in-the-loop (ViL) to test the security of CAVs safely.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.									0	0	0	0	0	0	0						2144801								Tennessee Technological University				2023-12-08	GRANTS:15517068		
P	KUUTTI S; PORAV H; UPCROFT B; NEWMAN P										Method for generating new adversarial scenario            involving autonomous vehicle and agent, involves            performing reinforcement learning to train agent using            proxy of autonomous vehicle software stack in            reinforcement learning environment					WO2023062392-A1	OXBOTICA LTD																									NOVELTY - The method involves performing reinforcement learning to train the agent using a proxy of an autonomous vehicle software stack in a reinforcement learning environment to generate episodes. The episodes each represents an adversarial scenario terminating in failure of the proxy of the autonomous vehicle software stack. A descriptors (20) are generated based on the or each episode. The descriptors are stored in a database. The descriptors clustered for the or each episode, and the stored descriptor comprises the cluster of descriptors stored in the database. A new descriptor generated by moving away from the cluster of descriptors in a descriptor space. USE - Computer-based method for generating trajectories of actors. ADVANTAGE - The method involves simulating a first scenario comprising an environment having an ego-vehicle, where set of actors is provided with a first actor and a set of objects is provided, and thus enables to reduce the risk to the AV and occupants of the AV. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer based method of generating an agent from a scenario involving an autonomous vehicle; anda transitory computer readable storage medium storing program for generating trajectories of actors. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the scenario of an ego-vehicle.10Autonomous vehicle12Vehicle14Pedestrian20Descriptor24Computer															0																		2023-08-10	DIIDW:202340389M		
P	KUUTTI S; PORAV H; UPCROFT B; NEWMAN P										Method for generating new adversarial scenario            involving autonomous vehicle and agent, involves            performing reinforcement learning to train agent using            autonomous vehicle software stack in reinforcement            learning environment to generate episodes					WO2023062393-A1	OXBOTICA LTD																									NOVELTY - The method involves performing reinforcement learning to train the agent using an autonomous vehicle software stack in a reinforcement learning environment to generate episodes, the episodes each representing an adversarial scenario terminating in a failure of the autonomous vehicle software stack. A descriptors (20) are generated based on the or each episode. The descriptors are stored in a database. The descriptors clustered for the or each episode, and the stored descriptors which comprises the cluster of descriptors stored in the database. A new descriptor is generated by moving away from the cluster of descriptors in a descriptor space. USE - Computer-based method for generating new adversarial scenario involving autonomous vehicle and agent. ADVANTAGE - The method involves simulating a first scenario comprising an environment having an ego-vehicle, where set of actors is provided with a first actor and a set of objects is provided, and thus enables to reduce the risk to the AV and occupants of the AV. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer implemented method of generating an agent from a scenario involving an autonomous vehicle; anda transitory computer readable medium storing program for generating an agent from a scenario involving an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the scenario of an ego-vehicle.10Autonomous vehicle12Vehicle14Pedestrian16Sidewalk20Descriptor															0																		2023-05-05	DIIDW:2023403886		
B	Ghazanfari, Behzad										Machine Learning-Based Decision Making in Autonomous Systems																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798557010252									Northern Arizona University, Electrical and Computer Engineering, Arizona, United States	Northern Arizona University				PQDT:66951049		
P	RAJKUMAR M; GORTHI R; DEVI S G; RAO T N; SRINIVASARAO P; GONUGUNTLA S										System for providing navigation of self-driving            car in highways, has reinforcement learning framework            implemented within processors to enable autonomous            vehicle to learn and adapt navigation strategy					IN202341076792-A	RAJKUMAR M; GORTHI R; DEVI S G; RAO T N; SRINIVASARAO P; GONUGUNTLA S																									NOVELTY - The system has an autonomous vehicle equipped with sensors, processors and control mechanisms. A reinforcement learning framework is implemented within the processors to enable the autonomous vehicle to learn and adapt a navigation strategy based on real-time sensory data and environmental conditions. The sensors are provided with cameras, lidars, radars, and global positioning systems for providing the autonomous vehicle with multi-modal data for environment perception. The reinforcement learning framework employs deep reinforcement learning algorithms to optimize navigation decisions including route planning, obstacle avoidance, and trajectory control. USE - System for providing navigation of an autonomous vehicle (claimed) i.e. self-driving car, in complex urban environments i.e. highways. ADVANTAGE - The system can navigate bustling city streets, thus ensuring safe, efficient, and reliable transportation in complex and challenging urban settings. The system defines a set of rewards and penalties that guide agent's decision-making process such that rewards can be associated with safe and efficient actions such as obeying traffic rules and avoiding collisions and penalties can be imposed for risky behaviors or traffic violations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) a method for providing autonomous vehicle navigation in complex urban environments using a reinforcement learning framework; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for providing navigation of an autonomous vehicle in complex urban environments.															0																		2024-01-27	DIIDW:2024055122		
P	WONGPIROMSARN T; COLLIN A; BELTA C; TEBBENS R D; HELOU B; BEIJBOM O O; KORING A; BERTA C; DUINTJER T R; HERAU B; OLOFF B O; BASSAM H; DOUINTIER T B; BELTACALIN; ANN C										Method for scoring trajectories of autonomous            vehicle through given traffic scenario using e.g.            linear support vector machine, involves planning            trajectory in environment using augmented route            planner, and operating vehicle along planned trajectory            using control circuit of vehicle					US2022063666-A1; CN114118658-A; GB2607835-A; GB2598408-A; GB2598408-B; KR2022029268-A; KR2529115-B1; KR2023068366-A; GB2616392-A; GB2607835-B	MOTIONAL AD LLC; APTIV TECHNOLOGIES LTD																									NOVELTY - The method (1000) involves generating a set of trajectories for a vehicle operating in an environment using processors (1001), where each trajectory is associated with a traffic scenario. A reasonableness score for each trajectory in the set of trajectories is predicted (1002) using the processors, where the reasonableness score is obtained from a machine learning model that is trained using input obtained from a set of human annotators, and a loss function that penalizes predictions of reasonableness scores that violate a rulebook structure. A route planner of the vehicle is augmented using the predicted reasonableness scores for the trajectories using the processors. A trajectory is planned in the environment using the augmented route planner by the processors. The vehicle is operated along the planned trajectory using a control circuit of the vehicle. USE - Method for scoring trajectories of an autonomous vehicle through a given traffic scenario using a machine learning model such as linear support vector machine, neural network and convolutional neural network trained on images (all claimed), to predict reasonableness scores for the trajectories. ADVANTAGE - The method enables using the machine learning model to predict reasonableness scores for autonomous vehicle trajectories for given traffic scenario, so that the predicted scores can be used to tune a route planner and performance of the route planner, compare two AV stacks, and compare reinforcement learning and any other desired application in efficient manner. DETAILED DESCRIPTION - The loss function is a hinge or slack loss function. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for scoring trajectories of an autonomous vehicle.Method for scoring trajectories of autonomous vehicle through given traffic scenario (1000)Generating set of trajectories for vehicle operating in environment (1001)Predicting reasonableness score for each trajectory in set of trajectories (1002)Using predicted reasonableness scores (1003)															0																		2022-04-01	DIIDW:202231365S		
P	KRYSTEK P; KWATRA S; WILSON J; BAYSINGER B										Method for implementing intelligent driving            characteristic adjustment for autonomous vehicles by a            processor involves determining a user experience            satisfaction level for one or more users during a            journey within an autonomous vehicle					US2020218271-A1; US11237565-B2	INT BUSINESS MACHINES CORP																									NOVELTY - The method (700) involves determining a user experience satisfaction level for one or more users during a journey within an autonomous vehicle according to historical user experience satisfaction levels, one or more user profiles, one or more contextual factors, or a combination (704). Dynamically adjusting one or more performance characteristics of the autonomous vehicle for the one or more users according to the historical user experience satisfaction levels, the one or more user profiles, if the user experience satisfaction level is less than a predetermined threshold (706). Determining a risk threshold for the cluster of users and use reinforcement learning to dynamically adjust the one or more performance characteristics based on a global satisfaction level of the cluster of users. USE - Method for implementing intelligent driving characteristic adjustment for autonomous vehicles by a processor. ADVANTAGE - Method for implementing intelligent driving characteristic adjustment for autonomous vehicles by a processor can allow cloud computing environment to offer infrastructure, platforms and/or software as services for which a cloud consumer does not need to maintain resources on a local computing device. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for implementing intelligent driving characteristic adjustment for autonomous vehicles; and(2) a computer program product. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for implementing intelligent driving characteristic adjustment for autonomous vehicles by a processor.Method (700)Determining a user experience satisfaction level for one or more users during a journey within an autonomous vehicle according to historical user experience satisfaction levels, one or more user profiles, one or more contextual factors, or a combination (704)Dynamically adjusting one or more performance characteristics of the autonomous vehicle for the one or more users according to the historical user experience satisfaction levels, the one or more user profiles, if the user experience satisfaction level is less than a predetermined threshold (706)															0																		2023-08-10	DIIDW:202062025S		
P	HALDER B										Method for managing autonomous vehicle application            operations with reinforcement learning, involves            communicating interruptible command from vehicle safety            manager to action integrity module to determine final            action for autonomous vehicle					US2019310649-A1; US10809735-B2	SAFEAI INC																									NOVELTY - The method involves providing an observed state of an autonomous vehicle as a feedback to a reinforcement learning (RL) model-agent that decides a next action for the autonomous vehicle. The observed state is provided to an epistemic uncertainty check (EUC) module to determine that that the input-state distribution varies from a training-state distribution based on a specified distance value between the input-state distributions from a training-state distribution, where the EUC module outputs an RL model confidence factor (RLMCF). The RLMCF is communicated from the RL model-agent and the interruptible command is communicated from a vehicle safety manager (VSM) (102) to an action integrity module (AIM) to determine a final action for the autonomous vehicle. USE - Method for managing autonomous vehicle application operations with RL. ADVANTAGE - The method enables providing robust and safe handling of RL model decision in real autonomous vehicle application. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computerized system for managing autonomous vehicle application operations with RL. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an autonomous vehicle management system.VSM (102)Applications (104)Middleware (106)Operating system (108)Sensors (110)															0																		2019-10-24	DIIDW:201984621Y		
P	ENGLARD B; GANDHI G; MAHESHWARI P										Non-transitory computer-readable medium storing            program for controlling autonomous vehicle, includes            instruction for generating grid path through            environment is based on cost maps by motion            planner					US2019113927-A1; US10606270-B2	LUMINAR TECHNOLOGIES INC																									NOVELTY - The non-transitory computer-readable medium includes instruction for generating a perception component (506) and prediction component configured to receive sensor data (502). An observed occupancy grid (508) indicative of which cells are currently occupied in a two-dimensional representation of an environment through the autonomous vehicle is moving based on the received sensor data. The navigation data is provided for guiding the autonomous vehicle through the environment towards a destination by mapping component (530). The several predicted occupancy grids and the navigation data are generated based on the observed occupancy grid by cost map generation component. A grid path through the environment is generated based on multiple cost maps by motion planner and generate decisions for maneuvering the autonomous vehicle toward the destination based on the grid path. USE - Non-transitory computer-readable medium storing program for controlling autonomous vehicle (claimed) using cost maps. ADVANTAGE - The performance e.g. safety and efficiency of the autonomous vehicle can be improved if the candidate decisions generated by the self-driving control architecture (SDCA) reflects a greater level of diversity. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method of controlling an autonomous vehicle; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an self-driving control architecture using a motion planner that is trained with reinforcement learning.Sensor data (502)Perception component (506)Occupancy grid (508)Mapping component (530)Control signals (546)															0																		2019-05-14	DIIDW:201934548M		
P	HOEL C; LAINE L										Method for providing reinforcement learning (RL)            agent for decision-making to be used in controlling            autonomous vehicle, involves initiating additional            training in which RL agent interacts with second            environment including autonomous vehicle					WO2021213617-A1; CN115413344-A; EP4139845-A1; US2023242144-A1	VOLVO TRUCK CORP; VOLVO AUTONOMOUS SOLUTIONS AB																									NOVELTY - The method (100) involves interacting an RL agent with a first environment including the autonomous vehicle, in multiple training sessions (110-1 to 110-K). Each training session includes a different initial value and yielding a state-action value function dependent on state and action. An uncertainty evaluation (114) is performed on the basis of a variability measure for multiple state-action value functions evaluated for multiple state-action pairs corresponding to possible decisions by the trained RL agent. The RL agent interacts with a second environment including the autonomous vehicle, in an additional training (116). The second environment differs from the first environment by an increased exposure to a subset of state-action pairs for which the variability measure indicates a relatively higher uncertainty. USE - Method for providing a reinforcement learning agent for decision-making to be used in controlling an autonomous vehicle. ADVANTAGE - The method effectively provides the reinforcement learning agent for decision-making to be used in controlling the autonomous vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an arrangement for controlling an autonomous vehicle;(2) a computer program; and(3) a data carrier carrying the computer program. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for providing a reinforcement learning agent for decision-making to be used in controlling an autonomous vehicle.Reinforcement learning method (100)Training sessions (110-1 to 110-K)Uncertainty evaluation (114)Additional training (116)															0																		2021-11-13	DIIDW:2021C0956M		
P	HUH K S; MIN K S; HAYOUNGKIM										Method for controlling autonomous vehicle using            deep reinforcement learning and driver assistance            system, involves determining action for vehicle control            using sensor and image data received from deep            reinforcement learning algorithm					KR2020095590-A; KR2166811-B1	UNIV HANYANG IUCF-HYU																									NOVELTY - The method involves receiving (S110) the measured sensor data and captured image data with deep reinforcement learning algorithm. An action is determined (S120) for vehicle control by using the sensor data and the image data received from the deep reinforcement learning algorithm. A vehicle is controlled (S130) by selecting a driver assistance system (DAS) according to the determined behavior. The input sensor data and the image data are purified respectively. The connected data is formed by connecting the refined sensor data and the image data. A Q value is obtained by inputting the connected data into a fully connected layer of the deep reinforcement learning algorithm. USE - Method for controlling autonomous vehicle using deep reinforcement learning and driver assistance system. ADVANTAGE - The deep reinforcement learning stably controls an autonomous vehicle by determining an appropriate driver assistance system for various situation through an algorithm that determines the optimal behavior using deep reinforcement learning and a control method of an autonomous vehicle using a driver assistance system. The path of the control of the autonomous vehicle is planned. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an autonomous vehicle control device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for controlling autonomous vehicle using deep reinforcement learning and driver assistance system. (Drawing includes non-English language text)Step for receiving input of measured sensor data and captured image data with deep reinforcement learning algorithm (S110)Step for determining an action for vehicle control using the sensor data and the image data received from the deep reinforcement learning algorithm (S120)Step for controlling a vehicle by selecting a DAS according to the determined behavior (S130)															0																		2020-08-31	DIIDW:2020777471		
P	KWON S D; LEE Y S; JIYEONG H; JU S										Method for personalizing autonomous driving            algorithm for enhancing satisfaction of specific user's            autonomous driving experience, involves setting driving            mode of autonomous vehicle to manual mode by            personalization device					KR2023063807-A	OBIGO INC																									NOVELTY - The method involves setting a driving mode of an autonomous vehicle to a manual mode by personalization device, and acquiring manual control data of the autonomous vehicle under situations according to an operation of a specific user. Automatic control data is generated by an autonomous driving module, and a personalization device is provided with a fitting module. An autonomous driving algorithm loaded in the autonomous driving module is set with reference to the first to Nth manual control data and the first to Nth automatic control data to the specific user. A determination is made whether instantaneous rate of change in speed or direction of the autonomous vehicle according to the input of the raw manual control data to the autonomous vehicle is equal to or greater than a first threshold. USE - The method is useful for personalizing an autonomous driving algorithm for enhancing the satisfaction of the specific user's autonomous driving experience. ADVANTAGE - The satisfaction of the user's autonomous driving experience is enhanced. The autonomous driving algorithm is personalized by performing inverse reinforcement learning with driving of a specific user as an optimal policy. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for personalizing the autonomous driving algorithm for enhancing satisfaction of specific user's autonomous driving experience. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for personalizing an autonomous driving algorithm for enhancing the satisfaction of a specific user's autonomous driving experience (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2023502526		
P	HUN B S										Apparatus for improving traffic based on            reinforcement learning at non-signal intersection for            autonomous vehicle cluster operation, has reinforcement            learning library environment construction unit that            optimizes traffic control with multi-agent deep            rein					KR2022102395-A; KR2461831-B1	UNIV PUKYONG NAT; SM CO LTD																									NOVELTY - The device has a simulation execution unit (100) that builds a simulation environment using simulation of urban mobility (SUMO) and delivers the data obtained from the speed, position, and sensor of the autonomous vehicle to a flow application unit (200). A reinforcement learning library environment construction unit (300) optimizes traffic control with multi-agent deep reinforcement learning using the flow. A result file generation unit transmits the obtained data from speed, location and sensor to the flow environment. A simulation initialization unit sets the simulation environment including human driver definition. A vehicle control module (23) controls the vehicle and delivers control information to the execution unit. A data sampling unit (32) samples data to be learned. An update unit receives the simulation state from an execution unit and updates the simulation status. A policy optimizer maximizes the reward of reinforcement learning for behavior. USE - The apparatus is useful for improving traffic based on reinforcement learning at non-signal intersection for autonomous vehicle cluster operation. ADVANTAGE - The self-driving vehicle platooning learning improves non-signal crossing traffic and ensures safety. The reward of reinforcement learning for behavior is maximized. The driving behavior of autonomous vehicles is learned in mixed traffic flow situation where clustered autonomous vehicles and human driver vehicles are mixed. The average travel speed in fully autonomous vehicle environment is improved. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for the method for reinforcement learning-based traffic improvement at non-signal intersections for group operation of autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of apparatus for improving traffic based on reinforcement learning at non-signal intersection for autonomous vehicle cluster operation (Drawing includes non-English language text).Simulation initialization unit (20)Vehicle control module (23)Data sampling unit (32)Simulation execution unit (100)Flow application unit (200)Reinforcement learning library environment construction unit (300)															0																		2022-08-16	DIIDW:202293832W		
P	NISHI T										Method for configuring autonomous operation            capability for vehicle, involves providing            reinforcement learning module with actor-critic module            to reduce active environment scanning by multiple            sensors as vehicle operates autonomously					US2019035275-A1; US10235881-B2; JP2019055768-A; JP6875330-B2	TOYOTA MOTOR ENG & MFG NORTH AMERICA INC; TOYOTA MOTOR ENG & MFG NORTH AMERICA																									NOVELTY - The method involves producing a state value function for an autonomous vehicle capability in relation to a first dataset by a processor, where the state value function includes a present state, a next state, and a state cost. A second dataset relating to a portion of vehicles is identified by the processor. A policy control gain for the autonomous vehicle capability is optimized in relation to the second dataset by the processor. The autonomous vehicle capability is operable to generate an autonomous vehicle action (124) for progressing to a next state based on the state value function in cooperation with the policy control gain and the autonomous vehicle capability. A reinforcement learning (RL) module (312) is provided with an actor-critic module (402) to reduce active environment scanning by multiple sensors as the vehicle operates autonomously. USE - Method for configuring an autonomous operation capability for a vehicle. Uses include but are not limited to a sports vehicle, a suburban utility vehicle, an outdoor vehicle, a pickup lorry, a passenger vehicle, a recreational vehicle, and a motorized land vehicle. ADVANTAGE - The method enables operating an autonomous vehicle dataset module to provide the reinforcement leaning module with the ability to provide continuous state and action spaces that engage in machine learning based on less system knowledge and without requiring active environment sensing and/or exploration, so that the passive dynamics data of the first dataset relating to the category, or model, of the vehicle are honed by the second dataset as relating to the state value functions and policies for improving reinforced learning efficiency with respect to improved implementation while reducing active scanning required for autonomous operations. The method enables operating an actor module to generate a prediction of an action of the autonomous vehicle capability based on an adaptive policy generated from a two-stage hierarchical configuration of a first and/or passive environment dynamic dataset and a second and/or control dynamics dataset when the RL module includes an actor-critic module. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vehicle control unit. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a reinforcement learning module.Autonomous vehicle action (124)RL module (312)Autonomous vehicle dataset module (314)Actor-critic module (402)Reinforcement feedback signal (404)															0																		2019-02-13	DIIDW:201912088L		
P	AL Q M H; QI X; CLIFFORD D H										Method for training an autonomous vehicle,            involves storing real world data that includes a            sequence of images of a road environment and generating            the sequence of images on the basis of a vehicle that            traverses the road environment					US2020387161-A1	GM GLOBAL TECHNOLOGY OPERATIONS INC																									NOVELTY - The method involves storing real world data that includes a sequence of images of a road environment in a data storage device. The sequence of images is generated on the basis of a vehicle that traverses the road environment. The sequence of images is processed with a deep reinforcement learning agent in an offline simulation environment. The deep reinforcement learning agent is associated with a control feature of the autonomous vehicle to obtain an optimized set of control policies. The autonomous vehicle is trained on the basis of the optimized set of control polices. A first image is obtained from the sequence of images and processes the first image with the deep reinforcement learning agent to obtain an action. USE - Method for training an autonomous vehicle. ADVANTAGE - Improves the training process by no longer relying on synthesized simulation environment data. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system that includes a data storage device; and(2) an autonomous vehicle that include multiple sensors. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the Method for training an autonomous vehicle.															0																		2023-08-10	DIIDW:2020C3467T		
P	CHOI Y										Method for predicting optimal speed trajectory of autonomous vehicle, involves generating virtual vehicle model by virtual vehicle learning unit using vehicle driving data collected from actual autonomous vehicle					KR2021090386-A	ELECTRONICS & TELECOM RES INST																									NOVELTY - The method involves generating a virtual vehicle model using vehicle driving data collected from an actual autonomous vehicle by the virtual vehicle learning unit. The driving data of the virtual vehicle model driving in the virtual environment using the generated virtual vehicle model input is predicted and outputted by a virtual environment agent. A speed trajectory is predicted by a controller agent in which target speed values are connected point by point using driving data of the virtual vehicle. A virtual environment model is generated by modeling the virtual environment. A compensation value is used as a value for learning the controller agent by a reinforcement learning algorithm. USE - Method for predicting optimal speed trajectory of the autonomous vehicle. ADVANTAGE - The dynamic characteristics of the autonomous vehicle are learned through the learning of two agents generated by artificial neural network and reinforcement learning, and the speed at which the experiential and inferential elements of the route are reflected. Stable autonomous driving is possible even for driving paths similar to learning data. The learning and driving in various modes are possible through a simple parameter change of the compensation value. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for predicting an optimal speed trajectory of an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for predicting the optimal speed trajectory of the autonomous vehicle. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202185220Q		
P	FAUST A; DEVIN M; CHEN Y J; MORLEY F; FURMAN V; FUERTES P C A										Method for implementing interactive simulated            autonomous vehicle agent for e.g. self-driving cars,            involves generating experience tuple, and providing            experience tuple as input to reinforcement learning            system for autonomous vehicle					US10254759-B1	WAYMO LLC																									NOVELTY - The method involves generating an immediate quality value for a predicted environment of an autonomous vehicle and an action as input to a context-specific quality model that generates immediate quality values that are specific to a particular driving context. An experience tuple is generated (350), where the experience tuple comprises initial environment observation, candidate action, and immediate quality value. The experience tuple is provided as input to a reinforcement learning system (360) for the autonomous vehicle. USE - Method for implementing an interactive simulated autonomous vehicle agent for self-driving cars, boats, and aircraft using a computer. Uses include but are not limited to mobile telephone, personal digital assistant (PDA), mobile audio or video player, game console, global positioning system (GPS) receiver and Universal Serial Bus (USB) flash drive device. ADVANTAGE - The method enables generating sufficient number of reliable and realistic experience tuples in a reasonable amount of time for different driving contexts efficiently. The method enables allowing a vehicle behavior model to be trained with automatically generated training data to reduce the amount of human-labeled data to be collected to reduce amount of time required to build and train the system. The method enables improving safety of learning driving maneuvers without putting a physical vehicle or humans at risk for learning to drive in dangerous situations or environments, thus ensuring the vehicle to perform smooth and safe action transitions. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a system for implementing an interactive autonomous vehicle agent(2) a computer program product comprising a set of instructions for implementing an interactive autonomous vehicle agent. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating an experience tuple for an interactive autonomous vehicle agent in a driving context.Step for receiving request to generate experience tuple for vehicle in driving context (310)Step for receiving candidate action and initial environment observation (320)Step for using context-specific quality model to compute immediate quality value for candidate action in initial environment (340)Step for generating experience tuple using initial environment observation, candidate action and computed immediate quality value (350)Step for providing experience tuple as input to reinforcement learning system for autonomous vehicle (360)															0																		2019-04-19	DIIDW:2019313346		
P	WEDEKIND D										Method for adapting driving behavior of autonomous            vehicle, involves training model to carry out driving            maneuver in traffic situation, training execution of            maneuver to optimize traffic flow, and carrying out            driving maneuver by autonomous vehicle based on model            depending on flow					DE102022106338-A1; CN116767272-A	JOYNEXT GMBH																									NOVELTY - The method involves defining a traffic situation with a driving maneuver to be carried out by an autonomous vehicle (217). A model is trained to carry out the driving maneuver in the traffic situation, where the model is based on a traffic flow related to the situation is taken into account and evaluated. An execution of the maneuver is trained in such a way that the traffic flow is optimized according to the evaluation. The maneuver is carried out in the situation by the autonomous vehicle based on the model depending on the flow, where the model uses reinforcement learning to particularly reward executions of maneuver. USE - Method for adapting driving behavior of autonomous vehicle. ADVANTAGE - The method enables avoiding excessively defensive driving behavior of the autonomous vehicle, which can lead to unnecessary driving delays and driving impairments of other vehicles. The method allows the model to be trained with data recorded in reality, so that the model can use reinforcement learning to reward executions of driving maneuvers that are in reality carried out by non-autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a street scene at an intersection.200Street scene201Intersection202First road204First lane217Autonomous vehicle															0																		2023-10-11	DIIDW:202398727M		
P	FUERTES P C A; FURMAN V; MORLEY F; CHEN Y J; DEVIN M; FAUST A										Method for detecting nearby objects by using            on-board sensors for autonomous vehicles, such as cars,            involves selecting action to be taken by autonomous            vehicle in initial environment from enumerated set of            candidate actions					US11067988-B1	WAYMO LLC																									NOVELTY - The method involves receiving an initial environment observation representing an initial environment of an autonomous vehicle when the autonomous vehicle is in a particular driving context. An action to be taken is selected by the autonomous vehicle in the initial environment from an enumerated set of candidate actions using a reinforcement learning system for the autonomous vehicle. A predicted environment observation representing a predicted environment of the autonomous vehicle is generated after the selected candidate action is taken by the autonomous vehicle in the initial environment. The initial environment observation and the selected candidate action is provided as input to a vehicle behavior model neural network trained to generate predicted environment observations. USE - Method for detecting nearby objects by using on-board sensors for autonomous vehicles, such as cars, boats, and aircraft. ADVANTAGE - Detection system is allowed to efficiently generate a sufficient number of reliable and realistic experience tuples in a reasonable amount of time for many multiple driving contexts. Quality metric engine can use a comfort feature that quantifies the level of driver comfort for the candidate action in the predicted environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for detecting nearby objects by using on-board sensors for autonomous vehicles;(2) a computer program product for detecting nearby objects by using on-board sensors for autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of method for detecting nearby objects by using on-board sensors for autonomous vehicles.Receiving request to generate an experience tuple for a vehicle in a particular driving context (310)Receiving candidate action and initial environment observation (320)Using vehicle behavior model to compute predicted environment observation for the candidate action in the initial environment (330)Generating an experience tuple using the initial environment observation, the candidate action, and the computed immediate quality value (350)Providing the experience tuple as input to a reinforcement learning system for the autonomous vehicle (360)															0																		2021-08-08	DIIDW:202181612U		
P	ISELE D; NAKHAEI S A; FUJIMURA K										Method for controlling an autonomous vehicle,            involves collecting scenario information from one or            more sensors mounted on a vehicle, and determining a            high-level option for a fixed time horizon based on the            scenario information					US2019332110-A1; US10990096-B2	HONDA MOTOR CO LTD																									NOVELTY - The method involves collecting scenario information from one or more sensors mounted on a vehicle (102), and determining a high-level option for a fixed time horizon based on the scenario information. A prediction algorithm is applied to the high-level option masking undesired low-level behaviors for completing the high-level option where a collision is predicted to occur. A restricted subspace of low-level behaviors is evaluated through a reinforcement learning system. USE - Method for controlling an autonomous vehicle. ADVANTAGE - The increased penalty for a timeout reduces the number of unsuccessful trials. Improved autonomous driving is ensured. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a vehicle for autonomous driving; and(2) a computer-readable medium storing computer executable code for autonomously controlling a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an operating environment of a reinforcement learning system for autonomous driving.Vehicle (102)Autonomous driving system (110)Electronic control unit (112)Processor (114)Memory (116)															0																		2019-11-11	DIIDW:201990096P		
P	SHALEV-SHWARTZ S; SHASHUA A; SHAMMAH S										Navigation system for host autonomous vehicle            using reinforcement learning techniques, has processing            device programmed to cause adjustment of navigational            actuator of host vehicle in response to determined            navigational action for host vehicle					US2021034068-A1; US11561551-B2	MOBILEYE VISION TECHNOLOGIES																									NOVELTY - The system has a processing device (110) programmed to analyze multiple images to identify a first object and a second object in the environment of a host vehicle. A first predefined navigational constraint implicated by the first object and a second predefined navigational constraint implicated by the second object are determined, where the first predefined navigational constraint and the second predefined navigational constraint are not satisfied. The second predefined navigational constraint has a priority higher than the first predefined navigational constraint. A navigational action for the host vehicle satisfying the second predefined navigational constraint is determined, but not satisfying the first predefined navigational constraint. An adjustment of a navigational actuator of the host vehicle is caused in response to the determined navigational action for the host vehicle. USE - For navigating an autonomous vehicle using reinforcement learning techniques. ADVANTAGE - The navigation system allows implementation of portions of the policy manually which ensures the safety of the policy, and implementation of other portions of the policy using reinforcement learning techniques which enables adaptivity to many scenarios, a human-like balance between defensive/aggressive behavior, and a human-like negotiation with other drivers. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method of navigating autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a side view representation of a vehicle including a system.Navigation system (100)Processing device (110)Image capture device (122)Navigate vehicle (200)															0																		2021-03-21	DIIDW:202116463F		
P	ZHANG H; ZHANG Y; CHEN G										Method for controlling autonomous vehicle based on            deep prediction network and deep reinforcement            learning, involves defining upper-level discrete            controller corresponding to control signal at bottom of            vehicle					CN115629608-A	UNIV SOUTHEAST																									NOVELTY - The method involves defining an upper-level discrete controller corresponding to a control signal at the bottom of the vehicle. The depth prediction network is built based on double-depth network of the decoder-decoder framework to initialize the weights of each network. The deep reinforcement learning training is conducted on the controlled vehicle. The weight of the network is iteratively updated until the reward value obtained by the controlled vehicle in one round of driving behavior reaches the preset level or the number of training rounds reaches the preset value. The trained deep prediction network and double deep network is deployed to the controlled vehicle. The position and speed of the vehicle is predicted in front of the controlled vehicle through the deep prediction network. USE - Method for controlling an autonomous vehicle based on a deep prediction network and deep reinforcement learning. ADVANTAGE - The method converts the track of the historical vehicle into image data, which can represent the influence of the vehicle interaction between the vehicle track prediction, reduces the whole model parameter, saves the training and calculation cost, compared with the individual vehicle equipped with prediction network, and reduces the calculation cost. The method introduces a channel attention mechanism in the prediction network of encoder-decoder frame, enhances the attention of the model influencing the characteristic of the track prediction and improves the prediction precision. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for the automatic driving vehicle control method based on depth prediction network and depth reinforcement learning to claim. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for controlling autonomous vehicle based on deep prediction network and deep reinforcement learning. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202313412T		
P	KWON M										Method for controlling road flow in road            information system using reinforcement learning-based            autonomous vehicle, involves receiving location and            speed of self-driving vehicles and non-autonomous            vehicles as status information at predetermined time            intervals					KR2023010498-A; KR2549744-B1	UNIV FOUND SOONGSIL IND COOP																									NOVELTY - The method involves receiving (S10) location and speed of self-driving vehicles and non-autonomous vehicles as status information at predetermined time intervals from a road information system using a selected deep reinforcement learning algorithm. An action of the self-driving vehicle is selected (S20), which is the strength of stepping on the accelerator or brake, based on the state information. A compensation value is derived (S30) for an action of an autonomous vehicle based on a target speed to be reached and the speeds of autonomous vehicles and non-autonomous vehicles. A policy is updated (S40) based on the compensation value. The learned self-driving car is executed together with the non-self-driving cars in the road information system based on the learning result and the location and speed status information of the real-time self-driving car and non-self-driving cars. USE - Method for controlling road flow in road information system using reinforcement learning-based autonomous vehicle e.g. self-driving car. ADVANTAGE - The road flow is smoothly induced by inducing all vehicles on the road to drive close to the target speed by using the self-driving car based on the deep reinforcement learning algorithm. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer-readable storage medium storing program for controlling road flow in road information system using reinforcement learning-based autonomous vehicle; anda road flow control device in a road information system using a deep reinforcement learning-based autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the autonomous driving learning step in the road flow control method in the road information system using the deep reinforcement learning-based autonomous vehicle. (Drawing includes non-English language text)S10Step for receiving location and speed of self-driving vehicles and non-autonomous vehicles as status information at predetermined time intervals from a road information system using a selected deep reinforcement learning algorithmS20Step for selecting the action of the self-driving vehicle, which is the strength of stepping on the accelerator or brake, based on the state informationS30Step for deriving the compensation value for an action of an autonomous vehicle based on a target speed to be reached and the speeds of autonomous vehicles and non-autonomous vehiclesS40Step for updating the policy based on the compensation valueS50Step for terminating the learning, when the predetermined time elapses															0																		2023-08-10	DIIDW:202310518T		
P	YANG M; LIU X; LI Z										Deep reinforcement learning based autonomous            vehicle driving behavior decision-making method,            involves calculating final action behavior value of            vehicle by learning structure if action behavior in the            experience pool is not found					CN111605565-A	KUNSHAN XIAOYANTANSUO INFORMATION TECHNOLOGY CO              LTD																									NOVELTY - The method involves obtaining current environment state around an autonomous vehicle. Action behavior of the autonomous vehicle in an experience pool is selected and outputted according to the current environment state and current behavior state of the autonomous vehicle. Final action behavior value of the autonomous vehicle is calculated by deep reinforcement learning structure if action behavior corresponding to current environment state in the experience pool is not found. Environmental information of road is received through an red green blue (RGB) camera. Information of obscured objects in the road is received through an infrared camera. Environmental sensing detection is performed on the environmental information. USE - Deep reinforcement learning based autonomous vehicle driving behavior decision-making method. ADVANTAGE - The method enables utilizing the RGB camera, the infrared camera and solid-state lidars to obtain current environmental status, which reduces sensor usage, forming an experience pool by imitating and learning driving experience of human drivers according to human driving habits to solve coexistence problem of manned and unmanned vehicles on the road and improving safety performance. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep reinforcement learning based autonomous vehicle driving behavior decision-making method. (Drawing includes non-English language text).															0																		2020-09-24	DIIDW:202090135J		
P	JAFARI T S R; GUPTA P; MEHDI S B; PALANISAMI P; PALANISAMY P										Method for performing lane change by autonomous            vehicle, involves evaluating lane data, map data,            vehicle data and participant data, and controlling            vehicle by processor to perform lane change based on            lane change action					CN112455441-A; US2021074162-A1	GM GLOBAL TECHNOLOGY OPERATIONS INC																									NOVELTY - The method involves determining a desired lane change by a processor. Lane change action is determined by the processor based on enhanced learning process and rule-based process. Lane data, map data, vehicle data and participant data are evaluated. A vehicle is controlled by the processor to perform lane change based on the lane change action, where the lane change action includes a gap between two vehicles on a road and an identifier performs a timing of the lane change. The lane change action is determined based on a reinforcement learning to satisfy a rule-based constraint. USE - Method for performing lane change by an autonomous vehicle. ADVANTAGE - The system controls the vehicle to perform lane change by the processor based on the lane action in an effective manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for performing lane change by an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating of a method for performing lane change by an autonomous vehicle.															0																		2021-03-27	DIIDW:202125278J		
P	ENGLARD B; MAHESHWARI P; KHILARI S C; RAMEZANI V R										Non-transitory computer-readable medium for            implementing self-driving control architecture for            controlling e.g. car, has set of instructions for            utilizing values of variables to generate decisions for            maneuvering autonomous vehicle					US2019113920-A1	LUMINAR TECHNOLOGIES INC																									NOVELTY - The medium has a set of instructions for utilizing signals descriptive of a current state of an environment and signals descriptive of a set of predicted future states of the environment to set values of a set of independent variables in objective equation, where the objective equation includes a set of terms that each correspond to different one of a set of driving objectives over a finite time horizon. Values of the set of dependent variables in the objective equation are determined by solving the objective equation subject to a set of constraints. The determined values of the dependent variables are utilized to generate decisions for maneuvering an autonomous vehicle (400) toward a destination. USE - Non-transitory computer-readable medium for implementing a self-driving control architecture for controlling an autonomous vehicle (claimed) i.e. car (from drawings). ADVANTAGE - The medium enables improving performance of the autonomous vehicle if candidate decisions generated by a self-driving control architecture reflect greater level of diversity. The medium enables utilizing a computing system to provide the generated decisions to operational subsystems so as to realize effectuate maneuver of the autonomous vehicle in accordance with the generated decisions. The medium enables training an arbitration model by using reinforcement learning for completely avoiding safety violations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for controlling an autonomous vehicle(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a top view of an autonomous vehicle.Autonomous vehicle (400)Accelerator (440)Brakes (442)Vehicle engine (444)Steering mechanism (446)															0																		2019-05-14	DIIDW:201934548S		
P	PATHAK S; NADKARNI V J; BAG S										System for controlling an autonomous vehicle,            comprises first sensor for detecting environment            characteristic, where driver characteristic input            device is configured to receive driver characteristic            corresponding to driving style of driver					EP3575172-A1; US2019367025-A1	VISTEON GLOBAL TECHNOLOGIES INC																									NOVELTY - The system comprises a first sensor for detecting an environment characteristic, where a driver characteristic input device is configured to receive a driver characteristic corresponding to a driving style of a driver. A controller includes a reinforcement learning adaptive cruise control that is in communication with the first sensor and the driver characteristic input device. The reinforcement learning adaptive cruise control is configured to determine a target behavior for the vehicle based on the environment characteristic and the driver characteristic, and selectively controls the vehicle based on the target behavior. USE - System for controlling a vehicle, such as an autonomous vehicle. ADVANTAGE - Ensures to control the vehicle to follow an other vehicle at a constant velocity, while maintaining a safe distance between the vehicle and the other vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a longitudinal control apparatus for controlling a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of a learning-based adaptive cruise control.															0																		2019-12-16	DIIDW:2019A1310D		
P	ZHAO Y; XUAN Z; MA K; SUN X; LIU L; LIN W										Method for managing speed control of autonomous            vehicle using human driving mode, involves comparing            vehicle control command with driving behavior standard            and verifying or modifying vehicle control command            according to comparison, and causing autonomous vehicle            to execute vehicle control command					CN117087681-A	TUSIMPLE																									NOVELTY - The method involves receiving a vehicle control command before controlling an autonomous vehicle to execute a vehicle control command. A vehicle control command with a driving behavior standard and verifying or modifying the vehicle control command is compared according to a comparison. The autonomous vehicle is used to execute the vehicle control command in accordance with the verified or modified vehicle control command. A driving behavior criteria is determined by training a reinforcement learning process comprising training by simulation to generate a data from the driving behavior criteria. The data is compared with data corresponding to human driving behavior during simulation. USE - Method for managing speed control of autonomous vehicle using human driving mode. ADVANTAGE - The method involves generating data corresponding to a desired human driving behavior, and training a human driving model module using an enhanced learning process, and ensures simple and efficient managing of the speed control of the autonomous vehicle using human driving mode. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:1. a device; and2. a computer-readable storage medium comprising a set of instructions for managing speed control of autonomous vehicle using human driving mode. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for managing speed control of autonomous vehicle using human driving mode (Drawing includes non-English language text).															0																		2023-12-13	DIIDW:2023C4073H		
P	NAGESHRAO S; JALES C B S; FILEV D P										System for interpreting data from a reinforcement            learning agent controller to control autonomous            vehicle, comprises a memory for storing instructions            that are executed by the processor that include            calculating multiple state-action values					DE102020108127-A1; US2020307577-A1; CN111830962-A; US11560146-B2	FORD GLOBAL TECHNOLOGIES LLC																									NOVELTY - The data interpreting system (10) comprises a processor (20), and a memory (22) for storing instructions that are executed by the processor. The instructions include calculating multiple state-action values based on sensor data representing an observed state through a deep neural network. A variety of linear models is generated that map the variety of state-action values to the sensor data. The processor is programmed to generate multiple linear models using an Evolving Takagi Sugeno model. USE - System for interpreting data from a reinforcement learning agent controller to control an autonomous vehicle. ADVANTAGE - Ensures to optimize a speed of the vehicle in relation to the lead vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block representation of a system for interpreting data from a reinforcement learning agent controller to control an autonomous vehicle. (Drawing includes non-English language text).Data interpreting system (10)Reinforcement learning agent controller (12)Fuzzy control (14)Processor (20)Memory (22)															0																		2020-10-19	DIIDW:2020958172		
P	LIU Y; OGUCHI K; QI X										System for learning optimal driving behavior for            autonomous vehicle i.e. car, has first stage training            module for training feature learning network during            first training stage, and second stage training module            for training decision action network during second            training stage					US2022388522-A1	TOYOTA MOTOR ENG & MFG NORTH AMERICA INC																									NOVELTY - The system has feature learning network for receiving sensor data from a vehicle as input and outputting spatial temporal feature embeddings. Decision action network receives the spatial temporal feature embeddings as input and outputs optimal driving policy for the vehicle. A first stage training module trains the feature learning network during first training stage by object detection loss. A second stage training module trains the decision action network during second training stage by reinforcement learning. Spatial feature learning network receives the sensor data as input and outputs spatial feature embeddings. USE - System for learning optimal driving behavior for an autonomous vehicle i.e. car (from drawings). ADVANTAGE - The system can provide an improved learning system for autonomous vehicles to learn optimal driving decisions and policy, and allows the decision action network to receive the spatial temporal feature embeddings as input and output the optimal driving policy for the vehicle, thus determining optimal driving behavior of the vehicle in crowded driving conditions where occlusions are present, and hence improving system performance and extendibility. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for learning optimal driving behavior for an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system for learning optimal driving behavior for autonomous vehicle i.e. car.100Autonomous vehicle optimal driving behavior learning system102Receiver vehicle104Transmitter vehicle106Road															0																		2023-08-10	DIIDW:2022F2820P		
P	WONGPIROMSARN T; COLLIN A; BELTA C; TEBBENS R D; HELOU B; BEIJBOM O O										Method for scoring trajectories for autonomous            vehicles, involves identifying trajectory with            inadequate performance by augmented route planner based            on metric associated with predicted reasonableness            scores, and changing parameters of model					US2022204033-A1; US11755015-B2	MOTIONAL AD LLC																									NOVELTY - The scoring method involves augmenting the route planner of a first vehicle using predicted reasonableness scores for trajectories by using one or more processors. The reasonableness scores are predicted by a trained machine learning model with parameters determined using a loss function that penalizes predictions of the scores that violate a rule book structure. One or more processors plan trajectories in an environment using the augmented route planner. One or more processors identify one trajectory with inadequate performance by the augmented route planner based on a first metric associated with the predicted reasonableness scores. One or more processors change the parameters of the augmented route planner in response to the identified trajectory. USE - Scoring method for trajectories of autonomous vehicles and/or other objects. ADVANTAGE - The method enables using a machine learning model to predict reasonableness scores for autonomous vehicle trajectories for a given traffic scenario, so that the predicted scores can be used to efficiently tune a route planner and performance of the route planner, compare two an autonomous vehicle stacks, and compare reinforcement learning and any other desired application. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a scoring system; (2) a non-transitory storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of an autonomous vehicle.Autonomous vehicle (100)Stereo video camera (122)Database (134)Server (136)Environment (190)															0																		2022-07-12	DIIDW:202286179V		
P	XIE J; WANG J; HUA G; HUANG Z										Autonomous vehicle lane keeping method based on            TD3 algorithm modified by exploration strategy,            involves collecting lot of training data through            interaction with environment and using data to learn,            updating algorithm and finally converging to optimal            strategy					CN114997048-A	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - The method involves calculating input automatic driving vehicle state and sensor information. In the training stage of the double-delay depth certainty strategy gradient algorithm, the noise is added to the action output by the double-delay depth certainty strategy gradient algorithm by using an Ornstein-Ulnbeck process for fully exploring a state space. The noise of the Ornstein-Ulnbeck process is subjected to weighted correction based on a path tracking method, so that invalid exploration of an automatic driving vehicle in the training process is reduced. Many experiments are carried out on a TORCS simulation platform. An automatic driving vehicle is guided by a double-delay depth certainty strategy gradient algorithm improved by an exploration strategy. Multiple training data are collected through interaction with the environment. The data are used for learning, the algorithm is updated, and finally the optimal strategy is converged. USE - Autonomous vehicle lane keeping method based on a TD3 algorithm modified by exploration strategy in field of deep reinforcement learning and automatic driving. ADVANTAGE - Improves the quality of the training samples obtained in the interaction process of an unmanned vehicle, and improves the performance of the algorithm. The exploration of the unmanned vehicle is biased to the correct direction, the proportion of low-efficiency samples in an experience playback pool is reduced, the algorithm is finally converged more quickly, and the strategy has better expressiveness. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of the autonomous vehicle lane keeping method based on a TD3 algorithm modified by exploration strategy.															0																		2022-10-12	DIIDW:2022B82047		
P	PARK T H; KIM M										Method for determining lane change of autonomous            driving vehicle using reinforcement learning in            vehicle-only road environment, involves receiving            vehicle information of surrounding vehicles through            vehicle to everything communication					KR2022116855-A; KR2514146-B1	UNIV CHUNGBUK NAT IND ACADEMIC COOP FOU																									NOVELTY - The method involves receiving (S101) vehicle information of surrounding vehicles through vehicle to everything (V2X) communication. The received vehicle information is used (S103) to select an important vehicle, which is a neighboring vehicle that has the greatest influence in determining a lane change of the autonomous vehicle. A lane change probability of the important vehicle is calculated (S105). The lane change probability is added (S107) to vehicle information. The pre-processing necessary for reinforcement learning is performed (S109) through a pre-processing network on the information obtained by adding the lane probability to the vehicle information. The reinforcement learning is performed (S111) by adding information about the autonomous driving vehicle to the information preprocessed in the preprocessing network, and a lane change determination result is outputted. USE - Method for determining lane change of autonomous vehicle using reinforcement learning in vehicle-only road environment. ADVANTAGE - The method enables determining the lane change of the autonomous vehicle using reinforcement learning in an automobile-only road environment so as to ensure real-time performance and flexibly respond flexibly to movement change of other vehicles. The method enables exhibiting better performance even in a road environment in which lanes change is performed by adding direct characteristic information on lane changes. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer-readable recording medium storing program for determining lane change of autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for determining a lane change of an autonomous vehicle using reinforcement learning in a vehicle-only road environment. (Drawing includes non-English language text)S101Step for receiving vehicle information of surrounding vehiclesS103Step for using received vehicle information to select important vehicleS105Step for calculating lane change probability of important vehicleS107Step for adding lane change probability to vehicle informationS109Step for performing pre-processing necessary for reinforcement learningS111Step for performing reinforcement learning by adding information about autonomous driving vehicle to information preprocessed in preprocessing network															0																		2023-08-10	DIIDW:2022A9721H		
P	LEE D										Reinforcement learning-based agent for collision avoidance and autonomous driving of autonomous vehicle, has operating environment that is provided in which speed control command is assigned by processing action value of learning model					KR2021063107-A	UNIV KUNSAN NAT IND ACAD COOP FOUND																									NOVELTY - The agent has a sensor fusion model (100) that creates a state memory-based state variable by fusion of multi-sensor data. An artificial neural network-based reinforcement learning model (200) outputs a behavior value for motion control of an autonomous moving object based on the state memory-based state variable of the sensor fusion model. An operating environment (300) is provided in which a speed control command is assigned by processing the action value of the artificial neural network-based reinforcement learning model. A multi-sensor (110) is provided with two sensors and generates state data by sensing surrounding objects. USE - Reinforcement learning-based agent for collision avoidance and autonomous driving of autonomous vehicle (claimed). ADVANTAGE - The performance is improved more and more because users train themselves. The behavior value directly outputs to the operating environment to exhibit simple collision avoidance and autonomous driving performance of the autonomous vehicle, and trained to improve collision avoidance and autonomous driving performance. The agent efficiently reduces the processing time by using the depth sensor and the ultrasonic sensor at the same time because the ultrasonic sensor estimates the area of pixels in the depth map image. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a reinforcement learning-based agent. (Drawing includes non-English language text)Sensor fusion model (100)Multi-sensor (110)Artificial neural network-based reinforcement learning model (200)Operating environment (300)Speed value conversion unit (310)															0																		2023-08-10	DIIDW:202162268N		
P	KWON M; LEE D										Method for solving congestion using deep            reinforcement learning-based autonomous driving            vehicle, involves operating autonomous vehicle based on            learned policy to determine behavior of autonomous            vehicle					KR2457914-B1; US2022363279-A1	UNIV FOUND SOONGSIL IND COOP																									NOVELTY - The method involves selecting an algorithm and a reward function from among multiple deep reinforcement learning for self-driving vehicle learning in an environment of a round road in which autonomous vehicles and non-autonomous vehicles operate. A deep neural network structure is determined according to the selected deep reinforcement learning algorithm. The selected deep reinforcement learning algorithm, autonomous driving is used based on state information and compensation information including the speed of the autonomous driving vehicle and the relative speed and relative position between the autonomous driving vehicle and the observable vehicle at each predetermined time learning a policy in which the vehicle speed is closest to constant speed driving. The autonomous vehicle is operated based on the learned policy to determine the behavior of the autonomous vehicle. USE - Method for solving congestion using deep reinforcement learning-based self-driving vehicle. ADVANTAGE - The decision-making model that controls the flow of roads in a vehicle-dense environment is provided. The algorithm for the most efficient driving is selected and applied by comparing and analyzing the driving patterns and performance of autonomous vehicles learned through each deep reinforcement learning algorithm. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer-readable storage medium storing program for solving congestion; anda device for solving congestion. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a stop-and-go wave phenomenon in a circular road that is an autonomous driving environment. (Drawing includes non-English language text)															0																		2022-11-24	DIIDW:2022D2444M		
P	HU Y; NAKHAEI S A; TOMIZUKA M; FUJIMURA K										Method for performing interaction-aware decision            making using simulated autonomous vehicle, involves            generating multi-goal, multi-agent, multi-stage and            interaction-aware decision making network policy based            on neural networks					US2019266489-A1; US11093829-B2	HONDA MOTOR CO LTD																									NOVELTY - The method involves training a first agent based on a first policy gradient and training a first critic based on a first loss function to learn goals in a single-agent environment, where the first agent is associated with a first agent neural network and the first critic is associated with a first critic neural network. A number of agents are trained based on the first policy gradient and training a second policy gradient and a second critic based on the first loss function and a second loss function to learn the goals in a multi-agent environment including the first agent and the number of agents using a Markov game to instantiate a second agent neural network, where each agent is associated with a driver type indicative of a level of cooperation for the respective agent. A multi-goal, a multi-agent, a multi-stage and an interaction-aware decision making network policy are generated based on the first agent neural network and the second agent neural network. USE - Method for performing interaction-aware decision making using a vehicle (claimed) i.e. simulated autonomous vehicle. ADVANTAGE - The method enables providing a multi-agent curriculum and distribution can be constructed to ensure sufficient training on difficult goal combinations that require cooperation, along with easier combinations for maintaining agent's ability to act toward goal. The method enables using Q-values associated with remaining subset of actions considered by the traffic simulator during simulation, thus mitigating amount of processing power and/or computing resources utilized during simulation and training of the autonomous vehicle in autonomous vehicle policy generation. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for interaction-aware decision making. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for performing cooperative multi-goal, multi-agent multi-stage reinforcement learning.Vehicle (170)Vehicle communication interface (172)Storage device (174)Controller (176)Vehicle system (178)															0																		2019-09-10	DIIDW:201974318K		
P	NIU Q; WANG H; CAO R; LI S; SHI W; XU J; YU Z; WANG J; LIU S										Multi-autonomous vehicle (AUV) collaborative            underwater data acquisition method, involves            continuously correcting moving track, and collecting            data of key node based on data collection method of            matrix complement type					CN115855226-A; CN115855226-B	UNIV QINGDAO SCI & TECHNOLOGY																									NOVELTY - The method involves setting underwater acoustic sensor nodes on an underwater cloth. Different sensing areas are formed. An AUV collecting sensing area is selected. A path of multi-AUV information collection is planned based on a deep reinforcement learning (DQN) method. A reward function is designed according to an information value. A moving track in a multi- AUV sailing process is continuously corrected. Data of a key node is collected based on the data collection method of a matrix complement type. USE - Multi-autonomous vehicle (AUV) collaborative underwater data acquisition method based on deep reinforcement learning (DQN) and matrix completion used in the field of marine data collection. ADVANTAGE - The method enables setting the underwater acoustic sensor node in the ocean area according to the correlation and timeliness between the node generating data and application needed data, calculating the value of each area generating information, and for subsequent AUV path planning, which greatly reduces the loss of the data value, planning multiple AUV information collecting path by using depth reinforcement learning method, and considering underwater acoustical sensor node drift caused by ocean current impact, continuously correcting AUV navigation track in the AUV driving process, and designing a data acquisition method of matrix compensation type, making AUV only need to collect data generated by key node, reducing the workload of data collection. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the multi-autonomous vehicle (AUV) collaborative underwater data acquisition method. (Drawing includes non-English language text).															0																		2023-04-30	DIIDW:202337721T		
P	SONG S; CIOCARLIE M										System for selecting action to be taken by            reinforcement learning agent e.g. robot, in            environment, has hardware processor coupled to memory,            where processor determines that variance meets            threshold, and requests identification of action to be            taken by reinforcement learning agent					US2023244229-A1	SONG S; CIOCARLIE M																									NOVELTY - The system has a hardware processor (502) coupled to a memory (504), where the processor determines a first variance for a first state of an environment, where the variance is based on reinforcement learning. The processor determines that the variance meets a threshold, requests an identification of a first action to be taken by a reinforcement learning agent from a human, receives the identification of the first action, and causes the first action to be taken by the agent in response to determining that the first variance meets the threshold, where the agent is one of an autonomous vehicle and a robot. The processor determines a second variance for a second state of the environment, and selects a second action based on the reinforcement learning policy. USE - System for selecting an action to be taken by a reinforcement learning agent e.g. autonomous vehicle and robot (all claimed), in an environment. ADVANTAGE - The system allows the reinforcement learning agent to be able to always interact with an environment optimally, provides an efficient way to select actions to be taken by the agent in a deterministic environment, and allows the agent to learn behavior of the environment efficiently, thus improving performance of the agent. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a method for selecting an action to be taken by a reinforcement learning agent in an environment; (2) a non-transitory computer-readable medium comprising a set of instructions for selecting an action to be taken by a reinforcement learning agent. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for selecting an action to be taken by a reinforcement learning agent.502hardware processor504Memory506input device controller508Input device514communication interfaces															0																		2023-08-17	DIIDW:202381165H		
P	LI R; ZHAO Y; LI X; WEI H; XU Z; ZHANG Y										Method for controlling automatic driving vehicle,            involves returning to step of obtaining real-time            traffic environment information of automatic driving            vehicle during driving process at current moment if            automatic driving is not ended					CN112249032-A; CN112249032-B; WO2022088798-A1; US2023365163-A1	INSPUR BEIJING ELECTRONIC INFORMATION IN																									NOVELTY - The method involves obtaining (S101) real-time traffic environment information of an autonomous vehicle during a driving process at a current moment. Real-time traffic environment information is mapped (S102) based on a preset mapping relationship to obtain mapped traffic environment information. A target deep reinforcement learning model is adjusted (S103) based on a pre-stored existing deep reinforcement learning model and mapped traffic environment information. A determination is made (S104) on whether to end the automatic driving. The method is returned (S105) to the step of obtaining the real-time traffic environment information of the automatic driving vehicle during a driving process at a current moment if the automatic driving is not ended in which the mapping relationship includes the mapping relationship between the real-time traffic environment information and the existing traffic environment information of the existing deep reinforcement learning model. USE - Method for controlling automatic driving vehicle. ADVANTAGE - The method avoided adjusting the target deep reinforcement learning model from scratch, speeds up the decision-making efficiency of the target deep reinforcement learning model, and performs fast and stable automatic driving. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an automatic driving system;(2) an automatic driving device; and(3) a computer readable storage medium storing program for performing automatic driving. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an automatic driving method. (Drawing includes non-English language text)Step for obtaining real-time traffic environment information of an autonomous vehicle during a driving process at a current moment (S101)Step for mapping real-time traffic environment information based on a preset mapping relationship to obtain mapped traffic environment information (S102)Step for adjusting target deep reinforcement learning model based on a pre-stored existing deep reinforcement learning model and mapped traffic environment information (S103)Step for determining whether to end the automatic driving (S104)Step for returning to step of obtaining the real-time traffic environment information of automatic driving vehicle during driving process at current moment if automatic driving is not ended (S105)															0																		2021-02-11	DIIDW:202111211U		
P	HOEL C; LAINE L										Method for controlling autonomous vehicle using            reinforcement learning agent, involves providing set of            training sessions in which reinforcement learning agent            interacts with environment including autonomous            vehicle					EP4086813-A1; US2022374705-A1; CN115392429-A	VOLVO AUTONOMOUS SOLUTIONS AB; HOEL C; LAINE L																									NOVELTY - The method involves providing a reinforcement learning (RL) agent (320) for decision-making to be used in controlling an autonomous vehicle (299). A set of training sessions is provided in which the RL agent interacts with an environment including the autonomous vehicle, where each training session includes a different initial value, and yields a state-action quantile function dependent on states and actions. A tentative decision relating to control of the vehicle is executed in dependence of two estimated uncertainties. The RL agent is provided with a neural network, where the training sessions employ an implicit quantile network. USE - Method for controlling an autonomous vehicle i.e. car using a reinforcement learning agent. ADVANTAGE - The aleatoric and epistemic uncertainty of outputs of the decision-making agent can be assessed effectively. The need for additional training of the RL agent is assessed based on aleatoric and epidemiologic uncertainty. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a method for providing a reinforcement learning; andan arrangement for controlling an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of an arrangement for controlling an autonomous vehicle.299Autonomous vehicle314Vehicle control interface320RL agent322First uncertainty estimator324Second uncertainty estimator															0																		2022-11-21	DIIDW:2022E0619D		
P	FENG Y; ZHAO X; HUI F; JING S										Lane change trajectory planning method for            autonomous vehicle, involves calculating vehicle state            of lane changing vehicle at each time point in changing            process by using vehicle transverse and longitudinal            discretization kinematics model through optimal            sequential acceleration decision information					CN114852105-A	UNIV CHANGAN																									NOVELTY - The method involves acquiring vehicle information. The optimal lane changing decision of the lane changing vehicles at the current moment is solved, according to vehicle information, a game lane changing decision model of lane changing vehicles and surrounding vehicles, a game income function considering safety and timeliness and a game income matrix. An automatic driving vehicle lane changing track planning model based on deep reinforcement learning is utilized to obtain optimal sequential acceleration decision information in the whole lane changing process, according to the optimal lane changing decision. The vehicle state of the lane changing vehicle at each time point in the lane changing process by using the vehicle transverse and longitudinal discretization kinematics model through the optimal sequential acceleration decision information is calculated, and the lane changing track of the lane changing vehicle is obtained according to the vehicle state of the lane changing vehicle. USE - Lane change trajectory planning method for autonomous vehicle. ADVANTAGE - The method completes the track changing planning of the automatic driving vehicle under the condition of considering safety, high efficiency, comfort and fuel economy. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a lane change trajectory planning system for autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the lane change trajectory planning method for autonomous vehicle. (Drawing includes non-English language text)															0																		2022-09-17	DIIDW:2022A6153B		
P	SHANKAR M; RAJPUT S										Method for training recurrent neural network for            e.g. autonomous car, utilizing reinforcement learning            model, involves training recurrent neural network based            on randomly selected sets of training representations            and driving states					EP4080413-A1	CONTINENTAL AUTONOMOUS MOBILITY US LLC																									NOVELTY - The method involves providing a recurrent neural network (RNN) configured to receive representations of surroundings of vehicles (T1). Training data comprising sets of training representations of surroundings of vehicles and corresponding driving states is provided (T2) to derive a ground truth. The RNN is trained (T3) based on randomly selected sets of training representations and corresponding driving states from the training data, where the sets of training representations of surroundings of vehicles are recorded during real drives of real vehicles, and the corresponding driving states are recorded during the respective real drives of the real vehicles. USE - Method for training a RNN for an autonomous vehicle i.e. autonomous car, utilizing a reinforcement learning model by a data processing system (claimed). ADVANTAGE - The method enables optimizing filters or kernels through an automated learning model, thus improving performance of the RNN in an efficient manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a computer program for storing a set of instructions for training a RNN for an autonomous vehicle utilizing a reinforcement learning model by a data processing system; anda computer-readable medium for storing a set of instructions for training a RNN for an autonomous vehicle utilizing a reinforcement learning model by a data processing system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for training a RNN for an autonomous vehicle utilizing a reinforcement learning model by a data processing system.T1Step for providing RNN configured to receive representations of surroundings of vehiclesT2Step for providing training data comprising sets of training representations of surroundings of vehicles and corresponding driving states to derive ground truthT3Step for training RNN based on randomly selected sets of training representations and corresponding driving states from training data															0																		2023-08-10	DIIDW:2022D2463U		
P	RAIKO P T; LEVIHN M										Method for evaluating varying-size action spaces            for autonomous vehicles using neural network-based            reinforcement learning models, involves transmitting            motion-control directives to implement particular            action of set to subsystem of vehicle					US11243532-B1	APPLE INC																									NOVELTY - The method involves identifying a first set of actions corresponding to a first state of an environment of a first vehicle. A first encoding of the first action and a second encoding of the second action are generated, where in the first encoding, the first target lane segment is indicated by a first color. The respective estimated value metric associated with individual actions of the first set is determined using a plurality of instances of a first machine learning model. An input data set of a first instance of the first machine learning model comprises the first encoding, and an input data set of a second instance of the first machine learning model comprises the second encoding. The motion-control directives to implement a particular action of the first set are transmitted to a motion-control subsystem of the first vehicle. The particular action is selected from the first set based on its estimated value metric. USE - Method for evaluating varying-size action spaces for autonomous vehicles using neural network-based reinforcement learning models. ADVANTAGE - The method involves determining a representation of the current state of the environment of an autonomous or partially-autonomous vehicle at various points in time during a journey, and identifying a corresponding set of feasible or proposed actions, and thus enables to make timely and reasonable decisions regarding an autonomous vehicle's trajectory in the context of unpredictable behaviors of other entities and incomplete or noisy data about static and dynamic components of the vehicle's environment remains a significant challenge. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following;1. System for evaluating varying-size action spaces for autonomous vehicles; and2. Non-transitory computer-accessible storage media storing program for evaluating varying-size action spaces for autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram illustrating the decision making scenario for an autonomous vehicle.Origin (215)Entrance ramp (234)Exit ramp (235)Autonomous vehicle (250)Autonomous vehicle motion planning problem characteristics (261)															0																		2022-02-19	DIIDW:202221117G		
P	SAXENA D M; BAE S; NAKHAEI S A; FUJIMURA K										System for generating model-free reinforcement            learning policy using autonomous vehicle e.g. car, has            simulator that is implemented via processor and memory            for building policy based on simulated traffic scenario            using actor-critic network					US2021086798-A1; US11465650-B2	HONDA MOTOR CO LTD																									NOVELTY - The system (100) has a processor (102) and a memory (104). A simulator (108) is implemented via the processor and the memory for generating a simulated traffic scenario including lanes, an ego-vehicle (170), a dead end position, and traffic participants. The dead end position is a position by which a lane change for the ego-vehicle is desired. The simulated traffic scenario is associated with an occupancy map, a relative velocity map, a relative displacement map, and a relative heading map at each time step within the simulated traffic scenario. The ego-vehicle and the traffic participants are modeled using a kinematic bicycle model. A policy is built based on the simulated traffic scenario using an actor-critic network. The policy is implemented on an autonomous vehicle. USE - System for generating model-free reinforcement learning policy using autonomous vehicle e.g. car, truck, van, minivan, sport utility vehicle (SUV), motorcycle, scooter, boat, personal watercraft, and aircraft, battery electric vehicle (BEV) and plug-in hybrid electric vehicle. ADVANTAGE - The controller is enabled to autonomously drive the vehicle around based on the policy network and to make autonomous driving decisions according to the generating a model-free reinforcement learning policy which occurred within the simulator, as the policy network is indicative of the policies or decisions which should be made based on the training or the simulation. The policies target the states and actions specifically in each repetition cycle, testing and improving the accuracy of the policy, to improve the quality of the model. The reinforcement is applied by continually re-running or re-executing the learning process based on the results of prior learning, effectively updating an old policy with a newer policy to learn from the results and to improve the policy. The policy is built in a model free fashion to mitigate capturing interactions from each traffic participant at each time step, which reduces the associated computational cost for the system. The simulation enables the policy to learn to repeatedly probe into a target road lane while finding a safe spot to move into. The size of the action space is greatly increased to achieve smooth behaviors with high enough fidelity via discrete control, which makes discrete control methods intractable. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for generating the model-free reinforcement learning policy;(2) an autonomous vehicle implementing a model-free reinforcement learning policy. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the system for model-free reinforcement learning.System for model-free reinforcement learning (100)Processor (102)Memory (104)Simulator (108)Vehicle (170)															0																		2021-05-01	DIIDW:202142201H		
P	KIM K; KIM Y; KIM I; KIM H; NAM W; BOO S; SUNG M; YEO D; RYU W; JANG T; JEONG K; JE H; CHO H; KIMKYEHYUN; KIM Y J; KIM I S; KIMHAGKYUNG; NAM W H; BOO S H; SUNG M C; RYU W J; JANG T U; JEONG K J; JE H M; CHO H J; JIN G; JIN R; JIN Y; JIN H; NAN Y; FU S; CHENG M; LV D; LIU Y; ZHANG T; ZHENG J; ZHU H; ZHAO H										Method for efficient resource allocation in            autonomous driving by reinforcement learning of            autonomous vehicle involves adjusting at least portion            of parameters used for neural network operation by            referring to reward by computing device					US10503174-B1; EP3690708-A1; KR2020096096-A; CN111507157-A; IN202024001165-A; JP2020125102-A; KR2277531-B1; JP6982897-B2; CN111507157-B	STRADVISION INC; STRATH VISUAL CO																									NOVELTY - The method involves enabling computing device to perform at least one neural network operation by referring to the attention sensor data to calculate one or more attention scores (210), and acquire at least one video data taken by at least portion of one or more cameras installed on the autonomous vehicle (200) by referring to the attention scores and generate at least one decision data for the autonomous driving by referring to the video data. The computing device operates autonomous vehicle by referring to the decision data to acquire at least one circumstance data representing change of circumstance of the autonomous vehicle in operation, and to generate at least one reward by referring to the circumstance data. The computing device adjusts at least portion of one or more parameters used for the neural network operation by referring to the reward. USE - Method for efficient resource allocation in autonomous driving by reinforcement learning of autonomous vehicle. ADVANTAGE - Performs resource allocation efficiently in autonomous driving by reinforcement learning which reduces power consumption of the autonomous vehicle. Provides virtual space where the autonomous vehicle optimizes the resource allocation by the reinforcement learning which reduces potential risks in the reinforcement learning. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing device for efficient resource allocation in autonomous driving by reinforcement learning of autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing is the schematic view illustrating attention scores calculated for performing the efficient resource allocation.Autonomous vehicle (200)Attention scores (210)Panoramic image (220)Specific directions (222,223)Region of interest panoramic image (230)															0																		2019-12-19	DIIDW:2019A2658F		
P	HOPPE S; LOU Z										Method for automatically influencing actuator such            as particular robot, machine, partially autonomous            vehicle or tool, involves defining upper confidence            bound as function of empirical unit and variance over            distribution of advantages					DE102019207410-A1; CN111984000-A; EP3741518-A1; EP3741518-B1	BOSCH GMBH ROBERT																									NOVELTY - The method involves providing (300) a state of an actuator or an environment of the actuator by an exploration strategy for learning a policy. An action for the automated influencing of the actuator depending on the state is defined (308) by the policy. A state value is defined as an expected value for a sum of rewards that are achieved when the policy is followed based on the state. A state action value is defined as an expected value for a sum of rewards that are achieved if any action is first carried out in the state and then the policy is carried out. The action that maximizes an empirical unit over a distribution is defined by the policy for the state. A state that locally maximizes an upper confidence bound is specified by the exploration strategy. The upper confidence bound is defined as a function of an empirical unit and a variance over the distribution of multiple advantages. USE - Method for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool. ADVANTAGE - The influencing of an actuator by reinforcement learning is improved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a device for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool; and(2) a computer program product having instructions for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating am method for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool.Step for providing a state of an actuator or an environment of the actuator by an exploration strategy for learning a policy (300)Step for moving actuator moved to a waypoint (302)Step for determining whether a collision of the actuator with an obstacle in the vicinity of the actuator occurs when moving from one waypoint to a next waypoint in the series (304)Step for judging whether the next waypoint can be reached when moving from one waypoint to a next waypoint in the series (306)Step for defining action for the automated influencing of the actuator depending on the state by the policy (308)															0																		2020-12-14	DIIDW:2020B65544		
P	NISHI T										Controlling method for autonomous vehicle,            involves applying passive actor-critic reinforcement            learning method to passively-collected data related to            vehicle operation, and Z-value is estimated using            linearized version of bellman equation					US2018011488-A1; US10061316-B2; JP2019031268-A; JP6856575-B2	TOYOTA MOTOR ENG & MFG NORTH AMERICA INC; TOYOTA MOTOR ENG & MFG NORTH AMERICA																									NOVELTY - The controlling method involves applying the passive actor-critic reinforcement learning method to passively-collected data related to the vehicle operation. A control policy is adapted for the vehicle to perform the vehicle operation. The control policy is configured for controlling the vehicle to merge the vehicle midway between the second vehicle and third vehicle. The Z-value is estimated using a linearized version of a bellman equation. The average cost is estimated under the optimal policy. USE - Controlling method for an autonomous vehicle, such as a hybrid vehicle. ADVANTAGE - The control policy is adapted for the vehicle to perform the vehicle operation with a minimum expected cumulative cost, and ensures allowing the communications between the nearby vehicles. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for optimizing the control policy for performing the vehicle operation; and(2) a computing system for optimizing he control policy for autonomously controlling the vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of the controlling method.Computing system (14)Communications interfaces (16)Steering system (18)Throttle system (20)Braking system (22)															0																		2018-02-16	DIIDW:201803909Q		
P	HOTSON G; MOOSAEI M; NARIYAMBUT MURALI V; JAIN J J; NARIYAMBUT M V										Method for operating autonomous vehicle, involves            receiving inputs from passenger, which is followed by            receiving sensor data, and updating control logic            according to inputs and sensor data to obtain updated            control logic					WO2018139993-A1; WO2018139993-A8; CN110225852-A; DE112017006530-T5; US2019382030-A1; US11318952-B2; CN110225852-B	FORD GLOBAL TECHNOLOGIES LLC																									NOVELTY - The autonomous vehicle operation method involves receiving (304) multiple inputs from a passenger of an autonomous vehicle. The sensor data is received (306) from the autonomous vehicle. The control logic of the autonomous vehicle is updated (310) according to the inputs and the sensor data to obtain updated control logic. USE - Method for operating an autonomous vehicle. ADVANTAGE - The deep reinforcement learning models are trained based on the feedback to promote actions that were rated highly by the passenger and uneventful ride and reduce occurrence of actions that are present in lowly rated rides or flagged as anomalies by the passenger. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for operating autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for updating control logic based on passenger feedback.Presenting interface (302)Receiving multiple inputs (304)Receiving sensor data (306)Training model per feedback (308)Updating control logic (310)															0																		2018-08-28	DIIDW:201860575J		
P	KIM K; KIM Y; KIM H; NAM W; BOO S; SUNG M; SHIN D; YEO D; RYU W; LEE M; LEE H; JANG T; JEONG K; JE H; CHO H; KIMKYEHYUN; KIM Y J; KIMHAGKYUNG; NAM W H; BOO S H; SUNG M C; SHIN D S; RYU W J; LEE M C; SOO L H; JANG T U; JEONG K J; JE H M; CHO H J; JIN G; JIN R; JIN H; NAN Y; FU S; CHENG M; SHEN D; LV D; LIU Y; LI M; LI J; ZHANG T; ZHENG J; ZHU H; ZHAO H; KIM I S										Learning method for use with an autonomous vehicle            for supporting a Reinforcement Learning (RL) by using            human driving data as training data, to perform a            personalized path planning by a learning device,            involves learning device					EP3690769-A1; US2020250486-A1; KR2020095378-A; CN111507501-A; IN202014003093-A; JP2020126646-A; US11074480-B2; JP6931937-B2; KR2373448-B1	STRADVISION INC; STRAD VISION INC																									NOVELTY - The method involves learning device. If actual circumstance vectors and information on actual actions performed at timings corresponding to the actual circumstance vectors by referring to actual circumstances, corresponding there to included in each of driving trajectories of the subject driver are acquired, a process of instructing an adjustment reward network is performed, which is built to operate as an adjustment reward function, to generate each of first adjustment rewards corresponding to each of the actual actions performed at each of the timings. The learning device instructs a first loss layer to generate one adjustment reward loss by referring to each of first personalized rewards and the actual prospective values, and to perform backpropagation by referring to the adjustment reward loss, to learn portion of parameters of the adjustment reward network. USE - Learning method for use with an autonomous vehicle for supporting a Reinforcement Learning by using human driving data as training data, to perform a personalized path planning by a learning device (claimed). ADVANTAGE - Provides a learning method for supporting a Reinforcement Learning algorithm by using human driving data as training data, to provide a personalized path planning, and then to provide a satisfied driving experience to passengers of an autonomous vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a testing method for using at least one personalized reward function to train a personalized Reinforcement Learning agent; and(2) a learning device for acquiring at least one personalized reward function, used for performing a Reinforcement Learning algorithm. DESCRIPTION Of DRAWING(S) - The drawing shows a block representation of a configuration of a learning device performing a learning method for supporting a Reinforcement Learning by using human driving data as training data, to perform a personalized path planning.Learning device (100)Communication portion (110)Memory (115)Processor (120)Adjustment reward network (130)															0																		2020-08-17	DIIDW:202073773Q		
P	HOEL C; LAINE L										Control method for autonomous vehicle using            reinforcement learning (RL) agent involves executing            tentative decision in dependence of estimated            uncertainty to control autonomous vehicle					WO2021213616-A1; CN115427966-A; EP4139844-A1; US2023142461-A1	VOLVO TRUCK CORP; VOLVO AUTONOMOUS SOLUTIONS AB																									NOVELTY - The method (100) involves interacting RL agent with an environment including the autonomous vehicle provided in several training sessions (110-1-110-K). Each training session is provided with a different initial value and a state-action value function Qk(s, a) dependent on state and action is yielded. The decision-making (112) is made in which the RL agent outputs tentative decision relating to control of the autonomous vehicle. Uncertainty estimation (114) is made on the basis of a variability measure for several state-action value functions evaluated for a state-action pair corresponding to each of the tentative decisions. The tentative decision is executed in dependence of the estimated uncertainty and vehicle control (116). USE - Method of controlling an autonomous vehicles such as trucks, buses, construction equipment, mining equipment and other heavy equipment operating in public or non-public traffic, using a RL agent to make decisions such as when to change lanes on a highway, or whether to stop or go at an intersection. ADVANTAGE - The method enables providing a safety criterion that determines whether the trained decision-making agent is confident enough about a particular decision, so that the agent can be overridden by a safety-oriented fallback decision in the negative case. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an arrangement for controlling an autonomous vehicle; and(2) a computer program for controlling an autonomous vehicle using a RL agent. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method.Method (100)Several training sessions (110-1-110-K)Step for decision-making in which the RL agent outputs tentative decision relating to control of the autonomous vehicle (112)Step for estimating uncertainty on the basis of a variability measure for several state-action value functions evaluated for a state-action pair corresponding to each of the tentative decisions (114)Step for executing tentative decision in dependence of the estimated uncertainty and vehicle control (116)															0																		2021-12-02	DIIDW:2021C0895X		
P	FELIP L J; ALVAREZ I J; ELLI M S; GONZALEZ A D I; TUREK J; ALVAREZ I; GONZALEZ A D; FELIP L J F										Map generation system for use with autonomous            vehicle, has analyzer that is configured to analyze            vector-field map to identify vectors of several cells            exceeding predetermined threshold values					US2020149898-A1; DE102020131240-A1; CN113052964-A; US11536574-B2	INTEL CORP																									NOVELTY - The map generation system has a data aggregator (305) that is configured to aggregate data received from autonomous vehicles (AVs) to generate aggregated data. A vector-field generator is configured to generate a vector-field map including several cells based on the aggregated data each cell having a corresponding vector. An analyzer is configured to analyze the vector-field map to identify vectors of the several cells exceeding predetermined threshold values. An analyzed signal is generated corresponding to the identified vectors and provide the analyzed signal to the AVs. USE - Map generation system for use with autonomous vehicle (claimed). ADVANTAGE - Since the MGS is configured to analyze the sum of magnitudes of vectors during traffic light transitions to adapt traffic control configurations, the overall accelerations in the vicinity is reduced. Since the machine learning model can be executed by a computing system, the performance of a specific task is improved. The accuracy is improved by providing positive or negative feedback to the reinforcement learning models. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory computer-readable storage medium storing a program for generating and analyzing acceleration-based vector field maps; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of map generation system.Controller (210)Data aggregator (305)Spatio-temporal vector field generator (310)Frequency analyzer (315)Magnitude analyzer (320)															0																		2020-05-28	DIIDW:202039879C		
P	WONGPIROMSARN T; COLLIN A; BELTA C; TEBBENS R D; HELOU B; BEIJBOM O O										Method for scoring trajectories for autonomous            vehicles, involves planning trajectory in environment            using augmented route planner, and operating vehicle            along planned trajectory using control circuit of            vehicle					US11203362-B1; DE102020128156-A1	MOTIONAL AD LLC																									NOVELTY - The method involves generating a set of trajectories for a vehicle operating in an environment, where each trajectory is associated with a traffic scenario. A reasonableness score is predicted for each trajectory, where the score is obtained from a machine learning model that is trained using input obtained from human annotators. A route planner of the vehicle (100) is augmented using the predicted scores for the trajectories. A trajectory in the environment is planned using the augmented planner. The vehicle is operated along the planned trajectory using a control circuit of a vehicle. A set of realizations of traffic scenarios is obtained. USE - Method for scoring trajectories for autonomous vehicles (claimed) and/or other objects. ADVANTAGE - The method enables using a machine learning model to predict reasonableness scores for AV trajectories for a given traffic scenario, so that the predicted scores can be used to tune a route planner and performance of the route planner, compare two AV stacks, and compare reinforcement learning and any other desired application in an efficient manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an autonomous vehicle.Autonomous vehicle (100)Steering control (102)Brakes (103)Server (136)Communication device (140)															0																		2022-01-16	DIIDW:2021E63670		
P	CAMPOS M L E; DE LA GUARDIA G R; GUZMAN L A K; GOMEZ G D; PARRA V J I; CAMPOS MACIAS L E; DE LA GUARDIA GONZALEZ R; GUZMAN LEGUEL A K; GOMEZ GUTIERREZ D; PARRA VILCHIS J I										Event filtering device for filtering multiple            event data, has processor that filters filter event            data, such that light frequency associated with filter            event data is not substantially same as reference            signal frequency					US2020223434-A1; EP3885975-A1; CN113442936-A; US11886968-B2	INTEL CORP																									NOVELTY - The device has processor (102) that determines a reference signal frequency associated with a transmitted light. An event data associated with a change in a light intensity of a pixel is received, such that the light intensity is associated with a light frequency. A select event data is identified from multiple event data, such that the light frequency associated with the select event data is substantially the same as the reference signal frequency. A filter event data is filtered from multiple event data, such that the light frequency associated with the filter event data is not substantially the same as the reference signal frequency. USE - Event filtering device for filtering multiple event data. ADVANTAGE - The reinforcement learning models can include positive or negative feedback to improve accuracy. The safety driving model includes a mathematical model for safety assurance that enables identification and performance of proper responses to dangerous situations such that self-perpetrated accidents can be avoided. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for filtering multiple event data; and(2) a device for calculating time to contact for autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram illustrating the autonomous vehicle.Vehicle (100)Processor (102)Memory (104)Antenna system (106)Measurement device (116)															0																		2020-07-27	DIIDW:202065140L		
P	NAGESHRAO S; TSENG H E; FILEV D P; BAKER R L; CRUISE C; DAEHLER L; MOHAN S; KUSARI A										Process for performing drive adaptive learning of            autonomous or semi-autonomous vehicle based on input of            vehicle sensor data, involves entering vehicle sensor            data into neural network, where first neural network            includes security agent					DE102019122826-A1; US2020065665-A1; CN110858325-A; US10733510-B2	FORD GLOBAL TECHNOLOGIES LLC																									NOVELTY - The process involves entering vehicle sensor data into a first neural network, where first neural network includes a security agent that determines a likelihood of unsafe vehicle operation. First neural network is adjusted at variety of times by using a periodically re-trained deep reinforcement learning agent with second neural network. The vehicle (110) is operated based on a vehicle action output from the first neural network. The vehicle sensor data is input by inputting a color video image into the first neural network. The probabilities of unsafe vehicle operation is determined by the security agent. USE - Process for performing drive adaptive learning of autonomous or semi-autonomous vehicle i.e land based vehicle, such as passenger car or light truck based on input of vehicle sensor data. ADVANTAGE - The deep neural network can be trained to enter vehicle transition states in response to sensor data input, which improves the driving of the vehicle by using the security agent block. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing system that includes a computer programmed to determine vehicle action based on the input of vehicle sensor data. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a traffic infrastructure system. (Drawing includes non-English language text).Vehicle (110)Infrastructure interface (111)Computing device (115)Sensor (116)Server computer (120)															0																		2020-03-16	DIIDW:202017338W		
P	RUSSELL A K; GEBRE M R; KHILARI S C; BURBANK I P; ENGLARD B; RAMEZANI V R; MAHESHWARI P										Method for determining scan pattern according to            which sensor equipped with scanner scans field of            regard, involves applying optimization scheme to            multiple objective functions to generate scan            pattern					US2022187463-A1; WO2022132451-A1; CN116583762-A	LUMINAR LLC; LUMINAR TECHNOLOGIES INC																									NOVELTY - The method involves obtaining a set of objective functions by processing hardware, where each objective function specifies a cost for a respective property of a scan pattern expressed in terms of operational parameters of a scanner. An optimization scheme is applied to the objective functions to generate the scan pattern by the processing hardware. A field of regard (FOR) (120A) is scanned according to the generated scan pattern. The objective functions include a velocity objective function specifying the cost for velocity at which the scanner scans a scan dimension of the FOR, and the velocity function generates higher cost for higher velocity. USE - Method for determining a scan pattern by which a sensor scans a FOR by using light detection and ranging (lidar) system (claimed) in an autonomous vehicle. Uses include but are not limited to car, automobile, motor vehicle, truck, bus, van, trailer, off-road vehicle, farm vehicle, lawn mower, construction equipment, golf cart, taxi, motorcycle, scooter, bicycle, skateboard, train, snowmobile, watercraft and spacecraft ADVANTAGE - The method enables determining the scan pattern in view of certain constrains of the scanning system, such as a maximum velocity or acceleration, and applying reinforcement learning techniques to account for higher-level goals, such as collision avoidance, when generating a scan pattern, thus accurately detecting and predicting lane boundary locations during autonomous vehicle operation. The method allows a control architecture to scan one or more regions of interest in a dense manner, thus yielding higher resolution. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of cognitive light detection and ranging configured to optimize multiple objective functions to generate a pattern for scanning the environment in an autonomous vehicle.Cognitive lidar (100)Control architecture (102)Vehicle (104)Vehicle environment (110)FOR (120A)															0																		2022-07-02	DIIDW:202279536X		
P	ILIEVSKI M; SMART M; TIEU K; NARANG A; KUMAVAT A; NARANG G										Method for context-aware decision making of            autonomous agent in autonomous vehicle field, involves            determining trajectory for autonomous agent based on            first output and operating autonomous agent based on            trajectory					US2021380130-A1; US11260882-B2	GATIK AI INC																									NOVELTY - The method involves determining a characterization of an environment of an autonomous agent. The characterization is mapped to a learned model from a set of multiple learned models. A first output is produced by using the learned model. A trajectory is determined for the autonomous agent based on the output. The autonomous agent is operated based on the trajectory. A predetermined set of contexts is labeled on a map, where the context is selected based on map and a pose of the agent. Each of the learned models is trained by adopting an inverse reinforcement learning algorithm. The action is selected from an action space defined by the model. USE - Method for context-aware decision making of an autonomous agent in the autonomous vehicle field. Uses include but are not limited to an automobile such as car, driverless car, bus, shuttle, taxi, ride-share vehicle, truck, semi-truck, etc., a watercraft such as boat, water taxi, etc., an aerial vehicle such as plane, helicopter, drone, etc., and a terrestrial vehicle such as two-wheeled vehicle, bike, motorcycle, scooter, etc. ADVANTAGE - The method enables utilizing a cloud computing as a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. The method allows a cloud consumer to unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider, and automatically and rapidly scale out and rapidly released to quickly scale in. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for context-aware decision making of an autonomous agent in the autonomous vehicle field.															0																		2022-01-15	DIIDW:2021E3380Y		
P	GOTO T; GOTO K										Learning device for use in autonomous vehicle, has            reward deriver that derives reward for action of            vehicle on basis of individual rewards and planner that            performs reinforcement learning that optimizes reward            derived by reward deriver					US2020070844-A1; JP2020035222-A; CN110874642-A; JP7048456-B2; US11498574-B2; CN110874642-B	HONDA MOTOR CO LTD																									NOVELTY - The learning device (300) comprises a planner (310) that is configured to generate information indicating an action of a vehicle. A reward deriver (360) is configured to derive several individual rewards obtained by evaluating each of several pieces of information to be evaluated, which include feedback information obtained from a simulator or an actual environment by inputting information based on the information indicating the action of the vehicle to the simulator (400). The reward deriver is configured to derive a reward for the action of the vehicle on the basis of several individual rewards. The planner performs reinforcement learning that optimizes the reward derived by the reward deriver. USE - Learning device for use in autonomous vehicle e.g. two-wheeled vehicle, three-wheeled vehicle and four-wheeled vehicle. ADVANTAGE - The learning device inputs the various pieces of information to the simulator through the process of the control operator, and efficiently acquires the feedback information. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a learning method of autonomous vehicle; and(2) a computer-readable non-transitory storage medium storing program for causing computer to learn for use in autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the learning device for use in autonomous vehicle.Learning device (300)Planner (310)Control operator (320)Reward deriver (360)Simulator (400)															0																		2020-03-16	DIIDW:202018966N		
P	MOZE M; GUILLEMARD F; AIOUN F; ZHAO H; SUN R; HU S										Method for trajectory of vehicle, involves selecting particular trajectory from set that mimicks human follow in initial situation and generates set containing admissible trajectories for predicted vehicle from initial situation					WO2021077446-A1	PSA AUTOMOBILES SA; UNIV PEKING																									NOVELTY - The method involves generating a set containing admissible trajectories for predicted vehicles from an initial situation. A particular trajectory from the set that mimics the one human follows in the same initial situation is selected. The admissible trajectories are generated in a Frenet frame, that is set at the initial position of the predicted vehicle and the Frenet frame is fixed during the whole trajectory prediction and is always relative to the predicted vehicle initial position. USE - Method for trajectory of vehicle used in autonomous driving. Can also be used for implementing prediction system for trajectory of vehicle (claimed). ADVANTAGE - The prediction method for the trajectory planning of an autonomous vehicle based on inverse reinforcement learning, predicts the trajectory of human-driven vehicles during driving. The trajectory of the predicted vehicle using the Frenet Frame minimizes the acceleration and make the velocity, acceleration, and jerk have continuity. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a prediction system for the trajectory of a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic view of the generated trajectories and human driving trajectory.															0																		2021-05-13	DIIDW:2021425745		
P	ZHANG K; ELWART S; STIMPSON A										Method for controlling autonomous vehicle (AV)            e.g. electric power train, involves generating            parameter tunings based on reinforcement learning            method, and updating AV controller based on parameter            tunings					US2020089244-A1; CN110901656-A; CN110901656-B	GREAT WALL MOTOR CO LTD; GREAT WALL MOTORS CO LTD																									NOVELTY - The method involves constructing a plant model based on a design of experiments (DOE) test matrix. A controller simulation is performed based on the constructed plant model. The performance data is generated based on the controller simulation. An unsupervised learning method is performed to identify regimes. A reinforcement learning method is performed based on the regimes. The parameter tunings are generated based on the reinforcement learning method. An AV controller is updated based on the parameter tunings. The DOE test matrix includes an entry radius, a curve radius, an exit radius, an entry length, a curve length, an exit length, an entry speed, a curve speed, an exit speed and direction. The DOE test matrix is replicated to refine the plant model. USE - Method for controlling autonomous vehicle (AV) such as electric power train. ADVANTAGE - The controller system communicates with the object detection system and the navigation system to operate a steering/acceleration profile for the host vehicle to avoid the potential collisions with other vehicles or objects. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a vehicle control system for controlling AV. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of a vehicle control system.Vehicle control system (5000)Controller (5130)Location device (5140)Image device (5150)Deep learning module (5240)															0																		2020-03-30	DIIDW:202021818L		
P	LYU C; WU J										Deep reinforcement learning model training method            for autonomous vehicle, involves minimizing loss            function of policy network, where loss function            comprises autonomous guidance component and human            guidance component					WO2022197252-A1; WO2022197252-A9	UNIV NANYANG TECHNOLOGICAL																									NOVELTY - The method involves minimizing a loss function of a policy network and the loss function of the policy network that comprises a human guidance component. An autonomous guidance component is provided zero when the state information is indicative of input of a human input signal at a machine. A value network is configured to estimate the value function based on the Bellman equation. A first value network is paired with a second value network and each value network having the same architecture for reducing or preventing overestimation. Each value network is coupled to a target value network and the policy network is coupled to a target policy network. USE - Method of training deep reinforcement learning model for autonomous control of machine i.e. autonomous vehicle (AV). ADVANTAGE - The method effectively trains the deep reinforcement learning model for autonomous control of the machine, thus improving the data-processing efficiency to ensure that human guidance-based deep reinforcement training (DRL) algorithms are feasible in practice, and reducing the requirements for human participants in human-guided DRL algorithms. The method improves the performance of DRL agents, and ensures the quality of the data collected and achieves an ideal improvement in performance. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a method for autonomous control of a machine;a system for training a deep reinforcement learning model for autonomous control of machine;a system for autonomous control of machine; anda non-transitory storage comprising machine-readable instructions for autonomous driving system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the proposed method with real-time human guidance.102Agent104Environment106Output action108State transition116Human guidance															0																		2022-10-09	DIIDW:2022C4105T		
P	ZHANG H; MA J; JIN Y; LIU P; MA H										Multi-agent reinforcement learning based            self-driving vehicle control method, involves using            optimal strategy obtained by multi-agent deep            reinforcement learning decision-making algorithm as            control input of autonomous vehicle					CN116394968-A	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves setting the cooperation and alliance methods of connected automated vehicle (CAVs). The vehicle is driving at different positions on different roads, and the observation area is within the set area. The set area size is let to be l*n, in which l represents the length of the area, and n represents the width of the area. The CAVs multi-agent subsystem is constructed. The multi-agent subsystem area is divided, based on vehicle to vehicle (V2V) communication and field of view threshold. The CAVs multi-agent deep reinforcement learning decision-making algorithm is designed. The CAV decision-making algorithm of multi-agent deep reinforcement learning is an end-to-end decision-making architecture, including an input layer, a neural network layer, an output layer and an environment interaction layer. The optimal strategy obtained by the multi-agent deep reinforcement learning decision-making algorithm is used as the control input of the autonomous vehicle. USE - Multi-agent reinforcement learning based self-driving vehicle control method for artificial intelligence (AI) field of automatic driving. ADVANTAGE - The method enables ensuring safety and efficiency on a high speed trunk lane and ensuring safety of a ramp merging comfort level, so that efficiency is guaranteed in a scene of fully automatic driving vehicle, thus improving quality of observation space and shortening resource utilization rate by using V2V communication technology to define CAV learning area, and hence improving vehicle safety, comfortable and efficient driving. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of CAV decision-making algorithm framework based on the rolling time domain of multi-agent reinforcement learning based self-driving vehicle control method. (Drawing includes non-English language text)															0																		2023-09-01	DIIDW:202374456K		
P	YANG J H; YANG J										Apparatus for controlling driving speed of            vehicle, has camera for acquiring image in front of            vehicle, and sensor acquiring driving speed of            vehicle					US2022111834-A1; CN114291107-A; KR2022046935-A; KR2022052429-A; EP3981660-A1; EP3981660-B1; US11794723-B2; CN114291107-B	HYUNDAI MOBIS CO LTD																									NOVELTY - The apparatus has a camera for acquiring an image in front of a vehicle. A sensor acquires a driving speed of the vehicle. A first neural network calculates a compensation steering angle based on comparing a driving steering angle with a calculated compensation steering angle. A second neural network sets a vehicle speed based on comparison of the compensation steering angles with a threshold value, where the driving angle comprises steering angle information collected when the vehicle is driven, and the calculated steering angle comprises the steering angle information learned by receiving the image and the driving speed. USE - Apparatus for controlling driving speed of a vehicle i.e. automobile during path following control using a conventional camera sensor. Uses include but are not limited to a lane departure warning system (LDWS), a lane keeping assist system (LKAS), a blind side warning (BSD), a smart cruise control (SCC) and an automatic emergency braking system (AEB). ADVANTAGE - The apparatus allows an autonomous vehicle to reach a destination without a driver's manipulation of a steering wheel, an accelerator pedal, and a brake. The apparatus utilizes an advanced driver assistance system (ADAS) that provides convenience and safety to drivers, and predicts a front road environment by using map information, and provides appropriate control and convenience services. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for controlling driving speed of a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an apparatus for controlling driving speed of a vehicle.Vehicle (100)Data storage (101)Sensor module (103)Convolutional Neural Network (105)Reinforcement Learning Neural Network (107)ongitudinal Controller (109)ransverse Controller (111)															0																		2022-05-04	DIIDW:202250741Y		
P	FUJIMURA K; KOCHENDERFER M; NAKHAEI S A; ISELE D F; BOUTON M										Method for reinforcement learning with iterative            reasoning, involves providing level-0 policy and            desired reasoning level, and training level-2 agent            based on level-1 first agent and second agent and            deriving level-two policy					US2021271988-A1	UNIV LELAND STANFORD JUNIOR; HONDA MOTOR CO LTD																									NOVELTY - The method involves providing a level-0 policy and a desired reasoning level n, and populating a training environment with a set of agents. A level-1 agent is trained based on the agents, where the agents follow an intelligent driver model (IDM) for longitudinal maneuvers. The training environment is populated with the agents associated with lane-change and lane-keeping behaviors, respectively. A set of level-2 agents is trained, and a set-2 policy is derived. A state associated with one of the agents includes a longitudinal position, a lateral position, longitudinal velocity, and lateral velocity. USE - Method for facilitating reinforcement learning with iterative reasoning for autonomous vehicles. Uses include but are not limited to cars, trucks, vans, minivans, sports utility vehicle, motorcycles, scooters, boats, personal watercraft, and aircraft. Can also be used in battery electric vehicles (BEV) and plug-in hybrid electric vehicles (PHEV). ADVANTAGE - The method enables training the level-2 agent based on the first agent and the second agent and deriving a level-1 policy, and populating the training environment with the first and second agents associated with a first behavior and a second behavior, so that the method enables providing a reinforcement learning with iterative reasoning in an effective manner. The method allows the autonomous vehicle to achieve the maneuver within a limited time and distance in an efficient manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for: a system for reinforcement learning with iterative reasoning. DESCRIPTION Of DRAWING(S) - The drawing shows a component block diagram of a network architecture associated with a system for reinforcement learning with iterative reasoning.Network architecture (100)Ego features (102)Vehicle features (104)Convolutional network (110)Layer (122)															0																		2022-01-08	DIIDW:2021A1817J		
P	FAN H; KONG Q; XIA Z; LIU C; CHEN Y; ZHU F										Computer-based method for generating motion            planning cost function for autonomous driving vehicle            (ADV), involves selecting highest ranked trajectory to            control ADV autonomously according to highest ranked            trajectory					US2020150671-A1; US11231717-B2	BAIDU USA LLC																									NOVELTY - The method involves collecting information for a driving environment surrounding the autonomous driving vehicle (101) (ADV) using sensors of the ADV. Sample trajectories are generated from a trajectory sample space for the driving environment. A reward is determined based on a reward model for each of the sample trajectories. The reward model is generated using a rank based conditional inverse reinforcement learning algorithm. The sample trajectories are ranked based on the determined rewards. A highest ranked trajectory is determined based on the ranking. The highest ranked trajectory is selected to control the ADV autonomously according to the highest ranked trajectory. USE - Computer-based method for generating motion planning cost function for autonomous driving vehicle (ADV). ADVANTAGE - The collision avoidance system automatically selects the maneuver that is both available and maximizes the safety of occupants of the autonomous vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory machine-readable medium storing program for generating motion planning cost function for autonomous driving vehicle; and(2) a computer-based method to train a rewards model for an autonomous driving vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a networked system.Autonomous driving vehicle (101)Control system (111)Wireless communication system (112)User interface system (113)Sensor system (115)															0																		2020-05-28	DIIDW:202039868T		
P	LEE D										Method for operating reinforcement learning-based            agent using state memory-based artificial neural            network, involves output behavior values into speed            control commands in operating environment and assigns            state memory-based state variables					KR2021063106-A	UNIV KUNSAN NAT IND ACAD COOP FOUND																									NOVELTY - The method involves generating (S10) a state memory-based state variable by fusing multi-sensor data by the sensor fusion model. The artificial neural network reinforcement learning model receives (S20) state memory-based state variables and outputs state memory-based state variables as behavior values for motion control of autonomous vehicles. The artificial neural network reinforcement learning model converts (S30) the output behavior values into speed control commands in the operating environment and assigns state memory-based state variables. USE - Method for operating reinforcement learning-based agent using state memory-based artificial neural network for collision avoidance and autonomous driving of autonomous vehicles. ADVANTAGE - The performance is improved more and more because you train yourself. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for operating a reinforcement learning-based agent using a state memory-based artificial neural network for collision avoidance and autonomous driving of an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method of operating a reinforcement learning-based agent using a state memory-based artificial neural network. (Drawing includes non-English language text)Step for generating a state memory-based state variable by fusing multi-sensor data by the sensor fusion model (S10)Step for receiving artificial neural network reinforcement learning model state memory-based state variables and outputs state memory-based state variables as behavior values for motion control of autonomous vehicles (S20)Step for converting the output behavior values into speed control commands in the operating environment and assigns state memory-based state variables (S30)															0																		2021-07-01	DIIDW:202162279L		
P	ANANDARAM H; RACHAPUDI V; RAJEYYAGARI S; BATCHA R R; RAMALINGAM V; RAGUPATHI T; SARAVANAN D; NATHAN V; RAJU G D; DEWANGAN N										Method for detecting and recognizing real-time            object using adaptive deep learning framework for e.g.            surveillance application, involves integrating            convolutional neural networks, recurrent neural            networks and reinforcement learning mechanisms					IN202321051662-A	ANANDARAM H; RACHAPUDI V; RAJEYYAGARI S; BATCHA R R; RAMALINGAM V; RAGUPATHI T; SARAVANAN D; NATHAN V; RAJU G D; DEWANGAN N																									NOVELTY - The method involves integrating convolutional neural networks, recurrent neural networks and reinforcement learning mechanisms. The parallel processing techniques are used to enable rapid analysis of large data sets for allowing real-time responses in time-critical applications. Deployment in small-scale and large-scale applications ranging from home security to city-wide management systems is enabled. Continuous learning and refinement of internal algorithms is permitted. USE - Method for detecting and recognizing a real-time object using an adaptive deep learning framework. Uses include but are not limited to autonomous vehicle i.e. self-driving car, robotics application, security system, industrial automation application, surveillance application and medical diagnostics application. ADVANTAGE - The method enables providing accurate and immediate object detection and recognition. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a real-time object detection and recognition system. DESCRIPTION Of DRAWING(S) - The drawing shows a functional block diagram of a real-time object detection and recognition system.															0																		2023-10-26	DIIDW:2023A5741D		
P	PHAN T; BADITHELA A										Method for generating trajectory for autonomous            vehicle by updating neural network of motion planner            for detecting building present in vehicle surrounding            environment, involves updating machine learning model            using processor to generate trajectory of vehicle by            applying multiple feature weights					US11640562-B1; WO2023146799-A1	MOTIONAL AD LLC																									NOVELTY - The method involves training a machine learning model to generate a trajectory for a vehicle using a processor, where the training generating a first trained machine learning model having multiple first feature weights. The machine learning model is trained to generate the correct trajectory training using the processor based on a first counterexample for which the first trained machine learning model fails to generate a correct trajectory for the vehicle, where the training generating a second trained machine learning model having multiple second feature weights. Multiple third feature weights are determined using the processor by updating multiple first feature weights based on multiple second feature weights. The machine learning model is updated using the processor to generate the trajectory of the vehicle by applying multiple third feature weights. USE - Method for generating a trajectory for vehicles e.g. an autonomous vehicle (AV) by updating a machine learning model e.g. a neural network (claimed) of a motion planner for detecting objects present in the vehicle surrounding environment. Uses include but are not limited to pedestrians, a cyclist, a structure e.g. a building, traffic signs and a fire hydrant, other vehicles e.g. cars, bicycles, and buses, and curbs. ADVANTAGE - The method enables generating the trajectory to avoid collisions between the vehicle and the objects present in the surrounding environment, and operating the vehicle in accordance with other desirable characteristics such as path length, ride quality, required travel time, observance of traffic rules, and adherence to driving practices. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a system for generating a trajectory for vehicles; (2) a non-transitory storage media storing instructions for generating the trajectory for vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the process of training a machine learning model through inverse reinforcement learning (IRL).															0																		2023-05-14	DIIDW:202345488X		
P	KARR R; LEE R										Storage system, has storage controllers for            delivering message to destination authority of one of            storage controllers responsive to achieving level of            redundancy for redundant copies of metadata regarding            message					US2022019505-A1	PURE STORAGE INC																									NOVELTY - The system (100) has storage controllers that are configured to initiate an action based on redundant copies of metadata, such that a source authority of one of multiple storage controllers receives a message, records the message redundantly throughout multiple storage controllers, and delivers the message to a destination authority of a further one of the storage controllers responsive to achieving a level of redundancy for the redundant copies of the metadata regarding the message. Multiple storage controllers comprises a zoned storage drive. USE - Storage system for artificial intelligence (AI) application used in predictive maintenance in manufacturing and related fields, healthcare applications such as patient data and risk analytics, retail and marketing deployments such as search advertising, social media advertising, supply chains solutions, fintech solutions such as business analytics and reporting tools, operational deployments such as real-time analytics tools, application performance management tools, and information technology infrastructure management tools. Can also be used in deep learning solution, deep reinforcement learning solution, artificial general intelligence solution, autonomous vehicle, cognitive computing solution, commercial unmanned aerial vehicle, conversational user interface, enterprise taxonomy, ontology management solution, machine learning solution, smart dust, smart robot, and smart workplace. ADVANTAGE - The method enables providing enhanced features or take advantage of unique aspects of Flash (RTM: Computer multimedia application) and other solid-state memory in an efficient manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a storage cluster; and(2) a method for storing data. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the first example system for data storage.Storage system (100)Storage array (102A)Storage array controller (110A)Local area network (160)Power distribution bus (172)															0																		2023-08-10	DIIDW:202214533G		
P	KIM K; KIM Y; KIM H; NAM W; BOO S; SUNG M; SHIN D; YEO D; RYU W; LEE M; LEE H; JANG T; JEONG K; JE H; CHO H; KIMKYEHYUN; KIM Y J; KIMHAGKYUNG; NAM W H; BOO S H; SUNG M C; SHIN D S; RYU W J; LEE M C; SOO L H; JANG T U; JEONG K J; JE H M; CHO H J; JIN G; JIN R; JIN H; NAN Y; FU S; CHENG M; SHEN D; LV D; LIU Y; LI M; LI J; ZHANG T; ZHENG J; ZHU H; ZHAO H; KIM I S										Method for an autonomous vehicle involves a            computing device acquiring multiple circumstance image            on surroundings of a subject vehicle, through a            panorama view sensor installed on the subject vehicle            and computing device instructing a CNN					US10726279-B1; EP3690719-A1; US2020250442-A1; KR2020095376-A; CN111507167-A; IN202014002874-A; JP2020126633-A; JP6895694-B2; KR2396272-B1; CN111507167-B	STRADVISION INC; STRAD VISION INC; STRATH VISUAL CO																									NOVELTY - The method involves computing device which acquired multiple circumstance image on surrounded of a subject vehicle, through a panorama view sensor which is installed on the subject vehicle. The computed device which is instructed a Convolutional Neural Network (CNN)(130) to apply at CNN operation to the circumstance image.The initial object information and initial confidence information is generated on the circumstance image. The re-detection process is performed N times in computed device so that N-th adjusted object information is generated. The final object information is generated by referred to the initial object information and K is an integer from 2 to N, and N is the number of the re-detection process to be performed by determination of the Reinforcement Learning (RL) agent. USE - Method for an autonomous vehicle. ADVANTAGE - The performance of the autonomous driving is maintained, computing power is reduced and more accurately object is detected. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing device for achieving better performance. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a computing device.Convolutional Neural Network (130)Fc Layer (132)Region Proposal Network (140)															0																		2020-08-10	DIIDW:202070560V		
P	VAN HOOF H; SCHMITT F; WOEHLKE J G										Method for controlling agent, such as robot is            autonomous vehicle, involves controlling agent in            accordance with control signals derived from evaluation            of control actions output by neural network in response            to input					US2023090127-A1; DE102021210533-A1; CN115857323-A	BOSCH GMBH ROBERT; BOSCH LLC ROBERT																									NOVELTY - The method involves obtaining (301) numerical values of a first set of state variables and a second set of state variables. The numerical values of the first set of state variables together with the numerical values of the second set of variables represent a current full state of the agent. The numerical values of the first set of state variables represent a current partial state of the robot. A state value prior is determined (302) for potential subsequent partial states following the current partial state. An input has a local crop of the state value prior and the numerical values of the second set of state variables representing together is supplied (303) with the numerical values of the first set of state variables. The current full state to a neural network configured to output an evaluation of control actions. The agent is controlled (304) with control signals derived from an evaluation of control actions output by the neural network in response to the input. USE - Method for controlling an agent, such as robot is an autonomous vehicle with legs or tracks or other kind of propulsion system, such as a deep sea or Mars rover. ADVANTAGE - The method enables utilizing a combination of a planning algorithm that guides reinforcement learning to improve data-efficiency. The method allows the agent to learn to perform desired behaviors with respect to a task specification, which controls action to take to reach a goal location in a robotic navigation scenario. The neural network can efficiently train to determine the control action evaluations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a controller configured to control an agent; and(b) a non-transitory computer-readable medium for storing a computer program for controlling an agent. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the method.301Obtaining numerical values of a first and a second set of state variables302Determining a state value prior303Supplying local crop of the state value prior and the numerical values of the first and second set of state variables304Controlling the agent															0																		2023-04-07	DIIDW:202331520K		
P	YOU C										Training method of autonomous driving strategy            using vehicle, involves configuring reinforcement            learning methods in training environment to obtain            optimal strategy for reinforcement learning models            based on state set, action set					CN112406904-A; CN112406904-B; HK40038331-A0; HK40038331-A1	TENCENT TECHNOLOGY SHENZHEN CO LTD																									NOVELTY - The method involves determining (601) the state set of the reinforcement learning model. The state set is set to represent the availability of target environment areas of the autonomous vehicle. The action set of the reinforcement learning model is determined (602). The action set represented the driving action of the autonomous vehicle. The state transition of the reinforcement learning model is determined (603). The reinforcement learning methods in the training environment is configured (605) to obtain the optimal strategy for reinforcement learning models based on the state set, action set, state transition and revenue function. The training environment is set to specify the probability of the obstacle performing each action in the action set. USE - Training method of autonomous driving strategy using vehicle (claimed). ADVANTAGE - The method enables realizing automatic driving strategy training process in a simple manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method of autonomous driving; and(2) a automatic driving device. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating training method of autonomous driving strategy using vehicle. (Drawing includes non-English language text)Step for determining the state set of the reinforcement learning model (601)Step for determining action set of the reinforcement learning model (602)Step for determining state transition of the reinforcement learning model (603)Step for determining the revenue function of the reinforcement learning model (604)Step for configuring reinforcement learning methods in the training environment (605)															0																		2021-03-25	DIIDW:202127493F		
P	INAM R; HATA A; TERRA A I										Method for risk management for autonomous device            using reinforcement learning-based risk management            node, involves initiating sending control parameter to            autonomous device to control action of autonomous            device					WO2021021008-A1; EP4007977-A1; US2022276650-A1; EP4007977-A4	TELEFONAKTIEBOLAGET ERICSSON L M																									NOVELTY - The method involves determining (601) state parameters from a representation of an environment that includes object, an autonomous device, and a set of safety zones for the autonomous device relative to the object. A reward value is determined (603) for the autonomous device based on evaluating a risk of a hazard with the object based on the determined state parameters and current location and current speed of the autonomous device relative to a safety zone from the set of safety zones. A control parameter is determined (605) for controlling action of the autonomous device based on the determined reward value. The sending the control parameter to the autonomous device is initiated (607) to control action of the autonomous device. The control parameter is dynamically adapted to reduce the risk of hazard with the object based on reinforcement learning feedback from the reward value. USE - Method for risk management for autonomous device such as robot and autonomous vehicle using reinforcement learning-based risk management node (claimed). ADVANTAGE - By enabling development of the more robust system can enhance safety of operation of autonomous devices while dynamically adapting to machine learned experiences during operation of the autonomous devices. The risk management node incorporates the current environment around autonomous device and state parameter of autonomous device in the improved manner in contrast to, using the predefined speed for the safety bubble. The risk management node can formulate the state and reward to minimize or reduce the potential risk. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a risk management node; and(2) a computer program product for risk management for autonomous device using reinforcement learning-based risk management node. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating the method for risk management for autonomous device using reinforcement learning-based risk management node.Step for determining state parameters (601)Step for determining reward value (603)Step for determining control parameter (605)Step for initiating the sending the control parameter to the autonomous device (607)															0																		2021-02-22	DIIDW:202113030Q		
P	WANG D; MEENAKSHI SUNDARAM A; KOTHARI R A; ROETTELER M H; KAPOOR A; MEENAKSHI S A										Computing device for training reinforcement            learning model, has processor receiving measurement            results in response to superposition queries from            quantum coprocessor, and updating policy function of            learning model based on results					WO2022164548-A1; US2022253743-A1	MICROSOFT TECHNOLOGY LICENSING LLC																									NOVELTY - The computing device (10) has a processor (12) that is configured to transmit instructions to encode a Markov decision process (MDP) model as a quantum oracle to a quantum coprocessor (20). The processor is configured to train a reinforcement learning model (50). The processor is configured to transmit several superposition queries to the quantum oracle encoded at the quantum coprocessor. The processor is configured to receive the measurement results in response to the superposition queries from the quantum coprocessor. The processor is configured to update a policy function of the reinforcement learning model based on the measurement results. The measurement results include an estimated optimal Q-function, an estimated optimal value function or an estimated optimal policy function for the reinforcement learning model. The quantum coprocessor is configured to compute the estimated optimal Q-function, the estimated optimal value function or the estimated optimal policy function. USE - Computing device such as personal computer, server computer, tablet computer, home-entertainment computer, network computing device, gaming device, mobile computing device, smartphone, smart wristwatch and head mounted augmented reality device for training agents to play board games or online video games with humans or other artificial intelligence (AI). ADVANTAGE - The method effectively trains the reinforcement learning model in the computing device, thus improving the performance of the computing system. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for training agents to play board games or online video games used with computing device. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the computing device when a reinforcement learning model configured to control an autonomous vehicle is generated.Computing device (10)Processor (12)Memory (14)Input device (16)Output device (18)Quantum coprocessor (20)Reinforcement learning model (50)															0																		2023-08-10	DIIDW:2022982118		
P	DOU L; SUN J; MEI D; XU Y										Identification and self-learning collaborative            control method of unmanned vehicle cluster system under            unknown model, used in e.g. civilian field, involves            finding optimal control strategy through policy            iterative learning algorithm to achieve collaborative            tracking control of unmanned vehicle					CN116700257-A	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves establishing a multi-unmanned vehicle system model, in which the multi-unmanned vehicle cluster system consists of N follower unmanned vehicles and a leader unmanned vehicle, communicating through a directed graph topology. The system data collected by each autonomous vehicle is used to identify and reconstruct the system model through reinforcement learning and data-driven methods. A distributed collaborative controller is designed for reinforcement learning and the corresponding cost function, based on the identified and reconstructed system model. The optimal control strategy is found through the policy iterative learning algorithm to achieve the optimal collaborative tracking control of the follower unmanned vehicle on the leader unmanned vehicle, under the designed reinforcement learning algorithm. USE - Identification and self-learning collaborative control method of unmanned vehicle cluster system under unknown model, used in military and civilian fields such as collaborative rescue, collaborative search and rescue, collaborative reconnaissance, and collaborative strike. ADVANTAGE - The method enables designing an optimal cooperative control solution based on enhanced learning, and providing an online self-adaptive learning cooperative control algorithm depending on online state information and input information to learn the optimal control strategy through Lyapunov stability analysis method, thus ensuring that the unmanned vehicle realizes the optimal tracking of the pilotage unmanned vehicle. The method completely releases the requirement of depending on vehicle dynamics in the traditional tracking control protocol, and realizes the efficient cooperative path tracking control. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the identification and self-learning collaborative control method of the unmanned vehicle cluster system under unknown model. (Drawing includes non-English language text).															0																		2023-10-01	DIIDW:202396023V		
P	HWANG S B; LEE K; KUM D; SUK K; LEE K B										Method for changing lane of e.g. bus, by lane            change system in global vehicle industry, involves            outputting path and velocity plan corresponding to            input driving state information of vehicle by using            lane change algorithm					US2023139133-A1; KR2023065861-A; WO2023080393-A1; EP4177124-A1	KOREA ADVANCED SCI & TECHNOLOGY INST																									NOVELTY - The method involves receiving driving state information of a vehicle. A path and a velocity plan corresponding to input driving state information of the vehicle is output by using a lane change algorithm, where the driving state information of the vehicle comprises state information of the vehicle comprising driving information of the vehicle and situation information for recognizing a situation of a surrounding vehicle on basis of the vehicle. The vehicle is controlled by transitioning from a ready stage to an approaching stage to adjust a longitudinal position for a retrieved gap, and bringing the vehicle to approach the retrieved gap in the transitioned approaching stage. USE - Method for changing a lane of an autonomous vehicle i.e. passenger car such as sedan or sports utility vehicle, lorry, and bus, by a lane change system (claimed), in a global vehicle industry. ADVANTAGE - Safe and consistent lane change is performed by dividing lane change into stages based on a finite state machine and setting a transition criterion for characteristics for each step. A lane change success rate is maximized without sacrificing safety of the vehicle because a deep reinforcement learning-based methodology is used when the vehicle performs the lane change. Commercialization of the vehicle is advanced by securing lane change performance of the vehicle in an essential lane change section in addition to a non-essential lane change case. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view illustrating a position and movement of a vehicle when the vehicle performs a lane change.															0																		2023-05-21	DIIDW:202347208G		
P	TREMBLAY J; FOX D; LEE M; FLORENSA C; RATLIFF N D; GARG A; TOZETO R F; RAMOS F T										Method for training robots to perform task,            involves moving robot to perform task under control of            second method that uses information generated by second            perception system as result of determining that robot            is in region					DE102020129425-A1; US2021146531-A1; CN112824061-A	NVIDIA CORP																									NOVELTY - The method involves moving a robot to be within a region under a control of a first method using a physical model based on information from a first perceptual system (106). An uncertainty of the information generated by the first perception system is determined. The determination is made that the robot is in the region based on the uncertainty. the robot is moved to perform a task under the control of a second method that uses information generated by a second perception system (108) as a result of determining that the robot is in the region. USE - Method for training robots to perform task for controlling autonomous vehicle such as passenger vehicle such as car, truck, bus, semi-tractor trailer truck, airplane and robotic vehicle. ADVANTAGE - The strengths of a model-based method (MBM) is combined with the strengths of a model-free method (MFM). The MBM is leveraged to provide efficient movement in an open space environment. The MBM is used to operate a robot in a room where collisions with the environment or people can easily be avoided. The model-based guideline is used toopen to navigate while avoiding obstacles, and an reinforcement learning (RL) algorithm and model-free guideline of architecture are used. The real world learning of a close fitting pen insertion task is achieved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer system;(2) a computer-readable medium storing program for training robots to perform task; and(3) a processor. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the method for training robots to perform task.Hole (104)First perception system (106)Second perception system (108)Uncertainty area (112)Model-free guideline (114)															0																		2021-06-06	DIIDW:202153672J		
P	JIAO X; DENG N; ZHOU W; CAO C; YANG D										Reliable learning-type automatic driving            decision-making method for automatic driving vehicle,            involves selecting decision with high value in both            learning decision and interpretable decision as final            reliable learning decision action					CN113879323-A; CN113879323-B	UNIV TSINGHUA																									NOVELTY - The method involves constructing interpretable decisions based on predetermined decision problems. The learned decision training is guided by the interpretable decision. The learning decision is trained by the decision problem. The learning decision with a high-value decision value function is obtained. The decision with high value in both the learning decision and the interpretable decision is selected as the final reliable learning decision action. USE - Reliable learning-type automatic driving decision-making method for automatic driving vehicle. ADVANTAGE - The method adjusts the value function evaluation process of the enhanced learning decision and makes the decision value function of the final generating strategy higher than a certain interpretive driving strategy so as to realize the reliability guarantee for the learning decision of the autonomous vehicle. The method can fully exert the decision-making ability of reinforcement learning for clear targets in a highly uncertain environment, and guarantee the lower bound of performance through interpretability strategies in order to ensure the high reliability of autonomous vehicles and to realize reliable learning decisions of autonomous vehicles. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a reliable learning-type automatic driving decision-making system; (2) a computer-readable storage medium storing programs for learning-type automatic driving decision-making for automatic driving vehicle; (3) a computing device.															0																		2022-02-18	DIIDW:202211424J		
P	TANG G; CAI Y; ZHANG W; CHENG D										Reinforcement learning test method for autonomous driving, involves obtaining automatic driving reference control data under automatic driving scene failure scene through interactive feedback learning of enhanced learning module and virtual environment					CN114139342-A	WUHAN KOTEI INFORMATICS CO LTD																									NOVELTY - The method involves driving an automatic driving vehicle to perform a failure scene driving test in a specific scene. An automatic driving system of the vehicle is used for collecting environment data and bicycle data. A virtual environment is constructed according to the collected environmental data and the bicycle data to simulate vehicle driving information and the environment information in the real environment. Automatic driving reference control data is obtained under the automatic driving scene failure scene through an interactive feedback learning of an enhanced learning module and the virtual environment. USE - Reinforcement learning test method for autonomous driving. ADVANTAGE - The method: utilizes the real failure scene to construct a virtual environment; inputs the virtual failure scene data into the reinforcement learning framework; can train security policies for specific scene; improves the safety of autonomous vehicles in this accident scene; and achieves the goal of reducing autonomous vehicle accidents. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a reinforcement learning autonomous driving test system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for reinforcement learning test for autonomous driving (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202238899K		
P	SILVER D; QUAN J; SCHAUL T										Method for controlling e.g. robot in simulated            environment to perform reinforcement learning tasks by            computing device, involves causing agent to perform            action specified by processing output of current            observation data					US2023244933-A1	DEEPMIND TECHNOLOGIES LTD																									NOVELTY - The method involves receiving current observation data characterizing current state of an environment (104), and processing the current observation data using a neural network (110) to generate an output that specifies an action to be performed by an agent (102) based on the current observation data, where the neural network is trained through a reinforcement learning module using respective values of an expected learning progress measure for piece of experience data. The agent is caused to perform the action specified by a processing output of the current observation data. USE - Method for controlling an agent e.g. mechanical agent such as robot, autonomous vehicle, or semi-autonomous vehicle, and electronic agent, in an environment e.g. real-world environment and simulated environment (all claimed), to perform reinforcement learning tasks by a computing device. Uses include but are not limited to a mobile phone, a tablet computer, a notebook computer, a music player, an electronic book reader, a laptop or desktop computer, a personal digital assistant (PDA), a smartphone, a game console, a global positioning system (GPS) receiver, and a portable storage device i.e. universal serial bus (USB) flash drive. ADVANTAGE - The method enables improving speed of training the neural network for selecting the actions to be performed by the agents by reducing amount of training data needed to effectively train the neural network, while reducing amount of computing resources necessary for training the neural network. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system for controlling an agent in an environment to perform a task;(2) a non-transitory computer storage medium comprising instructions for controlling an agent in an environment to perform a task. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a reinforcement learning system.102Agent104Environment110Neural network120Training engine130Replay memory															0																		2023-08-21	DIIDW:202381150R		
P	YU X; YANG X; SUN W; XUE X; LI Z; GAO H										Cross-sensor transfer learning based indoor            monocular navigation method for mobile robot e.g.            unmanned aerial vehicle, involves performing navigation            of robot according to heading angle at current moment,            and determining reward function					US2021333793-A1; US11561544-B2	HARBIN INST TECHNOLOGY																									NOVELTY - The method involves acquiring simulation single-line laser radar data of a mobile robot in a simulation model, where the simulation model is built in a Webots open source simulation environment. A laser radar monocular vision navigation model is determined according to a heading angle of the mobile robot at same moment and monocular camera data at corresponding moment and by using a Resnet18 network and a pre-trained YOLO v3 network. Navigation of the robot is performed according to the heading angle at the current moment. Reward function is determined. USE - Cross-sensor transfer learning based indoor monocular navigation method for a mobile robot such as unmanned aerial vehicle (UAV) and autonomous vehicle. ADVANTAGE - The method enables improving accuracy of mobile robot navigation with a monocular camera, acquiring a navigation angle of the mobile robot carrying the monocular cameras through monocular image data and improving navigation accuracy of the robot. The method enables obtaining the stable autonomous navigation model by using a virtual single-line laser radar as a sensor in a simulation environment based on deep deterministic policy gradient (DDPG) reinforcement learning method. The method enables binding real environment data collected by a single line laser radar and a monoclonal camera frame by frame. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a cross-sensor transfer learning based indoor monocular navigation system.															0																		2021-11-25	DIIDW:2021C1921R		
P	PENROD D C; DIKEMAN J M; GLENN J L; GERTISER K M; ROUTH S; LAWS S; RUTH S; GERTESER K M; DICKEMAN J M; PENROD D										Method for capturing die temperature data of            integrated circuit, involves obtaining temperature data            from thermal sensing device associated with die of IC,            and storing temperature data in storage component of            IC					US11293808-B1; EP4113083-A1; CN115541058-A; KR2023004248-A	DELPHI TECHNOLOGIES IP LTD																									NOVELTY - The method involves self-initializing an integrated circuit (IC) (25) by asserting a reset signal for a reset period, in response to an application of power to the IC. The reset signal is deasserted in an expiration of the reset period. Temperature data is obtained from a thermal sensing device associated with a die (27) of the IC, and the temperature data is stored in a storage component of the IC. A built-in self test (BIST) is conducted responsive to deassert the reset signal. A temperature read request is received from a requesting device. USE - Method for capturing die temperature data of an integrated circuit (IC) (claimed) used in vehicle e.g. autonomous or semi-autonomous vehicle, and automotive applications such as engine control units, power driving systems and antilock brake systems. Uses include but are not limited to desktop computers, laptop computers, mobile computing devices, tablet computing device, home appliances, stereos and medical devices. ADVANTAGE - The performance, accuracy and/or longevity of components in an IC depends on operating temperature. The self-heating of the IC die due to operation of self-heat components in the IC results in actual temperatures at locations within the IC being significantly higher than the reference temperature. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an IC. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an IC to receive power from a battery of the vehicle.IC (25)Die (27)Reset logic (RL) (30)Analog-to-digital converter (ADC) (60)Storage component (70)															0																		2022-05-21	DIIDW:202247592H		
P	OSTROVSKI G; DABNEY W C										Method for selecting action i.e. action to control            navigation e.g. steering of e.g. autonomous vehicle,            involves selecting action from set of possible actions            to be performed by agent using measures of central            tendency for actions					WO2019155061-A1; EP3701432-A1; US2020364557-A1; US11610118-B2; US2023196108-A1; US11887000-B2	DEEPMIND TECHNOLOGIES LTD																									NOVELTY - The method (500) involves processing action, current observation, and a probability value using a quantile function network including a set of network parameters, where the quantile function network is a neural network configured to process the action, the current observation, and the probability value in accordance with current values of the network parameters to generate a network output that indicates an estimated quantile value for the probability value. Measures of central tendency of the estimated quantile values generated by the quantile function network are determined (510). Action from a set of possible actions to be performed by an agent is selected (512) in response to the current observation using the measures of central tendency for the actions. USE - Method for selecting an action i.e. action to control navigation e.g. steering, and braking and/or acceleration of a vehicle e.g. autonomous or semi-autonomous land or air or sea vehicle, to be performed by a reinforcement learning agent interacting with an environment using a quantile function network. ADVANTAGE - The method enables training the agent in a quick manner so as to enable a system to consume less computational resources during training of the agent, and allowing the agent to control actions in an environment to increase efficiency by reducing resource usage and environmental impact of operations in the environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a system for selecting an action to be performed by a reinforcement learning agent interacting with an environment(2) a computer readable storage medium for storing a set of instructions for selecting an action to be performed by a reinforcement learning agent interacting with an environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for selecting an action to be performed by a reinforcement learning agent.Method for selecting action to be performed by reinforcement learning agent (500)Step for sampling probability value (504)Step for transforming probability value using distortion function (506)Step for generating estimated quantile values (508)Step for determining measures of central tendency of estimated quantile values generated by quantile function network (510)Step for selecting action from set of possible actions to be performed by agent (512)															0																		2019-09-04	DIIDW:201970850H		
P	SHALEV-SHWARTZ S; SHASHUA A; STEIN G; SHAMMAH S										System for navigating host vehicle, has processing            device for determining navigational action for            execution by host vehicle and causing adjustment of            navigational actuator of host vehicle in response to            determined action for vehicle					WO2019089591-A1; US2019283746-A1; US2020223451-A1; US11167756-B2; US11377102-B2; US2022242407-A1	MOBILEYE VISION TECHNOLOGIES																									NOVELTY - The system has a processing device for receiving a set of images representative of an environment of a host vehicle from a camera. The processing device analyzes the set of images to identify a navigational state associated with the host vehicle and obtains an indicator of an activity of an occupant of the host vehicle from a host vehicle component associated with an interior of the host vehicle. The processing device determines a navigational action for execution by the host vehicle and causes adjustment of a navigational actuator of the host vehicle in response to the determined navigational action for the host vehicle. USE - System for navigating a host vehicle (claimed). ADVANTAGE - The system provides a smoother ride for a user to improve user experiences while allowing for efficient navigation based on activity of occupants. The system allows implementation of parts of policy manually, which can ensure safety of the policy, and implementation of other parts of the policy using reinforcement learning technologies, which can enable adaptivity to many scenarios, a human-like balance between defensive/aggressive behavior, and a human-like negotiation with other drivers. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) an autonomous vehicle(2) a method for navigating a host vehicle(3) a host vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a representative image captured of an environment of a host vehicle, for determining a facing direction of a pedestrian.Scene (2900)Anticipated travel direction (2910)Pedestrian (2920)Region (2930)															0																		2019-06-07	DIIDW:2019404875		
P	ZHANG L; JING Y; CUI T										Method for autonomous driving rule learning based            on deep reinforcement learning, involves learning            driving rules for autonomous driving vehicles to            acquires position and speed of connected vehicles in            road network					CN111222630-A; CN111222630-B	UNIV BEIJING TECHNOLOGY																									NOVELTY - The method involves providing autonomous driving vehicle in vehicle queue vehicle-to-vehicle communication during the driving process. The position and speed of the connected vehicle in the road network are obtained. The autonomous driving vehicle is needed to adopt driving behaviors according to the driving status of the connected vehicle. The defined driving behavior of the autonomous driving vehicle is the acceleration of the vehicle. The speed of the autonomous driving vehicle updates the movement state. The basic goal of autonomous vehicle driving is to dissipate stop-and-stop waves in the road network. The acceleration threshold of autonomous driving vehicles is set to accel threshold. The driving strategy model of autonomous driving vehicle selects multi-layer perceptron (MLP). The driving rules of autonomous driving vehicle are learned. The probability value of driving behavior is outputted through the driving strategy model of autonomous driving vehicle. USE - Autonomous driving rule learning based on deep reinforcement learning. ADVANTAGE - The utilization of deep reinforcement learning improves the autonomous decision-making ability of vehicles. The driver of the non-standard operation and error operation influence of the running safety of the automobile is reduced, which improves the driving safety of a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrates the implementation method for autonomous driving rule learning based on deep reinforcement learning. (Drawing includes non-English language text)															0																		2020-06-22	DIIDW:2020520618		
P	ZADOROJNIY A; MASIN M; MASSING M										Method used for automated explanation of actions            of reinforcement learning in industrial automation,            involves calculating occupation measures for            state-action pairs, based on probabilities, and            receiving selection of action of interest					US2021073674-A1; CN112488307-A	INT BUSINESS MACHINES CORP																									NOVELTY - The method involves operating a hardware processor for automatically identifying features that drive a reinforcement learning model to recommend an action of interest. The features are identified based on occupation measures of state-action pairs associated with the reinforcement learning model. The reinforcement learning model is fitted (202) to generate a policy. The probabilities of the state-action pairs are calculated (204), based on the policy. The occupation measures are calculated (206) for the state-action pairs, based on the probabilities. A selection of the action of interest is received (208). The predefined threshold is a predefined number of state-action pairs which have the highest occupation measures. USE - Method for automated explanation of actions of reinforcement learning used in robotic, industrial automation, autonomous vehicle, automated medical diagnosis and treatment, computer game, algorithmic trading, etc. ADVANTAGE - The action of interest which is selected is facilitated by presenting a list of actions of the particular model to the user, from which the user can conveniently choose. The explanation of actions recommended by a reinforcement learning model is efficiently and accurately provided, by adapting occupation measures to the reinforcement learning domain. Many machine learning algorithms make black-box predictions and decisions, so that users are prevented from learning from the insight covertly gathered by these algorithms. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:1. a system for automated explanation of reinforcement learning actions;and2. a computer program product for automated explanation of reinforcement learning actions. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for automated explanation of reinforcement learning actions.Method for automated explanation of reinforcement learning actions (200)Step for fitting reinforcement learning model (202)Step for calculating probabilities of the state-action pairs (204)Step for calculating occupation measures (206)Step for receiving selection of the action of interest (208)															0																		2021-03-25	DIIDW:202127487J		
P	KREIDIEH A R; ZEIYNALI F Y; OGUCHI K; FARID Y Z										System for facilitating traffic-flow regulation            for i.e. car, through centralized lateral flow control,            has processor for executing machine-readable            instructions to receive aggregated macroscopic traffic            state information from section manager which            communicates with multiple connected vehicles					US2023126328-A1; JP2023062684-A	TOYOTA MOTOR ENG & MFG NORTH AMERICA INC; TOYOTA MOTOR CORP																									NOVELTY - The system has a memory (210) located for storing machine-readable instructions. A processor (205) is located for executing the machine-readable instructions to receive aggregated macroscopic traffic state information from a section manager (110) which communicates with multiple connected vehicles in a section of a roadway, process the aggregated macroscopic traffic state information using a reinforcement-learning-based model to determine target lateral flows for two or multiple lanes of the roadway in the section of the roadway and transmit target lateral flows to the section manager, where the section manager converts the target lateral flows to lane-change actions and transmits the lane-change actions to the connected vehicles, the reinforcement-learning-based model is based on a Markov decision process and the connected vehicles are autonomous vehicles. USE - System for facilitating traffic-flow regulation for an autonomous vehicle or a network-enabled manually driven vehicle i.e. car, through centralized lateral flow control. ADVANTAGE - The system improves safety and efficiency of traffic flow when small percentage of the vehicles on the roadway are connected vehicles by anticipating and repositioning the vehicles based on potential downstream congestion. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a non-transitory computer-readable medium including instructions for facilitating traffic-flow regulation for a vehicle through centralized lateral flow control; and(2) a method for facilitating traffic-flow regulation for a vehicle through centralized lateral flow control. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a system for facilitating traffic-flow regulation for a vehicle through centralized lateral flow control.110Section manager205Processor210Memory215Input module225Output module															0																		2023-05-13	DIIDW:202344540E		
P	ATOUI H; GONZALEZ BAUTISTA D; MAHTOUT I; GERARD D; MILANES M V; GONZALEZ B D; MONTERO MILANES V; MILANES V										Method for aiding driving of e.g. car, on road,            involves generating control response being optimized            with respect to current situation by reinforcement            learning of control response					WO2022128643-A1; FR3117971-A1; EP4263310-A1; FR3117971-B1; US2024034340-A1	RENAULT SAS																									NOVELTY - The method involves receiving a set of data (E1). The data received to determine a current situation of an autonomous motor vehicle is determined (E2). A control response to be provided with respect to the current situation is generated (E3). A command is sent (E4) to control the autonomous motor vehicle, where the command is a function of response to be provided. Another control response being optimized with respect to the current situation is generated (E5) by reinforcement learning of the latter response from control depending on quality information on the result of the command to control the autonomous motor vehicle. USE - Method for aiding driving of an autonomous motor vehicle (claimed) e.g. passenger vehicle such as car, lorry and motorcycle, on a road. Can also be used for a semi-autonomous motor vehicle and a bus. ADVANTAGE - The method enables effectively assisting driving of the autonomous motor vehicle on the road, and improving control of the autonomous vehicle on the road. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) a device for assisting driving of an autonomous motor vehicle on a road;(2) a computer program product comprising a set of instructions for assisting driving of an autonomous motor vehicle on a road; and(3) an autonomous motor vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for aiding driving of an autonomous motor vehicle on a road.Step for receiving set of data (E1)Step for determining data received to determine a current situation of an autonomous motor vehicle (E2)Step for generating control response to be provided with respect to the current situation (E3)Step for sending command to control autonomous motor vehicle (E4)Step for generating another control response being optimized with respect to current situation (E5)															0																		2022-07-27	DIIDW:202281606R		
P	OGUCHI K; ZEIYNALI F Y; KREIDIEH A R										System for coordinated vehicle lane assignment,            has memory for storing machine-readable instructions            executed by the processor to receive target lateral            flows for multiple lanes of roadway in section of            roadway that includes multiple connected vehicles, from            locality manager					US2023131614-A1	TOYOTA MOTOR ENG & MFG NORTH AMERICA INC																									NOVELTY - The system has a processor. A memory is used for storing machine-readable instructions executed by the processor to receive target lateral flows for multiple lanes (150) of a roadway (130) in a section of the roadway that includes multiple connected vehicles, from a locality manager. The target lateral flows are converted to a target number of connected vehicles N. A set of N connected vehicles, whose ranked distances (190) from a following vehicle in a target lane are greatest among the one or more connected vehicles in the section of the roadway, is selected for lane change when a direction of lane change is uniform among the set of N connected vehicles. The lane-change actions are transmitted to the set of N connected vehicles. USE - System for coordinated lane assignment to a vehicle, such as an autonomous vehicle (claimed). ADVANTAGE - By anticipating and repositioning vehicles in response to potential downstream congestion, such systems can greatly improve the safety and efficiency of traffic flow, even when only a small percentage of the vehicles on the roadway are connected vehicles. The coordinated lane-assignment strategies present promising strategies for improving the flow of vehicular traffic to alleviate traffic congestion. The system can be applied to both connected (network-enabled) manually driven vehicles and to connected autonomous vehicles. Setting y to zero results in RL module focusing entirely on improving the performance of the connected vehicles. The same model can then use all sampled information, thus improving the efficiency of the learning procedure. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a non-transitory computer-readable medium for coordinated vehicle lane assignment and storing instructions; and(2) a method for coordinated vehicle lane assignment. DESCRIPTION Of DRAWING(S) - The drawing shows an architecture diagram of a hierarchical traffic-flow regulation system.110But locality manager130Roadway159 Lanes170Legacy non-connected vehicles190Whose ranked distances															0																		2023-05-18	DIIDW:202344551T		
P	SYED A A; BACHET A E										System for operating fleet of vehicles, has            assignment unit that is configured to apply algorithm            to request batch to assign each of user requests to            respective vehicle of fleet					EP3985579-A1	BAYERISCHE MOTOREN WERKE AG																									NOVELTY - The system (100) has a receiving unit (110) for receiving a set of user requests (UR) for transportation of users to respective destinations. A batching decisions controller unit (120) batches the set of UR based on one of an origin and a destination of each user to form a request batch. An assignment unit (130) applies an algorithm to the request batch to assign each of the user requests to a respective vehicle of a fleet of vehicles. The controller unit includes a Reinforcement Learning-trained neural network for batching the UR. The system navigates and controls the vehicles according to determined travel paths. USE - System for operating a fleet of vehicles. ADVANTAGE - The efficient vehicle utilization and vehicle routing can be provided, thus reducing vehicle emissions, such as carbon dioxide emission. The processing times and the use of processing resources can be reduced. The overall complexity of the vehicle routing is reduced. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for operating a fleet of vehicle; and(2) a machine readable medium for operating a fleet of vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system for operating a fleet of vehicle.Autonomous vehicle (10)Method for operating a fleet of vehicle (100)Receiving Unit (110)Batchin decision controller unit (120)Assignment unit (130)															0																		2023-08-10	DIIDW:2022518312		
P	ZHANG D; YU C; LIU Y; ZHANG Q										Reinforcement learning based automatic driving            ship active fault-tolerant path tracking control            method, involves establishing overall control law of            automatic driving ship, so that state of non-linear            dynamic model tracks state of nominal model					CN117289695-A	UNIV SUN YAT-SEN; UNIV SUN YAT-SEN SHENZHEN																									NOVELTY - The method involves establishing a non-linear dynamic model of a three-degree-of-freedom model based on a mechanical module of an autonomous vehicle (ASV). A maneuvering module describes longitudinal, lateral and bow movements of the ASV under multiple external forces and torques from a propeller and a rudder. A nominal model is obtained based on the nonlinear dynamics model. An overall control law of an ASV is established such that a state of the non-linear dynamic model can track the state of a nominal model. A basic path tracking control law for ensuring basic tracking performance is determined. An error-tolerant robust control law is determined based on intensive learning for compensating system uncertainty and sensor faults. The sensor fault uses FDI detection and estimates size. USE - Reinforcement learning based automatic driving ship active fault-tolerant path tracking control method used for performing dangerous, complex and expensive maritime tasks e.g. global shipping, environmental monitoring and resource exploration. ADVANTAGE - The method enables reducing the dependence of the fault-tolerant control design on the model information by using the non-model characteristic of the intensified learning, improving the learning efficiency, and reducing the dependence on the sensor fault estimation precision. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a reinforcement learning based automatic driving ship active fault-tolerant path tracking control method. (Drawing includes non-English language text).															0																		2024-01-19	DIIDW:202402844R		
P	NEELAKANTAM N; SCHULTZ G										Method for orchestrating virtual storage system,            involves resuming cloud-based storage system            periodically and based on recovery objectives,            including refreshing copy of dataset that is maintained            by storage system					US2022035714-A1	PURE STORAGE INC																									NOVELTY - The method involves receiving recovery objectives associated with a dataset that is stored in a primary storage system. A cloud-based storage system (604) is created. The cloud-based storage system is suspended. The cloud based storage system is periodically resumed based on the recovery objectives. A copy of the dataset is refreshed that is maintained by the cloud based system. Hosts in the primary storage system are configured to utilize the cloud-based storage system upon a failure of the primary storage system. A performance level associated with components of the cloud-based storage system is decreased. USE - Method for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system for various application. Uses include but are not limited to deep learning solutions, deep reinforcement learning solutions, artificial general intelligence solution, autonomous vehicle, cognitive computing solution, commercial unmanned aerial vehicle (UAV) or drone, conversational user interface, enterprise taxonomy, ontology management solution, machine learning solution, smart robot and smart workplace. ADVANTAGE - The method enables utilizing cloud computing as a model of service delivery for enabling convenient on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. The method enables allowing cloud computing environment to offer infrastructure, platforms and/or software as services for which a cloud consumer not need to maintain resources on a local computing device. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) an apparatus for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system;(2) a computer program product comprising a set of instructions for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustarting a method for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system.Cloud computing environment (602)cloud-based storage system (604)Receiving request to write data to cloud-based storage system (606)Deduplicating data (608)Compressing data (610)															0																		2022-02-14	DIIDW:202218378C		
P	PONULAK F										Method for performing credit assignment for            artificial spiking network for e.g. autonomous vehicle,            involves determining credit based on relating network            output, and adjusting learning parameter associated            with unit based on credit					US2014025613-A1	PONULAK F																									NOVELTY - The method involves operating a spiking neuron network (106) comprising a spiking neuron (106-1) in accordance with reinforcement learning process capable of generating a network/unit output (112). A credit is determined based on relating the network output to contribution of a unit of a set of units. A learning parameter (130) associated with the unit is adjusted based on the credit, where the contribution of the unit is determined based on eligibility associated with the unit. The credit is determined for the unit based on a unit input (102), the unit output and a unit state. USE - Method for performing credit assignment for an artificial spiking network for a robotic apparatus (claimed) e.g. medical robot apparatus such as surgical robot apparatus and rover robot apparatus, autonomous vehicle, unmanned air vehicle, underwater vehicle, smart appliance such as ROOMBA (RTM: autonomous robotic vacuum cleaner), and robotic toys, used for painting or welding and for control applications e.g. HVAC applications and electromechanical devices applications, and games applications. Can also be used for machine vision applications, pattern detection applications, pattern recognition applications, object classification applications, signal filtering applications, data segmentation applications, data compression applications, data mining applications, optimization and scheduling applications and complex mapping applications. ADVANTAGE - The method enables providing implementation of reinforcement learning for large populations of neurons in an efficient manner. The method enables providing faster and more precise learning, thus reducing operational costs associated with operating learning networks due to a shorter amount of time that can be required to arrive at stable solution. The method enables controlling faster processes and learning precision performance in a reliable manner. DETAILED DESCRIPTION - The learning parameter is synaptic weight. INDEPENDENT CLAIMS are also included for the following:(1) a computer-implemented method for operating a set of data interfaces in a computerized network(2) a method for operating a neural network(3) a method for enhancing the learning performance of a neural network(4) a computerized robotic system(5) a robotic apparatus for performing accelerated learning performance. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an adaptive controller comprising a spiking neuron network operable in accordance with reinforcement learning process.Unit input (102)Spiking neuron network (106)Spiking neuron (106-1)Unit/network output (112)Learning parameter (130)															0																		2014-01-03	DIIDW:2014B89692		
P	HEE L W; KIM E; LEE J H										Robot logistics game device for providing            programming education for artificial intelligence-based            machine learning and coding to induce students to            actively participate in class, has administrator device            providing user environment for inputting result					KR2421655-B1	KYONGSANGNAM DO OFFICE EDUCATION																									NOVELTY - The robot logistics game device comprises a game board that is provided with a central area and a border area surrounding the center area. The border area is divided into four areas. The former area displays a color, where the latter area displays another color. The third area displays third color. An obstructing device is configured to move along a circular path. A first control device is provided for controlling a first game device. A second control device controls a second game device. An output unit outputs a manipulation program and a manipulation model building program. A data storage unit stores a steering model. An input unit inputs a corresponding user's command. An interface unit is connected with a corresponding game device, where an output unit is configured for outputting the manipulation program. The first sub-block of the former color and the third sub block of the third color are displayed together with a quick response (QR) code. USE - Robot logistics game device for providing programming education for artificial intelligence-based machine learning and coding to induce students to actively participate in class. ADVANTAGE - The device provides programming education for coding easily and efficiently through the robot logistics game so as to induce students to actively participate in a class, so that the students can have fun with AI, and understanding of underlying machine learning and coding principles can be supported. The device gradually applies reinforcement learning by correcting errors occurred in process of repeating certain actions to solve a problem, and supports students in learning concept of an autonomous vehicle lane recognition system. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of robot logistics game device for providing programming education for artificial intelligence-based machine learning.Game device (10)Game board (100)Block group (110)Moving unit (120)Control device (130)															0																		2022-08-05	DIIDW:202293801B		
P	CHAO M; KOSE C N; ROSALES R; KOSE CIHANGIR N										Device for triggering vehicular action of vehicle,            has processor that identifies features in subsets of            image data and determines state of occupants based on            tracked changes from states and trigger vehicular            action based on determined state					US2020242381-A1; EP3885976-A1; CN113511217-A; US11568655-B2	INTEL CORP																									NOVELTY - The device has a processor that is configured to identify features in subsets of image data detailing the occupants. The processor is configured to track changes over time of features over subsets of image data. The processor is configured to determine a state of the occupants based on the tracked changes from states. The processor is configured to trigger the vehicular action based on the determined state. The segments corresponds to a respective subset of subsets of image data, and each segment comprises a value corresponding to each of features. USE - Device for triggering vehicular action of vehicle such as automobile, aquatic vehicle, sub-aquatic vehicle and autonomous vehicle, for use in driving safety system. ADVANTAGE - The reinforcement learning models include positive or negative feedback to improve accuracy. A safety driving model or include a mathematical model for safety assurance that enables identification and performance of proper responses to dangerous situations such that self-perpetrated accidents are avoided. Since the order and magnitude of change of a motion are very critical in differentiating specific actions, the mechanisms and schemes provided are provide high accuracy and efficiency in identifying a state of a driver and passengers and using this information for a safety driving model. The device helps to capture the temporal information between selected frames using values of feature and to understand a direction of a motion together with the magnitude of change. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method to trigger vehicular action based on monitoring occupants of vehicle; and(2) a non-transitory computer readable media storing program for trigger vehicular action based on monitoring occupants of vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of driving safety system.Camera (702)Spatio-temporal system (704)Estimator (706)Driving model (708)Safety response (710)															0																		2020-08-14	DIIDW:202071156C		
P	HUANG Y; YUAN K; YANG S; WANG L; CHEN H										Scene self-adaptive decision planning method for            autonomous driving vehicle, involves outputting            steering wheel angle and accelerator opening to            actuator to realize function of autonomous            driving					CN115195784-A	UNIV TONGJI																									NOVELTY - The method involves obtaining dynamic traffic information of an autonomous vehicle to be planned. The dynamic traffic information is input into pre-trained decision-end output layer network to obtain a lane change decision. The dynamic traffic information is input into the pre-trained output layer network of planning end, and adjustment results of model predictive control (MPC) parameters used for trajectory planning of a vehicle are obtained under current working conditions. The lane change decision and MPC parameter adjustment results are input into a pre-trained horizontal and vertical progressive trajectory planning network, and a steering wheel angle and accelerator opening are output to an actuator to realize a function of autonomous driving. USE - Scene self-adaptive decision planning method for autonomous driving vehicle. ADVANTAGE - The method enables utilizing a reinforcement learning framework to rationally and fully use dynamic traffic information provided by a sensing system through construction of reinforcement learning, so that the reinforcement learning neural network can be trained in an efficient manner. The method allows a decision-making end output layer network in a training process at current time of the vehicle state information and the dynamic information as a state, lane or lane keeping as action, thus improving safety and comfort of the automatic driving automobile, driving efficiency as a reward, and driving efficiency. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an electronic device for using scene self-adaptive decision planning device for autonomous driving vehicle; and(2) a non-transitory machine-readable storage medium storing program for using scene self-adaptive decision planning device for autonomous driving vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an algorithm architecture. (Drawing includes non-English language text)															0																		2023-08-10	DIIDW:2022D31951		
P	GHOSH S; LINCOLN P D; RAMAMURTHY B S										Method for using machine learning to create model            that improves operation of controller of e.g. laptop,            involves providing model as trusted model that improves            operation of system controller by enabling controller            to perform system action					US2017364831-A1; WO2017223192-A1; DE112017002604-T5; JP2019526107-W; US10902349-B2	SRI INT																									NOVELTY - The method involves modifying a model (108) or input data (104) or a reward function (110) of the model, and re-training the model using one of the model, the input data or the reward function when a determination is made that the model does not satisfy a trust-related constraint (114). The model is provided as a trusted model that improves operation of a computer system controller (150) by enabling the computer system controller to perform a system action (152) within predetermined guarantees when the determination is made that the model satisfies the trust-related constraint. USE - Method for using machine learning to create a trusted model that improves operation of a controller of an autonomous computer system e.g. personal computer such as desktop, laptop, tablet, smart phone and wearable or body-mounted device, server and enterprise computer system, for life-critical or mission-critical applications. Uses include but are not limited to self-driving cars, cyber security applications, surgical robotics and aircrafts. ADVANTAGE - The method enables improving reliability of a computer system operation through the use of machine learning that provides guarantees of operation through the trusted model. The method allows the trusted model provided to a controller by a machine learning system to make the controller more trustworthy, thus making the system more reliable. The method enables facilitating maximum likelihood inverse reinforcement learning process by using a maximum likelihood estimation to a problem of inverse reinforcement learning to identify unknown reward functions from traces of initial input data. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a machine learning system for creating a trusted model that improves operation of a computer system controller(2) a non-transitory computer readable medium comprising a set of instructions for using machine learning to create a trusted model that improves operation of an autonomous vehicle controller. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an environment of a computing system including components for a machine learning system that uses a trusted model.Input data (104)Model (108)Reward function (110)Trust-related constraint (114)Computer system controller (150)System action (152)															0																		2018-01-05	DIIDW:2017873614		
P	XIA H; REN D; BAI Y; ZHANG T; JIA Q; LI K										Strategy transfer method for reinforcement            learning, involves executing next iteration process            based on updated first prediction model and updated            first policy information until sample data output by            first prediction model matches sample data at target            time T'					CN117390946-A	UNIV TSINGHUA; BEIJING SANKUAI ONLINE TECHNOLOGY CO LTD																									NOVELTY - The method involves obtaining (201) the first sample data at multiple simulation moments, based on the simulation scenario of the autonomous driving scenario. The first strategy information and the first prediction model are determined (202), based on multiple sets of first sample data. The following steps are iteratively executed (203) to train the first prediction model and the first policy information, based on the first policy information, the first prediction model and the automatic driving scenario. The second sample data at the target time T' in the autonomous driving scenario is obtained, in any iterative process, based on the first strategy information of this iterative process. The next iteration process is executed based on the updated first prediction model and the updated first policy information until the sample data output by the first prediction model matches the sample data at the target time T'. USE - Strategy transfer method for reinforcement learning applied in scenarios such as autonomous driving and intelligent workshop scheduling to conduct strategic planning in scenarios based on reinforcement learning. ADVANTAGE - The action to be performed by the autonomous vehicle is accurately predicted in the autonomous driving scenario, so that better results are achieved after executing the action, by migrating the prediction model and strategy information in the simulation scenario to the autonomous driving scenario through this method. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following: (1) a computer device; and (2) a computer readable storage medium storing program for strategy transfer for reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the strategy transfer method for reinforcement learning. (Drawing includes non-English language text)201Step for obtaining the first sample data at multiple simulation moments202Step for determining the first strategy information and the first prediction model203Step for training the first prediction model and the first policy information															0																		2024-02-10	DIIDW:202408759Y		
P	PALANISAMY P; MUDALIGE U P; CHEN Y; DOLAN J M; MUELLING K; PALANIZAMY P; MUDARIGUE U P; MILLING K										Method for learning lane-change policies through            actor-critic network architecture, involves generating            spatial context vector at spatial attention module, and            processing combined context vector to generate            hierarchical actions					US2020139973-A1; DE102019115707-A1; CN111137292-A; US10940863-B2; CN111137292-B	GM GLOBAL TECHNOLOGY OPERATIONS INC; UNIV CARNEGIE MELLON																									NOVELTY - The method involves processing image data received from an environment to learn the lane-change policies as a set of hierarchical actions through an actor network (110) over time. The action values are predicted through an action value function at a critic network (120). The learned importance weights are applied to each of the relevant regions of the image data to add importance to the relevant regions of the image data at the spatial attention module. A spatial context vector is generated at the spatial attention module (140). The spatial context vector is processed to learn temporal attention weights to be applied to past frames of image data to indicate relative importance in deciding which lane-change policy to select at a temporal attention module (160) of the actor network. A combined context vector is generated at the temporal attention module. The combined context vector is processed to generate the set of hierarchical actions through fully connected layer. USE - Method for learning lane-change policies through actor-critic network architecture. ADVANTAGE - The two streams of temporal attention and spatial attention boost the performance in deep reinforcement learning (DRL). The hierarchical action structure of sub-policies for lane changing behavior allows a vehicle to perform safe and efficient lane-changes which is a crucial feature for delivering fully autonomous vehicles on road. The total number of parameters in the network is reduced for more efficient training and testing. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an actor-critic network system; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows the block diagram of a DRL system.DRL system (100)Actor network (110)Critic network (120)Spatial attention module (140)Temporal attention module (160)															0																		2020-05-14	DIIDW:202037465B		
P	CHENTANEZ N; MUELLER-FISCHER M; MACKLIN M; MAKOVIICHUK V; JESCHKE S										Method for imitating reference object from motion            capture (MOCAP) video clip using physics simulator and            neural network (NN), involves adjusting movement of            target object using stability threshold and movement            agent					US2021082170-A1; US11113861-B2	NVIDIA CORP																									NOVELTY - The method involves tracking a reference object (306) using a movement agent of a target object (305). A movement of the target object is adjusted using a stability threshold and the movement agent. The movement agent utilizes an output from the NN to provide modifications to the movement. The output is indicative of joint torques, joint positionings, applied forces, or proportional derivative (PD) controller gain parameters. The output from the NN is generated utilizing simulation environment data, motion interference parameters, or target object information. The NN is one of a tracking NN or a recovery NN. USE - Method for imitating reference object from MOCAP video clip using physics simulator and NN. ADVANTAGE - The method can use the deep reinforcement learning technique combined with the physics-based MOCAP methods to allow the simulator to generate the higher quality video portion where the movements of the object or character appear smooth and natural. The motion correctors can be used to help determine how the human can react and move in various situations so that the autonomous vehicle can select the action that could minimize the damage and harm to the human. The recovery agent can be utilized to provide the NN parameters to bring the target object back to the position that satisfies the stability threshold using information from NN. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a video generator system for imitating reference object from MOCAP video clip using physics simulator and NN; and(2) a computer program product for imitating reference object from MOCAP video clip using physics simulator and NN. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of the target object movement agent process.Agent process (300)Target object (305)Reference object (306)Tracking samples (310a-310e,320a-320e,330a-330e)															0																		2023-08-10	DIIDW:202128084N		
P	SEO S; KIM S; YOON H; KIM C										Uncertainty conditional deep reinforcement            learning device, has uncertainty estimation unit            estimating epistemological uncertainty and feature            vector, and behavior estimation unit estimating            behavior for reward based on feature vector					KR2590791-B1	KOREA DEFENSE ACQUISITION PROGRAM ADMINI																									NOVELTY - The device has a feature extraction unit (100) that extracts a state as input for a feature vector. An uncertainty estimation unit (200) estimates epistemological uncertainty and the feature vector based on epistemic uncertainty and action. A behavior estimation unit (300) estimates behavior for a reward based on the state and the feature vector. An encoder outputs the state of the behavior. The uncertainty estimation unit utilizes a Monte-Carlo dropout method to determine the state, applies Monte-Carlo dropout to the encoder, and utilizes the variance of Monte Carlo samples as an uncertainty indicator. USE - Uncertainty conditional deep reinforcement learning device for an autonomous vehicle i.e. autonomous car. ADVANTAGE - The device checks whether the input state is well trained in the deep reinforcement learning model and determines appropriate action by considering uncertainty of the input condition. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a uncertainty conditional deep reinforcement learning method; anda computer-readable recording medium comprising a set of instructions for realizing uncertainty conditional deep reinforcement learning process. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a uncertainty conditional deep reinforcement learning device (Drawing includes non-English language text).100Feature extraction unit200Uncertainty estimation unit300Behavior estimation unit															0																		2023-11-17	DIIDW:2023B2447Q		
P	SCHAEFER M										Method for reinforcement learning for carrying out            control and/or regulation task of entity such as            mechanical or electrical component, involves changing            strategy of strategy module by correction factor            determined by divergence					DE102022108396-A1	PORSCHE AG F																									NOVELTY - The method involves determining modeled values for the properties and parameters of an entity (10) from a learning reinforcement module (400) to generate a second probability distribution for the respective properties and parameters. The first calculation results for the first probability distribution are generated by using mathematical functions and/or statistical methods. The second calculation results for the second probability distribution are generated by using mathematical functions and/or statistical methods. A divergence between the first calculation results and the second calculation results is determined. The strategy of a strategy module (470) is changed by a correction factor (550) determined by the divergence. USE - Method for reinforcement learning for carrying out control and/or regulation task of entity such as mechanical, electrical, electronic, mechatronic or hydraulic component in many areas of technology such as mechanical engineering and automotive technology. Uses include but are not limited to entity designed as larger unit including further components and assemblies such as motor vehicle with internal combustion engine or electric motor, autonomous vehicle, agricultural vehicle such as combine harvester, robot for use in production or services, watercraft or flying object such as airplane or drone, system or assembly such as braking system, drive train, electric motor or internal combustion engine for motor vehicle, or energy generation system such as photovoltaic system or a wind turbine, or a production system for producing a product, such as production facility for manufacture of motor vehicle, scientific analysis instrument, household appliance or medical device for diagnosing and supporting bodily functions, transportation system, supply chain system, stock market, energy supply system, and computer simulation, computer game, augmented reality, or another virtual object, and etc. ADVANTAGE - The possibilities for improving reinforcement learning in order to be able to carry out control and regulation tasks with high level of reliability, security, accuracy and efficient use of computing capacity is created. The system is able to carry out control and regulation tasks based on reinforcement learning more optimally and thus to reduce costs and the consumption of resources. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system for reinforcement learning for carrying out control and/or regulation tasks of an entity; and(2) computer program product for reinforcement learning for carrying out control and/or regulation tasks of an entity. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram illustrating the system for reinforcement learning for carrying out control and/or regulation task of entity.10Entity100Reinforcement learning system400Learning reinforcement module470Strategy module550Correction factor															0																		2023-10-26	DIIDW:2023A6843X		
P	SATTI J R; GREWAL A S; SHAHRIARI M										Method for implementing acceleration and            deceleration requests in behaviour-based adaptive            cruise control (ACC), involves processing set of            results with driver behaviour data determined by            reinforcement learning to correlate control actions            with driver behaviour data					DE102021132197-A1; CN114763139-A; US2022219695-A1; US11834042-B2	GM GLOBAL TECHNOLOGY OPERATIONS INC																									NOVELTY - The method involves executing adaptive cruise control to obtain a set of vehicle inputs about the operating environment and current operations of a host vehicle (10). A target vehicle operating in the vicinity of the host vehicle is identified. A set of target vehicle parameters about the target vehicle derived from sensed inputs is quantified. A state estimate of the host vehicle and the target vehicle is modelled by generating a set of velocity calculations and torque calculations about the vehicle. A set of results is generated from the reward function based on one or more modelled state estimates of the host vehicle and the target vehicle. The set of results are processed with driver behaviour data determined by the reinforcement learning to correlate one or more control actions with the driver behaviour data. USE - Method for implementing acceleration and deceleration requests in behaviour-based adaptive cruise control (ACC) using gain learning (RL) based on driving style of driver of vehicle such as motorcycle, truck, sport utility vehicle, recreational vehicle, watercraft, and airplane. ADVANTAGE - The behaviour of the driver can be assessed and target vehicle behavior can be detected to train intelligent model for adaptive cruise control function that correlates to a driver's driving style when operating a vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for implementing adaptive cruise control. DESCRIPTION Of DRAWING(S) - The drawing shows a functional block diagram of the autonomous or semi-autonomous vehicle with control system that controls vehicle actions based on the use of neural network for driver behaviour in vehicle control system.Vehicle (10)Transmission system (22)Braking system (26)Actuator system (30)Vehicle control system (100)															0																		2022-08-20	DIIDW:202289995H		
P	SUN Y; KING S; MA O										System for controlling interaction of apparatus            with e.g. self-closing door, for facilitating            navigation of e.g. machines, to perform wide range of            tasks in e.g. commercial settings, has door control            apparatus arranged on vehicle and provided with            controller, and control arm including camera					US2022388180-A1	UNIV CINCINNATI																									NOVELTY - The system has a door control apparatus (100) arranged on a vehicle (102) and provided with a controller. A frame includes a top frame portion and a bottom frame portion. A door control arm is integrated as a portion of the frame and moved vertically between the top frame portion and the bottom frame portion. A door control component includes a door handle contact protrusion with a curve-shaped end portion. The door handle contact protrusion extends perpendicularly relative to the frame. A door interaction arm is arranged on a surface of the frame, and extends perpendicularly relative to the frame. The door control arm includes a camera. The frame is formed by aluminum material. USE - System for controlling interaction of an apparatus with a door e.g. self-closing door, for facilitating navigation of a vehicle e.g. robot, autonomous vehicle and machines, to perform wide range of tasks in residential and commercial settings. Uses include but are not limited to manufacturing and machining complex components for automobiles, computers, surveilling surroundings, cleaning floors, inspecting areas of interest and delivering items from cleaning rooms. ADVANTAGE - The door control apparatus is mounted on the vehicle to autonomously unlock and open the self-closing door so as to effectively facilitate navigation of the vehicle. The controller of the door control apparatus can utilize the door control arm and sub-components of the door control arm so as to unlatch the door and pull or push the door to configure the door in a slightly open state and utilize one of two door-holding bars to hold the door to open for enabling navigation the vehicle through the door in an efficient and autonomous manner. The system utilizes a camera to transfer images to the controller so as to analyze the images and/or live video stream and facilitate movement of the vehicle and the door control apparatus toward the door. The controller can be trained on a reinforcement learning-based model to determine a set of actions that enable the door control apparatus to efficiently and effectively recharge a power supply component. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of a door control apparatus mounted on a vehicle.100Door control apparatus102Vehicle															0																		2023-08-10	DIIDW:2022F1871U		
P	LIN W; LIU L; SUN X; MA K; XUAN Z; ZHAO Y										Vehicle speed control system for autonomous            vehicles, has vehicle speed control module that            determines if proposed vehicle speed control command            conforms to desired human driving behaviors by use of            human driving model module					US2019072960-A1; WO2019051506-A1; US10656644-B2; CN111183073-A; CN111183073-B	TUSIMPLE; TUSIMPLE INC																									NOVELTY - The system (201) has a data processor (171) and a vehicle speed control module (200). The vehicle speed control module is configured to perform a vehicle speed control command validation operation for an autonomous vehicle. The vehicle speed control command validation operation generates data corresponding to desired human driving behaviors, trains a human driving model module (175) using a reinforcement learning process and the desired human driving behaviors. The vehicle speed control module receives a proposed vehicle speed control command (210), determines if the proposed vehicle speed control command conforms to the desired human driving behaviors by use of the human driving model module, and validates or modifies the proposed vehicle speed control command based on the determination. USE - Vehicle speed control system for autonomous vehicles. ADVANTAGE - The system uses the real-time generated trajectory and vehicle motion control command signal to safely and efficiently navigate the vehicle through a real world driving environment, while avoiding obstacles and safely controlling the vehicle. The speed control module and the human driving model module serve to enable the modeling and modification of the vehicle speed control command for the vehicle based on a comparison of a proposed vehicle speed control command with corresponding normal human driving behavior data maintained by the human driving model module. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for managing speed control of autonomous vehicles; and(2) a non-transitory machine-useable storage medium that comprises instructions for performing a method for managing speed control of autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows the block diagram of the vehicle speed control system.Data processor (171)Human driving model module (175)Vehicle speed control module (200)Vehicle speed control system (201)Proposed vehicle speed control command (210)															0																		2019-04-26	DIIDW:201921864K		
P	MILTON S										Non-transitory machine-readable medium for            performing vehicle data analytics on            remotely-controlled vehicles, semi-autonomous vehicles            and fully autonomous vehicle, involves providing second            feedback indicator to operator based on second            value					US2021312725-A1; WO2021202602-A1; CA3174392-A1; EP4126619-A1	MOOVE AI																									NOVELTY - The medium have set of instructions for providing a first feedback feedback indicator to a vehicle operator based on a first action value by using a vehicle. Multiple sensor measurements is performed, while the first feedback indicator is provided to the vehicle operator. Multiple sensor measurements is associated with the vehicle by using multiple vehicle sensors or multiple roadside sensors. A reward value is determined based on the target vehicle state and multiple sensor measurements. A second action value is determined based on reward value based on policy by using a deep reinforcement learning system, and the policy comprises mapping of a possible state to perform possible action. A second feedback indicator is provided to the operator based on second value. USE - Non-transitory machine-readable medium for performing vehicle data analytics on remotely-controlled vehicles, semi-autonomous vehicles and fully autonomous vehicles. ADVANTAGE - The medium have set of instructions for computationally performing operations on devices of a vehicle by using a computing layer, thus quickly performing searching process to a subset of available tiles corresponding to the data model through the entire map, and calculating significant amount of time or computational resources, so that noise can be injected during process, thus improving accuracy of training operation, and hence increasing security and privacy of data during FL training operation. The medium have set of instructions for creating application to perform neural network operations across multiple parameters, thus allowing deeper understanding of the latent space of various previously disparate records or data such as vehicle sensor data, operator profile data, road network graph data, and online profile data. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a first computing environment.First vehicle (102)Vehicle sensor (104)Wireless network interface (105)Onboard computing device (106)Vehicle agent (108)Anomalous object (110)Base station (120)Local computer data center (122)Local computing agent (128)															0																		2022-01-08	DIIDW:2021B50006		
P	SCHMITT F; VAN HOOF H; WOEHLKE J G; WOLCK J G; SCHMIDT F; WOEHLKE H V; VAN H H										Method for controlling a robot, involves receiving            an indication of a target configuration to be reached            from an initial configuration of the robot, and            determining a coarse-scale value map by value            iteration					EP3904973-A1; CN113671942-A; US2021341904-A1; EP3904973-A4	BOSCH GMBH ROBERT																									NOVELTY - The method (300) involves receiving (301) an indication of a target configuration to be reached from an initial configuration of the robot. The coarse-scale value map is determined (302) by value iteration. The transition probabilities are determined using a transition probability model mapping coarse-scale states and coarse-scale actions to transition probabilities for coarse-scale states. The fine-scale sub-goal from the coarse-scale value map is determined. The fine-scale control actions are performed (305) to reach the determined fine-scale sub-goal by an actuator of the robot. The sensor data is obtained to determine the fine-scale states reached as a result of performing the fine-scale control actions for each fine-scale state of the resulting sequence of fine-scale states of the robot. The next coarse-scale state of the sequence of coarse-scale states is determined (306) from the last fine-scale state of the sequence of fine-scale states. USE - Method for controlling robot such as autonomous vehicle. ADVANTAGE - The method allows controlling a robot by training with less observation data since only a transition probability model needs to be learned for the high-level planner as opposed to learning the value iteration procedure itself end-to-end with the low-level continuous control policy via reinforcement learning, which is a more difficult learning task. The method is computationally more efficient than approaches learning a value iteration model (such as HiDe) since no backpropagation of gradients through the planner is necessary. The data-efficiency of RL algorithms is high in case the provided reward signal is very sparse. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a robot controller;(2) a computer program for controlling robot; and(3) a computer-readable medium storing program for controlling robot. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart of a method for controlling robot.Method for controlling robot (300)Step for receiving indication of a target configuration to be reached from an initial configuration of the robot (301)Step for determining a coarse-scale value map by value iteration (302)Step for performing fine-scale control actions to reach the determined fine-scale sub-goal by an actuator of the robot (305)Step for determining the next coarse-scale state of the sequence of coarse-scale states from the last fine-scale state of the sequence of fine-scale states (306)															0																		2021-12-19	DIIDW:2021D9534L		
J	Liu, Hao; Kiumarsi, Bahare; Kartal, Yusuf; Taha Koru, Ahmet; Modares, Hamidreza; Lewis, Frank L.				Wang, He/JCO-3900-2023; liu, hao/HTN-9816-2023; xu, li/GNH-3667-2022; kartal, yusuf/GVU-4564-2022; Yu, ZH/KBC-6889-2024; Zhou, heng/JCN-6493-2023; Zhang, Jinfan/JPK-7588-2023; zhang, yimeng/JLL-7337-2023; wang, wenjuan/JGD-0428-2023; zhang, xueying/JMB-7808-2023; Jiang, Yuan/JED-3759-2023; wu, meng/JPK-1930-2023; Liu, Ying/ISU-1216-2023	wang, wenjuan/0000-0002-4220-8817; Koru, Ahmet Taha/0000-0001-8191-2324					Reinforcement Learning Applications in Unmanned Vehicle Control: A Comprehensive Overview								UNMANNED SYSTEMS				11	01			17	26				10.1142/S2301385023310027					AUG 2022		Article; Early Access		2023	This paper briefly reviews the dynamics and the control architectures of unmanned vehicles; reinforcement learning (RL) in optimal control theory; and RL-based applications in unmanned vehicles. Nonlinearities and uncertainties in the dynamics of unmanned vehicles (e.g. aerial, underwater, and tailsitter vehicles) pose critical challenges to their control systems. Solving Hamilton-Jacobi-Bellman (HJB) equations to find optimal controllers becomes difficult in the presence of nonlinearities, uncertainties, and actuator faults. Therefore, RL-based approaches are widely used in unmanned vehicle systems to solve the HJB equations. To this end, they learn the optimal solutions by using online data measured along the system trajectories. This approach is very practical in partially or completely model-free optimal control design and optimal fault-tolerant control design for unmanned vehicle systems.									7	0	0	0	0	0	7			2301-3850	2301-3869										Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R ChinaMichigan State Univ, E Lansing, MI 48823 USAUniv Texas Arlington, Res Inst, Ft Worth, TX 76118 USA				2022-09-09	WOS:000848727300001		
C	Zhang, Kai; Wang, Guile; Hu, Jinwen; Xu, Zhao; Guo, Chubing						Peng, C; Sun, J				Path Planning Technology of Unmanned Vehicle Based on Improved Deep Reinforcement Learning								2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)		Chinese Control Conference						8392	8397											Proceedings Paper	2021	2021	As the basic problem of unmanned vehicle navigation control, path planning has been widely studied. Reinforcement learning (RL) has been found an effective way of path optimization for the highly nonlinear and unmodeled dynamics. However, the RL based methods suffer from the "dimension disaster" under the high-dimension state spaces. In this paper, the path planning of an unmanned vehicle with collision avoidance is considered, and an improved Deep Q-Network (DQN) algorithm is proposed to reduce the computation load in the high-dimension state space. First, the states, actions and rewards are determined based on the task requirement, and a smoothing function is defined as an additional penalty term to modify the basic reward function. Then, the two-dimension grid of the state space is mapped to a gray image, which is applied as the input of a neural network, i.e., the Q-Network. Finally, simulation results show that the modified DQN algorithm is more stable and the fluctuation frequency is significantly reduced.					40th Chinese Control Conference (CCC)40th Chinese Control Conference (CCC)	JUL 26-28, 2021JUL 26-28, 2021	TCCT; CAA; Syst Engn Soc China; Shanghai Univ; Chinese Acad Sci, Acad Math & Syst Sci; China Soc Ind & Appl Math; Asian Control Assoc; IEEE Control Syst Soc; Inst Control, Robot & Syst; Soc Instrument & Control Engineers; Shanghai Jiao Tong Univ; Shanghai Assoc AutomatTCCT; CAA; Syst Engn Soc China; Shanghai Univ; Chinese Acad Sci, Acad Math & Syst Sci; China Soc Ind & Appl Math; Asian Control Assoc; IEEE Control Syst Soc; Inst Control, Robot & Syst; Soc Instrument & Control Engineers; Shanghai Jiao Tong Univ; Shanghai Assoc Automat	Shanghai, PEOPLES R CHINAShanghai, PEOPLES R CHINA	0	0	0	0	0	0	0			2161-2927		978-988-15638-0-4									CETC Key Lab Data Link Technol, Xian 710068, Peoples R ChinaNorthwestern Polytech Univ, Sch Automat, Xian 710129, Peoples R ChinaNorthwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China				2021-01-01	WOS:000931046708089		
C	Marvi, Zahra; Kiumarsi, Bahare			IEEE							Safe Off-policy Reinforcement Learning Using Barrier Functions								2020 AMERICAN CONTROL CONFERENCE (ACC)		Proceedings of the American Control Conference						2176	2181				10.23919/acc45564.2020.9147584							Proceedings Paper	2020	2020	This paper presents a safe off-policy reinforcement learning (RL) scheme to design optimal controllers for systems with uncertain dynamics. The utility function for which its optimization achieves a desired behavior is augmented with a control barrier function (CBF) candidate providing a platform for merging safety planning and optimal control design. A damping factor is included in the CBF providing a design tool to specify the relative importance of performance and safety. As one of the main contributions of this paper, it is shown that by iterative approximation of the value function, the safety properties of CBF are certified which bridges the broad capability of barrier functions into the learning-based approaches. Then, the safety of control system is proved accordingly. Stability and optimality of the control system in a safe condition are verified. Afterward, an off-policy RL algorithm is used to obtain the safe and optimal controller without requiring full knowledge about the system dynamics. The efficiency of the proposed method is demonstrated on the lane changing as an automotive control problem.					American Control Conference (ACC)American Control Conference (ACC)	JUL 01-03, 2020JUL 01-03, 2020	Amer Automat Control Council; Int Federat Automat ControlAmer Automat Control Council; Int Federat Automat Control	Denver, CODenver, CO	12	0	0	0	0	0	12			0743-1619	2378-5861	978-1-5386-8266-1									Michigan State Univ, Dept Elect & Comp Engn, 428 S Shaw Lane,Engn Bldg, E Lansing, MI 48824 USA				2021-03-02	WOS:000618079802023		
J	Ma, Chengdong; Liu, Jianan; He, Saichao; Hong, Wenjing; Shi, Jia				Hong, Wenjing/C-7258-2011	Hong, Wenjing/0000-0003-4080-6175; Liu, Jianan/0009-0002-7179-0129					Confrontation and Obstacle-Avoidance of Unmanned Vehicles Based on Progressive Reinforcement Learning								IEEE ACCESS				11				50398	50411				10.1109/ACCESS.2023.3278597							Article	2023	2023	The core technique of unmanned vehicle systems is the autonomous maneuvering decision, which not only determines the applications of unmanned vehicles but also is the critical technique many countries are competing to develop. Reinforcement Learning (RL) is the potential design method for autonomous maneuvering decision-making systems. Nevertheless, in the face of complex decision-making tasks, it is still challenging to master the optimal policy due to the low learning efficiency caused by the complex environment, high dimensional state, and sparse reward. Inspired by the human learning process from simple to complex, we propose a novel progressive deep RL algorithm for policy optimization in unmanned autonomous decision-making systems in this paper. The proposed algorithm divides the training of the autonomous maneuvering decision into a sequence of curricula with learning tasks from simple to complex. Finally, through the self-play stage, the iterative optimization of the policy is realized. Furthermore, the confrontation environment with two unmanned vehicles with obstacles is analyzed and modeled. Finally, the simulation leads to the one-to-one adversarial tasks demonstrate the effectiveness and applicability of the proposed design algorithm.									1	0	0	0	0	0	1			2169-3536											Xiamen Univ, Coll Chem & Chem Engn, Dept Chem & Biochem Engn, Xiamen 361005, Peoples R ChinaXiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China				2023-06-23	WOS:001005637500001		
C	Mason, Federico; Drago, Matteo; Zugno, Tommaso; Giordani, Marco; Boban, Mate; Zorzi, Michele			IEEE	Giordani, Marco/T-4215-2019; Zorzi, Michele/JZT-7485-2024; Zorzi, Michele/GQQ-2252-2022; Boban, Mate/JKH-9873-2023; Drago, Matteo/ABB-2438-2020	Giordani, Marco/0000-0002-0575-1781; Drago, Matteo/0000-0001-5321-0309; Mason, Federico/0000-0001-5681-1695; Boban, Mate/0000-0003-0668-7627					A Reinforcement Learning Framework for PQoS in a Teleoperated Driving Scenario								2022 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC)		IEEE Wireless Communications and Networking Conference						114	119				10.1109/WCNC51071.2022.9771590							Proceedings Paper	2022	2022	In recent years, autonomous networks have been designed with Predictive Quality of Service (PQoS) in mind, as a means for applications operating in the industrial and/or automotive sectors to predict unanticipated Quality of Service (QoS) changes and react accordingly. In this context, Reinforcement Learning (RL) has come out as a promising approach to perform accurate predictions, and optimize the efficiency and adaptability of wireless networks. Along these lines, in this paper we propose the design of a new entity, integrated at the RAN level that implements PQoS functionalities with the support of an RL framework. Specifically, we focus on the design of the reward function of the learning agent, able to convert QoS estimates into appropriate countermeasures if QoS requirements are not satisfied. We demonstrate via ns-3 simulations that our approach achieves better results in terms of QoS and Quality of Experience (QoE) performance of end users in a teleoperated driving scenario.					IEEE Wireless Communications and Networking Conference (IEEE WCNC)IEEE Wireless Communications and Networking Conference (IEEE WCNC)	APR 10-13, 2022APR 10-13, 2022	IEEEIEEE	Austin, TXAustin, TX	5	0	0	0	0	0	5			1525-3511		978-1-6654-4266-4									Univ Padua, Dept Informat Engn, Padua, ItalyMunich Res Ctr, Huawei Technol, Munich, Germany				2022-08-04	WOS:000819473100020		
J	Xu Guoyan; Zong Xiaopeng; Yu Guizhen; Su Hongjie							徐国艳; 宗孝鹏; 余贵珍; 苏鸿杰			A Research on Intelligent Obstacle Avoidance of Unmanned Vehicle Based on DDPG Algorithm			基于DDPG的无人车智能避障方法研究				汽车工程	Automotive Engineering				41	2			206	212	1000-680X(2019)41:2<206:JYDDWR>2.0.TX;2-E										Article	2019	2019	An intelligent obstacle avoidance scheme for unmanned vehicle based on reinforcement learning is proposed in this paper. In view of that the movement of unmanned vehicle must meet both interior and exterior constraints,including vehicle dynamics constraints and traffic rule constraints and its output must be continuous, which the traditional reinforcement learning cannot assure,an improved deep deterministic policy gradient algorithm is proposed to tackle continuous motion space issue and achieve the continuous output of steering wheel angle and acceleration. Multi-source sensor data fusion is adopted to fulfill the state input of unmanned vehicle obstacle avoidance algorithm and both interior and exterior constraints are added to make output motion more reasonable and effective. Finally a simulation is conducted on the open-source simulation platform TORCS and the effectiveness and robustness of the algorithm verified.			本文中提出一种基于强化学习的无人车智能避障方法。鉴于无人车运动必须满足内外约束,包括汽车动力学约束和交通规则约束,且动作输出必须连续,而传统强化学习无法应对连续动作空间问题,提出了一种改进的DDPG算法,解决连续动作空间问题,实现转向盘转角和加速度的连续输出;采取多源传感器数据融合,满足无人车避障算法的状态输入;增加车辆内外约束条件,使输出动作更合理有效。最后,在开源仿真平台TORCS进行仿真,验证了算法的有效性和鲁棒性。						3	10	0	0	0	0	13			1000-680X											北京航空航天大学交通科学与工程学院, 北京 100191, 中国School of Transportation Science and Engineering, Beihang University, Beijing 100191, China	北京航空航天大学交通科学与工程学院School of Transportation Science and Engineering, Beihang University			2019-06-04	CSCD:6455721		
J	Mukherjee, Sayak; Perez-Rapela, Daniel; Forman, Jason L.; Panzer, Matthew B.										Generating Human Arm Kinematics Using Reinforcement Learning to Train Active Muscle Behavior in Automotive Research								JOURNAL OF BIOMECHANICAL ENGINEERING-TRANSACTIONS OF THE ASME				144	12					121008			10.1115/1.4055680							Article	DEC 1 2022	2022	Computational human body models (HBMs) are important tools for predicting human biomechanical responses under automotive crash environments. In many scenarios, the prediction of the occupant response will be improved by incorporating active muscle control into the HBMs to generate biofidelic kinematics during different vehicle maneuvers. In this study, we have proposed an approach to develop an active muscle controller based on reinforcement learning (RL). The RL muscle activation control (RL-MAC) approach is a shift from using traditional closed-loop feedback controllers, which can mimic accurate active muscle behavior under a limited range of loading conditions for which the controller has been tuned. Conversely, the RL-MAC uses an iterative training approach to generate active muscle forces for desired joint motion and is analogous to how a child develops gross motor skills. In this study, the ability of a deep deterministic policy gradient (DDPG) RL controller to generate accurate human kinematics is demonstrated using a multibody model of the human arm. The arm model was trained to perform goal-directed elbow rotation by activating the responsible muscles and investigated using two recruitment schemes: as independent muscles or as antagonistic muscle groups. Simulations with the trained controller show that the arm can move to the target position in the presence or absence of externally applied loads. The RL-MAC trained under constant external loads was able to maintain the desired elbow joint angle under a simplified automotive impact scenario, implying the robustness of the motor control approach.									0	0	0	0	0	0	0			0148-0731	1528-8951										Univ Virginia, Ctr Appl Biomech, 4040 Lewis & Clark Dr, Charlottesville, VA 22911 USA				2022-11-18	WOS:000878264100002	36128755	
J	Hu, B.; Chen, J.; Lin, Y.; Tan, S.										Vehicle Following Hybrid Control Algorithm Based on DRL and PID in Intelligent Network Environment								SAE Technical Papers								2022	01-7113				10.4271/2022-01-7113							Conference Proceedings	2022	2022	Deep reinforcement learning (DRL) has not been widely used in the engineering field yet because RL needs to be learned through 'trial and error', which makes the application of this kind of algorithm in real physical environment more difficult, and it is impossible to carry out 'trial and error' learning on real vehicles. By analyzing the motion state of the vehicle in the car following mode, the algorithm that combined traditional longitudinal motion control with DRL improves the safety of RL in the real physical environment and the poor adaptability of the traditional longitudinal motion control algorithm. In this paper, the longitudinal motion of the unmanned vehicle is taken as the research object, and the PID algorithm is combined with the Deep Deterministic Policy Gradient (DDPG) algorithm to control the longitudinal motion of the unmanned vehicle. The research results show that the longitudinal motion control hybrid algorithm performed better than the single PID algorithm or DDPG algorithm in the vehicle following control. This strategy establishes the relationship between the vehicle longitudinal control and the state of both the ego vehicle and the front vehicle, while also considers the randomness of the front vehicle motion in the iterative learning process, which improves the safety, comfort and following performance.					SAE 2022 Intelligent and Connected Vehicles SymposiumSAE 2022 Intelligent and Connected Vehicles Symposium	20222022		Shanghai, ChinaShanghai, China	0	0	0	0	0	0	0			2688-3627											Chongqing Univ. of Technol., Chongqing, China				2023-03-17	INSPEC:22695677		
J	Evdokimova, T.S.; Sinodkin, A.A.; Tiurikov, M.I.; Fedosova, L.O.; Kalyashina, A.V.										Developing an unmanned vehicle local trajectory using a reinforcement learning algorithm								IOP Conference Series: Materials Science and Engineering				1086				012015 (6 pp.)	012015 (6 pp.)				10.1088/1757-899X/1086/1/012015							Conference Paper; Journal Paper	2021	2021	This article describes the algorithm development for constructing a local trajectory for an unmanned vehicle or for implementation in an ADAS system using the reinforcement learning method. A special part is dedicated to reinforcement learning. One of the methods that is best suitable for the task conditions will also be implemented. This method will allow bypassing obstacles and reaching the specified short target points.					5th International Scientific and Practical Seminar Mobility of Transport and Technological Vehicles (MTTV 2020)5th International Scientific and Practical Seminar Mobility of Transport and Technological Vehicles (MTTV 2020)	12-13 Nov. 202012-13 Nov. 2020		Nizhny Novgorod, RussiaNizhny Novgorod, Russia	0	0	0	0	0	0	0			1757-8981											Nizhny Novgorod State Tech. Univ. n.a. R.E. Alekseev, Nizhny Novgorod, RussiaDept. of Laser Technol., KNRTU KAI, UK				2021-05-14	INSPEC:20549428		
C	Zhai, Weitong; Wang, Xiangrong; Greco, Maria S.; Gini, Fulvio			IEEE	GRECO, MARIA/JFB-5740-2023						Reinforcement Learning based Integrated Sensing and Communication for Automotive MIMO Radar								2023 IEEE RADAR CONFERENCE, RADARCONF23													10.1109/RADARCONF2351548.2023.10149653							Proceedings Paper	2023	2023	Integrated sensing and communication (ISAC) is a promising technique in vehicular transportation thanks to its substantial gains in size, cost, power consumption, electromagnetic compatibility and spectrum congestion. In this paper, we propose a reinforcement learning (RL) based ISAC system with a multi-input-multi-output (MIMO) automotive radar. The target sensing and downlink communication are separately performed by dividing the transmit antennas into two non-overlapping but interweaving subarrays. We first design a RL framework to adaptively allocate the proper number of transmit antennas for the two subarrays under any unknown environment. The training is performed in the metrics of Cramer-Rao Bound (CRB) of direction of arrival (DOA) estimation for sensing and receive signal-to-noise (SNR) for communications, respectively. We proceed to propose a co-design method to jointly optimize the configurations of the two subarrays to further enhance the sensing accuracy with a constrained communication quality. The resultant problem is converted into the convex form via convex relaxation. Simulations are provided to demonstrate the adaptability and effectiveness of the proposed RL based ISAC system under the unkown environment.					IEEE Radar Conference (RadarConf)IEEE Radar Conference (RadarConf)	MAY 01-05, 2023MAY 01-05, 2023	IEEE; Aerosp & Elect Syst Soc; IEEE Lone Star Sect; Massachusetts Inst Technol, Lincoln Lab; Lockheed Martin; Continental Elect; Georgia Tech Res Inst; Rohde & Schwarz; NL; Northrop Grumman; Samtec; Comtech; SW Res Inst; ARC Technol Solut; NSI MI; AMETEK; ISL; Univ Oklahoma, Adv Radar Res Ctr; Rantec Microwave Syst; Boeing; Anduril; Cent Intelligence Agcy; OPHIR; Berkeley Nucleon Corp; Leonardo DRS; CAES; Interface Concept; Empower RF Syst Inc; TechWay; TTM Technologies; Adv Test Equipment Corp; MathWorks; MetronIEEE; Aerosp & Elect Syst Soc; IEEE Lone Star Sect; Massachusetts Inst Technol, Lincoln Lab; Lockheed Martin; Continental Elect; Georgia Tech Res Inst; Rohde & Schwarz; NL; Northrop Grumman; Samtec; Comtech; SW Res Inst; ARC Technol Solut; NSI MI; AMETEK; ISL; Univ Oklahoma, Adv Radar Res Ctr; Rantec Microwave Syst; Boeing; Anduril; Cent Intelligence Agcy; OPHIR; Berkeley Nucleon Corp; Leonardo DRS; CAES; Interface Concept; Empower RF Syst Inc; TechWay; TTM Technologies; Adv Test Equipment Corp; MathWorks; Metron	San Antonio, TXSan Antonio, TX	1	0	0	0	0	0	1					978-1-6654-3669-4									Beihang Univ, Sch Elect & Informat Engn, Beijing, Peoples R ChinaUniv Pisa, Dept Informat Engn, Pisa, Italy				2023-08-18	WOS:001031599600113		
J	Egan, Daniel; Zhu, Qilun; Prucka, Robert					Zhu, Qilun/0000-0003-2995-2470; Egan, Daniel/0000-0002-4584-6427					A Review of Reinforcement Learning-Based Powertrain Controllers: Effects of Agent Selection for Mixed-Continuity Control and Reward Formulation								ENERGIES				16	8					3450			10.3390/en16083450							Review	APR 2023	2023	One major cost of improving the automotive fuel economy while simultaneously reducing tailpipe emissions is increased powertrain complexity. This complexity has consequently increased the resources (both time and money) needed to develop such powertrains. Powertrain performance is heavily influenced by the quality of the controller/calibration. Since traditional control development processes are becoming resource-intensive, better alternate methods are worth pursuing. Recently, reinforcement learning (RL), a machine learning technique, has proven capable of creating optimal controllers for complex systems. The model-free nature of RL has the potential to streamline the control development process, possibly reducing the time and money required. This article reviews the impact of choices in two areas on the performance of RL-based powertrain controllers to provide a better awareness of their benefits and consequences. First, we examine how RL algorithm action continuities and control-actuator continuities are matched, via native operation or conversion. Secondly, we discuss the formulation of the reward function. RL is able to optimize control policies defined by a wide spectrum of reward functions, including some functions that are difficult to implement with other techniques. RL action and control-actuator continuity matching affects the ability of the RL-based controller to understand and operate the powertrain while the reward function defines optimal behavior. Finally, opportunities for future RL-based powertrain control development are identified and discussed.									0	0	0	0	0	0	0				1996-1073										Clemson Univ, Dept Automot Engn, Clemson, SC 29634 USA				2023-05-19	WOS:000977041700001		
P	XU G; ZONG X; YU G										Reinforcement learning-based obstacle-avoiding            system for unmanned vehicle, has executing part for            realizing obstacle-avoidant function by rotating            unmanned vehicle through calculated steering wheel            angle					CN107065890-A; CN107065890-B	UNIV BEIHANG																									NOVELTY - The system has a system main body provided with a sensing part, a decision part, a controlling part and an implementing part. The sensing part detects obstacle area using single laser radar. Determination is made to check an obstacle for an unmanned vehicle, if unmanned vehicle requires control, the controlling part controls state information input through reinforcement learning model, where the model is obtained through trial and error to converge to steady state. The executing part realizes obstacle-avoidant function by rotating unmanned vehicle through calculated steering wheel angle. USE - Reinforcement learning-based obstacle-avoiding system for unmanned vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a reinforcement learning-based obstacle-avoiding method for unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a reinforcement learning-based obstacle-avoiding method for unmanned vehicle. '(Drawing includes non-English language text)'															0																		2017-12-29	DIIDW:201758048M		
B	Zhai, W.; Wang, X.; Greco, M.S.; Gini, F.										Reinforcement Learning based Integrated Sensing and Communication for Automotive MIMO Radar								2023 IEEE Radar Conference (RadarConf23)								1	6				10.1109/RadarConf2351548.2023.10149653							Conference Paper	2023	2023	Integrated sensing and communication (ISAC) is a promising technique in vehicular transportation thanks to its substantial gains in size, cost, power consumption, electromag-netic compatibility and spectrum congestion. In this paper, we propose a reinforcement learning (RL) based ISAC system with a multi-input-multi-output (MIMO) automotive radar. The target sensing and downlink communication are separately performed by dividing the transmit antennas into two non-overlapping but interweaving subarrays. We first design a RL framework to adaptively allocate the proper number of transmit antennas for the two subarrays under any unknown environment. The training is performed in the metrics of Cramer-Rao Bound (CRB) of direction of arrival (DOA) estimation for sensing and receive signal-to-noise (SNR) for communications, respectively. We proceed to propose a co-design method to jointly optimize the configurations of the two subarrays to further enhance the sensing accuracy with a constrained communication quality. The resultant problem is converted into the convex form via convex relaxation. Simulations are provided to demonstrate the adaptability and effectiveness of the proposed RL based ISAC system under the unkown environment.					2023 IEEE Radar Conference (RadarConf23)2023 IEEE Radar Conference (RadarConf23)	1-5 May 20231-5 May 2023	IEEE; AESS; Lincoln Laboratory, Massachusetts Institute of Technology; LOCKHEED MARTIN; Continental Electronics; Georgia Tech. Research Institute; NI; ROHDE & SCHWARZ; NORTHROP GRUMAN; samtec; COMTECH; Southwest Research Institute; ARC Technology Solutions; NSI MI AMETEK; The University of Oklahoma; RANTEC; ISL; ANDURIL; OPHIR; CENTRAL INTELLIGENCE AGENCY; BOEING; CONSTELLI; LInC; CAES; AEC; TTM Technologies; IC; Quantic ELECTRONICS; BNC; LEONARDO DRS; TECHWAY; MathWorks; EMPOWER RF SYSTEMS, INC.; METRONIEEE; AESS; Lincoln Laboratory, Massachusetts Institute of Technology; LOCKHEED MARTIN; Continental Electronics; Georgia Tech. Research Institute; NI; ROHDE & SCHWARZ; NORTHROP GRUMAN; samtec; COMTECH; Southwest Research Institute; ARC Technology Solutions; NSI MI AMETEK; The University of Oklahoma; RANTEC; ISL; ANDURIL; OPHIR; CENTRAL INTELLIGENCE AGENCY; BOEING; CONSTELLI; LInC; CAES; AEC; TTM Technologies; IC; Quantic ELECTRONICS; BNC; LEONARDO DRS; TECHWAY; MathWorks; EMPOWER RF SYSTEMS, INC.; METRON	San Antonio, TX, USASan Antonio, TX, USA	0	0	0	0	0	0	0					978-1-6654-3669-4									Sch. of Electron. & Inf. Eng., Beihang Univ., Beijing, ChinaDept. of Inf. Eng., Univ. of Pisa, Pisa, Italy				2023-08-19	INSPEC:23320020		
J	Jeong, Min S.; Jang, Jin H.; Lee, Eun S.					-, jangjinhyeog | gonghagdaehag jeonjagonghagbu | janghagjogyo | han-yangdae(ERICA)/0009-0007-7199-3694					Optimal IPT Core Design for Wireless Electric Vehicles by Reinforcement Learning								IEEE TRANSACTIONS ON POWER ELECTRONICS				38	11			13262	13272				10.1109/TPEL.2023.3297740							Article	NOV 2023	2023	In this article, optimal inductive power transfer (IPT) core structures for wireless electric vehicle (WEV), which can be derived by optimal reinforcement learning (RL) algorithms, are newly proposed. Because the IPT cannot be theoretically analyzed to find a maximum value of mutual inductance for the optimal core structure design, intuitive and iterative process based on finite element method analysis are usually implemented. This conventional method, however, is not preferred due to numerous possible combinations and computation times. For this reason, RL algorithms are designed to optimize nonlinear system design, enabling the WEV IPT to be efficiently designed with high mutual inductance, even in the presence of severe misalignment conditions. Contrary to the conventional RL algorithm for the IPT core design, the proposed RL algorithm can follow higher mutual inductance by shorter episodes; hence, 50% of computation time reduction and 2% of maximum mutual inductance were achieved. A prototype of WEV IPT system designed by the proposed RL algorithm was fabricated, satisfying the standard J2954 of the society of automotive engineers for WPT3/Z3 case. As a result, it is found that the proposed WEV IPT can be manufactured, considering the desired number of cores for reasonable cost and weight of the vehicle assembly.									0	0	0	0	0	0	0			0885-8993	1941-0107										Hanyang Univ, Sch Elect Engn, ERICA Campus, Ansan 15588, South Korea				2024-01-13	WOS:001105213800002		
P	JIANG H; CHEN C; MA B; LOU M; LU J										Unmanned vehicle reinforcement learning training            environment construction method involves disconnecting            simulation environment model from reinforcement            learning module when simulation environment model is            closed					CN111795700-A	UNIV ZHEJIANG																									NOVELTY - The method involves using the real on-board camera of the real unmanned vehicle to collect real scene pictures as the real domain data set. A CycleGAN network is established, the real domain data set and the enhanced simulation domain data set are used as the input of the two generators in the CycleGAN network respectively. The API interface is established between the simulation environment model and the reinforcement learning module. The simulation environment model is connected to the reinforcement learning module, and the trained CycleGAN model is loaded when the simulation environment is loaded. The real simulation environment picture is collected as the input of the CycleGAN model, the simulated real scene picture is obtained, and the simulated real scene picture is used as the input of the reinforcement learning module for intensive training. When the simulation environment model is closed, the simulation environment model is disconnected from the reinforcement learning module. USE - Unmanned vehicle reinforcement learning training environment construction method. ADVANTAGE - The unmanned vehicle camera collects real scene pictures in reality. Since the simulated real scene pictures input by the reinforcement learning algorithm during training are very similar to the real scene pictures, the trained algorithm can be directly transferred or transferred to the real scene after fine-tuning. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an unmanned vehicle reinforcement learning training system based on the construction method. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an unmanned vehicle reinforcement learning training environment construction method. (Drawing includes non-English language text)															0																		2020-11-05	DIIDW:2020A4320D		
P	YU X; FENG P; TIAN Y; LUO J										Simulation verification system for            virtual-real-combined reinforcement learning algorithm            in robot or multi-robot system or providing from            sensing to control end to end strategy, has simulation            unmanned vehicle/machine replaced by mirror image            unmanned vehicle/machine					CN116108627-A	UNIV BEIHANG																									NOVELTY - The system has a simulation platform provided with a simulation unmanned vehicle/machine as an intelligent body and a mirror image unmanned machine. An object platform is provided with an action capturing system, an obstacle mirror image, an object unmanned vehicle/machine and a related communication device. A data range is processed to be consistent with a data format and range obtained from a real object. The simulation unmanned vehicle/machine is replaced by a mirror image unmanned vehicle/machine. A mirror image unmanned vehicle/machine operation data to obtain a virtual reality algorithm transplantation effect. USE - Simulation verification system for virtual-real-combined reinforcement learning algorithm in robot or multi-robot system or providing from sensing to control end to end strategy. ADVANTAGE - The simulation verification method of virtual-real combined reinforcement learning algorithm fully verifies the feasibility and reliability of the algorithm in the actual complex environment, and has low cost. The virtual combination of the reinforcement learning for simulating verification method, strong real-time performance, experiment fidelity and high reliability. The simulation environment for training reinforcement learning butt joint with the actual intelligent vehicle physical model, real time mapping of real vehicle in the simulation environment, the state observation in simulation environment is consistent with actual Intelligent vehicle, finally transferring simulation environment-based training-based reinforcement learning strategy to actual intelligent vehicles, finishing the complex task. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the simulation verification system for virtual-real-combined reinforcement learning algorithm.															0																		2023-08-10	DIIDW:2023541590		
P	LIU H; LIU C; HAN L; ZHAN Z; BEI W										Reinforcement learning based unmanned vehicle            obstacle avoiding method, involves utilizing penalty            functions and evaluation functions to score multiple            groups of trajectories and evaluate obstacle avoidance            performance of trajectories					CN112906542-A; CN112906542-B	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves training a dynamic window obstacle avoidance model using a reinforcement learning algorithm under pre-obtained environmental constraints to obtain a prediction window. The environment constraints are utilized to indicate shape and size of each obstacle in the area in front of an unmanned vehicle. Distance between each obstacle and the unmanned vehicle is determined. A kinematics model is constructed. Judgment is made to check whether the prediction window is a dynamic window after the position is adjusted according to the environmental constraints. A set of trajectories is generated for each set of sampling speeds based on the prediction window and the speed sampling constraints. Multiple sets of speeds are sampled. The speed sampling constraint is pre-built based on the driving state. USE - Reinforcement learning based unmanned vehicle obstacle avoiding method. ADVANTAGE - The method enables improving obstacle-avoiding performance of the unmanned vehicle and realizing active obstacle avoidance of the unmanned vehicle in an effective manner. DETAILED DESCRIPTION - Pre-built obstacle avoidance penalty functions and evaluation functions are utilized to score multiple groups of the trajectories and evaluate obstacle avoidance performance of each group of the trajectories. A trajectory with the highest score is selected from each group of the trajectories as the target trajectory. Sampling speed corresponding to the target trajectory is outputted to a control system of the unmanned vehicle. The unmanned vehicle corresponding to the target trajectory is driven according to the sampling speed. INDEPENDENT CLAIMS are also included for the following:(1) a reinforcement learning based unmanned vehicle obstacle avoiding device; and(2) a computer-readable storage medium comprises a set of instructions for avoiding unmanned vehicle obstacle based on reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a reinforcement learning based unmanned vehicle obstacle avoiding method. (Drawing includes non-English language text).															0																		2021-07-15	DIIDW:202167952M		
C	Pathak, Shashank; Bag, Suvam; Nadkarni, Vijay						Strand, M; Dillmann, R; Menegatti, E; Ghidoni, S				A Generalised Method for Adaptive Longitudinal Control Using Reinforcement Learning								INTELLIGENT AUTONOMOUS SYSTEMS 15, IAS-15		Advances in Intelligent Systems and Computing		867				464	479				10.1007/978-3-030-01370-7_37							Proceedings Paper	2019	2019	Adaptive cruise control (ACC) seeks intelligent and adaptive methods for longitudinal control of the cars. Since more than a decade, high-end cars have been equipped with ACC typically through carefully designed model-based controllers. Unlike the traditional ACC, we propose a reinforcement learning based approach - RL-ACC. We present the RL-ACC and its experimental results from the automotive-grade car simulators. Thus, we obtain a controller which requires minimal domain knowledge, is intuitive in its design, can accommodate uncertainties, can mimic human-like behaviour and may enable human-trust in the automated system. All these aspects are crucial for a fully autonomous car and we believe reinforcement learning based ACC is a step towards that direction.					15th International Conference on Intelligent Autonomous Systems (IAS)15th International Conference on Intelligent Autonomous Systems (IAS)	JUN 11-15, 2018JUN 11-15, 2018	DHBW; KIT; FZI; Sick AG; Bosch AG; IntelDHBW; KIT; FZI; Sick AG; Bosch AG; Intel	Baden-Baden, GERMANYBaden-Baden, GERMANY	2	0	0	0	1	0	2			2194-5357	2194-5365	978-3-030-01370-7; 978-3-030-01369-1									Visteon Elect GmbH, RaumFabrik 33b, D-76227 Karlsruhe, GermanyVisteon Corp, 2901 Tasman Dr, Santa Clara, CA 95054 USA	Visteon Elect GmbHVisteon Corp			2019-01-01	WOS:000833525300037		
J	Pengfei Liu; Yimin Liu; Tianyao Huang; Yuxiang Lu; Xiqin Wang										Cognitive Radar Using Reinforcement Learning in Automotive Applications [arXiv]								arXiv								11 pp.	11 pp.											Journal Paper	24 April 2019	2019	The concept of cognitive radar (CR) enables radar systems to achieve intelligent adaption to a changeable environment with feedback facility from receiver to transmitter. However, the implementation of CR in a fast-changing environment usually requires a well-known environmental model. In our work, we stress the learning ability of CR in an unknown environment using a combination of CR and reinforcement learning (RL), called RL-CR. Less or no model of the environment is required. We also apply the general RL-CR to a specific problem of automotive radar spectrum allocation to mitigate mutual interference. Using RL-CR, each vehicle can autonomously choose a frequency subband according to its own observation of the environment. Since radar's single observation is quite limited compared to the overall information of the environment, a long short-term memory (LSTM) network is utilized so that radar can decide the next transmitted subband by aggregating its observations over time. Compared with centralized spectrum allocation approaches, our approach has the advantage of reducing communication between vehicles and the control center. It also outperforms some other distributive frequency subband selecting policies in reducing interference under certain circumstances.									0	0	0	0	0	0	0														Dept. of Electron. Eng., Tsinghua Univ., Beijing, ChinaBeijing Radar Res. Inst., Beijing, China				2020-03-23	INSPEC:18925496		
P	CHEN J; LI Z; SUN J										Unmanned aerial vehicle detection track planning            method based on depth reinforcement learning, involves            establishing detection track optimization problem of            unmanned vehicle according to Markov Decision process            model					CN115562345-A; CN115562345-B	BEIJING INST TECHNOLOGY																									NOVELTY - The unmanned aerial vehicle detection track planning method involves constructing a Markov decision process model of unmanned aerial vehicle detection trajectory planning. A detection track optimization problem of an unmanned vehicle is established according to the Markov Decision process model. An unmanned vehicle detection tracking optimization problem is designed to design reinforcement learning to a solving algorithm. The unmanned vehicle observation is inputted to design the reinforcement learning. The solving algorithm is used to obtain the unmanned vehicle inspection trajectory based on the deep reinforcement learning based on a planning strategy. USE - Unmanned aerial vehicle detection track planning method based on depth reinforcement learning. ADVANTAGE - The method realizes the trajectory planning of the unmanned aerial vehicle detection signal field under the condition that the distribution of the unmanned aerial vehicle dynamics model and the signal field to be detected is completely unknown, obtains sufficient information at the shortest time and reaches the predetermined target, and has high practical value. DESCRIPTION Of DRAWING(S) - The drawing shows a graphical representation of unmanned aerial vehicle detection track planning method based on depth reinforcement learning.															0																		2023-01-21	DIIDW:202305972P		
P	NATARAJAN V; ACHARYA G; M R; BAXI A S; K G A; VINCENT S M; NATARAJAN; BAXI A; SHAGAYA V										Apparatus for constructing complex assembly of            objects by robot using reinforcement learning action            primitives, has movement manager for commanding robot            to construct physical assembly of objects based on            sequences of action primitives					US2019275671-A1; DE102020106714-A1; US11345030-B2; DE102020106714-B4	INTEL CORP																									NOVELTY - The apparatus has a construction manager for determining sequences of reinforcement learning (RL) action primitives based on object location goals and associated assembly goals determined for respective ones of objects depicted in an imaged assembly of objects. A movement manager commands a robot (102) to construct a physical assembly of objects based on the sequences of RL action primitives, where the physical assembly of objects corresponds to the imaged assembly of objects. An object-to-goal mapper generates an object-to-goal map based on the object location goals, the associated assembly goals, and initial locations of physical objects to be included in the physical assembly of objects. USE - Apparatus for constructing a complex assembly of objects by a robot using RL action primitives in different applications. Uses include but are not limited to smart retail and warehouse logistics applications, smart manufacturing assembly lines, automotive component fabrication and assembly applications and consumer electronics fabrication and assembly applications. ADVANTAGE - The apparatus provides an RL-based approach that utilizes relatively simple and non-complex RL action primitives that are combinable into a myriad of configurations and/or sequences. The apparatus constructs the complex assembly of objects by the robots using the RL action primitives corresponding to relatively simple robotic actions and/or robotic movements. The apparatus reduces tedious and labor-intensive process of manually programming the robots to perform complex assembly tasks. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a non-transitory computer-readable storage medium for storing a set of instructions for constructing a complex assembly of objects by a robot using RL action primitives(2) a method for constructing a complex assembly of objects by a robot using RL action primitives. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an environment in which a robot is implemented to construct a complex assembly of objects using RL action primitives.Robot (102)Computing device (104)Robotic arm (108)Display (112)Assembly picture (126)															0																		2019-10-01	DIIDW:201978366N		
B	Ahadi-Sarkani, A.; Elmalaki, S.										ADAS-RL: Adaptive Vector Scaling Reinforcement Learning For Human-in-the-Loop Lane Departure Warning								CPHS21: Proceedings of the First International Workshop on Cyber-Physical-Human System Design and Implementation								13	18				10.1145/3458648.3460008							Conference Paper	18 May 2021	2021	Multiple sensory modalities are fast becoming a key instrument in the future of the automotive industry. Collision avoidance, lane departure warning, and self-parking are examples of Advanced Driver Assistance Systems (ADAS) that are becoming possible with the adoption of more sensors. Moreover, thanks to the recent advances in mobile computing and wearable devices, the driver is now equipped with advanced sensory systems. This rich sensory environment paves the way to integrate the human factor into the loop of computation of ADAS to provide a personalized experience. In this paper, we introduce ADAS-RL, a Reinforcement Learning based algorithm that integrates the behavior and reactions of the driver with the vehicle context to continuously adapt and tune the warning interventions of Lane Departure Warning System (LDW). We validated ADAS-RL against human drivers using CARLA simulator. Our evaluation shows a significant enhancement in the driver experience compared to the standard LDW systems. ADAS-RL shows the ability to track the changes in driving behavior and adapt the frequency of warnings allowing drivers to stay within a reasonable distance (around 1.75m) from lane markings with a significant decrease in the false warnings.					CPHS21: First International Workshop on Cyber-Physical-Human System Design and ImplementationCPHS21: First International Workshop on Cyber-Physical-Human System Design and Implementation	18 May 202118 May 2021	IEEE Signal Processing SocietyIEEE Signal Processing Society	Nashville, TN, USANashville, TN, USA	4	0	0	0	1	0	4					978-1-4503-8440-7									Univ. of California, Irvine, Irvine, CA, USA				2021-07-30	INSPEC:20757889		
B	Chen, Y.; Zhang, Y.; Chen, J.; Zhao, J.; Li, K.; Wang, L.						Long, S.; Dhillon, B.S.				Deep Reinforcement Learning Algorithm and Simulation Verification Analysis for Automatic Control of Unmanned Vehicles								Man-Machine-Environment System Engineering: Proceedings of the 22nd International Conference on MMESE. Lecture Notes in Electrical Engineering (941)								279	86				10.1007/978-981-19-4786-5_39							Conference Paper	2023	2023	This study conducted research mainly on the proven applicability of controlling the unmanned vehicle using a deep reinforcement learning algorithm and relative performance improvements. In specific, this study chose the AirSim platform developed by Microsoft as the simulation environment and conducted simulations mainly in the indoor parking lot Unreal 4 environment. In the simulations, the deep reinforcement learning method applied is Deep Q Networks for its effectiveness as well as simplicity. To improve the performance of the trained network, object detection methodology YOLO v3 is applied as the detection algorithm for the unmanned vehicle, and the network is improved using the output of object detection as its input to accelerate the training process. The implementation of the algorithms has efficiently proven the feasibility of using deep reinforcement learning agents for the unmanned vehicle in the project and the implementation of effective object detection.					International Conference on Man-Machine-Environment System EngineeringInternational Conference on Man-Machine-Environment System Engineering	21-23 Oct. 202221-23 Oct. 2022		Beijing, ChinaBeijing, China	0	0	0	0	0	0	0					978-981-19-4786-5									Yangzhou Collaborative Innovation Res. Inst. Co., Ltd., Yangzhou, ChinaSch. of Aeronaut. Sci. & Eng., Beihang Univ., Beijing, ChinaAstronaut Center of China, Beijing, ChinaDept. of Mech. Eng., Univ. of Ottawa, Ottawa, ON, Canada				2023-03-17	INSPEC:22692185		
J	Man Xunyu; Liu Yuansheng; Qi Han; Yan Chao; Yang Rujin							满恂钰; 刘元盛; 齐含; 严超; 杨茹锦			Autonomous Navigation Exploration and Map Construction for Unmanned Vehicles in Unknown Environments			未知环境下无人车自主导航探索与地图构建				汽车技术	Automobile Technology					11			34	40	1000-3703(2023)11<34:WZHJXW>2.0.TX;2-2										Article	2023	2023	For the problem that the autonomous navigation exploration algorithm is easy to fall into the local area,this paper proposed an exploration algorithm combining sampling and deep reinforcement learning.First,the Long-Short-Term Memory (LSTM) network was used locally to obtain the historical pose information of the unmanned vehicle to avoid repeated exploration of the explored area;secondly,the optimal action of the deep reinforcement learning strategy was used to output using deep reinforcement learning and the reward function was designed to encourage the unmanned vehicle to fully explore the unknown area;Finally,the horizontal movement factor of the unmanned vehicle was considered to generate a global exploration path conforming to its current attitude by solving the Asymmetric Travel Salesman Problem (ATSP).In the 2 000 s mine tunnel simulation environment,compared with the Technologies for Autonomous Robot Exploration (TARE) algorithm,the proposed algorithm increased the exploration area by 346.3 m~2 and reduced the total driving distance by 209.4 m;in the real scene test,the exploration algorithm completed the exploration of the underground garage with an area of 3 444.3 m~2 and returned to the starting point in 1 014 s and built the environment map.			针对自主导航探索算法易陷入局部区域的问题,提出了融合采样与深度强化学习的探索算法。首先,局部采用长短期记忆(LSTM)网络获得无人车历史位姿信息进而避免重复走向已探索区域;其次,利用深度强化学习输出策略最优的动作并设计奖励函数以激励无人车充分探索未知区域;最后,考虑无人车水平移动因素,通过解非对称旅行商问题(ATSP)生成一条符合其当前姿态的全局探索路径。2 000 s矿道仿真环境中,所提出的算法相较于无人机自主探索(TARE)算法,探索面积增加346.3 m~2,总行驶距离减少209.4 m;在真实场景试验中,该探索算法用时1 014 s完成面积为3 444.3 m~2的地下车库探索返回起点,并完成环境地图构建。						0	0	0	0	0	0	0			1000-3703											北京联合大学, 北京市信息服务工程重点实验室, 北京 100101, 中国北京联合大学,机器人学院, 北京 100101, 中国北京联合大学,智慧城市学院, 北京 100101, 中国Beijing Union University, Beijing Key Laboratory of Information Service Engineering, Beijing 100101, ChinaCollege of Robotics,Beijing Union University, Beijing 100101, ChinaSmart City College,Beijing Union University, Beijing 100101, China	北京联合大学北京联合大学,机器人学院北京联合大学,智慧城市学院Beijing Union UniversityCollege of Robotics,Beijing Union UniversitySmart City College,Beijing Union University			2024-01-27	CSCD:7597773		
J	Han, Wei; Guo, Fang; Su, Xichao				Han, Wei/B-2609-2008	, Mr. Fang GUO/0000-0001-9893-1994					A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem								ALGORITHMS				12	11					222			10.3390/a12110222							Article	NOV 2019	2019	The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.									15	4	0	0	0	0	18				1999-4893										Naval Aviat Univ, Dept Airborne Vehicle Engn, Yantai 264001, Peoples R China	Naval Aviat Univ			2019-12-13	WOS:000500012900011		
B	Wu, X.; Wedernikow, E.; Nitsche, C.; Huber, M.F.										Towards Optimal Energy Management Strategy for Hybrid Electric Vehicle with Reinforcement Learning								2023 IEEE Intelligent Vehicles Symposium (IV)								1	7				10.1109/IV55152.2023.10186787							Conference Paper	2023	2023	In recent years, the development of Artificial Intelligence (AI) has shown tremendous potential in diverse areas. Among them, reinforcement learning (RL) has proven to be an effective solution for learning intelligent control strategies. As an inevitable trend for mitigating climate change, hybrid electric vehicles (HEVs) rely on efficient energy management strategies (EMS) to minimize energy consumption. Many researchers have employed RL to learn optimal EMS for specific vehicle models. However, most of these models tend to be complex and proprietary, making them unsuitable for broad applicability. This paper presents a novel framework, in which we implement and integrate RL-based EMS with the open-source vehicle simulation tool called FASTSim. The learned RL-based EMSs are evaluated on various vehicle models using different test drive cycles and prove to be effective in improving energy efficiency.					2023 IEEE Intelligent Vehicles Symposium (IV)2023 IEEE Intelligent Vehicles Symposium (IV)	4-7 June 20234-7 June 2023	IEEEIEEE	Anchorage, AK, USAAnchorage, AK, USA	0	0	0	0	0	0	0					979-8-3503-4691-6									Dept. Cyber Cognitive Intelligence (CCI), Fraunhofer IPA, USAInst. of Ind. Manuf. & Manage. IFF, Univ. of Stuttgart, Stuttgart, UK				2023-08-10	INSPEC:23486922		
C	Moradi, Mehrdad; Oakes, Bentley James; Saraoglu, Mustafa; Morozov, Andrey; Janschek, Klaus; Denil, Joachim			IEEE Comp Soc	Oakes, Bentley/AAS-2811-2020; Morozov, Andrey/V-1033-2019; Moradi, Mehrdad/AAW-7249-2020	Oakes, Bentley/0000-0001-7558-1434; Moradi, Mehrdad/0000-0001-8748-069X					Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection								50TH ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS WORKSHOPS (DSN-W 2020)								102	109				10.1109/DSN-W50199.2020.00028							Proceedings Paper	2020	2020	Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.					50th IEEE/IFIP Annual International Conference on Dependable Systems and Networks (DSN)50th IEEE/IFIP Annual International Conference on Dependable Systems and Networks (DSN)	JUN 29-JUL 02, 2020JUN 29-JUL 02, 2020	IEEE; IFIP; IEEE Comp Soc; Intel; King Abdullah Univ Sci & Technol; Oracle; Commonwealth Cyber Initiat; Univ Politecnica Valencia, DISCA; Univ Politecnica ValenciaIEEE; IFIP; IEEE Comp Soc; Intel; King Abdullah Univ Sci & Technol; Oracle; Commonwealth Cyber Initiat; Univ Politecnica Valencia, DISCA; Univ Politecnica Valencia	Valencia, SPAINValencia, SPAIN	12	0	0	0	0	0	13					978-1-7281-7263-7									Univ Antwerp, Antwerp, BelgiumFlanders Make Vzw, Antwerp, BelgiumTech Univ Dresden, Dresden, Germany	Flanders Make Vzw			2020-01-01	WOS:000853340600018		
P	WANG X; ZHANG Y; ZOU L; LI B										Global path planning method for unmanned vehicles,            involves generating motion path of unmanned vehicle,            according to evaluation index of path planning result            output by deep enhanced neural network					CN111061277-A; WO2021135554-A1; CN111061277-B; US2022196414-A1; US11747155-B2	GOERTEK INC																									NOVELTY - The global path planning method involves establishing (S110) describing the sequence decision-making process of unmanned vehicle path planning through a reinforcement learning method. Unmanned vehicle status, environmental status described using map pictures, and evaluation indicators of route planning results. The map picture and the state of the unmanned vehicle in the current task scene is inputted (S120) into the deep reinforcement learning neural network after training. The motion path of the unmanned vehicle is generated (S130), according to the evaluation index of the path planning result output by the deep enhanced neural network. USE - Global path planning method for unmanned vehicles. ADVANTAGE - The global path planning method identifies the environmental information in the scene through the map picture. The map features are extracted through deep neural network simplifies the modeling process of map scenes. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for unmanned vehicle global path planning device. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating global path planning method for unmanned vehicles. (Drawing includes non-English language text)Step for establishing describing sequence decision-making process of unmanned vehicle path planning through reinforcement learning method (S110)Step for inputting map picture and state of unmanned vehicle in current task scene into deep reinforcement learning neural network after training (S120)Step for generating motion path of unmanned vehicle, according to evaluation index of path planning result output by deep enhanced neural network (S130)															0																		2020-05-18	DIIDW:2020365414		
P	LI X; WANG Z; LI G; ZHOU Y; LU P; GENG S; HE B										Method for planning motion of robot based on            digital twinning and reinforcement learning, involves            obtaining route and linear velocity and angular            velocity information for controlling unmanned vehicle            movement					CN115903825-A	UNIV TONGJI																									NOVELTY - The method involves constructing a virtual scene corresponding to real scene by using digital twinning technology. The virtual scene is constructed for performing training process in reinforcement learning. Training in the virtual scene is performed to obtain a local path planning and obstacle avoidance algorithm. The real scene and the virtual scene are used to map for finishing motion planning task of an unmanned vehicle in the real scene and updating the virtual scene. Unmanned aerial vehicle observation information of the virtual space and the real space is combined after finishing training. Route and linear velocity and angular velocity information is obtained for controlling unmanned vehicle movement. USE - Method for planning motion of a robot based on digital twinning and reinforcement learning. Uses include but are not limited to product design, product manufacturing, medical analysis and engineering construction fields. ADVANTAGE - The method enables combining the digital twinning and depth reinforcement learning and training the unmanned vehicle to realize autonomous navigation algorithm. The information obtained by unmanned vehicle assembling sensor is used for updating the virtual scene, and the position and speed of unmanned vehicle in real scene can be monitored online in real-time through virtual scene and state information. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for planning motion of a robot based on digital twinning and reinforcement learning (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202342500S		
P	DUAN Z; REN Y; YANG T; ZHANG Z; LV X										Method for planning path of unmanned vehicle based            on reinforcement learning layered double-sensing            domain, involves realizing path planning of unmanned            vehicle based on reinforcement learning of target            hierarchical dual perception domain					CN114578834-A; CN114578834-B	UNIV PEKING																									NOVELTY - The method involves reading two-dimensional pixel map of an unmanned vehicle driving area. Coordinate conversion and global alignment process is performed to obtain a world map for path planning. An obstacle position is extracted from the world map. A current unmanned vehicle position state and a target position state are obtained. A passable path is obtained according to the global map search. A safety area corridor is obtained along the passable paths. A sub-target point sequence is obtained by using a target layering process according to a safety area corridors. The corresponding data of the unmanned vehicle to be planned is inputted, and the model output get the planned smooth path. The path planning of the unmanned vehicle based on the reinforcement learning of the target hierarchical dual perception domain is realized. USE - Method for planning path of unmanned vehicle based on reinforcement learning layered double-sensing domain. ADVANTAGE - The method is set to adapt to the map of different dimensions, and avoids the dimensional disaster in the training, the calculation efficiency is high, applicability is strong, and meets the actual requirement of real-time path planning. By using the target layering method to make the sub-target layer at the middle portion of the safety area corridor, avoiding the situation that the path is close to the barrier due to the search algorithm. The obstacle sensing domain and the target discovery domain double sensing domain reduce observation input, decoupling the obstacle sensing and target discovery separately. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating the process for planning path of unmanned vehicle based on reinforcement learning layered double-sensing domain. (Drawing includes non-English language text)															0																		2023-08-10	DIIDW:202278131W		
C	Fang, Xing; Zhang, Qichao; Gao, Yinfeng; Zhao, Dongbin			IEEE							Offline Reinforcement Learning for Autonomous Driving with Real World Driving Data								2022 IEEE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						3417	3422				10.1109/ITSC55140.2022.9922100							Proceedings Paper	2022	2022	Since traditional reinforcement learning (RL) approaches need active online interaction with the environment, previous works are mainly investigated in the simulation environment rather than the real world environment, especially for safety-critical applications. Offline RL has recently emerged as a promising data-driven learning paradigm to learn a policy from offline dataset directly. It seems that offline RL is well suited for autonomous driving, as it is feasible to collect offline naturalized driving dataset. However, it remains unclear how to deploy offline RL with real world driving dataset only including observation data, and whether current offline RL algorithms work well to learn a driving policy than imitation learning? In this paper, we provide an offline RL benchmark for autonomous driving including the dataset, baselines, and a data driven simulator1. First, we summarize and introduce the popular offline RL baseline methods. Then, we construct an offline RL dataset for the car following task based on the real world driving dataset INTERACTION. A data driven simulator is applied to obtain augmented data and test the driving policy. Further, we deploy four popular offline algorithms and analyze their performances under different datasets including real world driving data and augmented data. Finally, related conclusions and discussions are given to analyze the critical challenge for offline RL in autonomous driving.					IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)	OCT 08-12, 2022OCT 08-12, 2022	IEEEIEEE	Macau, PEOPLES R CHINAMacau, PEOPLES R CHINA	0	0	0	0	0	0	0			2153-0009		978-1-6654-6880-0									Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R ChinaUniv Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R ChinaUniv Elect Sci & Technol China, Sch Math Sci, Chengdu 611731, Peoples R ChinaPeng Cheng Lab, Shenzhen, Peoples R ChinaUniv Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China				2023-04-26	WOS:000934720603069		
P	GUO D; ZHOU Z; YU Y; ZHANG Z										Automatic driving decision system based on maximum            entropy layered reinforcement learning, is configured            to be trained based on the maximum entropy layered            reinforcement learning method to obtain automatic            driving strategies					CN116257065-A	UNIV NANJING																									NOVELTY - The automatic driving decision-making system is configured to be trained based on the maximum entropy layered reinforcement learning method to obtain automatic driving strategies adapted to different road conditions and automatic selection strategies for driving strategies. The car automatically selects a reasonable driving strategy according to the road conditions. The system constructs an unmanned vehicle simulation control environment to train the automatic driving strategy. The system includes the driving strategy value network module, which is used for evaluating the driving strategy, a driving strategy selection policy network module used for unmanned vehicles to select driving strategies according to their own state, and a driving strategy group network module for unmanned vehicles to determine driving actions according to the selected driving strategy and its own state. USE - Automatic driving decision-making system based on maximum entropy layered reinforcement learning. ADVANTAGE - The automatic driving decision system ensures that the automatic driving strategy learning cost is reduced and the learning efficiency of the strategy is improved. The learning efficiency is improved and the trial error cost is also reduced. The unmanned vehicle can effectively learn multiple driving strategies for automatic driving decision and automatically selecting the optimal driving strategy for driving according to the unmanned vehicle state, which is suitable for unmanned vehicle training in the automatic driving environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a training method of automatic driving strategy based on maximum entropy layered reinforcement learning; (2) an automatic driving decision method based on maximum entropy layered reinforcement learning; (3) a computer device; (4) a computer-readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows an unmanned vehicle automatic driving decision flow chart for the automatic driving decision system based on maximum entropy layered reinforcement learning. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202365392Y		
C	Ahadi-Sarkani, Armand; Elmalaki, Salma			ACM							ADAS-RL: Adaptive Vector Scaling Reinforcement Learning For Human-in-the-Loop Lane Departure Warning								CPHS'21: PROCEEDINGS OF THE 2021 THE FIRST ACM INTERNATIONAL WORKSHOP ON CYBER-PHYSICAL-HUMAN SYSTEM DESIGN AND IMPLEMENTATION								7	12				10.1145/3458648.3460008							Proceedings Paper	2021	2021	Multiple sensory modalities are fast becoming a key instrument in the future of the automotive industry. Collision avoidance, lane departure warning, and self-parking are examples of Advanced Driver Assistance Systems (ADAS) that are becoming possible with the adoption of more sensors. Moreover, thanks to the recent advances in mobile computing and wearable devices, the driver is now equipped with advanced sensory systems. This rich sensory environment paves the way to integrate the human factor into the loop of computation of ADAS to provide a personalized experience. In this paper, we introduce ADAS-RL, a Reinforcement Learning based algorithm that integrates the behavior and reactions of the driver with the vehicle context to continuously adapt and tune the warning interventions of Lane Departure Warning System (LDW). We validated ADAS-RL against human drivers using CARLA simulator. Our evaluation shows a significant enhancement in the driver experience compared to the standard LDW systems. ADAS-RL shows the ability to track the changes in driving behavior and adapt the frequency of warnings allowing drivers to stay within a reasonable distance (around 1.75m) from lane markings with a significant decrease in the false warnings.					First ACM International Workshop on Cyber Physical Human System Design and ImplementationFirst ACM International Workshop on Cyber Physical Human System Design and Implementation	MAY 18, 2021MAY 18, 2021	ACM SIGBED, IEEE Signal Proc Soc and IEEE CSACM SIGBED, IEEE Signal Proc Soc and IEEE CS	Nashville, TNNashville, TN	0	0	0	0	0	0	0					978-1-4503-8440-7									Univ Calif Irvine, Irvine, CA 92697 USA				2021-01-01	WOS:000936022300002		
J	Seowoo Jang; Namwoo Kang										Generative Design by Reinforcement Learning: Maximizing Diversity of Topology Optimized Designs [arXiv]								arXiv								18 pp.	18 pp.											Journal Paper	17 Aug. 2020	2020	Generative design is a design exploration process in which a large number of structurally optimal designs are generated in parallel by diversifying parameters of the topology optimization while fulfilling certain constraints. Recently, data-driven generative design has gained much attention due to its integration with artificial intelligence (AI) technologies. When generating new designs through a generative approach, one of the important evaluation factors is diversity. In general, the problem definition of topology optimization is diversified by varying the force and boundary conditions, and the diversity of the generated designs is influenced by such parameter combinations. This study proposes a reinforcement learning (RL) based generative design process with reward functions maximizing the diversity of the designs. We formulate the generative design as a sequential problem of finding optimal parameter level values according to a given initial design. Proximal Policy Optimization (PPO) was applied as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. This study also proposes the use of a deep neural network to instantly generate new designs without the topology optimization process, thus reducing the large computational burdens required by reinforcement learning. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.									0	0	0	0	0	0	0														Dept. of Electr. & Comput. Eng., Seoul Nat. Univ., Seoul, South Korea				2021-01-29	INSPEC:20234654		
C	Wang, Yiquan; Wang, Jingguo; Yang, Yu; Li, Zhaodong; Zhao, Xijun				wu, jialin/JPY-4408-2023		Fu, W; Gu, M; Niu, Y				An End-to-End Deep Reinforcement Learning Model Based on Proximal Policy Optimization Algorithm for Autonomous Driving of Off-Road Vehicle								PROCEEDINGS OF 2022 INTERNATIONAL CONFERENCE ON AUTONOMOUS UNMANNED SYSTEMS, ICAUS 2022		Lecture Notes in Electrical Engineering		1010				2692	2704				10.1007/978-981-99-0479-2_248							Proceedings Paper	2023	2023	Most conventional unmanned vehicle control algorithms require human adjustment of parameters and design of precise rules, thus failing to adapt quickly to multiple situations when facing complex environments in the wild. To address these problems, this paper adopts an end-to-end deep reinforcement learning model based on proximal policy optimization algorithm to control the steering, speed and braking of an unmanned vehicle, allowing it to autonomously learn motion control strategies from perceptionmap in un-known environments. A novel environment simulator which contains variable passable areas and obstacles is also proposed to support agents to achieve target reward. The proposed agent model has been proved to receive the highest reward over SAC and has the ability to overcome the complexity of the wild environment generated by the simulator.					International Conference on Autonomous Unmanned Systems (ICAUS)International Conference on Autonomous Unmanned Systems (ICAUS)	SEP 23-25, 2022SEP 23-25, 2022		Xian, PEOPLES R CHINAXian, PEOPLES R CHINA	0	0	0	0	0	0	0			1876-1100	1876-1119	978-981-99-0481-5; 978-981-99-0479-2; 978-981-99-0478-5									China North Artificial Intelligence & Innovat Res, Beijing, Peoples R ChinaJiuquan Satellite Launch Ctr, Jiuquan, Gansu, Peoples R China	China North Artificial Intelligence & Innovat ResJiuquan Satellite Launch Ctr			2023-08-12	WOS:001002244202068		
B	Luo, Z.; Zhou, J.; Wen, G.										Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee								2022 13th Asian Control Conference (ASCC)								1893	8				10.23919/ASCC56756.2022.9828057							Conference Paper	2022	2022	It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique. Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee. By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained. Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast. Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.					2022 13th Asian Control Conference (ASCC)2022 13th Asian Control Conference (ASCC)	4-7 May 20224-7 May 2022		Jeju, South KoreaJeju, South Korea	2	0	0	0	0	0	2					978-89-93215-23-6									Dept. of Syst. Sci., Southeast Univ., Nanjing, ChinaSch. of Autom., Nanjing Univ. of Sci. & Technol., Nanjing, China				2022-11-01	INSPEC:21950912		
B	Wang, Y.; Wang, J.; Yang, Y.; Li, Z.; Zhao, X.						Fu, W.; Gu, M.; Niu, Y.				An End-to-End Deep Reinforcement Learning Model Based on Proximal Policy Optimization Algorithm for Autonomous Driving of Off-Road Vehicle								Proceedings of 2022 International Conference on Autonomous Unmanned Systems (ICAUS 2022). Lecture Notes in Electrical Engineering (1010)								2692	2704				10.1007/978-981-99-0479-2_248							Conference Paper	2023	2023	Most conventional unmanned vehicle control algorithms require human adjustment of parameters and design of precise rules, thus failing to adapt quickly to multiple situations when facing complex environments in the wild. To address these problems, this paper adopts an end-to-end deep reinforcement learning model based on proximal policy optimization algorithm to control the steering, speed and braking of an unmanned vehicle, allowing it to autonomously learn motion control strategies from perception map in un-known environments. A novel environment simulator which contains variable passable areas and obstacles is also proposed to support agents to achieve target reward. The proposed agent model has been proved to receive the highest reward over SAC and has the ability to overcome the complexity of the wild environment generated by the simulator.					International Conference on Autonomous Unmanned SystemsInternational Conference on Autonomous Unmanned Systems	23-25 Sept. 202223-25 Sept. 2022		Xi'an, ChinaXi'an, China	0	0	0	0	0	0	0					978-981-99-0479-2									China North Artificial Intelligence & Innovation Res. Inst., Beijing, ChinaJiuquan Satellite Launch Centre, Jiuquan, ChinaUnmanned Syst. Res. Inst., Northwestern Polytech. Univ., Xi'an, ChinaBeijing HIWING Sci. & Technol. Inf. Inst., Beijing, ChinaColl. of Intelligence Sci. & Technol., Nat. Univ. of Defense Technol., Changsha, China				2023-07-14	INSPEC:23378709		
P	WANG M; CHEN S; SONG W; WANG K										Method for realizing unmanned vehicle formation on            motorways based on multi-agent reinforcement learning,            involves carrying out trajectory planning according to            each vehicle so that each vehicle performs specified            action in action decision					CN113255998-A; CN113255998-B	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves obtaining environmental information as observation input in a trained Q-MIX network to obtain action decisions of each unmanned vehicle to realize the formation. A training environment is initialized. Environmental information of the training environment is inputted as observations into the Q-MIX network to obtain an action decision of each unmanned vehicle. Trajectory planning according to each unmanned vehicle is carried out based on the action decision so that each unmanned vehicle performs specified action in the action decision. Corresponding reward value is obtained when each unmanned vehicle performs the specified action. Judgment is made to check whether distance between any two unmanned vehicles is greater than set threshold or collision occurs. Average speed of all unmanned vehicles is determined. Lateral displacement and longitudinal displacement of the unmanned vehicle are determined. USE - Method for realizing unmanned vehicle formation on motorways based on multi-agent reinforcement learning. ADVANTAGE - The method enables utilizing multi-intelligent body reinforcement learning process for changing strategy, combining S-T diagram track optimization process, calculating accurate control quantity and increasing control constraint and improving safety of the unmanned vehicle. DETAILED DESCRIPTION - A mean square error loss function corresponding to each unmanned vehicle is constructed according to sum of reward value. The Q-MIX network is updated according to the mean square error loss function. The Q-MIX network is resumed with the updated Q-MIX network until the set number of repetitions is reached. A final Q-MIX network is obtained. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for realizing unmanned vehicle formation on motorways based on multi-agent reinforcement learning. (Drawing includes non-English language text).															0																		2022-01-08	DIIDW:202197617B		
B	Shouqing Lu										Automatic Tracking Method of Unmanned Vehicle Trajectory Based on Reinforcement Learning								AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture								1596	9				10.1145/3495018.3495448							Conference Paper	2021	2021	Up to now, unmanned driving is still a challenging research field in academia at home and abroad. The vehicle trajectory tracking technology is a very critical and urgent link, because it provides important information for intelligent traffic monitoring. The reinforcement learning method is an important method for learning in an unknown environment. In the field of artificial intelligence machine learning, reinforcement learning research has made great progress in theory, algorithm and application, and has become a current hotspot in research. Unmanned vehicle trajectory tracking is one of the key technologies in the field of unmanned driving research. It uses built-in sensors to perceive the environment, uses trajectory planning algorithms to generate the required path in real time, and the decision system selects the best path. Finally, the built-in path tracking controller implement it. This article mainly adopts the experimental analysis method to discuss how to break through the problem of automatic trajectory tracking technology in the support of enhanced learning by unmanned vehicles, and compare and analyze the expected yaw rate and actual yaw rate and frequency of the target vehicle. According to the experimental research results, the expected yaw rate and actual yaw rate of the unmanned vehicle trajectory automatic tracking test are relatively close, and the test system in this test has a certain tracking effect.					AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced ManufactureAIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture	23-25 Oct. 202123-25 Oct. 2021		Manchester, UKManchester, UK	0	0	0	0	0	0	0					978-1-4503-8504-6									Chongqing Vocational Coll. of Transp., Chongqing, China				2022-11-01	INSPEC:21639779		
P	WANG Y; HUANG G; LI Z; HOU M; YUAN X										Method for performing depth reinforcement learning            for unmanned vehicle path planning in three-dimensional            large size terrain environment, involves using current            planning point as center to expand outwards for ten            unit step lengths					CN115979287-A	UNIV HUNAN																									NOVELTY - The method involves inputting global map observation information to a dynamic global channel. Local map observing information is inputted to the dynamic local channel. Energy consumption and driving time of an unmanned vehicle are evaluated by a multi-target reward function. Historical empirical data reaching a target position is extracted from an experience buffer pool as a training set. Three-dimensional map data is stored in a three-dimensional large-scale terrain environment. A depth reinforcement learning search strategy is adopted based on prior experience rebroadcast. A current planning point is used as a center to expand outwards for ten unit step lengths. USE - Method for performing high-efficient depth reinforcement learning for unmanned vehicle path planning in a three-dimensional large size terrain environment. Uses include but are not limited to video game, robot, intelligent driving and recommendation system. ADVANTAGE - The method enables inputting the global map observation information to the dynamic global channel, and thus ensuring simple and efficient learning process of depth reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for performing high-efficient depth reinforcement learning for unmanned vehicle path planning (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202347278J		
P	WANG X; SHEN S; WANG L; HUANG Y; QIN X										Method for controlling formation of multi-unmanned            vehicle based on depth reinforcement learning            technology, involves providing formation generating            strategy and formation holding strategy for            multi-unmanned vehicle long distance formation            according to multiunmanned vehicle global            planning					CN116069023-A	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - The method involves obtaining state information of each unmanned vehicle, where the state information comprises coordinate, yaw angle, linear speed, angular speed and front 180 degrees in an obstacle and a distance of the unmanned vehicle. An action of each unmanned vehicle currently needed to execute is calculated according to a policy network. A reward value and the state information of the next time of each unmanned vehicle currently needed to be executed are obtained, where all actions of the unmanned vehicle need to be executed form a combined action. A sample from an experience buffer pool is read according to an evaluation network. A minimum loss function is calculated. USE - Method for controlling a formation of a multi-unmanned vehicle based on a depth reinforcement learning technology. ADVANTAGE - The method enables improving a success rate of the multiunmanned vehicle in a dynamic obstacle environment by training a neural network, so that the multi-unmanned vehicle can reach an expected formation of a global stability. DETAILED DESCRIPTION - A parameter of the target strategy network and the target evaluation network is updated to optimize a multi-unmanned vehicle formation control strategy, where the multi-unmanned vehicle formation control strategy comprises a formation generating strategy and a formation holding strategy, the formation generating strategy makes all unmanned vehicle tracking to a target point, and the formation holding strategy corrects the unmanned vehicle track, which deviates from the unmanned vehicle. A multi-unmanned vehicle overall planning is performed according to a soft actor critic (SAC) algorithm. The formation generating strategy and the formation holding strategy are provided for multi-unmanned vehicle long distance formation according to a multiunmanned vehicle global planning.An INDEPENDENT CLAIM is included for a system for controlling a formation of a multi-unmanned vehicle based on depth reinforcement learning technology. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for controlling a formation of a multi-unmanned vehicle based on a depth reinforcement learning technology. (Drawing includes non-English language text).															0																		2023-06-08	DIIDW:2023514470		
P	ZHOU P; WANG J; ZHANG S; LI Z; LIANG H										Multi-element characteristic fusion automatic            driving course reinforcement learning training method            for use in automatic driving of vehicle i.e. car,            involves generating current optimal trajectory of            unmanned vehicle in real time according to obtained            optimal driving behavior					CN115169951-A	CHINESE ACAD SCI HEFEI INST PHYS SCI																									NOVELTY - The method involves collecting (S1) vehicle surrounding environmental information by using a vehicle-mounted sensor according to a training task for global path planning. A traffic state information vector is obtained. A local occupied grid image of a vehicle current position is obtained based on a Fleiner coordinate system. An action space and reward function of an action decision reinforcement learning building depth reinforcement learning backbone network model are designed (S2). A current optimal trajectory of an unmanned vehicle is generated (S4) in real time according to the obtained optimal driving behavior. USE - Multi-element characteristic fusion automatic driving course reinforcement learning training method for use in automatic driving of a vehicle i.e. car. ADVANTAGE - The method makes the automatic driving vehicle automatically learn the optimal driving strategy by data driving so as to finish the navigation task in the urban structured road environment. The multi-resolution trajectory planning algorithm based on sampling, generates the current optimal trajectory of the unmanned vehicle in real time according to the obtained optimal driving behavior. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the multi-element characteristic fusion automatic driving course reinforcement learning training method for use in automatic driving of a vehicle. (Drawing includes non-English language text).S1Step for collecting vehicle surrounding environmental information by using vehicle-mounted sensor according to training task for global path planningS2Step for designing action space and reward function of action decision reinforcement learning building depth reinforcement learning backbone network modelS3Step for inputting inputting extracted multivariate characteristic into constructed depth reinforcement learning backbone network modelS4Step for generating current optimal trajectory of unmanned vehicle in real time according to obtained optimal driving behavior															0																		2023-08-10	DIIDW:2022D0218L		
J	Yun, Lingxiang; Wang, Di; Li, Lin					Yun, Lingxiang/0000-0002-2978-3243					Explainable multi-agent deep reinforcement learning for real-time demand response towards sustainable manufacturing								APPLIED ENERGY				347						121324			10.1016/j.apenergy.2023.121324					JUN 2023		Article; Early Access		2023	The demand response (DR) plays a significant role in manufacturing system energy management and sustainable industrial development. Current literature on DR management for manufacturing systems has mostly focused on day-ahead production scheduling, whose effectiveness is limited due to the lack of flexibility to control the production line in real time. The development of reinforcement learning (RL) possesses huge potential for realtime production control to address the flexibility issue. However, since production is the top priority for any manufacturing system, a trustful and explainable RL for manufacturing system energy management that can ensure the production requirements is necessary for this application. This study proposes an explainable multiagent deep RL method, where the analytical manufacturing system model is applied to decompose the systemlevel energy management objective and production requirement to the agent level. Based on the decomposed task, the agent can then form a safe action subset that is interpretable to achieve the original system-level production requirement while learning to reduce energy costs under DR. The proposed RL method, which is referred to as decomposed multi-agent deep Q-network (DMADQN), is applied to control a section of an automotive assembly line using one year of DR electricity price data to validate its performance. Results show that the proposed method ensures the achievement of the production requirement while providing better DR energy management performance in both RL training and testing phases. In addition, the proposed approach can outperform the day-ahead scheduling approach and save up to an additional 30.7% of energy costs under dynamic DR.									1	0	0	0	0	0	1			0306-2619	1872-9118										Univ Illinois, Dept Mech & Ind Engn, Chicago, IL 60607 USA				2023-08-10	WOS:001037144900001		
P	CHEN J; WANG G; LI B; SUN J										Social navigation method for mobile robots based            on reinforcement learning, involves finishing social            navigation of mobile robot after training depth            reinforcement learning network frame as actual            navigation of optimal strategy					CN115456851-A	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves constructing a model of the social navigation problem. A deep reinforcement learning network framework is constructed according to the model of the social navigation problem. The deep reinforcement learning network framework constructed is trained. The deep reinforcement learning network framework trained is used as the optimal strategy for actual navigation to complete the social navigation of mobile robots. The deep reinforcement learning network framework includes an interaction module, historical feature extraction module, pooling module and planning module. USE - Social navigation method for mobile robots used in unmanned distribution and other fields, based on reinforcement learning in electronic game. ADVANTAGE - The method enables providing a more accurate model for an unmanned vehicle sensing an actual crowd environment under same experimental condition, so that the algorithm can make the unmanned vehicle to effectively avoid the crowd and faster reach a target position. The method allows a mobile robot social navigation method based on reinforcement learning to comprehensively consider crowd time and space feature, thus improving efficiency of the social navigation algorithm. The planning module plans a feasible path according to unmanned vehicle state and the extracted crowd characteristic, thus pre-training the model by simulating learning of expert experience, and hence accelerating model convergence speed. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the construction method of mobile robot and crowd spatio-temporal map.															0																		2023-08-10	DIIDW:2022F4358X		
P	GUO Q; MA W; LAN W; XU R; FENG X										Method for diagnosing fault of unmanned vehicle            based on depth reinforcement learning and expert            knowledge base, involves circulating step except            initial parameter configuration until convergence or            reaches maximum training wheel number					CN115659178-A	SHAANXI ZHIN TECHNOLOGY CO LTD																									NOVELTY - The method involves randomly dividing a training sample into a training set and a test set. The training set fault data is inputted as a state space. A fault type of the training set is input as an action space, discount rate, learning rate and corrupt heart rate. A Q network parameter is initialized. A parameter of a target Q network is initialized until a maximum training wheel number is reached. A gradient descent algorithm is performed on a loss function. A neural network parameter and a target network parameter are updated. A test set is utilized to model test to obtain a deep reinforcement learning knowledge system. USE - Method for performing fault diagnosis of an unmanned vehicle based on depth reinforcement learning and expert knowledge base. Uses include but are not limited to an automatic driving unmanned vehicle, a computer-driving unmanned vehicle and a vehicle capable of detecting and identifying a road and an obstacle. ADVANTAGE - The method enables updating the knowledge system and explanation library, so as to effectively improve the detection range of the fault, thus reducing the occurrence of safety accident caused by the fault of the unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the unmanned vehicle fault diagnosis method based on depth reinforcement learning expert knowledge base. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202317308E		
J	Marvi, Zahra; Kiumarsi, Bahare										Safe reinforcement learning: A control barrier function optimization approach								INTERNATIONAL JOURNAL OF ROBUST AND NONLINEAR CONTROL				31	6			1923	1940				10.1002/rnc.5132					AUG 2020		Article; Early Access		2021	This article presents a learning-based barrier certified method to learn safe optimal controllers that guarantee operation of safety-critical systems within their safe regions while providing an optimal performance. The cost function that encodes the designer's objectives is augmented with a control barrier function (CBF) to ensure safety and optimality. A damping coefficient is incorporated into the CBF which specifies the trade-off between safety and optimality. The proposed formulation provides a look-ahead and proactive safety planning and results in a smooth transition of states within the feasible set. That is, instead of applying an optimal controller and intervening with it only if the safety constraints are violated, the safety is planned and optimized along with the performance to minimize the intervention with the optimal controller. It is shown that addition of the CBF into the cost function does not affect the stability and optimality of the designed controller within the safe region. This formulation enables us to find the optimal safe solution iteratively. An off-policy reinforcement learning (RL) algorithm is then employed to find a safe optimal policy without requiring the complete knowledge about the system dynamics, while satisfies the safety constraints. The efficacy of the proposed safe RL control design approach is demonstrated on the lane keeping as an automotive control problem.									66	6	0	0	0	0	76			1049-8923	1099-1239										Michigan State Univ, Dept Elect & Comp Engn, 428 S Shaw Lane,Room 2120, E Lansing, MI 48824 USA				2020-08-24	WOS:000558186300001		
P	IN C T; WON J										Apparatus for determining path of e.g. UAV, in            airspace by using reinforcement learning, has airspace            path inference unit predicting flight path of vehicle            through reinforcement learning of inference of agent            node at vector point nodes					KR2023032309-A	CLROBUR CO LTD																									NOVELTY - The apparatus (1) has space vector point nodes for representing environmental elements of an airspace of an unmanned vehicle as an environment for reinforcement learning. A starting node corresponding to a starting point of the unmanned vehicle and an arrival point are set in the space vectors point nodes. An airspace environment recognition unit (10) sets an arrival node. An agent node setting unit (21) is configured to set agent nodes that perform reinforcement learning at the space vector points. A flight path of the vehicle is predicted by an airspace path inference unit through reinforcement learning of a flight path inference of the agent node. USE - Apparatus for determining a path of an unmanned mobile vehicle i.e. UAV, in the airspace by using reinforcement learning. ADVANTAGE - The apparatus determines and expresses optimal flight path avoiding obstacles according to 3D buildings and terrain in space through an automatic path determination process using a reinforcement learning technique in a space vector point node environment in an airspace expressed in 4 dimensions. The apparatus is convenient to select and determine routes individually on a 2D or 3D map to determine the flight route of the unmanned vehicle, so that difficulties in performing tasks that require direct visual observation and judgment, and consequently unpredictable accidents are reduced. The apparatus allows a user to easily obtain a safe route for the unmanned vehicle by setting starting and ending points while solving problems such as yagi and lack of automation. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the apparatus for determining the path of the unmanned mobile vehicle (Drawing includes non-English language text).1Apparatus for determining path of unmanned mobile vehicle10Airspace environment recognition unit13Node unit14Start node setting unit21Agent node setting unit															0																		2023-04-07	DIIDW:202327481E		
J	Raio, Stephen; Corder, Kevin; Parker, Travis W.; Shearer, Gregory G.; Edwards, Joshua S.; Thogaripally, Manik R.; Park, Song J.; Nelson, Frederica F.										Reinforcement Learning as a Path to Autonomous Intelligent Cyber-Defense Agents in Vehicle Platforms								APPLIED SCIENCES-BASEL				13	21					11621			10.3390/app132111621							Article	NOV 2023	2023	Technological advancement of vehicle platforms exposes opportunities for new attack paths and vulnerabilities. Static cyber defenses can help mitigate certain attacks, but those attacks must generally be known ahead of time, and the cyber defenses must be hand-crafted by experts. This research explores reinforcement learning (RL) as a path to achieve autonomous, intelligent cyber defense of vehicle control networks-namely, the controller area network (CAN) bus. We train an RL agent for the CAN bus using Toyota's Portable Automotive Security Testbed with Adaptability (PASTA). We then apply the U.S. Army Combat Capabilities Development Command (DEVCOM) Army Research Laboratory's methodology for quantitative measurement of cyber resilience to assess the agent's effect on the vehicle testbed in a contested cyberspace environment. Despite all defenses having similar traditional performance measures, our RL agent averaged a 90% cyber resilience measurement during drive cycles executed on hardware versus 41% for a naive static timing defense and 98% for the bespoke timing-based defense. Our results also show that an RL-based agent can detect and block injection attacks on a vehicle CAN bus in a laboratory environment with greater cyber resilience than prior learning approaches (1% for convolutional networks and 0% for recurrent networks). With further research, we believe there is potential for using RL in the autonomous intelligent cyber defense agent concept.									0	0	0	0	0	0	0				2076-3417										US Army, Combat Capabil Dev Command DEVCOM, Army Res Lab, Aberdeen, MD 21005 USAParsons Corp, Aberdeen, MD 21005 USAICF Int, Columbia, MD 21046 USA	Parsons CorpICF Int			2023-12-03	WOS:001099465600001		
J	Zhang, Li; Li, Zhao; Ren, Huali; Yu, Xiao; Ma, Yuxi; Zhang, Quanxin										Knowledge graph and behavior portrait of intelligent attack against path planning								INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS				37	10			7110	7123				10.1002/int.22874					MAR 2022		Article; Early Access		2022	The broad application of artificial intelligence (AI) shows more and more vulnerabilities. Adversaries have more opportunities to attack AI systems. For example, unmanned vehicles may be interfered with by adversaries in path planning, resulting in unmanned vehicles being unable to move according to the planned route, and even serious safety problems. On the other side, the portrait technology can extract highly refined characteristics of different attack strategies, so that unmanned vehicles can defend themselves based on the characteristics of each attack. Existing research lacks intelligent attack research on path planning in the field of unmanned vehicles, and lacks portraits of attack behaviors in this scenario. This paper combines multiagent reinforcement learning technology, time-series segmentation clustering technology, and knowledge graph technology to study the portrait technology of adversary intelligent attack behavior in the field of unmanned vehicle path planning. First, the simulation results of unmanned vehicle path planning are obtained, and the steps of adversary attack behavior are extracted by using Toeplitz inverse covariance-based clustering time-series segmentation cluster technology. Second, the knowledge graph is used to save the attack strategy, so as to form the attack behavior portrait of unmanned vehicle path planning. The test on the Neo4j platform shows that our method is universal, can effectively describe the attack steps for unmanned vehicle path planning, and provides the basis for attack detection to establish the defense system of unmanned vehicles.									3	0	0	0	0	0	3			0884-8173	1098-111X										Commun Univ Zhejiang, Dept Media Engn, Hangzhou, Peoples R ChinaBeijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R ChinaGuangzhou Univ, Inst Artificial Intelligence & Blockchain, Guangzhou, Peoples R ChinaShandong Univ Technol, Dept Comp Sci & Technol, Zibo, Peoples R China				2022-03-27	WOS:000770873400001		
J	Voogd, K.; Allamaa, J.P.; Alonso-mora, J.; Son, T.D.										Reinforcement Learning from Simulation to Real World Autonomous Driving using Digital Twin [arXiv]								arXiv																				Journal Paper	27 Nov. 2022	2022	Reinforcement learning (RL) is a promising solution for autonomous vehicles to deal with complex and uncertain traffic environments. The RL training process is however expensive, unsafe, and time consuming. Algorithms are often developed first in simulation and then transferred to the real world, leading to a common sim2real challenge that performance decreases when the domain changes. In this paper, we propose a transfer learning process to minimize the gap by exploiting digital twin technology, relying on a systematic and simultaneous combination of virtual and real world data coming from vehicle dynamics and traffic scenarios. The model and testing environment are evolved from model, hardware to vehicle in the loop and proving ground testing stages, similar to standard development cycle in automotive industry. In particular, we also integrate other transfer learning techniques such as domain randomization and adaptation in each stage. The simulation and real data are gradually incorporated to accelerate and make the transfer learning process more robust. The proposed RL methodology is applied to develop a path following steering controller for an autonomous electric vehicle. After learning and deploying the real-time RL control policy on the vehicle, we obtained satisfactory and safe control performance already from the first deployment, demonstrating the advantages of the proposed digital twin based learning process.									0	0	0	0	0	0	0																		2023-05-05	INSPEC:22979267		
J	Nantogma, Sulemana; Zhang, Shangyan; Yu, Xuewei; An, Xuyang; Xu, Yang					Nantogma, Sulemana/0000-0002-5103-2526					Multi-USV Dynamic Navigation and Target Capture: A Guided Multi-Agent Reinforcement Learning Approach								ELECTRONICS				12	7					1523			10.3390/electronics12071523							Article	APR 2023	2023	Autonomous unmanned systems have become an attractive vehicle for a myriad of military and civilian applications. This can be partly attributed to their ability to bring payloads for utility, sensing, and other uses for various applications autonomously. However, a key challenge in realizing autonomous unmanned systems is the ability to perform complex group missions, which require coordination and collaboration among multiple platforms. This paper presents a cooperative navigating task approach that enables multiple unmanned surface vehicles (multi-USV) to autonomously capture a maneuvering target while avoiding both static and dynamic obstacles. The approach adopts a hybrid multi-agent deep reinforcement learning framework that leverages heuristic mechanisms to guide the group mission learning of the vehicles. Specifically, the proposed framework consists of two stages. In the first stage, navigation subgoal sets are generated based on expert knowledge, and a goal selection heuristic model based on the immune network model is used to select navigation targets during training. Next, the selected goals' executions are learned using actor-critic proximal policy optimization. The simulation results with multi-USV target capture show that the proposed approach is capable of abstracting and guiding the unmanned vehicle group coordination learning and achieving a generally optimized mission execution.									0	0	0	0	0	0	0				2079-9292										Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R ChinaChina North Artificial Intelligence & Innovat Res, Beijing 100072, Peoples R ChinaNankai Univ, Coll Artificial Intelligence, Tianjin 300350, Peoples R China	China North Artificial Intelligence & Innovat Res			2023-05-06	WOS:000971798000001		
J	Luo, Jie; Wang, Zhong-Xun; Pan, Kang-Lu										Reliable Path Planning Algorithm Based on Improved Artificial Potential Field Method								IEEE ACCESS				10				108276	108284				10.1109/ACCESS.2022.3212741							Article	2022	2022	In order to solve the "minimum trap" of Artificial Potential Field and the limitation of traditional path planning algorithm in dynamic obstacle environment, a path planning algorithm based on improved artificial potential field is proposed. Firstly, a virtual potential field detection circle model (VPFDCM) with adjustable radius is proposed to detect the "minimum trap" formed by the repulsion field of obstacles in advance. And the motion model of unmanned vehicle is established. Combined with the improved reinforcement learning algorithm based on Long Short-Term Memory(LSTM), the radius of virtual potential field detection circle is adjusted to achieve effective avoidance of dynamic obstacles. The reliable online collision free path planning of unmanned vehicle in semi closed dynamic obstacle environment is realized. Finally, the reliability and robustness of the algorithm are verified by MATLAB simulation. The simulation results show that the improved artificial potential field can effectively solve the problem of unmanned vehicle falling into the "minimum trap" and improve the reliability of unmanned vehicle movement. Compared with the traditional artificial potential field method, the improved artificial potential field method can achieve more than 90% success rate in obstacle avoidance.									10	0	0	0	0	0	10			2169-3536											Yantai Univ, Sch Phys & Elect Informat, Yantai 264005, Peoples R China				2022-10-29	WOS:000870211300001		
J	Yoon, Yeomyung; Lee, Hojeong; Kim, Hyogon										Deep reinforcement learning-based dual-mode congestion control for cellular V2X environments								ELECTRONICS LETTERS				59	20					e12984			10.1049/ell2.12984							Article	OCT 2023	2023	The Society of Automotive Engineers (SAE) J2945/1 standard for Dedicated Short Range Communication (DSRC) environmentutilizes transmit (Tx) power control and rate control elements for the periodic BSM transmissions, which are intended to work in a complementary manner. An equivalent standard for the cellular vehicle-to-everything (C-V2X) communication environment is J3161/1, but it eliminates Tx power control and uses only rate control. However, the consequence is the degraded update delay of neighbouring vehicles' kinematics, potentially undermineing driving safety. In this Letter, the authors propose to retain the dual-mode control in the C-V2X environment and find a policy through reinforcement learning (RL) to adjust the rate control function to maintain synergy. Moreover, the authors can extract the RL-created policy from the neural network so that it can be explicitly specified in the standard, and downloaded and used more conveniently by vehicles. Finally, the RL-generated policy achieves a better packet delivery frequency than J2945/1 or J3161/1.Current V2X standards typically specify human-designed heuristic control algorithms. Their drawbacks are frequently discovered later by more thorough simulation experiments that were not done during standardization. This Letter shows that AI-generated control can be automatically obtained while running simulation, and better performing than heuristic algorithms, with the case of congestion control. Moreover, the policy can be extracted from the neural network and explicitly specified in the standard and used by vehicles instead of the neural network.image									0	0	0	0	0	0	0			0013-5194	1350-911X										Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea				2023-10-24	WOS:001080569200001		
J	Zielinski, Kallil M. C.; Hendges, Lucas, V; Florindo, Joao B.; Lopes, Yuri K.; Ribeiro, Richardson; Teixeira, Marcelo; Casanova, Dalcimar				Kaszubowski Lopes, Yuri/Z-4925-2019; Teixeira, Marcelo/I-2860-2014; Casanova, Dalcimar/J-1588-2012; Zielinski, Kallil Miguel Caparroz/AFU-7376-2022; Ribeiro, Richardson/AAX-1924-2021; Florindo, Joao/S-5823-2019	Kaszubowski Lopes, Yuri/0000-0002-4627-5590; Teixeira, Marcelo/0000-0002-1008-7838; Casanova, Dalcimar/0000-0002-1905-4602; Florindo, Joao/0000-0002-0071-0227; Ribeiro, Richardson/0000-0002-5630-5204; Zielinski, Kallil/0000-0001-9395-6287					Flexible control of Discrete Event Systems using environment simulation and Reinforcement Learning								APPLIED SOFT COMPUTING				111						107714			10.1016/j.asoc.2021.107714					JUL 2021		Article; Early Access		2021	Discrete Event Systems (DESs) are classically modeled as Finite State Machines (FSMs), and controlled in a maximally permissive, controllable, and nonblocking way using Supervisory Control Theory (SCT). While SCT is powerful to orchestrate events of DESs, it fail to process events whose control is based on probabilistic assumptions. In this research, we show that some events can be approached as usual in SCT, while others can be processed using Artificial Intelligence. We present a tool to convert SCT controllers into Reinforcement Learning (RL) simulation environments, from where they become suitable for intelligent processing. Then, we propose a RL-based approach that recognizes the context under which the selected set of stochastic events occur, and treats them accordingly, aiming to find suitable decision making as complement to deterministic outcomes of the SCT. The result is an efficient combination of safe and flexible control, which tends to maximize performance for a class of DES that evolves probabilistically. Two RL algorithms are tested, State-Action-Reward-State-Action (SARSA) and N-step SARSA, over a flexible automotive plant control. Results suggest a performance improvement 9 times higher when using the proposed combination in comparison with non-intelligent decisions. (C) 2021 Elsevier B.V. All rights reserved.									10	0	0	0	0	0	10			1568-4946	1872-9681										Univ Tecnol Fed Parana, Grad Program Elect Engn, Curitiba, PR, BrazilUniv Estadual Campinas, Inst Math Stat & Sci Comp, Campinas, Brazil				2021-11-08	WOS:000711075800005		
J	Xue, Lei; Ma, Bei; Liu, Jian; Mu, Chaoxu; Wunsch, Donald C.					Mu, Chaoxu/0000-0003-1055-9513					Extended Kalman Filter Based Resilient Formation Tracking Control of Multiple Unmanned Vehicles via Game-Theoretical Reinforcement Learning								IEEE TRANSACTIONS ON INTELLIGENT VEHICLES				8	3			2307	2318				10.1109/TIV.2023.3237790							Article	MAR 2023	2023	In this paper, we discuss the resilient formation tracking control problem of multiple unmanned vehicles (MUV). A dynamic leader-follower distributed control structure is utilized to optimize the performance of the formation tracking. For the follower of the MUV, the leader is a cooperative unmanned vehicle, and the target of formation tracking is a non-cooperative unmanned vehicle with a nonlinear trajectory. Therefore, an extended Kalman filter (EKF) observer is designed to estimate the state of the target. Then the leader of the MUV is adjusted dynamically according to the state of the target. In order to describe the interactions between the follower and dynamic leader, a Stackelberg game model is constructed to handle the hierarchical decision problems. At the lower layer, each follower responds by observing the leader's strategy, and the potential game is used to prove a Nash equilibrium among all followers. At the upper layer, the dynamic leader makes decisions depending on the response of all followers to reaching the Stackelberg equilibrium. Moreover, the Stackelberg-Nash equilibrium of the designed game theoretical model is proven. A novel reinforcement learning-based algorithm is designed to achieve the Stackelberg-Nash equilibrium of the game. Finally, the effectiveness of the method is verified by a variety of formation tracking simulation experiments.									2	0	0	0	0	0	2			2379-8858	2379-8904										Southeast Univ, Sch Automat, Key Lab Measurement & Control Complex Syst Engn, Minist Educ, Nanjing 210096, Peoples R ChinaTianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R ChinaMissouri Univ Sci & Technol, Dept Elect & Comp Engn, Rolla, MO 65409 USA				2023-06-12	WOS:000981348100027		
P	CHEN L; GAO S; CUI L; WANG T										Underwater unmanned vehicle safety opportunity            routing method on reinforcement learning by using            computer device, involves using underwater unmanned            aircraft to primary filter node, and setting            state-action value updating function					CN114025405-A; CN114025405-B	UNIV HARBIN ENGINEERING																									NOVELTY - The method involves using an underwater unmanned aircraft to primary filter node in a communication range. A trust evaluation model is established according to the node of primary filter. The node of the preliminary filter is evaluated by using the trust evaluation model. An evaluation element of evaluation model is composed of two parts of direct trust value DTValue and indirect trust value ITValue. An element input fuzzy logic system is evaluated to obtain evaluation node comprehensive trust value. The node comprehensive trust value updated to a meeting node trust value dynamic table is evaluated. A strengthening learning is used to perform routing selection. A state-action value updating function and a reward function are set. USE - The method is useful for underwater unmanned vehicle safety opportunity routing on reinforcement learning by using a computer device (claimed). ADVANTAGE - The method: improves the network performance of underwater unmanned vehicle networking; reduces the delay of underwater information transmission; increases the delivery rate of messages; and realizes safety and high efficiency of information transmission of unmanned aerial vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an underwater unmanned vehicle safety opportunity routing device on reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows an implementation process of underwater unmanned vehicle safety opportunity routing method on reinforcement learning (Drawing includes non-English language text).															0																		2022-03-24	DIIDW:202228492E		
P	HAO M; WU H; WANG Y; WANG J; JIAO R										Method for controlling intelligent unmanned aerial            vehicle formation maintaining based on deep            reinforcement learning, involves using            proportional-integral-derivative cascade controller to            receive control instruction to operate unmanned vehicle            for completing formation and maintenance					CN116820134-A	ZHIYANG INNOVATION TECHNOLOGY CO LTD																									NOVELTY - The method involves establishing an unmanned aerial vehicle flying dynamics model and a kinematics model according to flying mechanics principle. An unmanned vehicle relative motion model is designed according to a virtual long machine topology structure. A proportional-integral-derivative (PID) cascade controller of a stable-attitude-track of an unmanned vehicle is designed. Markov decision process of a MAPPO intelligent body of the unmanned vehicle is designed. Input of an intelligent agent is provided with a state space. Output of the intelligent agent is contained with an unmanned vehicle control instruction. The PID cascade controller is used to receive the control instruction to operate the unmanned vehicle for completing formation and maintenance. USE - Method for controlling an intelligent unmanned aerial vehicle formation maintaining based on deep reinforcement learning in different fields e.g. robot, game, finance and traffic. ADVANTAGE - The method enables improving the intelligence, robustness and accuracy of the unmanned aerial vehicle formation. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram illustrating a method for controlling an intelligent unmanned aerial vehicle formation maintaining based on deep reinforcement learning. (Drawing includes non-English language text).															0																		2023-10-26	DIIDW:2023A5350A		
P	DAI B; ZHAO D; XIAO L; NIE Y; SHANG E; WANG X										Depth reinforcement learning and A-star search            algorithm based method for planning path of unmanned            vehicles, involves driving unmanned vehicle in off-road            environment and collecting three-dimensional point            cloud data, and utilizing path traveled by unmanned            vehicle as path planning result					CN115933629-A	CHINESE PEOPLES LIBERATION ARMY ACAD																									NOVELTY - The method involves driving an unmanned vehicle in an off-road environment and collecting three-dimensional (3D) point cloud data. A path traveled by the unmanned vehicle is recorded. Multiple grid obstacle maps are generated from the point cloud data. Target points are generated in the current grid map from the path traveled by the unmanned vehicle. The grid obstacle maps are utilized to represent a simulation environment constructed by the rectangle of the unmanned vehicle. An A-star algorithm is utilized to generate a guiding path from each obstacle map. The guidance path is utilized to design a reward function. Data augmentation and curriculum learning are realized to train a unmanned vehicle agent in the simulation environment using a deep neural network. The path traveled by the unmanned vehicle is utilized as a path planning result of the corresponding obstacle map. USE - Depth reinforcement learning and A-star search algorithm based method for planning path of unmanned vehicles. ADVANTAGE - The method enables realizing path planning of unmanned vehicles of different off-road scenes and different sizes. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a depth reinforcement learning and A-star search algorithm based method for planning path of unmanned vehicles. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202350062J		
J	Yao, Jialing; Ge, Zhen					Yao, Jialing/0000-0002-0474-9858					Path-Tracking Control Strategy of Unmanned Vehicle Based on DDPG Algorithm								SENSORS				22	20					7881			10.3390/s22207881							Article	OCT 2022	2022	This paper proposes a deep reinforcement learning (DRL)-based algorithm in the path-tracking controller of an unmanned vehicle to autonomously learn the path-tracking capability of the vehicle by interacting with the CARLA environment. To solve the problem of the high estimation of the Q-value of the DDPG algorithm and slow training speed, the controller adopts the deep deterministic policy gradient algorithm of the double critic network (DCN-DDPG), obtains the trained model through offline learning, and sends control commands to the unmanned vehicle to make the vehicle drive according to the determined route. This method aimed to address the problem of unmanned-vehicle path tracking. This paper proposes a Markov decision process model, including the design of state, action-and-reward value functions, and trained the control strategy in the CARLA simulator Town04 urban scene. The tracking task was completed under various working conditions, and its tracking effect was compared with the original DDPG algorithm, model predictive control (MPC), and pure pursuit. It was verified that the designed control strategy has good environmental adaptability, speed adaptability, and tracking performance.									9	0	0	0	0	0	9				1424-8220										Nanjing Forestry Univ, Sch Automot & Transportat Engn, Nanjing 210037, Peoples R China				2022-11-04	WOS:000873357000001	36298232	
J	Jang, Seowoo; Yoo, Soyoung; Kang, Namwoo										Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs								COMPUTER-AIDED DESIGN				146						103225			10.1016/j.cad.2022.103225					FEB 2022		Article; Early Access		2022	Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention. (c) 2022 Elsevier Ltd. All rights reserved.									13	0	0	0	0	0	13			0010-4485	1879-2685										Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South KoreaSookmyung Womens Univ, Dept Mech Syst Engn, Seoul, South KoreaKorea Adv Inst Sci & Technol, Cho Chun Shik Grad Sch Green Transportat, Daejeon, South Korea				2022-04-28	WOS:000781652200004		
J	Levin, J.; Correll, R.; Ide, T.; Suzuki, T.; Saito, T.; Arai, A.										Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes [arXiv]								Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes [arXiv]																				Preprint	2024	2024	Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number of trucks and nodes, and then embedded into a large supply chain to yield solutions for larger numbers of trucks and nodes. We test our approach on a real supply chain environment arising in the operations of Japanese automotive parts manufacturer Aisin Corporation, and find that our algorithm outperforms Aisin's previous best solution.									0	0	0	0	0	0	0																		2024-02-08	INSPEC:24464038		
P	SUN L; QIN W; ZHAI J; DI J										deep learning based reinforcement vehicle speed            decision method, involves constructing deep            reinforcement learning structure of Actor-Critic frame,            and performing tracking of vehicle at low speed running            under city congestion condition					CN109213148-A; CN109213148-B	UNIV SOUTHEAST																									NOVELTY - The method involves receiving position, speed and acceleration information of front and rear unmanned vehicle as an environment state by using a network in real-time. deep reinforcement learning structure of an Actor-Critic frame is constructed by using environment state and current state of the unmanned vehicle. Training process is performed on the deep reinforcement learning structure to update a Critic network parameter and an Actor network parameter such that certain safe distance is established between the unmanned vehicle and front and rear vehicles. Automatic tracking of the unmanned vehicle is performed at low speed running under city congestion condition. USE - deep learning based reinforcement vehicle speed decision method. ADVANTAGE - The method enables increasing comfort degree of driving, improving traffic safety and smoothness of traffic lane and decision process. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based reinforcement vehicle speed decision method. '(Drawing includes non-English language text)'															0																		2019-03-29	DIIDW:201907935R		
P	YANG L; WANG Y; REN F; LIU J; WANG L										Deep reinforcement learning based intelligent            unmanned vehicle driving end-to-end decision-making            method, involves constructing deep reinforcement            network, and outputting value of action network when            target network operation is completed					CN113104050-A; CN113104050-B	UNIV TIANJIN TECHNOLOGY																									NOVELTY - The method involves constructing and training a deep reinforcement learning network, where the deep reinforcement learning network comprises an activity network, an Eval action network and a Target Actor network. Training or Eval action network action values are output when target action network operation is completed. Current time environment state is received by the Eval action network. Discrete space action is output. Multiple groups of samples are adjusted from a playback experience pool, where the current time environment state comprises front road environment state and vehicle self state. The front road environment state is a front road characteristic code, where the vehicle state comprises a vehicle driving speed, steering wheel angle, accelerator pedal opening degree and brake pedal opening degree. USE - Deep reinforcement learning based intelligent unmanned vehicle driving end-to-end decision-making method. ADVANTAGE - The method enables requiring less environment data so as to effectively reduce cost, constructing a deep strengthening learning network with high learning efficiency, and increasing training speed and improving exploration efficiency of the intelligent body. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep reinforcement learning based intelligent unmanned vehicle driving end-to-end decision-making method. (Drawing includes non-English language text).															0																		2022-01-08	DIIDW:2021815943		
J	Huang Yuzhou; Wang Lisong; Qin Xiaolin							黄昱洲; 王立松; 秦小麟			Bi-level Path Planning Method for Unmanned Vehicle Based on Deep Reinforcement Learning			一种基于深度强化学习的无人小车双层路径规划方法				计算机科学	Computer Science				50	1			194	204	1002-137X(2023)50:1<194:YZJYSD>2.0.TX;2-W										Article	2023	2023	With the wide application of intelligent unmanned vehicles,intelligent navigation,path planning and obstacle avoidance technology have become important research contents.This paper proposes model-free deep reinforcement learning algorithms DDPG and SAC,which use environmental information to track to the target point,avoid static and dynamic obstacles,and can be generally suitable for different environments.Through the combination of global planning and local obstacle avoidance,it solves the path planning problem with better globality and robustness,solves the obstacle avoidance problem with better dynamicity and generalization,and shortens the iteration time.In the network training stage,PID,A* and other traditional algorithms are combined to improve the convergence speed and stability of the method.Finally,a variety of experimental scenarios such as navigation and obstacle avoidance are designed in the robot operating system ROS and the simulation program gazebo.Simulation results verify the reliability of the proposed approach,which takes the global and dynamic nature of the problem into account and optimizes the generated paths and time efficiency.			随着智能无人小车的广泛应用,智能化导航、路径规划和避障技术成为了重要的研究内容。文中提出了基于无模型的DDPG和SAC深度强化学习算法,利用环境信息循迹至目标点,躲避静态与动态的障碍物并且使其普适于不同环境。通过全局规划和局部避障相结合的方式,该方法以更好的全局性与鲁棒性解决路径规划问题,以更好的动态性与泛化性解决避障问题,并缩短了迭代时间;在网络训练阶段结合PID和A*等传统算法,提高了所提方法的收敛速度和稳定性。最后,在机器人操作系统ROS和仿真程序gazebo中设计了导航和避障等多种实验场景,仿真实验结果验证了所提出的兼顾问题全局性和动态性的方法具有可靠性,生成的路径和时间效率有所优化。						0	0	0	0	0	0	0			1002-137X											南京航空航天大学计算机科学与技术学院, 南京, 江苏 211106, 中国College of Computer Science and Technology,Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu 211106, China	南京航空航天大学计算机科学与技术学院College of Computer Science and Technology,Nanjing University of Aeronautics and Astronautics			2023-06-02	CSCD:7400834		
J	Yudin, D. A.; Skrynnik, A.; Krishtopik, A.; Belkin, I; Panov, A., I				Panov, Aleksandr I./L-9171-2013; Yudin, Dmitry/G-4347-2011	Panov, Aleksandr I./0000-0002-9747-3837; Yudin, Dmitry/0000-0002-1407-2633; Belkin, Ilya/0000-0002-5901-0137					Object Detection with Deep Neural Networks for Reinforcement Learning in the Task of Autonomous Vehicles Path Planning at the Intersection								OPTICAL MEMORY AND NEURAL NETWORKS				28	4			283	295				10.3103/S1060992X19040118							Article	OCT 2019	2019	Among a number of problems in the behavior planning of an unmanned vehicle the central one is movement in difficult areas. In particular, such areas are intersections at which direct interaction with other road agents takes place. In our work, we offer a new approach to train of the intelligent agent that simulates the behavior of an unmanned vehicle, based on the integration of reinforcement learning and computer vision. Using full visual information about the road intersection obtained from aerial photographs, it is studied automatic detection the relative positions of all road agents with various architectures of deep neural networks (YOLOv3, Faster R-CNN, RetinaNet, Cascade R-CNN, Mask R-CNN, Cascade Mask R-CNN). The possibilities of estimation of the vehicle orientation angle based on a convolutional neural network are also investigated. Obtained additional features are used in the modern effective reinforcement learning methods of Soft Actor Critic and Rainbow, which allows to accelerate the convergence of its learning process. To demonstrate the operation of the developed system, an intersection simulator was developed, at which a number of model experiments were carried out.									18	1	0	0	0	0	19			1060-992X	1934-7898										Natl Res Univ, Moscow Inst Phys & Technol, Moscow 141701, RussiaRussian Acad Sci, Fed Res Ctr Comp Sci & Control, Artificial Intelligence Res Inst, Moscow 119333, Russia				2019-10-01	WOS:000704695800005		
B	Qiao, Zhiqian										Reinforcement Learning for Behavior Planning of Autonomous Vehicles in Urban Scenarios																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798538168347									Carnegie Mellon University, Electrical and Computer Engineering, Pennsylvania, United States	Carnegie Mellon University				PQDT:64663684		
C	Biswas, Atriya; Anselma, Pier G.; Emadi, Ali			IEEE	Anselma, Pier Giuseppe/AAP-1275-2021; emadi, ali/AAL-3732-2020	Anselma, Pier Giuseppe/0000-0002-8646-4310; 					Real-Time Optimal Energy Management of Electrified Powertrains with Reinforcement Learning								2019 IEEE TRANSPORTATION ELECTRIFICATION CONFERENCE AND EXPO (ITEC)		IEEE Transportation Electrification Conference and Expo											10.1109/itec.2019.8790482							Proceedings Paper	2019	2019	Reinforcement learning (RL) algorithm is employed in solving energy management problem for electrified powertrain in real-world driving scenarios and the application process is streamlined. A near-global optimal control policy is articulated for the energy management system (EMS) using Q-learning algorithm which is real-time implementable. The core of the EMS is an updating optimal control policy in the form of a changing look-up table comprising near-global optimal action value function (Q-values) corresponding to all feasible state-action combinations. Using the updating control policy, the EMS can optimally decide power-split between electric machines (EMs) and internal combustion engine (ICE) in real-world driving situations.					IEEE Transportation Electrification Conference and Expo (ITEC)IEEE Transportation Electrification Conference and Expo (ITEC)	JUN 19-21, 2019JUN 19-21, 2019	IEEE; IEEE Power Elect Soc; IEEE Power & Energy Soc; IEEE Ind Applicat SocIEEE; IEEE Power Elect Soc; IEEE Power & Energy Soc; IEEE Ind Applicat Soc	Detroit, MIDetroit, MI	12	0	0	0	0	0	12			2377-5483	2473-7631	978-1-5386-9310-0									McMaster Univ, MARC, Hamilton, ON, CanadaPolitecn Torino, Dept Mech & Aerosp Engn DIMEAS, Turin, Italy				2019-12-30	WOS:000502391500035		
J	Liu, Pengfei; Liu, Yimin; Huang, Tianyao; Lu, Yuxiang; Wang, Xiqin				Huang, Tianyao/ABI-3871-2020	Liu, Pengfei/0000-0002-1292-6148					Decentralized Automotive Radar Spectrum Allocation to Avoid Mutual Interference Using Reinforcement Learning								IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS				57	1			190	205				10.1109/TAES.2020.3011869							Article	FEB 2021	2021	Nowadays, mutual interference among automotive radars has become a problem of wide concern. In this article, a decentralized spectrum allocation approach is presented to avoid mutual interference among automotive radars. Although decentralized spectrum allocation has been extensively studied in cognitive radio sensor networks, two challenges are observed for automotive sensors using radar. First, the allocation approach should be dynamic as all radars are mounted on moving vehicles. Second, each radar does not communicate with the others so it has quite limited information. A machine learning technique, reinforcement learning, is utilized because it can learn a decision-making policy in an unknown dynamic environment. As a single radar observation is incomplete, a long short-term memory recurrent network is used to aggregate radar observations through time so that each radar can learn to choose a frequency subband by combining both the present and past observations. Simulation experiments are conducted to compare the proposed approach with other common spectrum allocation methods such as the random and myopic policy, indicating that our approach outperforms the others.									22	3	0	0	0	0	24			0018-9251	1557-9603										Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R ChinaBeijing Radar Res Inst, Beijing 100015, Peoples R China	Beijing Radar Res Inst			2021-03-31	WOS:000617426500015		
J	Wang, Y.; Chen, Q.; Ma, H.; Jiang, Y.										Research on intelligent combat decision making based on deep reinforcement learning								Proceedings of SPIE								128032D (9 pp.)	128032D (9 pp.)				10.1117/12.3009215							Conference Paper; Journal Paper	2023	2023	With the operational advantages of unmanned combat platforms in modern war gradually appearing, the research of unmanned combat platforms has become the focus of all circles. In order to realize intelligent and autonomous unmanned operation in a real sense, a combat mission computer based on AI development board was proposed to be built as the control core of unmanned vehicles, simulate the operational mobility situation diagram of unmanned vehicles, and use the deep reinforcement learning network DQN to establish angle and distance decision-making network, so as to realize intelligent mobility decision-making of unmanned vehicles. The experiment verified that the unmanned vehicle can maneuver to the target area autonomously, which proved that the deep reinforcement learning network can realize the feasibility of platform autonomous and intelligent decision-making, and provided a feasible technical approach and theoretical support for the construction of combat mission computer to realize intelligent, autonomous and unmanned combat in a real sense.					Fifth International Conference on Artificial Intelligence and Computer Science (AICS 2023)Fifth International Conference on Artificial Intelligence and Computer Science (AICS 2023)	20232023		Wuhan, ChinaWuhan, China	0	0	0	0	0	0	0			1996-756X											Naval Aviation Univ.32018 Units, China				2024-02-01	INSPEC:24412217		
P	HUANG Z; QU Z										Deep reinforcement learning based unmanned layered            motion decision control method, involves controlling            target driving behavior of unmanned vehicle to finish            target driving behavior when meta-action instruction is            changed					CN113264043-A	UNIV BEIJING TECHNOLOGY																									NOVELTY - The method involves receiving a specific driving action instruction by a meta-action decision-making layer. A series of meta-action instruction is output to a vehicle control layer. A basic task of the vehicle control layer is performed to maintain a lane keeping driving state of an unmanned vehicle. A vehicle running state is changed according to the meta-action instruction when the meta-action instruction is received. A target driving behavior of the unmanned vehicle is controlled to finish the target driving behavior when the meta-action instruction is changed. USE - Deep reinforcement learning based unmanned layered motion decision control method. ADVANTAGE - The method enables avoiding discrete decision problem by abstract decomposition of driving behavior and environmental factor analysis. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep reinforcement learning based unmanned layered motion decision control method. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2021994041		
B	Huang, Zhe										Distributed Reinforcement Learning for Autonomous Driving																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798374404128									Carnegie Mellon University, Robotics Institute, Pennsylvania, United States	Carnegie Mellon University				PQDT:68300582		
B	Chen, J.; Shi, Z.; Wang, X.; Wu, J.										UBER: An Unreal Engine Based Simulation Platform with Extensibility and Real-time Capability*								2023 IEEE International Conference on Real-time Computing and Robotics (RCAR)								322	7				10.1109/RCAR58764.2023.10249599							Conference Paper	2023	2023	In this paper, we propose a real-time platform called UBER, which stands for Unreal Engine Based simulation platform with Extensibility and Real-time capability. It provides a visualization way to the online tests of unmanned vehicle simulation with Unreal Engine 5 (UE5). By building a TCP communication module to exchange message between two ends in UBER, we successfully solve the problem that no code script can be operated in blueprints of UE5. Excellent physics components in UE5 makes simulation visualization more real and beautiful compared with other Reinforcement Learning environments, and by accessing the python scripts outside of blueprints, various of elements, such as scenario components and environmental settings, etc., can be easily modified. Furthermore, a series of reinforcement learning and formation control algorithms are implemented on UBER to show its extensibility and real-time capability, proving that our TCP communication method brings forth a good solution to the problem that UE5 does not support python scripts within blueprint.					2023 IEEE International Conference on Real-time Computing and Robotics (RCAR)2023 IEEE International Conference on Real-time Computing and Robotics (RCAR)	20232023	Cyborg and Bionic Systems; Beijing NOKOV Science & Technology Co.; Shanghai Society of Aeronautics; Galleon (Shanghai) Consulting Co., LtdCyborg and Bionic Systems; Beijing NOKOV Science & Technology Co.; Shanghai Society of Aeronautics; Galleon (Shanghai) Consulting Co., Ltd	Datong, ChinaDatong, China	0	0	0	0	0	0	0					979-8-3503-2718-2									Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, ChinaDept. of Civil Eng., Shanghai Jiao Tong Univ., Shanghai, ChinaDept. of Microelectron. Sci. & Eng., Shanghai Jiao Tong Univ.Dept. of Autom., Shanghai Jiao Tong Univ., Shanghai, China				2023-10-06	INSPEC:23747402		
P	HU W; XU Q; ZHANG Y; WANG G										Deep reinforcement learning based ground unmanned            vehicle autonomous driving method, involves analyzing            driving environment image by learning neural network,            and executing vehicle action by control module until            reaching ending state					CN113552883-A	UNIV JILIN																									NOVELTY - The method involves obtaining a driving environment image and a semantic segmentation environment image. Pixel information in the driving environment image is analyzed. A standard reference path is automatically generated according to a road pixel. The driving environment image is pre-processed. The pre-processed driving environment image is conveyed into a deep reinforcement learning neural network. The driving environment image is analyzed by the learning neural network for predicting vehicle action to obtain the intelligent decision result. The pre-processed driving environment image, the predicted vehicle action, a driving state score and a current driving state are stored in an experience pool module. Reverse propagation is performed to the learning neural network. Neural network parameter is adjusted. Control instruction is transmitted to a vehicle model control module. The vehicle action is executed by the vehicle model control module until reaching an ending state. USE - Deep reinforcement learning based ground unmanned vehicle autonomous driving method. ADVANTAGE - The method enables obtaining the ideal actual driving track under the complex road condition and different vehicle models. The method enables realizing environment perception-intelligent decision-local path planning-vehicle action, simplifying the calculation process, reducing the calculation amount, and improving the precision of the calculation, performing the unmanned monitoring training for deep reinforcement learning neural network for autonomous driving. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep reinforcement learning based ground unmanned vehicle autonomous driving system.															0																		2021-11-25	DIIDW:2021C3630D		
P	ZHENG X; LI S; CHEN T; LUO X										Method for controlling constrained hybrid vehicle            formation based on deep reinforcement learning            strategy, involves executing, circularly reciprocating            and completing autonomous training of hybrid vehicle            formation longitudinal queue model under safety            constraint					CN116382297-A	UNIV YANSHAN																									NOVELTY - The method involves establishing a third-order nonlinear dynamics model of a manned vehicle and a third-order nonlinear engine model of an unmanned vehicle. An updated actor-reviewer network is taken as a target network. Weight parameters of the target network are updated by adopting deterministic strategy gradient to obtain an optimal action value function. Optimal control strategy is input into the unmanned vehicle of a hybrid vehicle formation longitudinal queue model. A state of the unmanned vehicle of the hybrid vehicle formation longitudinal queue model is updated. Autonomous training of the hybrid vehicle formation longitudinal queue model is executed, circularly reciprocated and completed under safety constraint. USE - Method for controlling constrained hybrid vehicle formation based on a deep reinforcement learning strategy. ADVANTAGE - The method enables inputting the optimal control strategy into the unmanned vehicle of the hybrid vehicle formation longitudinal queue model, and updating the state, thus solving problem of autonomous training of the hybrid vehicle formation on premise of safety. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating a method for controlling constrained hybrid vehicle formation based on a deep reinforcement learning strategy.															0																		2023-08-10	DIIDW:202373297U		
B	Nagasai, L.S.; Sriprasath, V.J.; SajithVariyar, V.V.; Sowmya, V.; Aniketh, K.; Sarath, T.V.; Soman, K.P.						Reddy, V.S.; Prasad, V.K.; Jiacun Wang; Reddy, K.T.V.				Electric Vehicle Steering Design and Automated Control Using CNN and Reinforcement Learning								Soft Computing and Signal Processing. Proceedings of 3rd ICSCSP 2020. Advances in Intelligent Systems and Computing (AISC 1325)								513	23				10.1007/978-981-33-6912-2_46							Conference Paper	2021	2021	Autonomous vehicles are one of the engrossing technological trends in the present automotive industry. These vehicles enticed substantial attention in industry as well as in academia. With the rising trend in research and development of autonomous vehicles, it is important to keep in mind the safety, control, and cost effectiveness of the system. The cost and implementation of self-driving technologies hinder the development of similar systems in academia and research. In this paper, we are mainly focused on developing a vision system, an automated steering system in an electric vehicle platform for academia and research. The developed system has a provision to incorporate deep learningConvolutional neural network (CNN) and reinforcement learning (RL) for automated steering control. The proposed automated steering model uses end-to-end learning and reinforcement learning for predicting the steering angles with at most 85% accuracy and control the steering.					Soft Computing and Signal Processing. 3rd ICSCSP 2020Soft Computing and Signal Processing. 3rd ICSCSP 2020	21-22 Aug. 202021-22 Aug. 2020		Hyderabad, IndiaHyderabad, India	1	0	0	0	0	0	1					978-981-33-6911-5									Sch. of Eng., Dept. of EEE, Amrita Vishwa Vidyapeetham, Coimbatore, IndiaCenter for Comput. Eng. & Networking (CEN), Coimbatore, IndiaDept. of Electron. & Commun. Eng., Malla Reddy Coll. of Eng. & Technol., Secunderabad, IndiaDept. of Comput. Sci. & Eng., Jawaharlal Nehru Technol. Univ. Hyderabad, Hyderabad, IndiaDept. Comput. Sci. & Software Eng., Monmouth Univ., West Long Branch, NJ, USADept. of Electron. & Commun. Eng., Sir Visvesvaraya Inst. of Technol., Nashik, India				2021-08-20	INSPEC:20819244		
B	Huang, J.; Tan, Q.; Ma, J.; Han, L.										Path planning method using dyna-q algorithm under complex urban environment								2022 China Automation Congress (CAC)								6776	81				10.1109/CAC57257.2022.10054800							Conference Paper	2022	2022	Path planning and obstacle avoidance problems are now the focus of robotics research. This paper uses the Dyna-Q reinforcement learning algorithm to implement an obstacle avoidance and a path planning algorithm for unmanned ground vehicle(UGV) under urban environment. Using the reinforcement learning algorithm, we calculate the waypoints of the unmanned vehicle and achieve obstacle avoidance tasks and path planning using a vector field. Finally, we use a PID controller on unmanned aerial vehicle (UAV) to realize the air-ground collaboration task. The algorithms and the agents' modeling in this paper are implemented in the lab's simulation platform.					2022 China Automation Congress (CAC)2022 China Automation Congress (CAC)	25-27 Nov. 202225-27 Nov. 2022		Xiamen, ChinaXiamen, China	0	0	0	0	0	0	0					978-1-6654-6533-5									Sino-French Eng. Sch., Beihang Univ., Beijing, ChinaChina Acad. of Launch Vehicle Technol., Beijing, ChinaInst. Yuhang, Beihang Univ., Hangzhou, China				2023-03-30	INSPEC:22777483		
P	ZANG Z; LV C; ZUO Y; WEI L; LI Z; GONG J										Ground unmanned vehicle chassis movement and            target strike cooperative control method, involves            using sensor information of unmanned vehicle on ground            as input, using trained enhanced learning parameter            model for cooperative control					CN113705115-A; CN113705115-B	BITIV BEIJING TECHNOLOGY CO LTD; BEIJING INST TECHNOLOGY																									NOVELTY - The method involves building a simulation scene corresponding to a real vehicle environment. A strengthening learning parameter model is built. The strengthening learning parameters model is provided with a full connection layer, a state value network, an action value network and a dropout network. The reinforcement learning parameters are trained and tested by using the simulation scenario to obtain a trained reinforcement learning model. The sensor information of the unmanned vehicle on the ground is used as an input. The trained reinforced learning parameters is used for cooperative control of the movement and target attack of the ground unmanned vehicle chassis. USE - Ground unmanned vehicle chassis movement and target striking cooperative control method for ground unmanned vehicle i.e., unmanned mining vehicle, unmanned fire truck, and unmanned special vehicle. ADVANTAGE - The method realizes ground unmanned vehicle chassis movement and target striking collaborative control, realizes cooperation of an autonomous mobile module and an autonomous task module, shortens finishing time of task and improves task execution effect. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a ground unmanned vehicle chassis movement and target striking collaborative control system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating a ground unmanned vehicle chassis movement and target striking cooperative control method (Drawing includes non-English language text).Specifically implementing step (101)															0																		2023-08-10	DIIDW:2021E2576T		
P	YU Y; ZHAN D; ZHOU Z; YU F; CHEN X; LUO F; ZHANG Y; GUAN C										Method for keeping unmanned lane keeping based on            maximum entropy reinforcement learning framework,            involves selecting action with largest probability            value for unmanned vehicle to execute for trained            strategy model according to action probability            distribution outputted by network in use phase					CN113276852-A; CN113276852-B	UNIV NANJING																									NOVELTY - The method involves creating an unmanned vehicle simulation environment. An interaction is made with an environment, and a sample data is collected and stored in a buffer pool by an unmanned vehicle. A random strategy is used to sample from the buffer pool to update a state value function network, action value function network and strategy network. A soft update method is used to update a target state value function network. The collecting to using steps are repeated until the strategy network is about to converge. An entropy coefficient is set in an optimization objective of the state value network to zero and a training is continued until the strategy network converges completely. An action with a largest probability value is selected for the unmanned vehicle to execute for the trained strategy model according to an action probability distribution outputted by the network in the use phase. USE - Method for keeping unmanned lane keeping based on maximum entropy reinforcement learning framework. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the frame of method for keeping unmanned lane keeping based on maximum entropy reinforcement learning framework. (Drawing includes non-English language text)															0																		2022-01-08	DIIDW:202199737X		
P	GAO M										Method for controlling unmanned vehicle by            electronic device, involves mapping area of target            vehicle with grid map, and obtaining control            instruction of target vehicle by input deep            pre-training reinforcement learning network					CN110658820-A	BEIJING JINGDONG QIANSHI TECHNOLOGY CO																									NOVELTY - The method involves mapping an area of a target vehicle with a grid map. A position of the target vehicle is located in the grid map. State information of the area of the target vehicle is obtained. A control instruction of the target vehicle is obtained by an input deep pre-training reinforcement learning network. The target vehicle is controlled according to the control instruction, where the control instruction comprises a forward command or brake command. The area of the target vehicle is divided into a set of grid units. USE - Method for controlling an unmanned vehicle by an electronic device (claimed). ADVANTAGE - The method enables controlling the unmanned vehicle in safe and efficient manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) an unmanned vehicle control device(2) an electronic device(3) a computer-readable storage medium storing a set of instructions for controlling an unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for controlling an unmanned vehicle by an electronic device. '(Drawing includes non-English language text)'															0																		2020-01-31	DIIDW:202007541B		
J	Lee, Sanghoon; Kim, Jinyoung; Wi, Gwangjin; Won, Yuchang; Eun, Yongsoon; Park, Kyung-Joon					Won, Yuchang/0000-0001-7115-9658; Wi, Gwangjin/0000-0002-0152-002X; Lee, sanghoon/0000-0002-8160-8952; Park, Kyung-Joon/0000-0003-4807-6461					Deep Reinforcement Learning-Driven Scheduling in Multijob Serial Lines: A Case Study in Automotive Parts Assembly								IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS													10.1109/TII.2023.3292538					AUG 2023		Article; Early Access		2023	Multijob production (MJP) is a class of flexible manufacturing systems, which produces different products within the same production system. MJP is widely used in product assembly, and efficient MJP scheduling is crucial for productivity. Most of the existing MJP scheduling methods are inefficient for multijob serial lines with practical constraints. We propose a deep reinforcement learning (DRL)-driven scheduling framework for multijob serial lines by properly considering the practical constraints of identical machines, finite buffers, machine breakdown, and delayed reward. We analyze the starvation and the blockage time, and derive a DRL-driven scheduling strategy to reduce the blockage time and balance the loads. We validate the proposed framework by using real-world factory data collected over six months from a tier-one vendor of a world top-three automobile company. Our case study shows that the proposed scheduling framework improves the average throughput by 24.2% compared with the conventional approach.									0	0	0	0	0	0	0			1551-3203	1941-0050										Daegu Gyeongbuk Inst Sci & Technol, Dept Elect Engn & Comp Sci, Daegu 42988, South Korea				2023-11-24	WOS:001095722000002		
P	HAN J; WU X; REN L; CHEN J; ZENG X; ZHANG F; YANG C; LIU J										Training method for unmanned ship track generation            model based on graph neural network and deep            reinforcement learning, involves giving priority to            selecting experiences with high sampling probability to            train unmanned boat track generation model					CN116952235-A	UNIV HUAZHONG SCI & TECHNOLOGY																									NOVELTY - The training method involves building a track generation scene, which includes unmanned boats, moving obstacles and target locations. The environment in the scene, the actions performed by the unmanned boat, rewards, and the environment are set after the unmanned boat performs the actions as parameters of the deep reinforcement learning method. A memory pool is constructed with a fixed capacity to store experience and experience sampling probability. The experience includes the environment at a certain moment, the actions performed by the unmanned vehicle, rewards and the environment after the unmanned vehicle performs the action. An unmanned ship track generation model is constructed and trained. The priority is given to selecting experiences with high sampling probability to train the unmanned boat track generation model. USE - Training method for an unmanned ship track generation model based on graph neural network and deep reinforcement learning. ADVANTAGE - The method effectively improves the success rate of track planning. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a unmanned boat track generation method based on graph neural network and deep reinforcement learning;(2) a training system of unmanned boat track generation model based on graph neural network and deep reinforcement learning;(3) an unmanned boat track generation system based on graph neural network and deep reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a training method for unmanned ship track generation model based on graph neural network and deep reinforcement learning. (Drawing includes non-English language text).															0																		2023-11-30	DIIDW:2023C1440V		
J	Dinh-Son Vu; Alsmadi, A.										Trajectory Planning of a CableBased Parallel Robot using Reinforcement Learning and Soft Actor-Critic								WSEAS Transactions on Applied and Theoretical Mechanics				15				165	72				10.37394/232011.2020.15.19							Journal Paper	2020	2020	Industry 4.0 introduces the use of modular stations and better communication between agents to improve manufacturing efficiency and to lower the downtime between the customer and its final product. Among novel mechanisms that have a high potential in this new industrial paradigm are cable-suspended parallel robot (CSPR): their payload-to-mass ratio is high compared to their serial robot counterpart and their setup is quick compared to other types of parallel robots such as Gantry system, popular in the automotive industry but difficult to set up and to adapt while the production line changes. A CSPR can cover the workspace of a manufacturing hall and providing assistance to operators before they arrive at their workstation. One challenge is to generate the desired trajectories, so that the CSPR could move to the desired area. Reinforcement Learning (RL) is a branch of Artificial Intelligence where the agent interacts with an environment to maximize a reward function. This paper proposes the use of a RL algorithm called Soft Actor-Critic (SAC) to train a two degrees-of-freedom (DOFs) CSPR to perform pick-and-place trajectory. Even though the pick-and-place trajectory based on artificial intelligence has been an active research with serial robots, this technique has yet to be applied to parallel robots.									0	0	0	0	0	0	0			1991-8747											Mech. Eng. Dept., American Univ. of the Middle East, Egaila, Kuwait				2020-01-01	INSPEC:21021467		
P	CHEN M; LI Y; LIU Q; LV S; XU Y; LIU Y										Attention model and depth reinforcement learning            based automatic unmanned vehicle driving            decision-making method, involves inputting current            state to decision module to obtain updated current            maximum probability executing driving action					CN112965499-A; CN112965499-B	HARBIN INST TECHNOLOGY SHENZHEN GRADUATE																									NOVELTY - The method involves initializing an automatic driving environment. An observation sequence of a vehicle front camera is obtained through the driving environment. A sensing module is modeled by a building self-attention model and a long-short-time memory network. The sensing module is trained by an automatic encoder model. A Q-value of a decision module is calculated according to a depth deterministic strategy algorithm. The current state is input to the decision module to obtain updated current maximum probability executing driving action. Low-dimensional feature in the observation sequence is extracted. USE - Attention model and depth reinforcement learning based automatic unmanned vehicle driving decision-making method. ADVANTAGE - The method enables reducing dimension of observation data by using the sensing module, and improving utilization rate of the data sample by introducing priority experience playback process so as to improve training speed of algorithm and driving safety in a complex road environment. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram representing an attention model and depth reinforcement learning based automatic unmanned vehicle driving decision-making process. (Drawing includes non-English language text).															0																		2021-07-21	DIIDW:202168617E		
P	DING Z; DING D; WANG Y; WEI G; E G										Combined unmanned vehicle knowledge conversion            reinforcement learning method, involves recording            online learning experience to memory to expand case            library, and learning experience application in case            base according to different tasks					CN109740741-A; CN109740741-B	UNIV SHANGHAI SCI & TECHNOLOGY																									NOVELTY - The method involves establishing BP neural network mapping relationship between target tasks through task learning experience source to initialize mapping relationship of the target tasks. The task learning experience source is stored. Linear perceptron learning action mapping relationship between source domain and target domain is established by using a case-based reasoning mechanism. Online learning experience is recorded to a memory to expand a case library when learning the target tasks. An experience application is learned in a case base according to different tasks. USE - Combined unmanned vehicle knowledge conversion reinforcement learning method. ADVANTAGE - The method enables avoiding dimension disaster and improving unmanned vehicle self-skill learning speed and efficiency. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an unmanned vehicle self-skill learning method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a case-based reasoning mechanism. '(Drawing includes non-English language text)'															0																		2019-07-17	DIIDW:201944617N		
P	ZHONG G; XIU C; MIAO C; XU S; BU X										Navigation method for unmanned vehicle based on deep reinforcement learning, involves obtaining first depth image with repeating size of minimum value and threshold value until minimum value is greater than set threshold value					CN113670306-A	GUANGZHOU AUTOMOBILE GROUP CO LTD																									NOVELTY - The navigation method involves obtaining a first depth image through an RGBD depth camera on an unmanned vehicle. A second depth image is obtained through a linear interpolation. A relative positioning of a starting point is calculated by a wheel speed milemeter of the unmanned vehicle when the minimum value is greater than a set threshold value. A Markov state space is constructed. A next action is randomly or determined according to a deep learning network. The minimum value and the threshold value are repeated until the minimum values are greater than the set threshold values. USE - Navigation method for unmanned vehicle based on deep reinforcement learning. ADVANTAGE - The method makes the network learning efficiency higher, the error convergence value is smaller, so that the obstacle avoiding effect of the unknown environment is better, and the collecting efficiency of the map is improved.															0																		2022-01-19	DIIDW:2021D7746U		
P	HU H; ZHOU C; LI T; CHEN Y; ZHANG Y; YANG H; ZHOU Y; SHI D										Energy constraint multi-machine search method            based on depth reinforcement learning, involves            obtaining Double deep Q-network reinforcement learning            algorithm by training target network and playback            method					CN115933734-A	TIANJIN BINHAI ARTIFICIAL INTELLIGENCE																									NOVELTY - The method involves obtaining a current view based on a mathematical model of an unmanned aerial vehicle constructed in advance. The current view is pre-processed and inputted to a Convolutional neural network (CNN) for feature extraction to obtain extracted feature. The extracted feature is input into a pre-trained Double deep Q-network (DDQN) reinforcement learning algorithm to obtain an action of the unmanned vehicle. Determination is made to check whether the action of unmanned vehicle is executable by a safety controller. The action of drone is determined. USE - Energy constraint multi-machine search method based on depth reinforcement learning for autonomous robot used in dangerous complex physical environment. Uses include but are not limited to planet detection, reconnaissance, rescue, mowing and cleaning. ADVANTAGE - The method providing a penalty function based on minimum distance, which can make an exploration system to ensure higher exploration efficiency and make the system to obtain higher return rate, so that the unmanned aerial vehicle can return take-off landing area before energy is exhausted while the exploration task is finished. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for the multi-machine search system based on energy constraint of depth reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of an energy constraint multi-machine search method based on depth reinforcement learning for autonomous robot used in dangerous complex physical environment. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2023480083		
B	Yan, Yushu										Reinforcement Learning Based Decentralized Automotive Radar Spectrum Allocation																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798534673296									Stevens Institute of Technology, Electrical Engineer, New Jersey, United States	Stevens Institute of Technology				PQDT:64568175		
J	He, Zichen; Dong, Lu; Sun, Changyin; Wang, Jiawei				sun, chang/ITV-6759-2023; SUN, CHANG/GXM-3680-2022						Asynchronous Multithreading Reinforcement-Learning-Based Path Planning and Tracking for Unmanned Underwater Vehicle								IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS				52	5			2757	2769				10.1109/TSMC.2021.3050960					FEB 2021		Article; Early Access		2022	The underwater unmanned vehicle (UUV) is widely used in various marine operations, in which path planning and trajectory tracking are the critical technologies to achieve autonomous motion planning. Unlike previous research methods, this article proposes the asynchronous multithreading proximal policy optimization-based path planning (AMPPO-PP) and trajectory tracking (AMPPO-TT) algorithms and applies these two methods to different task scenarios of UUVs. Taking advantage of the AMPPO, the expensive online computational procedure is converted to an offline training process. The proposed algorithms enable the UUV to learn autonomous planning, tracking, and emergency obstacle avoiding. Besides, the algorithm architecture of the AMPPO-PP and the AMPPO-TT is described in detail. By refining the reward in each timestep and utilizing the reward-shaping trick, the reward sparsity is avoided. The goal-distance heuristic reward function is used to make the UUV explore more directionally. Various simulation environments are developed from simple to complex, along with multiple comparative experiments to verify the effectiveness of the proposed algorithms.									23	2	0	0	1	0	25			2168-2216	2168-2232										Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R ChinaSoutheast Univ, Sch Automat, Nanjing 210096, Peoples R China				2021-12-28	WOS:000732313400001		
B	Wenbin Chen; Guo Xie; Wenjiang Ji; Rong Fei; Xinhong Hei; Siyu Li; Jialin Ma						Mingxuan Sun; Huaguang Zhang				Decision Making for Overtaking of Unmanned Vehicle Based on Deep Q-learning								Proceedings of the 2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)								350	3				10.1109/DDCLS52934.2021.9455523							Conference Paper	2021	2021	Overtaking decision under dynamic environment is one of the key research directions. Traditional methods are difficult to solve such a complex optimization decision-making problem. In recent years, reinforcement learning algorithm is developing continuously, which can be applied to solve the overtaking decision problem of unmanned vehicles. Reinforcement learning generates new data through the continuous interaction between agent and environment, and feeds back to the agent for corresponding learning behavior, so as to improve its own behavior strategy. In this paper, Deep Q-learning (DQN) is used to solve the overtaking decision-making problem on one-way through two lanes. A one-way through two lane traffic environment is built in Python to train and test the algorithm.					2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)	14-16 May 202114-16 May 2021	IEEE Beijing Sect.IEEE Beijing Sect.	Suzhou, ChinaSuzhou, China	4	0	0	0	0	0	4					978-1-6654-2423-3									Xi'an Univ. of Technol., Xi'an, China				2021-09-10	INSPEC:20890868		
J	Deng, Yingjie; Gong, Mingde; Ni, Tao				Ni, Tao/HHR-9207-2022	Deng, Yingjie/0000-0003-0819-9619					Double-channel event-triggered adaptive optimal control of active suspension systems								NONLINEAR DYNAMICS				108	4			3435	3448				10.1007/s11071-022-07360-3					APR 2022		Article; Early Access		2022	An event-triggered adaptive fuzzy optimal control strategy is proposed for a quarter-car electromagnetic active suspension system, where the stiffness and the road input are unknown. The event-triggered mechanism is utilized in both sensor-to-controller (SC) and controller-to-actuator (CA) channels, such that communication saving is achieved in double channels. Two separate triggering conditions are constructed to guarantee optimal performance and stability. Via the reinforcement learning (RL) method, the critic-actor architecture of fuzzy logic systems (FLSs) is constructed to approximate the solution of Hamilton-Jacobi-Bellman (HJB) equation, where there are two critics with one actor. To overcome the "jumps of virtual control laws" (JVCL) problem arising in the backstepping-based ETC (see Deng et al. in ISA Trans. 117:28-39 (2021)), undetermined continuous virtual control laws are constructed for analysis. An event-triggered adaptive observer is fabricated to estimate the unknown road input. It is proved that all the estimating and tracking errors are semi-globally uniformly ultimately bounded (SGUUB). Simulation verifies the effectiveness of the proposed scheme.									6	0	0	0	0	0	6			0924-090X	1573-269X										Yanshan Univ, Sch Mech Engn, Hebei Prov Key Lab Heavy Machinery Fluid Power Tr, Qinhuangdao 066044, Hebei, Peoples R ChinaYanshan Univ, Sch Vehicle & Energy, Qinhuangdao 066044, Hebei, Peoples R China				2022-04-16	WOS:000777276900001		
B	Moradi, M.; Oakes, B.J.; Saraoglu, M.; Morozov, A.; Janschek, K.; Denil, J.										Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection								2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W). Proceedings								102	9				10.1109/DSN-W50199.2020.00028							Conference Paper	2020	2020	Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.					2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)	29 June-2 July 202029 June-2 July 2020		Valencia, SpainValencia, Spain	3	0	0	0	0	0	3					978-1-7281-7263-7									Univ. of Antwerp, Antwerp, BelgiumTech. Univ. Dresden, Dresden, Germany				2020-09-04	INSPEC:19855600		
B	Gorodnichev, M.										On the Applicability of Reinforced Learning in the Route Selection Task of an Unmanned Vehicle								2023 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO								1	9				10.1109/SYNCHROINFO57872.2023.10178444							Conference Paper	2023	2023	This paper presents the process of describing and investigating methods for solving the optimal route search problem, choosing the means for developing a software module, including consideration of the required tools, description of the project structure, and features of the algorithm used. An introduction to the field of optimal route finding is given through the basic terms of graph theory, which is a leading framework for the field of reinforcement learning. The process of developing a learning environment is reviewed, starting with the selection of the environment and an explanation of its possible properties. Then the research of selected multi-agent learning algorithms is described in order to make a final comparison according to the most important criteria. Neural network architectures, hyperparameter learning tables and quality graphs of the learning models are plotted.					2023 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO)2023 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO)	20232023	IEEE; Institute of Radio and Information Systems Association (IRIS)IEEE; Institute of Radio and Information Systems Association (IRIS)	Pskov, RussiaPskov, Russia	0	0	0	0	0	0	0					979-8-3503-4831-6									Moscow Tech. Univ. of Commun. & Informatics, Moscow, Russia				2023-08-03	INSPEC:23454387		
J	Alomrani, Mhd Ali; Tushar, Mosaddek Hossain Kamal; Kundur, Deepa										Detecting State of Charge False Reporting Attacks via Reinforcement Learning Approach								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	10			10467	10476				10.1109/TITS.2023.3281476					JUN 2023		Article; Early Access		2023	The increased push for green transportation has been apparent to address the alarming increase in atmospheric CO2 levels, especially in the last five years. The success and popularity of Electric Vehicles (EVs) have led many carmakers to shift to developing clean cars in the next decade. Moreover, many countries around the globe have set aggressive EV target adoption numbers, with some even aiming to ban gasoline cars by 2050. Unlike their gasoline-based counterparts, EVs comprise many sensors, communication channels, and decision-making components vulnerable to cyberattacks. Hence, the unprecedented demand for EVs requires developing robust defenses against these increasingly sophisticated attacks. In particular, recently proposed cyberattacks demonstrate how malicious owners may mislead EV charging networks by sending false data to unlawfully receive higher charging priorities, congest charging schedules, and steal power. This paper proposes a learning-based detection model that can identify deceptive electric vehicles. The model is trained on an original dataset using real driving traces and a malicious dataset generated from a reinforcement learning agent. The Reinforcement Learning (RL) agent is trained to create intelligent and stealthy attacks that can evade simple detection rules while also giving a malicious EV high charging priority. We evaluate the effectiveness of the generated attacks compared to handcrafted attacks. Moreover, our detection model trained with RL-generated attacks displays greater robustness to intelligent and stealthy attacks.									1	0	0	0	0	0	1			1524-9050	1558-0016										Huawei Noahs Ark lab, Toronto, ON L3R 5Y1, CanadaUniv Toronto, Dept Elect & Comp Engn, Toronto, ON M5S, CanadaUniv Dhaka, Dept Comp Sci & Engn, Dhaka 1000, Bangladesh	Huawei Noahs Ark lab			2023-07-10	WOS:001012513000001		
J	Loffredo, Alberto; May, Marvin Carl; Matta, Andrea; Lanza, Gisela					Lanza, Gisela/0000-0002-0481-4613; May, Marvin Carl/0000-0002-9361-6685; MATTA, ANDREA/0000-0003-3902-2007					Reinforcement learning for sustainability enhancement of production lines								JOURNAL OF INTELLIGENT MANUFACTURING													10.1007/s10845-023-02258-2					NOV 2023		Article; Early Access		2023	The importance of sustainability in industry is dramatically rising in recent years. Controlling machine states to achieve the best trade-off between production rate and energy demand is an effective method for improving the energy efficiency of production systems. This technique is referred to as energy-efficient control (EEC) and it triggers machines in a standby state with low power requests. Reinforcement Learning (RL) algorithms can be used to successfully control production systems without the requirement of prior knowledge about system parameters. Due to the difficulty in acquiring comprehensive information about system dynamics in real-world scenarios, this is considered an important factor. The goal of this work is to create a novel RL-based model to apply EEC to multi-stage production lines with parallel machine workstations without relying on full knowledge of the system dynamics. Numerical results confirm model benefits when applied to a real line from the automotive sector. Further experiments confirm the effectiveness and generality of the approach.									0	0	0	0	0	0	0			0956-5515	1572-8145										Politecn Milan, Mech Engn Dept, Via G La Masa 1, I-20156 Milan, ItalyKarlsruhe Inst Technol KIT, wbk Inst Prod Sci, Kaiserstr 12, D-76131 Karlsruhe, Germany				2023-12-17	WOS:001105442700004		
P	HONG W; MA C; SHI J										Method for counteracting and avoiding obstacle of            e.g. military vehicle, with gradual depth reinforcement            learning, involves obtaining real-time decision of            executor of neural network training for avoiding human            carrier and obstacle					CN116243727-A	UNIV XIAMEN																									NOVELTY - The method involves using a progressive self-game SAC algorithm to self-adjust training course difficulty and executing a self-game process, generating decision data of an unmanned vehicle and putting the decision data into an experience playback pool to update different course learning data, averagely sampling latest data of the experience playback pool and updating a parameter of a criter neural network, and finishing a learning course (S4). Criter and neural network learning courses are made, real-time decision of an executor of neural network training is obtained for avoiding a human carrier and an obstacle (S5). USE - Method for counteracting and avoiding obstacle of an unmanned vehicle e.g. military vehicle and civil unmanned vehicle, with gradual depth reinforcement learning. ADVANTAGE - The method enables providing the progressive self-game SAC algorithm through progressive course learning, avoiding sparse appreciation, improving learning efficiency, improving generalization capability, and generalizing a static task to a random dynamic task. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for counteracting and avoiding obstacle of an unmanned vehicle with gradual depth reinforcement learning. (Drawing includes non-English language text).S1Step for solving modeling by longger process according to kinematics model of unmanned vehicleS2Step for simulating independent decision process of unmanned vehicle system through computer, and generating simulation data of unmanned vehicle motion process and decision behaviorS3Step for designing and optimizing form, size and number of criticizer of neural network progressive self-game SAC neural network, size and numberS4Step for using progressive self-game SAC algorithm to self-adjust training course difficulty and executing self-game process, generating decision data of unmanned vehicle and putting decision data into experience playback pool to update different course learning data, averagely sampling latest data of experience playback pool and updating parameter of criter neural network, and finishing learning courseS5Step for making criter and neural network learning courses, obtaining real-time decision of executor of neural network training for avoiding human carrier and obstacle															0																		2023-07-29	DIIDW:202364392H		
J	Ding, Lige; Zhao, Dong; Cao, Mingzhe; Ma, Huadong										When Crowdsourcing Meets Unmanned Vehicles: Toward Cost-Effective Collaborative Urban Sensing via Deep Reinforcement Learning								IEEE INTERNET OF THINGS JOURNAL				8	15			12150	12162				10.1109/JIOT.2021.3062569							Article	AUG 1 2021	2021	Mobile crowdsensing (MCS) and unmanned vehicle sensing (UVS) provide two complementary paradigms for large-scale urban sensing. Generally, MCS has a lower cost but often confronts sensing imbalance and even blind areas due to the limitation of human mobility, whereas UVS is often capable of completing more demanding tasks at the expense of limited energy supply and hardware cost. Thus, it is significant to investigate whether we could integrate the two paradigms for high-quality urban sensing in a cost-effective collaborative way. However, it is nontrivial due to complex and long-term optimization objectives, uncontrolled dynamics, and a large number of heterogeneous agents. To address the collaborative sensing problem, we propose an actor-critic-based heterogeneous collaborative reinforcement learning (HCRL) algorithm, which leverages several key ideas: local observation to handle expanded state space and extract the states of neighbor nodes, generalized model to avoid environment nonstationarity and ensure the scalability and stability of network, and proximal policy optimization to prevent the destructively large policy updates. Extensive simulations based on a mobility model and a realistic trace data set are conducted to confirm that HCRL outperforms the state-of-the-art baselines.									2	0	0	0	0	0	2			2327-4662											Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China				2021-08-09	WOS:000678340800028		
P	HUANG Z; QU Z										Deep reinforcement learning supporting            multi-driving behavior based unmanned vehicle movement            decision control method, involves resuming expected            driving speed that improving the front vehicle being            changed or vehicle speed					CN113264059-A; CN113264059-B	UNIV BEIJING TECHNOLOGY																									NOVELTY - The method involves decomposing a driving action into a key action decision and a key action execution. The driving action is induced to change a vehicle transverse and a longitudinal expected state when driving. An action instruction is introduced to realize decision control of supporting multiple driving actions. An upper layer of a model is provided with an independent action decision layer corresponding to the driving action. The action instruction is executed by a uniform vehicle control layer. An unmanned vehicle is resumed to an expected driving speed when a front vehicle is changed. USE - Deep reinforcement learning based multi-driving behavior supporting unmanned vehicle movement decision control method. ADVANTAGE - Method enables dividing driving action into key action decision and key action execution, realizing decision control of supporting multiple driving behavior, avoiding repeated modeling of the same task, reducing the complexity of the model, introducing depth enhanced learning algorithm, adopting intelligent body and the environment interactive learning mode, avoiding the problem of data difficult to obtain and complex data pre-processing work, solving the problem of movement decision control of multi-driving behavior and the problem of supporting multi-driving behavior based on the learning end to end movement decision control model, finishing different driving behavior according to corresponding upper behavior instruction, and realizing autonomous driving of the vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of deep reinforcement learning supporting a multi-driving behavior-based unmanned vehicle movement decision control method. (Drawing includes non-English language text).															0																		2022-01-08	DIIDW:202199278S		
J	Yu Ling-li; Shao Xuan-ya; Long Zi-wei; Wei Ya-dong; Zhou Kai-jun										Intelligent land vehicle model transfer trajectory planning method of deep reinforcement learning								Control Theory & Applications				36	9			1409	22				10.7641/CTA.2018.80341							Journal Paper	Sept. 2019	2019	Aiming at the problem of unmanned vehicles model automobiles tracking error and excessive dependence in the traditional motion planning, a method of unmanned vehicle path planning based on deep reinforcement learning model migration is proposed. First, an abstract model of the real environment is extracted. The model uses the deep deterministic policy gradient (DDPG) and the vehicle dynamics model to jointly train the enhanced learning model that approximates the optimal intelligent driving. Secondly, the actual scenario problem is migrated through the model migration strategy. In the virtual abstract model, the control and trajectory sequences are calculated according to the trained deep reinforcement learning model in the environment; then, the optimal trajectory sequence is selected according to the evaluation function in the real environment. The experimental results show that the proposed method can process the continuous input state and generate a continuously controlled corner control sequence to reduce the lateral tracking error. At the same time, the model can improve the generalization performance of the model and reduce the excessive dependence.									2	5	0	0	0	0	7			1000-8152											Sch. of Autom., Central South Univ., Changsha, ChinaSch. of Comput. & Inf. Eng., Hunan Univ. of Commerce, Changsha, China				2019-09-01	INSPEC:20336156		
J	Liu, Teng; Huo, Weiwei; Lu, Bing; Li, Jianwei										Reinforcement Learning-Based Co-Optimization of Adaptive Cruise Speed Control and Energy Management for Fuel Cell Vehicles								ENERGY TECHNOLOGY				12	1								10.1002/ente.202300541					OCT 2023		Article; Early Access		2024	With the development of intelligent autodriving vehicles, the co-optimization of speed control and energy management under the insurance of safe and comfortable driving has become a vital issue. Herein, the adaptive cruise control scenario is discussed. A co-optimization method for speed control and energy management for fuel cell vehicles is suggested to delay the degradation of energy sources while preserving fuel cell efficiency. A reward function based on a reinforcement learning (RL) algorithm is developed to optimize the safety coefficient, comfortability, car-following efficiency, and economy at the speed control level. The RL agent learns to control vehicle speed while avoiding collisions and maximizing the cumulative rewards. To handle the problem of energy management, an adaptive equivalent consumption minimization strategy, which takes into account the deterioration of energy sources, is implemented at the energy management level. The results indicate that the suggested method reduces the demand power by 1.7%, increases the lifetime of power sources, and reduces equivalent hydrogen consumption by 9.4% compared to the model predictive control.This study focuses on co-optimizing speed control and energy management for fuel cell vehicles in the adaptive cruise control scenario. A reinforcement learning algorithm optimizes safety, comfort, car-following efficiency, and economy. An equivalent consumption minimization strategy considers energy source degradation, resulting in reduced power demand, prolonged source lifespan, and decreased hydrogen consumption compared to model predictive control.image (c) 2023 WILEY-VCH GmbH									0	0	0	0	0	0	0			2194-4288	2194-4296										Beijing Informat Sci & Technol Univ, Inst Electromech Engn, Beijing 100192, Peoples R ChinaBeijing Informat Sci & Technol Univ, Collaborat Innovat Ctr Elect Vehicles Beijing, Beijing 100192, Peoples R ChinaBeijing Inst Technol, Shenzhen Automot Res Inst, Shenzhen 518118, Peoples R ChinaBeijing Inst Technol, Sch Mech Engn, Beijing 100081, Peoples R China				2023-11-05	WOS:001088252500001		
P	DAI B; LIU Y; NIE Y; SHANG E; WANG X										Unmanned vehicle layered vehicle following control            method based on depth reinforcement learning, involves            training target recognition algorithm to identify            target vehicle and deploy target recognition algorithm            on following vehicle					CN115617040-A	CHINESE PEOPLES LIBERATION ARMY ACAD																									NOVELTY - The method involves training the target recognition algorithm to identify the target vehicle and deploy the target recognition algorithm on the following vehicle. The coordinates of the target vehicle relative to the following vehicle based on the algorithm recognition results is calculated, and the kinematic model is used to calculate the distance between the two vehicles. The coordinates of the following vehicle are taken as the origin. The direction of the front of the vehicle is the y-axis to establish a right-handed coordinate system. The coordinates of the target car relative to the following car are (x, y). The action space is a binary group. The lower layer used the underlying control algorithm based on control theory to calculate the specific acceleration and angular velocity. USE - Unmanned vehicle layered vehicle following control method based on depth reinforcement learning. ADVANTAGE - The method enables providing a vehicle following control method of a real scene facing an unknown target vehicle kinematics model. The method ensures that the vehicle following intelligent body neural network model can be directly applied on the real vehicle without migration operation. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the unmanned vehicle layered vehicle following control method based on depth reinforcement learning. (Drawing includes non-English language text)															0																		2023-08-10	DIIDW:202315889M		
J	Xu, Wenchao; Guo, Song; Ma, Shiheng; Zhou, Haibo; Wu, Mingli; Zhuang, Weihua				Zhuang, Weihua/AAH-2576-2020; Guo, Song/AAZ-4542-2020; Zhou, Haibo/ABE-8194-2020	Zhuang, Weihua/0000-0003-0488-511X; Guo, Song/0000-0001-9831-2202; 					Augmenting Drive-Thru Internet via Reinforcement Learning-Based Rate Adaptation								IEEE INTERNET OF THINGS JOURNAL				7	4			3114	3123				10.1109/JIOT.2020.2965148							Article	APR 2020	2020	Drive-thru Internet has been considered as an effective Internet access method for Internet of Vehicles (IoV). Through the opportunistic vehicle-to-roadside WiFi connection, it can provide high throughput performance with low communication cost for IoV applications, such as intelligent transportation system, automotive infotainment, etc. However, its usability is highly affected by a fundamental issue called rate adaptation (RA), which is to adjust the modulation and coding rate to adapt to the dynamic wireless channel between the vehicle and the roadside access point (AP). Conventional WiFi RA schemes are designed for indoor or quasistatic scenarios and do not account for the channel variations in drive-thru Internet. In this article, we study the limitation of applying existing RA schemes in drive-thru Internet and propose a reinforcement learning (RL)-based RA scheme to capture the potential channel variation patterns and efficiently select the rate for every vehicle's egress frame. Simulation results demonstrate that the proposed RA scheme outperforms the existing schemes in network throughput and that the efficiency of the learning model can be generalized under various conditions. The proposed RA method can provide useful inspirations for designing robust and scalable link adaptation protocols in IoV.									9	0	0	0	0	0	9			2327-4662											Hong Kong Polytech Univ, Dept Compuing, Hong Kong, Peoples R ChinaShanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R ChinaNanjing Univ, Sch Sci & Engn, Nanjing 210093, Peoples R ChinaUniv Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2V 2L2, Canada				2020-06-15	WOS:000537136400052		
J	Trilling, Jens; Schumacher, Axel; Zhou, Ming					Trilling, Jens/0009-0005-4268-6260					Reinforcement learning based agents for improving layouts of automotive crash structures								APPLIED INTELLIGENCE													10.1007/s10489-024-05276-6					JAN 2024		Article; Early Access		2024	The topology optimization of crash structures in automotive and aeronautical applications is challenging. Purely mathematical methods struggle due to the complexity of determining the sensitivities of the relevant objective functions and restrictions according to the design variables. For this reason, the Graph- and Heuristic-based Topology optimization (GHT) was developed, which controls the optimization process with rules derived from expert knowledge. In order to extend the collected expert rules, the use of reinforcement learning (RL) agents for deriving a new optimization rule is proposed in this paper. This heuristic is designed in such a way that it can be applied to many different models and load cases. An environment is introduced in which agents interact with a randomized graph to improve cells of the graph by inserting edges. The graph is derived from a structural frame model. Cells represent localized parts of the graph and delineate the areas where agents can insert edges. A newly developed shape preservation metric is presented to evaluate the performance of topology changes made by agents. This metric evaluates how much a cell has deformed by comparing its shape in the deformed and undeformed state. The training process of the agents is described and their performance is evaluated in the training environment. It is shown how the agents and the environment can be integrated as a new heuristic into the GHT. An optimization of the frame model and a vehicle rocker model with the enhanced GHT is carried out to assess its performance in practical optimizations.									0	0	0	0	0	0	0			0924-669X	1573-7497										Univ Wuppertal, Optimizat Mech Struct, Gaussstr 20, D-42119 Wuppertal, NRW, GermanyAltair Engn, Main St, Irvine, CA 92614 USA	Altair Engn			2024-01-24	WOS:001143910300001		
P	CAO J; LI P; LIANG D										Agent behavior tree generation method based on            reinforcement learning and genetic algorithm, involves            updating action space using retained parent behavior            tree and generated child behavior tree, and remaining            position of parent behavior tree unchanged, and filling            vacant position					CN116932535-A	PLA ACAD MILITARY SCI WAR RES INST																									NOVELTY - The method involves initializing (S1) behavior tree library. The action space is initialized (S2). The accumulated value estimate is reset (S3), and a table equal to the size of the action space is set. The reinforcement learning training is performed (S4). The reinforcement learning is used to perform a training iteration, and environmental information is taken as input, and the value estimate of the behavior tree in the action space is used as the output. Determination is made (S5) whether the behavior tree achieves the goal. The action space is updated (S10) using the retained parent behavior tree and the generated child behavior tree. The position of the parent behavior tree is remained unchanged, and the vacant position is filled by child behavior tree. USE - Intelligent portion agent behavior tree generation method based on reinforcement learning and genetic algorithm for use in application such as game intelligent agent, robot and unmanned vehicle. ADVANTAGE - The method enables providing a control strategy for automatically generating the action tree expression for multiple applications such as game intelligent agent, robot and unmanned vehicle, thus reducing cost. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an intelligent agent behaviour tree generation system based on enhanced learning and genetic algorithm. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating agent behavior tree generation method based on reinforcement learning and genetic algorithm. (Drawing includes non-English language text)S1Step for initializing behavior tree libraryS10Step for initializing action spaceS2Step for resetting accumulated value estimateS3Step for performing reinforcement learning trainingS4Step for determining whether behavior tree achieves goalS5Step for updating action space															0																		2023-11-12	DIIDW:2023B2796S		
P	CHEN Q; XIA X; LI X										Method for scheduling an unmanned aerial vehicle            based on multi-intelligent body reinforcement learning,            involves updating MADDPG algorithm effectively strategy            in multi-agent environment, and optimal or near-optimal            unmanned plane position scheduling strategy in MDP            model is found					CN117055619-A	UNIV GUILIN ELECTRONIC TECHNOLOGY																									NOVELTY - The method involves obtaining environment information from a system, where the environment information comprises position information of an internet-of-things (IoT) node, task issuing condition, unmanned aerial vehicle resource and position information. Judgment is made to check whether there is a new task based on the new environment information. A task is distributed for an unmanned vehicle. A dispatching decision of the unmanned vehicle is performed. A next motion state of the drone is determined by using a multi-intelligent body reinforcement learning algorithm. The drone is used as an intelligent body in the multi-Intelligent structure reinforcement learning process. A real or simulation environment is continuously performed in the real or simulated environment. The MADDPG algorithm effectively updates the strategy in the multi-agent environment, and the optimal or near-optimal unmanned plane position scheduling strategy in the MDP model is found. USE - Method for scheduling an unmanned aerial vehicle based on multi-intelligent body reinforcement learning for mobile user equipments (UEs) in a short time. ADVANTAGE - The method optimizes the autonomy and efficiency of the individual unmanned aerial vehicle while ensuring the whole performance when the multiple unmanned aerial vehicles are dispatched according to the resource demand, and improves the system effect and the unmanned aerial device energy efficiency. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of method for scheduling unmanned aerial vehicle based on multi-intelligent body reinforcement learning. (Drawing includes non-English language text)															0																		2023-12-07	DIIDW:2023C2926P		
P	XU Z; HUANG Z										Unmanned decision and control method based on federal deep reinforcement learning, involves sending current model parameter to federal server which performs federal aggregation and then sends to each client					CN113885491-A	UNIV BEIJING TECHNOLOGY																									NOVELTY - The method involves issuing an initialized federal model to the client participating in the federal by the federal server. The required scene data is acquired from a scene and the scene data is processed into data dimensions input by a neural network by the client. The deep reinforcement learning training is carried out on the client participating in the federation locally according to a local data set by using a federation model to obtain a new client model. An aggregation request is initiated to a client regularly by a federal server, current model parameters are sent to the federal server the client, and federal aggregation is performed and then the federate aggregation to each client is issued by the federal server. USE - Federal deep reinforcement learning-based unmanned decision-making and control method. ADVANTAGE - The method ensures that the client data is not local, performing federal learning training, realizing the effect of decision and control of the unmanned vehicle in different scenes. The unmanned vehicle can finish the driving under different test scenes, and can keep the more stable speed and vehicle control. DESCRIPTION Of DRAWING(S) - The drawing shows an architecture of a federal deep reinforcement learning-based unmanned decision and control model. (Drawing includes non-English language text)															0																		2023-08-10	DIIDW:202211200B		
P	ZANG B; LI L; LONG L; JIANG C; ZHANG W; LI Y										Method for performing intelligent navigation of            multi-unmanned aerial vehicle based on deep            reinforcement learning, involves establishing            multi-frame four-rotor unmanned aerial vehicle model in            three-dimensional simulation environment					CN116242364-A	UNIV XIDIAN																									NOVELTY - The method involves establishing a multi-frame four-rotor unmanned aerial vehicle model in a three-dimensional simulation environment and generating an environment comprising an obstacle and a target point. Global observation information of multi-dimensional feature fusion of unmanned aerial vehicles, local observation information and discrete action space is set. A value evaluation index of unmanned vehicle state is defined based on reward function of Euclidean distance. A strategy network and a state value network are designed. A current execution of an action is determined according to local observing information of the unmanned vehicle. The current unmanned vehicle execution of the action score is evaluated according to the global observation information. A temporary experience pool is designed for storing interactive information. USE - Method for performing intelligent navigation of a multi-unmanned aerial vehicle based on deep reinforcement learning used in military field or civil field. ADVANTAGE - The method enables combining two-dimensional image information and one-dimensional state information input to reinforcement learning network training, so that the unmanned aerial vehicle can fully detect surrounding environment to make better action under specific state, and designing machine obstacle avoidance function in reward function, so as to realize better obstacle avoidance navigation effect. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a flight path corresponding to an unmanned aerial vehicle (Drawing includes non-English language text).															0																		2023-07-29	DIIDW:202364390B		
P	HU W; XU Q; ZHANG Y; WANG G										Deep reinforcement learing based intelligent            ground unmanned vehicle decision method, involves            performing reverse propagation to deep reinforcement            learning decision network, and adjusting decision            network parameter until deep reinforcement network            convergence					CN113553934-A	UNIV JILIN																									NOVELTY - The method involves collecting vehicle information and environmental information around a vehicle. A deep reinforcement learning decision network is deeply enhanced to analyze and calculate the vehicle information. A driver driving characteristic expression is obtained through vehicle information to obtain a vehicle environment characteristic expression through the environmental information. An intelligent decision result is given to a current driving environment. A current driving state termination signal is stored in an experience pool module. An experience is extracted in the experience pool modules. A decision network parameter is adjusted until a decision network convergence process is performed. USE - Deep reinforcement learing based intelligent ground unmanned vehicle decision method. ADVANTAGE - The method simplifies calculation process, and reduces calculation amount, so that the real time is guaranteed, and avoids generating any disturbance in the whole training process to the driver, and correctly drives the vehicle by the driver to finish the training of the network. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep reinforcement learing based intelligent ground unmanned vehicle decision system.															0																		2021-12-09	DIIDW:2021C46896		
J	Kulmer, F.; Wolf, M.; Ramsauer, C.					Wolf, Matthias/0000-0002-9277-8543					Medium-term Capacity Management through Reinforcement Learning - Literature review and concept for an industrial pilot-application								Procedia CIRP								1065	70				10.1016/j.procir.2022.05.109							Journal Paper	2022	2022	Empty storage shelves and long customer lead times due to a sharp rise in market demand from industries on the one hand (e.g., pharmaceutical products). On the other hand, increasing short-term working or unemployment due to a rapid decline in demand (e.g., automotive). Current supply and demand gaps caused by the COVID-19 pandemic remind us that successful competition in volatile business environments requires rapid adjustments of production capacities. Capacity management (CM) addresses these adjustments by adapting production capacity to market demand. Operations managers of flexible manufacturing systems can adjust the capacity by using various levers (e.g., overtime, used machines, ...). To guide these managers, decision support systems (DSS) exist for short-term CM (e.g., shop floor scheduling). However, due to complexity and runtime problems, the decision-making process for medium-term CM is usually carried out with low technical support. Increases in computing power and advances in algorithm performance over the past decades have enabled Machine Learning to solve ever more complex problems such as the aforementioned issues. Reinforcement Learning (RL) in particular has shown good performance in solving short-term CM problems when compared to humans or other established heuristics. In this work we review the current literature for CM using RL in flexible manufacturing systems. We identify an existing lack of knowledge within the overlap of medium-term CM and RL. However, good performance of RL in short-term CM indicates that an application in medium-term CM should be evaluated. In addition, we propose a concept of a method for medium-term CM based on RL to support operations managers in the decision-making process. The resulting DSS could have a significant impact on production performance, especially in terms of capacity adjustment speed. All rights reserved Elsevier.									0	0	0	0	0	0	0			2212-8271											Inst. of Innovation & Ind. Manage., Graz Univ. of Technol., Graz, Austria				2023-10-26	INSPEC:23898862		
P	FENG B; LIU W; YAN X; SHE H; DU Z; YE H; SHI Y; YANG X										Water-air amphibious unmanned aircraft path            planning method based on reinforcement learning            involves selecting amphibious unmanned aircraft            execution path planning task area, and finishing global            path planning according to different working            scenes					CN114089762-A	UNIV JIANGSU SCI & TECHNOLOGY																									NOVELTY - The method involves selecting an amphibious unmanned aircraft execution path planning task area. An electronic chart is extracted corresponding to the area S of the data for three-dimensional environment modelling. A markov decision process (MDP) of amphibious unmanned aircraft path planning is constructed. A starting point and a target point are determined. A global path planning is completed according to the different working scenes of the amphibious unmanned vehicle based on a deep Q network (DQN) algorithm according to the MDP of the amphibious unmanned vehicle path planning. USE - The method is useful for planning water-air amphibious unmanned aircraft path based on reinforcement learning. ADVANTAGE - The method: improves the planning range to dozens of kilometers; considers the movement characteristics of amphibious unmanned aircraft; combines with the DQN algorithm to quickly find an optimal path that suits its working scene; and is simple and valid. DESCRIPTION Of DRAWING(S) - The drawing shows a logic step diagram of the water-air amphibious unmanned aircraft path planning method based on reinforcement learning (Drawing includes non-English language text).															0																		2022-04-02	DIIDW:202235782X		
P	YANG B; HUANG K; WANG L; YU Y										Method for optimizing unmanned aerial vehicle            auxiliary mobile edge computing resource, involves            solving optimization problem by deep reinforcement            learning algorithm to obtain optimal resource            distribution solution					CN116528250-A	UNIV NORTHEASTERN																									NOVELTY - The method involves obtaining distribution condition of a user calculation task based on Poisson point process. An unmanned aerial vehicle is pre-deployed according to the distribution condition. The unmanned aerial vehicles are determined according to a determination of a coverage radius of the unmanned vehicle and an unmanned vehicle number and a position. A system model is constructed based on an unmanned aerial Vehicle auxiliary MEC system model. An optimization problem is obtained based on the system model of the drone-assisted MEC. An optimal resource distribution solution is obtained by a deep reinforcement learning algorithm. USE - Method for optimizing resource of an unmanned aerial vehicle auxiliary mobile edge computing (MEC) system based on NOMA in a communication field and strengthening learning technology field. ADVANTAGE - The method enables solving the optimization problem by a deep reinforcement learning algorithm to obtain an optimal resource distribution solution. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a method for optimizing resource of an unmanned aerial vehicle auxiliary MEC system based on NOMA (Drawing includes non-English language text).															0																		2023-09-15	DIIDW:202392566H		
P	DENG F; WU X; HUANG K; HUANG J; LIU H; ZHANG X										Automatic amphibious unmanned vehicle mode            switching and ground obstacle avoidance training            method, involves obtaining trained decision neural            network when preset number of training period is not            reached					CN115718485-A	UNIV TSINGHUA																									NOVELTY - The method involves constructing a training scenario based on a Gazebo simulator. The training scenario is provided with a road-air amphibious unmanned vehicle, an obstacle cone and a training scene of a wall. Current time sensing information is input into a decision neural network based on machine learning principle reinforcement learning. Judgment is made to check whether to switch a motion mode. An action instruction is sent to the Gazebos simulator. Motion-related data of each step is stored into an experience pool. Multiple groups of data are randomly extracted from the experience pool based on n-step time sequence difference method. A trained decision neural network is obtained. USE - Automatic amphibious unmanned vehicle mode switching and ground obstacle avoidance training method. ADVANTAGE - The method enables quickly providing sufficient and high-quality data based on simulation environment, reducing time for generating mature mode switching strategy, utilizing reinforcement learning algorithm to effectively eliminate problem that the state action value is high in reinforcement training, reducing estimation error of the state action value and starting training period by a training scene building module for obtaining the trained decision neural network. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a Automatic amphibious unmanned vehicle mode switching and ground obstacle avoidance training system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an automatic amphibious unmanned vehicle mode switching and ground obstacle avoidance training method. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202324284V		
P	SHEN Y; GAO Z; XIA Z; FENG L; WANG H										Method for controlling cooperative steering of            human-vehicle based on cooperation model reinforcement            learning corner weight distribution, involves            distributing weights of controller and driver model in            real-time manner					CN115062539-A	UNIV HEFEI TECHNOLOGY																									NOVELTY - The method involves distributing weight of a controller of an unmanned vehicle and a driver model in real-time manner by using a DQN intelligent body, and updating a policy network of the unmanned vehicle in real-time manner through a vehicle state and an evaluation network of the unmanned vehicle. Policy network iteration is finished after a certain iteration times, finishing training, and keeping network parameter of last updated policy network is not changed, where at this time, the evaluation network does not participate in the weight distribution process of the controller and the driver model. Weights of the controller and the driver model are distributed in real-time manner by the the DQN intelligent body through the last updated policy network. USE - Method for controlling cooperative steering of a human-vehicle based on cooperation model reinforcement learning corner weight distribution in an unmanned driving field in traffic transportation. ADVANTAGE - The method enables generating the intelligent body to satisfy expected standard in the iterative process to coordinate and distribute an output corner of the driver and the controller. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer readable storage medium for storing a set of instructions for controlling cooperative steering of a human-vehicle based on cooperation model reinforcement learning corner weight distribution. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a device for controlling cooperative steering of a human-vehicle. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2022C2419Y		
J	Bragato, F.; Lotta, T.; Ventura, G.; Drago, M.; Mason, F.; Giordani, M.; Zorzi, M.										Towards Decentralized Predictive Quality of Service in Next-Generation Vehicular Networks [arXiv]								arXiv																				Journal Paper	22 Feb. 2023	2023	To ensure safety in teleoperated driving scenarios, communication between vehicles and remote drivers must satisfy strict latency and reliability requirements. In this context, Predictive Quality of Service (PQoS) was investigated as a tool to predict unanticipated degradation of the Quality of Service (QoS), and allow the network to react accordingly. In this work, we design a reinforcement learning (RL) agent to implement PQoS in vehicular networks. To do so, based on data gathered at the Radio Access Network (RAN) and/or the end vehicles, as well as QoS predictions, our framework is able to identify the optimal level of compression to send automotive data under low latency and reliability constraints. We consider different learning schemes, including centralized, fully-distributed, and federated learning. We demonstrate via ns-3 simulations that, while centralized learning generally outperforms any other solution, decentralized learning, and especially federated learning, offers a good trade-off between convergence time and reliability, with positive implications in terms of privacy and complexity.									0	0	0	0	0	0	0																		2023-03-23	INSPEC:22734399		
P	YU H; FENG Y; OU J; LIU J; LIU Y										Method for smartly controlling automotive thermal            management based on adversarial reinforcement learning,            involves determining whether control agent and            adversarial agent have both converged, if that does not            converge					CN116787995-A	CHONGQING CHANGAN AUTOMOBILE CO LTD																									NOVELTY - The method involves collecting (S1) system environmental status data related to the thermal system in the vehicle. A thermal system model is established (S2) based on the system environmental status data obtained in S1. The initial state of the thermal system model is set (S3). The adversarial agent is used (S4) to give a random environmental disturbance d, and the control agent is used to give a control action a. A reinforcement learning algorithm is used (S6) to train the control agent based on the data set. Check whether the control agent and the adversarial agent have both converged is determined or not (S7), if that does not converge, return to S3. The process ends if convergence occurs. USE - Method for smartly controlling automotive thermal management based on adversarial reinforcement learning. ADVANTAGE - The method effectively deal with the problems of untimely control and unguaranteed comfort temperature, save energy under time-varying working conditions, and improve the control effect. The control smart agent has high robustness to the unknown disturbance in the real environment and reduces the risk of actual application DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a computer system; and (2) a computer readable storage medium storing program for smartly controlling automotive thermal management based on adversarial reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for smartly controlling automotive thermal management based on adversarial reinforcement learning. (Drawing includes non-English language text).S1Step for collecting system environmental status data related to the thermal system in the vehicleS2Step for establishing thermal system model based on the system environmental status dataS3Step for setting initial state of the thermal system modelS4Step for using adversarial agent to give a random environmental disturbance d, and the control agent is used to give a control action aS6Step for using a reinforcement learning algorithm to train the control agent based on the data setS7Step for determining whether the control agent and the adversarial agent have both converged, if that does not converge															0																		2023-10-30	DIIDW:2023A41498		
B	Wenwei Lin; Chen Zhu; Wenzhang Zhu; Sixin Shen										Charging Scheduling Strategies of Cooperated Car-hailing Operating Business for Electric Taxis								2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)								407	13				10.1109/ICWCSG53609.2021.00088							Conference Paper	2021	2021	With the popularity of electric vehicles (EVs), the replacement on the traditional fuel taxis to the electric taxis (e-taxis) has been gradually emerged in a creasing number of cities. Compared with the ordinary EVs, e-taxis require frequent charging due to their long daily mileage and high total power consumption. Additionally, the disorderly charging behaviors of EVs have caused congestion in charging stations (CSs) and have seriously affected the normal business of e-taxi drivers. This paper proposes a management architecture that combines the charging network and the car-hailing operating network of e-taxis. With the goal of minimizing the charging cost of e-taxis, a set of scheduling strategy which combines car-hailing operating business and charging plan is designed based on reinforcement learning (RL). The result shows that this strategy can effectively lower the charging cost of e-taxis by reducing the charging queue time in the CSs.					2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)	13-15 Aug. 202113-15 Aug. 2021		Hangzhou, ChinaHangzhou, China	0	0	0	0	0	0	0					978-1-6654-2598-8									Xiamen Univ., Xiamen, ChinaGuangzhou Transp. Univ., Guangzhou, China				2022-04-08	INSPEC:21550639		
B	Ko, Sangjin										Reinforcement Learning Based Decision Making for Self Driving & Shared Control Between Human Driver and Machine																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798380354769									Texas A&M University, Mechanical Engineering, Texas, United States	Texas A&M University				PQDT:85120342		
B	Zhang, C.; Jing, G.; Zuo, S.; Su, M.; Liu, Q.										Inverse Reinforcement Learning in Automatic Driving Decision								2022 2nd International Conference on Algorithms, High Performance Computing and Artificial Intelligence (AHPCAI)								701	4				10.1109/AHPCAI57455.2022.10087395							Conference Paper	2022	2022	With the urgent need of automatic driving on urban roads, autonomous unmanned system must complete the driving task considering safety, efficiency and comfort. For the planning and decision-making module, reinforcement learning can learn human strategies in a human-like manner. However, the reward function is difficult to be determined manually, and inverse reinforcement learning (IRL) can find a reasonable reward function that explains the human strategy. In this paper, the machine learning method on unmanned system is studied, and the IRL based on maximum entropy is introduced to learn the reward function. Experiments on the real-world nuScenes dataset is implemented by setting the features of reward function that conforms to urban environmental constraints. Finally, a reasonable reward function is obtained, which demonstrates the weights of the features can describe the trajectory of unmanned vehicle under the urban road.					2022 2nd International Conference on Algorithms, High Performance Computing and Artificial Intelligence (AHPCAI)2022 2nd International Conference on Algorithms, High Performance Computing and Artificial Intelligence (AHPCAI)	21-23 Oct. 202221-23 Oct. 2022		Guangzhou, ChinaGuangzhou, China	0	0	0	0	0	0	0					979-8-3503-3478-4									Beijing Res. Inst., China Telecom Corp. Ltd., Beijing, ChinaBeijing Inst. of Technol., Beijing, China				2023-04-22	INSPEC:22889862		
C	Xu, Lifan; Zheng, Ruxin; Sun, Shunqiao			IEEE	zheng, ruxin/IWM-6792-2023; Xu, Lifan/B-6367-2017						A DEEP REINFORCEMENT LEARNING APPROACH FOR INTEGRATED AUTOMOTIVE RADAR SENSING AND COMMUNICATION								2022 IEEE 12TH SENSOR ARRAY AND MULTICHANNEL SIGNAL PROCESSING WORKSHOP (SAM)								316	320				10.1109/SAM53842.2022.9827815							Proceedings Paper	2022	2022	We present a deep reinforcement learning approach to design an automotive radar system with integrated sensing and communication. In the proposed system, sparse transmit arrays with quantized phase shifter are used to carry out transmit beamforming to enhance the performance of both radar sensing and communication. Through interaction with environment, the automotive radar learns a reward that reflects the difference between mainlobe peak and the peak sidelobe level in radar sensing mode or communication user feedback in communication mode, and intelligently adjust its beamforming vector. The Wolpertinger policy based actioncritic network is introduced for beamforming vector learning, which solves the dimension curse due to huge beamforming action space.					IEEE 12th Sensor Array and Multichannel Signal Processing Workshop (SAM)IEEE 12th Sensor Array and Multichannel Signal Processing Workshop (SAM)	JUN 20-23, 2022JUN 20-23, 2022		Trondheim, NORWAYTrondheim, NORWAY	0	0	0	0	0	0	0					978-1-6654-0633-8									Univ Alabama, Dept Elect & Comp Engn, Tuscaloosa, AL 35487 USA				2023-02-23	WOS:000922095000064		
P	TAN L; LI B; WANG L; XIA J										Multi-unmanned aerial vehicle air charging and            task scheduling method based on deep reinforcement            learning e.g. used in mobile application, involves            calculating resource optimal allocation strategy of            unmanned aerial vehicle and optimal unloading decision            of user equipment					CN114048689-A; CN114048689-B	UNIV NANJING INFORMATION SCI & TECHNOLOG																									NOVELTY - The method involves obtaining a position of each user and a base station in an environment according to data collected by a third-party. A deployment position of an unmanned aerial vehicle group is initialized. A calculation resource of each unmanned vehicle is pre-set based on the optimization model of the unmanned vehicle group energy consumption minimum as an optimization target. An unloading decision of a user equipment is solved by using a differential evolution algorithm based on a current position of a current unmanned vehicle and an offload decision of the user equipment. An unmanned vehicle deployment strategy is optimized based on an un-install decision of an user equipment and a calculation resource allocation strategy of unmanned vehicle. An optimal resource optimal allocation strategy is calculated based on user equipment optimization decision when an absolute value of an energy consumption value is less than a preset threshold value. USE - Multi-unmanned aerial vehicle air charging and task scheduling method based on deep reinforcement learning used in mobile application, mobile online game, augmented reality enhancing and intelligent navigation. ADVANTAGE - The method involves constructing a multi-unmanned aerial vehicle group auxiliary edge calculation model, and presetting the calculation resource of each unmanned aerial vehicle, and thus enables improving the endurance ability of the unmanned aerial vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the multi-unmanned aerial vehicle air charging and task scheduling method. (Drawing includes non-English language text).															0																		2022-03-24	DIIDW:202230039V		
P	YANG F; LIU Y; LIANG X; ZHANG L; LI M; XU Y; LIU Z; CHEN L; FENG Y; ZHANG Y										Method for calling unmanned aerial vehicle            knowledge model time sharing based on reinforcement            learning, involves using final evaluation value of            unmanned aerial vehicles knowledge model at different            time to perform time-sharing call					CN115470894-A; CN115470894-B	UNIV NAT DEFENSE TECHNOLOGY																									NOVELTY - The method involves obtaining multiple unmanned aerial vehicle knowledge models to be invoked. The unmanned vehicle knowledge model is provided with a cruise model, a reconnaissance model and a strike model. The cruise model is used to execute the task to the target area within the preset period. The environment of all unmanned vehicle information model timely feedback value is obtained. The accumulated discount feedback is obtained of all drone information models. The option strategy function of each drone information model is calculated according to the cumulative discount feedback and the multi-step time state transition probability of the drone knowledge model. A neural network is used as the evaluation function of the drones information model. An update formula of the evaluation functions is constructed according to option strategy functions. The update formula is utilized to update the neural network to obtain the updated neural network. USE - Method for performing time-sharing calling of an unmanned aerial vehicle knowledge model based on reinforcement learning. Uses include but are not limited to aerial photography, agriculture, express transportation, disaster rescue, wild animal observation, infectious disease monitoring, surveying and mapping, news report, power inspection, disaster relief and video shooting fields. ADVANTAGE - The method enables improving working efficiency of the unmanned aerial vehicle in the execution task. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an unmanned aerial vehicle knowledge model time-sharing calling device based on reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method for performing time-sharing calling of an unmanned aerial vehicle knowledge model based on reinforcement learning. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2022F6762E		
J	Koch, Lucas; Roeser, Dennis; Badalian, Kevin; Lieb, Alexander; Andert, Jakob				Andert, Jakob/W-4495-2017	Andert, Jakob/0000-0002-6754-1907; Badalian, Kevin/0000-0002-5593-0227					Cloud-Based Reinforcement Learning in Automotive Control Function Development								VEHICLES				5	3			914	930				10.3390/vehicles5030050							Article	SEP 2023	2023	Automotive control functions are becoming increasingly complex and their development is becoming more and more elaborate, leading to a strong need for automated solutions within the development process. Here, reinforcement learning offers a significant potential for function development to generate optimized control functions in an automated manner. Despite its successful deployment in a variety of control tasks, there is still a lack of standard tooling solutions for function development based on reinforcement learning in the automotive industry. To address this gap, we present a flexible framework that couples the conventional development process with an open-source reinforcement learning library. It features modular, physical models for relevant vehicle components, a co-simulation with a microscopic traffic simulation to generate realistic scenarios, and enables distributed and parallelized training. We demonstrate the effectiveness of our proposed method in a feasibility study to learn a control function for automated longitudinal control of an electric vehicle in an urban traffic scenario. The evolved control strategy produces a smooth trajectory with energy savings of up to 14%. The results highlight the great potential of reinforcement learning for automated control function development and prove the effectiveness of the proposed framework.									0	0	0	0	0	0	0				2624-8921										Rhein Westfal TH Aachen, Teaching & Res Area Mechatron Mobile Prop, D-52074 Aachen, GermanydSPACE GmbH, Rathenaustr 26, D-33102 Paderborn, Germany	dSPACE GmbH			2023-10-16	WOS:001075052500001		
C	Biswas, Atriya; Anselma, Pier Giuseppe; Rathore, Aashit; Emadi, Ali			IEEE	Anselma, Pier Giuseppe/AAP-1275-2021; emadi, ali/AAL-3732-2020	Anselma, Pier Giuseppe/0000-0002-8646-4310; Rathore, Aashit/0000-0001-7065-5561					Comparison of Three Real-Time Implementable Energy Management Strategies for Multi-mode Electrified Powertrain								2020 IEEE TRANSPORTATION ELECTRIFICATION CONFERENCE & EXPO (ITEC)		IEEE Transportation Electrification Conference and Expo						514	519				10.1109/itec48692.2020.9161549							Proceedings Paper	2020	2020	Three real-time implementable energy management system (EMS) strategies have been articulated for forward simulation vehicle model with an electrified powertrain. Rule-based strategy and equivalent consumption minimization strategy (ECMS) have been profoundly used as a competent real-time implementable EMS strategy for electrified powertrain. Rein-forcement learning (RL) is relatively new as a real-time EMS controller. All these three controllers have been articulated for model-in-the-loop (MIL) simulation. A comparison among state-of-the art RL-based controller, widely accredited ECMS, and rule-based control strategies is very crucial in order to analyze strengths and weaknesses of each of these strategies at the MIL and to make them apposite for the subsequent phases of utilitarian controller development.					IEEE Transportation Electrification Conference and Expo (ITEC)IEEE Transportation Electrification Conference and Expo (ITEC)	JUN 22-26, 2020JUN 22-26, 2020	IEEEIEEE	Chicago, ILChicago, IL	5	0	0	0	0	0	5			2377-5483	2473-7631	978-1-7281-4629-4									McMaster Univ, McMaster Automot Resource Ctr MARC, Hamilton, ON, CanadaPolitecn Torino, Dept Mech & Aerosp Engn DIMEAS, Turin, Italy				2021-03-19	WOS:000620344100091		
P	WU S; WANG Z; HAO T; HOU Z										Automatic driving method for intersection scene            and related device, involves combining single-step            recurrent neural network in action neural network and            review neural network respectively based on            actor-critic reinforcement learning framework					CN113264064-A; CN113264064-B	ZHIXING QIANLI BEIJING TECHNOLOGY CO LTD																									NOVELTY - The method involves obtaining (S101) driving data of unmanned vehicles and environmental data of intersections. An action space is constructed (S102) according to the driving data. The state space according to the driving data and the environmental data is constructed (S103). The driving action of the unmanned vehicle is predicted (S104) based on the action space and the state space using intersection behavior model. The behavior of the unmanned vehicle at the intersection is controlled (S105) based on the driving action. The intersection behavior model is pre-built in the following manner. The single-step recurrent neural network is combined in the action neural network and the review neural network respectively based on the actor-critic reinforcement learning framework. USE - Automatic driving method for intersection scene and related device of unmanned vehicle using electronic device (claimed). ADVANTAGE - The automatic driving method provides a new solution for the behavioral decision-making of unmanned vehicle intersections, which effectively deals with various scenes at intersections. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an automatic driving device for intersection scene; and(2) a non-transitory storage medium storing program for automatic driving method. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the automatic driving method for intersection scene and related device. (Drawing includes non-English language text)Step for obtaining driving data of unmanned vehicles and environmental data of intersections (S101)Step for constructing action space (S102)Step for constructing state space according to driving data and environmental data (S103)Step for predicting driving action of unmanned vehicle (S104)Step for controlling behavior of unmanned vehicle at intersection (S105)															0																		2023-08-10	DIIDW:2021A02341		
B	Zhou, B.; Yi, J.; Zhang, X.										Learning to navigate on the rough terrain: A multi-modal deep reinforcement learning approach								2022 IEEE 4th International Conference on Power, Intelligent Computing and Systems (ICPICS)								189	94				10.1109/ICPICS55264.2022.9873725							Conference Paper	2022	2022	How to enable safe navigation of unmanned vehicles on complex and rough terrain is challenging and meaningful research. In this paper, we propose an end-to-end reinforcement learning local navigation method with multi-modal data fusion, which effectively combines the intrinsic perception, such as Inertial Measurement Unit (IMU) measurements, and the extrinsic perception, such as Three-dimensional (3D) point clouds and images, of an unmanned vehicle. A specific feature extraction network is constructed for each modal data, and the total network is effectively trained using a modal separation learning method. The experimental results show that the proposed method can effectively address various obstacles such as rough roads, vegetation obstacles, and water pool disturbances to achieve autonomous and safe navigation of unmanned vehicles in off-road scenarios.					2022 IEEE 4th International Conference on Power, Intelligent Computing and Systems (ICPICS)2022 IEEE 4th International Conference on Power, Intelligent Computing and Systems (ICPICS)	29-31 July 202229-31 July 2022	IEEE; Shenyang Normal UniversityIEEE; Shenyang Normal University	Shenyang, ChinaShenyang, China	2	0	0	0	0	0	2					978-1-6654-6773-5									Sch. of Mech. & Power Eng., East China Univ. of Sci. & Technol., Shanghai, China				2022-11-01	INSPEC:22027860		
J	Panov, A., I				Panov, Aleksandr I./L-9171-2013	Panov, Aleksandr I./0000-0002-9747-3837					Simultaneous Learning and Planning in a Hierarchical Control System for a Cognitive Agent								AUTOMATION AND REMOTE CONTROL				83	6			869	883				10.1134/S0005117922060054							Article	JUN 2022	2022	The tasks of behavior planning and decision-making learning in a dynamic environment are usually divided and considered separately in control systems for intelligent agents. A new unified hierarchical formulation of the problem of simultaneous learning and planning (SLAP) is proposed in the context of object-oriented reinforcement learning, and an architecture of a cognitive agent that solves this problem is described. A new algorithm for learning actions in a partially observed external environment is proposed using a reward signal, an object-oriented subject description of the states of the external environment, and dynamically updated action plans. The main properties and advantages of the proposed algorithm are considered, including the lack of a fixed cognitive cycle necessitating the separation of planning and learning subsystems in earlier algorithms and the ability to construct and update the model of interaction with the environment, thus increasing the learning efficiency. A theoretical justification of some provisions of this approach is given, a model example is proposed, and the principle of operation of a SLAP agent when driving an unmanned vehicle is demonstrated.									2	0	0	0	0	0	2			0005-1179	1608-3032										Russian Acad Sci, Fed Res Ctr Comp Sci & Control, Moscow 119333, RussiaMoscow Inst Phys & Technol, Dolgoprudnyi 141701, Moscow Oblast, Russia				2022-07-18	WOS:000821017000005		
C	Yu, Lingli; Shao, Xuanya; Yan, Xiaoxin			IEEE							Autonomous Overtaking Decision Making of Driverless Bus Based on Deep Q-learning Method								2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017)								2267	2272											Proceedings Paper	2017	2017	The autonomous overtaking maneuver is a valuable technology in unmanned vehicle field. However, overtaking is always perplexed by its security and time cost. Now, an autonomous overtaking decision making method based on deep Q-learning network is proposed in this paper, which employs a deep neural network(DNN) to learn Q function from action chosen to state transition. Based on the trained DNN, appropriate action is adopted in different environments for higher reward state. A series of experiments are performed to verify the effectiveness and robustness of our proposed approach for overtaking decision making based on deep Q-learning method. The results support that our approach achieves better security and lower time cost compared with traditional reinforcement learning methods.					IEEE International Conference on Robotics and Biomimetics (ROBIO)IEEE International Conference on Robotics and Biomimetics (ROBIO)	DEC 05-08, 2017DEC 05-08, 2017	IEEE; IEEE Robot & Automat Soc; City Univ Hong Kong; Beijing Inst Technol; Univ Hong Kong; Univ Macau; Shenzhen Acad RobotIEEE; IEEE Robot & Automat Soc; City Univ Hong Kong; Beijing Inst Technol; Univ Hong Kong; Univ Macau; Shenzhen Acad Robot	Macau, PEOPLES R CHINAMacau, PEOPLES R CHINA	15	0	0	0	0	0	15					978-1-5386-3742-5									Cent South Univ, Sch Informat Sci & Engn, Changsha, Peoples R China				2017-01-01	WOS:000576754200370		
C	Kerbel, Lindsey; Ayalew, Beshah; Ivanco, Andrej; Loiselle, Keith										Residual Policy Learning for Powertrain Control								IFAC PAPERSONLINE				55	24			111	116				10.1016/j.ifaco1.2022.10.270							Proceedings Paper	2022	2022	Eco-driving strategies have been shown to provide significant reductions in fuel consumption. This paper outlines an active driver assistance approach that uses a residual policy learning (RPL) agent trained to provide residual actions to default power train controllers while balancing fuel consumption against other driver-accommodation objectives. Using previous experiences, our RPL agent learns improved traction torque and gear shifting residual policies to adapt the operation of the powertrain to variations and uncertainties in the environment. For comparison, we consider a traditional reinforcement learning (RL) agent trained from scratch. Both agents employ the off-policy Maximum A Posteriori Policy Optimization algorithm with an actor-critic architecture. By implementing on a simulated commercial vehicle in various car-following scenarios, we find that the RPL agent quickly learns significantly improved policies compared to a baseline source policy but in some measures not as good as those eventually possible with the RL agent trained from scratch. Copyright (c) 2022 The Authors. This is an open access article under the CC BY-NC-ND license					10th IFAC Symposium on Advances in Automotive Control (AAC)10th IFAC Symposium on Advances in Automotive Control (AAC)	AUG 29-31, 2022AUG 29-31, 2022	Int Federat Automat Control, Tech Comm 7 1 Automot Control; Gen Motors Co; Cummins Inc; PACCAR Inc; Ford Motor Co; Honda R & D Americas; Jobs Ohio; Schaeffler Grp; BorgWarner Inc; Int Federat Automat Control, Tech Comm 7 4 Transportat Syst; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; IEEE CSS Tech Comm Automot ControlsInt Federat Automat Control, Tech Comm 7 1 Automot Control; Gen Motors Co; Cummins Inc; PACCAR Inc; Ford Motor Co; Honda R & D Americas; Jobs Ohio; Schaeffler Grp; BorgWarner Inc; Int Federat Automat Control, Tech Comm 7 4 Transportat Syst; Int Federat Automat Control, Tech Comm 7 5 Intelligent Autonomous Vehicles; IEEE CSS Tech Comm Automot Controls	Ohio State Univ, Columbus, OHOhio State Univ, Columbus, OH	2	0	0	0	0	0	2			2405-8963											Clemson Univ, Dept Automot Engn, Greenville, SC 29607 USAAllison Transmiss Inc, One Allison Way, Indianapolis, IN 46222 USA	Allison Transmiss Inc			2022-11-09	WOS:000872024300018		
J	Yan Hao; Liu Xiaozhu; Shi Ying							闫浩; 刘小珠; 石英			Lane-change Control for Unmanned Vehicle Based on REINFORCE Algorithm and Neural Network			基于REINFORCE算法和神经网络的无人驾驶车辆变道控制				交通信息与安全	Journal of Transport Information and Safety				39	1			164	172	1674-4861(2021)39:1<164:JYRSFH>2.0.TX;2-Q										Article	2021	2021	For lane change and overtaking of unmanned vehicles,the paper studies the lane change control strategy of unmanned vehicles based on the REINFORCE algorithm and neural network.The feedback,control input,and output limit requirement of the vehicle dynamics model are determined.The REINFORCE algorithm is used to design the structure of the neural network controller and the training plan of the controller.For too large data value and variance of the experience pool,a preprocessing method of the experience pool data is proposed to improve the controller training plan.Besides analyzing sparse reward distribution in the reinforcement learning process,a reward shaping solution based on logarithmic function is proposed combined with the running condition of unmanned vehicles.Compared with PID and LQR controllers,the experiment is carried out.The results show that the proposed control strategy has smaller maximum error compared with PID,with a safer lane-change process.The performance of the control strategy is similar to LQR,which proves its feasibility for the lane change control task of unmanned vehicles.Also,the execution time of the control strategy in different platforms is recorded to prove its real-time performance and feasibility in lightweight platforms.			针对无人驾驶车辆变道超车场景,研究基于REINFORCE算法和神经网络技术的无人驾驶车辆变道控制策略。通过车辆动力学模型确定模型的反馈量、控制量和输出限幅要求;设计神经网络控制器的结构,根据REINFORCE算法设计控制器训练方案;分析经验池数据数值和方差过大的问题,提出1种经验池数据预处理的方法以改进控制器训练方案;结合无人驾驶车辆运行场景,分析和研究强化学习过程中产生的奖励分布稀疏问题,并针对该问题提出1种基于对数函数的奖励塑造解决方案;与PID控制器和LQR控制器进行对比实验验证。实验结果表明,与PID相比,该控制策略有更小的最大误差,变道过程更安全;与LQR相比,该控制策略性能表现接近,以此证明其用于无人驾驶车辆变道控制任务的可行性。此外,记录在不同平台下该控制策略的执行时间以证明其实时性和在轻量级平台运行的可行性。						0	2	0	0	0	0	2			1674-4861											武汉理工大学自动化学院, 武汉, 湖北 430070, 中国School of Automation,Wuhan University of Technology, Wuhan, Hubei 430070, China	武汉理工大学自动化学院School of Automation,Wuhan University of Technology			2021-07-25	CSCD:6954680		
J	Li, Zhenyu; Zhou, Aiguo										RDDRL: a recurrent deduction deep reinforcement learning model for multimodal vision-robot navigation								APPLIED INTELLIGENCE				53	20			23244	23270				10.1007/s10489-023-04754-7					JUL 2023		Article; Early Access		2023	Existing deep reinforcement learning-based mobile robot navigation relies largely on single-modal visual perception to perform local-scale navigation. However, multimodal visual fusion-based global navigation is still under technical exploration. Visual navigation necessitates that agents drive safely in structured, changing, and even unpredictable environments; otherwise, inappropriate operations may result in mission failure and even irreversible damage to life and property. We propose a recurrent deduction deep learning model (RDDRL) for multimodal vision-robot navigation to address these issues. We incorporate a recurrent reasoning mechanism (RRM) into the reinforcement learning model, which allows the agent to store memory, predict the future, and aid in policy learning. Specifically, the RRM first stores current observations and states by learning a parameterized environment model and then predicts future transitions. The RRM then performs a self-assessment on the predicted behavior and perceives the consequences of the current policy, producing a more reliable decision-making process. Furthermore, to obtain global-scale behavioral decision-making, information from scene recognition, semantic segmentation, and pose estimation are fused and used as partial observations of the RDDRL. A large number of simulated experiments based on CARLA scenarios, as well as test results in real-world scenarios, show that RDDRL outperforms state-of-the-art RL methods in terms of driving stability and safety. The results show that by training the agent, the collision rate in the global decision-making of the unmanned vehicle decreases from 0.2 % in the training state to 0.0 % in the test state.									0	0	0	0	0	0	0			0924-669X	1573-7497										Shanghai Inst Technol, Sch Mech Engn, Shanghai 201400, Peoples R ChinaTongji Univ, Sch Mech Engn, Shanghai 201804, Peoples R China				2023-07-20	WOS:001023986500003		
P	LI Z; LI H; WANG C; JIN L										Method for planning unmanned aerial vehicle            unknown environment path based on deep reinforcement            learning, involves building simulation environment of            unmanned aerial vehicle path planning, and performing            unmanned aerial vehicle route planning in actual            environment					CN116257085-A	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - The method involves building a simulation environment of unmanned aerial vehicle path planning. A corresponding action space and reward function are designed according to flight characteristics of an unmanned vehicle and simulation environment. A proximal policy optimization (PPO) algorithm model is constructed with an actor-critic network. A self-attention module is added in the Actor-critic network structure. A route planning and obstacle avoidance training of the unmanned vehicle is performed based on the improved proximal policy optimization model. Unmanned aerial vehicle route planning is performed in the actual environment. USE - Method for planning path of an unmanned aerial vehicle i.e. four-rotor aerial vehicle, in an unknown environment based on deep reinforcement learning. ADVANTAGE - The method reduces the training time, improves the convergence speed of the network, and the network structure is stable, the parameter is small, and it is suitable for being deployed on small mobile device e.g. computing power computer, and improves autonomous path planning capability of the unmanned aerial vehicle in the unknown environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for planning unmanned aerial vehicle unknown environment path based on deep reinforcement learning (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2023653995		
P	JIANG J; MAO K; CUI H; WU Q; ZHU Q; ZHOU T; WANG L; HUANG Y										Method for performing combined search of unmanned            aerial vehicle and pan-tilt interference source based            on reinforcement learning, involves determining            vertical angle of time slots by tripod head until            unmanned vehicle flies to upper portion of interference            source					CN114281101-A; CN114281101-B	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - The method involves dividing a time average of a search task into multiple time slots with the same length. The time slot is divided into a cloud stage and an unmanned aerial vehicle stage. A directional antenna is controlled by a tripod head. A vertical angle of the time slot tripod head is determined. A horizontal angle is determined as a flight direction. A fixed step length is determined to the flight direction to reach a position. The vertical angle of the time slots is determined by the tripod head until the unmanned vehicle flies to an upper portion of an interference source. USE - Method for performing combined search of an unmanned aerial vehicle and pan-tilt interference source based on reinforcement learning. ADVANTAGE - The method enables reducing multi-path effect, shadow fading and influence of complex ground environment, controlling the directional antenna by the tripod head to scan a measuring signal to avoid a use of a complex array antenna. DESCRIPTION Of DRAWING(S) - The drawing show a flow diagram illustrating a method for performing combined search of an unmanned aerial vehicle and pan-tilt interference source based on reinforcement learning (Drawing includes non-English language text).															0																		2022-05-17	DIIDW:202252452T		
P	WANG W; CAI C; YOU Z; LI X; SONG Q; ZHANG Y; FENG X; CHEN H; WEI W; CAI X										Method for generating underwater unmanned vehicle            track in complex dynamic environment, involves            utilizing multi-objective reinforcement learning            framework to adopt environment state in main scene,            unmanned underwater vehicle action and training samples            of reward function for training					CN115657678-A	NO 719 RES INST CHINA SHIPBUILDING IND C																									NOVELTY - The method involves determining a main scene, a first sub-scene and a second sub-scene of track planning of an unmanned underwater vehicle (UUV). Environmental state of the two sub-scenes is input into a trained double-depth Q-network model with a priority experience playback mechanism to predict actions of the UUV in the two sub-scenes at the next moment. A trained multi-objective reinforcement learning framework based on integrated learning is utilized to predict two sub-scenes at the next moment. Predicted actions of the two sub-scenes at the next moment are selected through the trained multi-objective reinforcement learning framework based on ensemble learning with reference to the environmental state of the main scene, so as to prevent the underwater unmanned submersible from obstacles when there is a collision threat. USE - Method for generating UUV track in complex dynamic environment. Can also be used in military, civil and ocean fields. ADVANTAGE - The method enables utilizing a multi-objective reinforcement learning framework to judge the importance of two strategies of avoiding obstacles in different environmental states and going to a destination, thus improving the success rate of track planning and ensuring safe and efficient tracks. DETAILED DESCRIPTION - An optimal action is selected to realize planning of the track of the UUV. The multi-objective reinforcement learning framework is utilized to adopt environment state in the main scene, the UUV action and training samples of a reward function for training.INDEPENDENT CLAIMS are included for:(1) a system for generating UUV track in complex dynamic environment;(2) a computer readable storage medium comprises a set of instructions for generating UUV track in complex dynamic environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating underwater unmanned vehicle track in complex dynamic environment. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202316624U		
J	Zhang, Yuxiang; Gao, Bingzhao; Guo, Lulu; Guo, Hongyan; Chen, Hong				shen, jickie/GSN-7389-2022; Zhang, Yuxiang/GSJ-2574-2022	Zhang, Yuxiang/0000-0001-9047-9585					Adaptive Decision-Making for Automated Vehicles Under Roundabout Scenarios Using Optimization Embedded Reinforcement Learning								IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS				32	12			5526	5538				10.1109/TNNLS.2020.3042981							Article	DEC 2021	2021	The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions. In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout. The promotion is the modified actor of the Actor-Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly. Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency. When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making. More notably, the modified actor matches human drivers' behaviors, macroscale behavior captures the human mind's jump, and medium-scale behaviors are preferentially adjusted through driving skills. To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network. In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time. To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly. The results show that the proposed method has high algorithm efficiency and better system performance.									23	1	0	0	1	0	24			2162-237X	2162-2388										Jilin Univ, State Key Lab Automot Simulat & Control, Changchun 130025, Peoples R ChinaUniv Georgia, Ctr Cyber Phys Syst, Athens, GA 30602 USAJilin Univ, Dept Control Sci & Engn, Changchun 130025, Peoples R ChinaTongji Univ, Clean Energy Automot Engn Ctr, Shanghai, Peoples R China				2022-01-03	WOS:000724480600030	33378264	
P	WANG H; LI Y; SUN X; LIU Q; ZHOU R; CAI Y										Navigation obstacle avoidance control method            combining path planning and reinforcement learning,            involves using improved dynamic window planning            algorithm to generate a plurality of path sampling            spaces, and obtaining optimal track of navigation            obstacle avoidance by using reinforcement            learning					CN114564016-A	UNIV JIANGSU																									NOVELTY - The method involves using a global planning algorithm to plan a passable path of an automatic driving vehicle according to a starting point and a target point in a driving task. Multiple path sampling spaces are generated by using an improved dynamic window planning algorithm (DWA). Each track is evaluated by using evaluation function. The evaluation function is provided with an included angle with the target position, whether there is an obstacle and a distance from the obstacle on the track. An optimal track of navigation obstacle avoidance is obtained by using reinforcement learning. USE - Navigation obstacle avoidance control method combining path planning and reinforcement learning. ADVANTAGE - The method combines the kinematics model of the vehicle with the interpretability, thus improving the safety in the vehicle navigation process. The method does not excessively depend on the model precision of the controlled object, thus greatly reducing the complexity of the solving process, and hence improving the real-time calculation efficiency of the automatic driving vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) Navigation obstacle control system combining path planning and reinforcement learning; and(2) Reinforcement learning network model applied to unmanned vehicle navigation obstacle-avoiding control system. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating of the navigation obstacle avoidance control method combining path planning and reinforcement learning (Drawing includes non-English language text).															0																		2022-07-19	DIIDW:202277432K		
P	LENG S; SI D; WANG L; ZHANG K										Migration learning based unmanned aerial vehicle            content caching decision method, involves putting            interactive data into buffer area to execute training            step to solve corresponding content caching decision            problem					CN113596160-A; CN113596160-B	UNIV CHINA ELECTRONIC SCI & TECHNOLOGY																									NOVELTY - The method involves reducing content cache problem within service range by using a first unmanned aerial vehicle. Data is obtained from a buffer area to perform environment interaction process. Data training process is performed to establish a learning model to solve content caching problem. Cache decision problem is reduced by using an unmanned vehicle when the unmanned vehicle reaches adjacent area. Interactive data of a partial buffer area is transmitted to a second unmanned vehicle by using the first unmanned vehicle. Interactive data is processed and obtained by using the second unmanned vehicle. The interactive data is put into the buffer area to execute training step to solve corresponding content caching decision problem. The processed interactive data is stored in the buffer according to greedy strategy. USE - Migration learning based unmanned aerial vehicle content caching decision method. ADVANTAGE - The method enables determining cache content under constraint of self-caching capacity, optimizing the user to reduce content total time delay problem, considering network state dynamics, selecting enhanced learning algorithm to solve. The method enables transferring previous experience data of the unmanned machine to complete interaction, process, and reducing time to collect interactive data, calculating the resource, implementing faster convergence reinforcement learning algorithm, reducing actual problem, improving content decision efficiency. The method enables reducing content cache decision problem by using deep reinforcement learning (DRL) process, minimizing total content of the user to reduce service range acquisition time delay at same time, and satisfying user requirements and determining dynamic state. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a migration learning based unmanned aerial vehicle content caching decision method. (Drawing includes non-English language text).															0																		2022-01-19	DIIDW:2021D96967		
C	Han, Yuqi						Dai, Q; Shimura, T; Zheng, Z				An adaptive obstacle avoidance algorithm of collaborative unmanned vehicles in dynamic scenes with monocular cameras								OPTOELECTRONIC IMAGING AND MULTIMEDIA TECHNOLOGY VIII		Proceedings of SPIE		11897						1189705			10.1117/12.2602756							Proceedings Paper	2021	2021	Monocular camera is widely used in robots and unmanned vehicles because it is low cost and easy to calibrate. However, the lacks of depth information hinders accurate estimation of the position and physical size of obstacles, which is specially important for a unmanned vehicle platform. To solve this problem, we propose a collaborative structure to accurately acquire the position of static or dynamic obstacles based on partial observations from multiple monocular cameras. After that, a reinforcement learning based obstacle avoidance algorithm is proposed for unmanned vehicles under unknown environments. Specifically, we discuss the influence of obstacles' moving orientations on the performance of obstacles adaptive avoidance. Simulation results verify the feasibility of the proposed approach.					Conference on Optoelectronic Imaging and Multimedia Technology VIIIConference on Optoelectronic Imaging and Multimedia Technology VIII	OCT 10-12, 2021OCT 10-12, 2021	SPIE; Chinese Opt SocSPIE; Chinese Opt Soc	Nantong, PEOPLES R CHINANantong, PEOPLES R CHINA	0	0	0	0	0	0	0			0277-786X	1996-756X	978-1-5106-4644-5; 978-1-5106-4643-8									Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China				2022-05-19	WOS:000792677300004		
J	Chen, Anthony Siming										Adaptive Optimal Control Via Reinforcement Learning : Theory and Its Application to Automotive Engine Systems																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0														University of Bristol (United Kingdom), England	University of Bristol (United Kingdom)				PQDT:68299261		
B	Xu, L.; Zheng, R.; Sun, S.										A Deep Reinforcement Learning Approach for Integrated Automotive Radar Sensing and Communication								2022 IEEE 12th Sensor Array and Multichannel Signal Processing Workshop (SAM)								316	20				10.1109/SAM53842.2022.9827815							Conference Paper	2022	2022	We present a deep reinforcement learning approach to design an automotive radar system with integrated sensing and communication. In the proposed system, sparse transmit arrays with quantized phase shifter are used to carry out transmit beamforming to enhance the performance of both radar sensing and communication. Through interaction with environment, the automotive radar learns a reward that reflects the difference between mainlobe peak and the peak sidelobe level in radar sensing mode or communication user feedback in communication mode, and intelligently adjust its beamforming vector. The Wolpertinger policy based action-critic network is introduced for beamforming vector learning, which solves the dimension curse due to huge beamforming action space.					2022 IEEE 12th Sensor Array and Multichannel Signal Processing Workshop (SAM)2022 IEEE 12th Sensor Array and Multichannel Signal Processing Workshop (SAM)	20-23 June 202220-23 June 2022	The Research Council of Norway; DNV; NORDIC SEMICONDUCTOR; SINTEFThe Research Council of Norway; DNV; NORDIC SEMICONDUCTOR; SINTEF	Trondheim, NorwayTrondheim, Norway	0	0	0	0	0	0	0					978-1-6654-0633-8									Dept. of Electr. & Comput. Eng., Univ. of Alabama, Tuscaloosa, AL, USA				2022-11-01	INSPEC:21903604		
B	Cui, Z.; Wang, Y.; Bian, N.; Chen, H.										Reward Machine Reinforcement Learning for Autonomous Highway Driving: An Unified Framework for Safety and Performance								2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)								1	6				10.1109/CVCI59596.2023.10397271							Conference Paper	2023	2023	Developing a safe and highly effective policy for autonomous vehicles (AVs) continues to pose a significant challenge in machine learning. In this study, we propose a novel reinforcement learning approach with reward machine. By tailoring the reward function to the specific needs of AVs in highway scenarios, we enable them to make more informed and efficient decisions. Our focus is on designing a reward function to formalize traffic rules, which is crucial for achieving safe and effective AV behavior on highways. To address this problem, we propose several innovative ideas that go beyond existing algorithmic techniques, specifically aimed at facilitating exploration and exploitation on different operations. To our knowledge, this is the first reinforcement learning algorithm that can integrate the safe distance with autonomous highway driving, aiming at the Vienna Convention on road traffic. Experimental results demonstrate the effectiveness of the proposed approach, which significantly improves AVs' safety and performance on highways.					2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)	20232023		Changsha, ChinaChangsha, China	0	0	0	0	0	0	0					979-8-3503-4048-8									Clean Energy Automotive Eng. Center Sch. of Automotive Studies, Tongji Univ., Shanghai, ChinaDept. of Control Sci. & Eng., Tongji Univ., Shanghai, ChinaDongfeng Tech. Center, Dongfeng Motor Corp., Wuhan, China				2024-02-15	INSPEC:24508417		
B	Davila, Christian										On Achieving Acceptable Levels of Safety Risk in a Reinforcement Learning Environment																												Dissertation/Thesis	Jan 01 2023	2023										0	0	0	0	0	0	0					9798380165570									Washington University in St. Louis, Mechanical Engineering & Materials, Missouri, United States	Washington University in St. Louis				PQDT:85143708		
P	LIU G; LUO Y; SHENG J										Determining unmanned vehicle ethics behavior comprises obtaining barrier characteristic data on two lanes, inputting barrier characteristic data into ethics decision model, and determining ethics behavior of the unmanned vehicle					CN112926748-A	UNIV EAST CHINA JIAOTONG; NANCHANG INST TECHNOLOGY																									NOVELTY - Determining unmanned vehicle ethics behavior comprises obtaining barrier characteristic data on two lanes. The barrier characteristic data is inputted into an ethics decision model. An ethics behavior of an unmanned vehicle is determined. The ethics decision model is provided with multiple groups of the two lanes according to the barrier characteristic data. A statistical result of a forward excitation number of the barrier characteristic data is obtained. The ethics behavior of the unmanned vehicle is determined according to the statistical result of the positive excitation number corresponding to the obstacle feature data on the two lanes. USE - The method is useful for determining unmanned vehicle ethics behavior. ADVANTAGE - The method: realizes the ethics decision of deep reinforcement learning mode from bottom to top from the surrounding environment and people; and avoids partiality and discriminatory shortcomings in the process of human ethical decision-making. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) system for determining unmanned vehicle ethics behavior; and(2) intelligent automobile. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for determining unmanned vehicle ethics behavior (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202165777U		
P	GAO X; WAN K; YANG H; KANG P; GAN Z; BAI S; LI B										Unmanned aerial vehicle flight decision method            based on meta-reinforcement learning parallel training            algorithm, involves testing unmanned aerial vehicle            flight decision model based on meta reinforcement            learning algorithm, to evaluate flight decision            performance					CN114895697-A	UNIV NORTHWESTERN POLYTECHNICAL																									NOVELTY - The method involves constructing an unmanned aerial vehicle flight control model. The unmanned aerial vehicle flight control rigid body model is used, in order to real-time resolve the position and posture information of the unmanned aerial vehicle. The state space of an unmanned vehicle flight decision is constructed according to a Markov decision process, an action space and a reward function. The multi-task experience pool is constructed for training sample data by a storing meta reinforcement learning algorithm. Different flight environment and unmanned aerial vehicle state are initialized, to realize unmanned aerial vehicle element reinforcement learning decision model by parallel training in multiple environments. The new flight environment and unmanned aerial vehicle state are initialized, and the unmanned aerial vehicle flight decision model is tested based on the meta reinforcement learning algorithm, to evaluate flight decision performance. USE - Method for unmanned aerial vehicle flight decision based on meta-reinforcement learning parallel training algorithm. ADVANTAGE - The method solves the problem that the generalization performance of the SAC algorithm is not enough by training the strategy in multiple environments, it can integrally optimize the unmanned aerial vehicle flight decision strategy, in the new environment, and it can be converged by the less step training. The generalization capability and universality of the strategy is effectively improved. DESCRIPTION Of DRAWING(S) - The diagram shows a schematic view illustrating the process for unmanned aerial vehicle flight decision based on meta-reinforcement learning parallel training algorithm. (Drawing includes non-English language text)															0																		2022-09-17	DIIDW:2022A6956F		
P	KOCHAR R										System for optimizing fuel and energy consumption            by automobile paint shop electro-deposition oven, has            fan operated based on corresponding optimal speed that            leads to optimization of energy consumption					IN202341089975-A	BERT LABS PRIVATE LTD																									NOVELTY - The system has a digital twin to simulate an electrodeposition (ED) oven for baking and curing a coated automobile component in an automobile paint shop. A reinforcement learning (RL) agent receives training using environment models derived from the digital twin of the ED oven. The RL agent learns control strategies through interaction with the environment models for generating policies, where each policy includes opening percentage of gas dampers and speed of fans e.g. recirculating fan. The fan is operated based on corresponding optimal speed that leads to optimization of energy consumption. USE - System for optimizing fuel and energy consumption by an automobile paint shop electro-deposition oven for baking and curing coated metal components. Uses include but are not limited to automotive, aerospace, and consumer goods. ADVANTAGE - The system optimizes fan speeds, damper positions, and fuel flow rates, and significantly reduces energy and fuel consumption. The system provides valuable insights into oven's operation, thus enabling continuous improvement and fine-tuning of a coating process. The system enhances efficiency, and drives innovation in process management. The system provides more responsive and efficient operation, and is capable of quickly addressing deviations from optimal conditions. The system continuously monitors and analyzes system's performance so as to predict maintenance needs, thus reducing unexpected downtimes and prolonging equipment's lifespan. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for optimizing fuel and energy consumption by an automobile paint shop electro-deposition oven.															0																		2024-02-10	DIIDW:202411149N		
J	Sommer Obando, Hermann										Reinforcement learning framework for the self-learning suppression of clutch judder in automotive drive trains																												Dissertation/Thesis	Jan 01 2016	2016										0	0	0	0	0	0	0														Karlsruher Institut fuer Technologie, Universitaet Karlsruhe (TH) (Germany), Germany	Karlsruher Institut fuer Technologie, Universitaet Karlsruhe (TH) (Germany)				PQDT:66756350		
C	Heimrath, Andreas; Froeschl, Joachim; Rezaei, Razieh; Lamprecht, Martin; Baumgarten, Uwe						Miraz, MH; Excell, PS; Ware, A; Soomro, S; Ali, M				Reflex-Augmented Reinforcement Learning for Operating Strategies in Automotive Electrical Energy Management								2019 INTERNATIONAL CONFERENCE ON COMPUTING, ELECTRONICS & COMMUNICATIONS ENGINEERING (ICCECE)								62	67				10.1109/iccece46942.2019.8941819							Proceedings Paper	2019	2019	This paper introduces reflex-augmented reinforcement learning (RARL) for operating strategies in automotive electrical energy management. RARL makes it possible to overcome the limitations of rule-based decision systems (RBDS) and to face the increasing complexity in a vehicle's electrical energy system. We suggest a deep Q-learning-based RARL approach for operating strategies determining the behavior of the electrical energy system. This also provides a general approach to realize reinforcement learning in cybernetic management systems for safety-critical applications. In a simulation-based study of more than 50 hours of driving with an extensive model of a vehicular electrical energy system, we show that RARL-based operating strategies fulfill the major requirements of a real vehicle. Compared to an RBDS, RARL requires less effort to design an operating strategy of this level of performance. Furthermore, we evaluate different variants of the biologically-inspired reflex of RARL enabling the application in safety-critical systems. Finally, we do not only provide an approach to replace the RBDS, but we also suggest that RARL is a key to integrate further sources of information into decision-making to enhance electrical energy management.					International Conference on Computing, Electronics and Communications Engineering (iCCECE)International Conference on Computing, Electronics and Communications Engineering (iCCECE)	AUG 22-23, 2019AUG 22-23, 2019	Int Assoc Educators & Researchers; IEEE, United Kingdom & Ireland Sect; British Comp Soc, N Wales Branch; IEEE, Bahrain Sect; IEEE Commun Soc, Bahrain Chapter; IEEEInt Assoc Educators & Researchers; IEEE, United Kingdom & Ireland Sect; British Comp Soc, N Wales Branch; IEEE, Bahrain Sect; IEEE Commun Soc, Bahrain Chapter; IEEE	London Metropolitan Univ, London, ENGLANDLondon Metropolitan Univ, London, ENGLAND	0	0	0	0	0	0	0					978-1-7281-2138-3									BMW AG, R&D Elect, Munich, GermanyTech Univ Munich, Dept Informat, Munich, Germany				2020-04-24	WOS:000525754800013		
B	Velamati, S.; Padmaja, V.						Saini, H.S.; Singh, R.K.; Tariq Beg, M.; Sahambi, J.S.				Policy Space Exploration for Linear Quadratic Regulator (LQR) Using Augmented Random Search (ARS) Algorithm								Innovations in Electronics and Communication Engineering. Proceedings of the 8th ICIECE 2019. Lecture Notes in Networks and Systems (LNNS 107)								791	7				10.1007/978-981-15-3172-9_74							Conference Paper	2020	2020	Considering the recent developments in embedded systems and automotive industry, it is quite evident that in very near future many application-based electronic devices will adapt the automation in its daily based activities. This automation will make the devices more powerful and will enhance its services. Currently, automation is the result of the algorithms which are pre-coded into the devices, but its future is the result of algorithms which enable devices to learn from environment in which it needs to work. It can be achieved utilizing the resources developed for a particular domain popularly known as reinforcement learning (RL). Main objective of this paper is to enable an agent to explore a policy for achieving a control of dynamic system such that it will be capable to find an optimal solution to solve the environment. It can be achieved using an algorithm known as augmented random search algorithm. To improve the training speed, we will use concept of multiprocessing and environment-specific customizations along with ARS algorithm.					Innovations in Electronics and Communication Engineering. 8th ICIECE 2019Innovations in Electronics and Communication Engineering. 8th ICIECE 2019	2-3 Aug. 20192-3 Aug. 2019		Hyderabad, IndiaHyderabad, India	0	0	0	0	0	0	0					978-981-15-3171-2									Dept. of Electron. & Commun. Eng., VNR Vignana Jyothi Inst. of Eng. & Technol., Hyderabad, IndiaGuru Nanak Instn., Hyderabad, IndiaGuru Nanak Instn. Tech. Campus, Hyderabad, IndiaDept. of Electron. & Commun. Eng., Jamia Millia Islamia, New Delhi, IndiaIndian Inst. of Technol. Ropar, Rupnagar, India				2020-06-05	INSPEC:19596718		
C	Xu, Risheng; Kuehl, Marvin; von Hasseln, Hermann; Nowotka, Dirk			ACM							Reducing Overall Path Latency in Automotive Logical Execution Time Scheduling via Reinforcement Learning								PROCEEDINGS OF 31ST INTERNATIONAL CONFERENCE ON REAL-TIME NETWORKS AND SYSTEMS, RTNS 2023								212	223				10.1145/3575757.3593658							Proceedings Paper	2023	2023	The Logical Execution Time paradigm is a promising approach for achieving time-deterministic communication on multi-core CPUs. Task scheduling under this paradigm is a variant of the Multi-Row Facility Layout Problem, which is known to be NP-hard. In this paper, we propose using reinforcement learning to reduce the overall path latency among all scheduled runnables while adhering to other constraints, such as schedulability, load balance, and data contention control. The neural networks, also known as agents, are trained on a real-world automotive powertrain project. We compare two schedules generated by the agents to the current one and one produced by a genetic algorithm. The agent trained with the Proximal Policy Optimization algorithm demonstrated the best performance. Additionally, we investigate the generalization ability of the agents against software updates, and the results show that our agents are well-generalized.					31st International Conference on Real-Time Networks and Systems (RTNS)31st International Conference on Real-Time Networks and Systems (RTNS)	JUN 07-08, 2023JUN 07-08, 2023		Dortmund, GERMANYDortmund, GERMANY	0	0	0	0	0	0	0					978-1-4503-9983-8									Univ Kiel, Kiel, GermanyMercedes Benz AG, Sindelfingen, Germany				2023-12-28	WOS:001108881100020		
B	Ferens, M.; Hortelano, D.; De miguel, I.; Duran barroso, R.J.; Aguado, J.C.; Ruiz, L.; Merayo, N.; Fernandez, P.; Lorenzo, R.M.; Abril, E.J.				Hortelano, Diego/Q-5064-2017	Hortelano, Diego/0000-0001-7776-6862					Deep Reinforcement Learning Applied to Computation Offloading of Vehicular Applications: A Comparison								2022 International Balkan Conference on Communications and Networking (BalkanCom)								31	5				10.1109/BalkanCom55633.2022.9900545							Conference Paper	2022	2022	An observable trend in recent years is the increasing demand for more complex services designed to be used with portable or automotive embedded devices. The problem is that these devices may lack the computational resources necessary to comply with service requirements. To solve it, cloud and edge computing, and in particular, the recent multi-access edge computing (MEC) paradigm, have been proposed. By offloading the processing of computational tasks from devices or vehicles to an external network, a larger amount of computational resources, placed in different locations, becomes accessible. However, this in turn creates the issue of deciding where each task should be executed. In this paper, we model the problem of computation offloading of vehicular applications to solve it using deep reinforcement learning (DRL) and evaluate the performance of different DRL algorithms and heuristics, showing the advantages of the former methods. Moreover, the impact of two scheduling techniques in computing nodes and two reward strategies in the DRL methods are also analyzed and discussed.					2022 International Balkan Conference on Communications and Networking (BalkanCom)2022 International Balkan Conference on Communications and Networking (BalkanCom)	22-24 Aug. 202222-24 Aug. 2022	TII; ZTETII; ZTE	Sarajevo, Bosnia and HerzegovinaSarajevo, Bosnia and Herzegovina	0	0	0	0	0	0	0					978-1-6654-8764-1									Aalborg Univ., DenmarkUniv. de Valladolid, Valladolid, SpainUniv. Rey Juan Carlos, Madrid, Spain				2022-10-20	INSPEC:22112925		
P	CHEN J; XU Y; GUO Y; LI Z; SUN J										Multi-vehicle cooperative detection system track            planning method based on multi-intelligent body            strengthening learning for robot intelligent and            control field, involves establishing multiple unmanned            vehicle detection track optimization problem according            to Markov process model					CN116820093-A	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves constructing a Markov decision process model of multiple unmanned vehicle track planning facing a space-time signal field detection task. Multiple unmanned vehicle detection track optimization problem is established according to the Markov process model. Multiple unmanned vehicle detection track are obtained based on multi-intelligent body reinforcement learning planning strategy. USE - Multi-vehicle cooperative detection system track planning method based on multi-intelligent body strengthening learning for robot intelligent and control field. ADVANTAGE - The method solves the track planning problem in the space-time signal field task cooperatively detected by multiple unmanned vehicles under the condition that the unmanned vehicle dynamic model and the space time signal field distribution to be detected are completely unknown, so that the system obtains enough information in the shortest time and reaches the predetermined target. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of a multi-vehicle cooperative detection system track planning method. (Drawing includes non-English language text).															0																		2023-10-26	DIIDW:2023A54034		
S	Klarmann, Noah; Malmir, Mohammadhossein; Josifovski, Josip; Plorin, Daniel; Wagner, Matthias; Knoll, Alois C.				Knoll, Alois/AAN-8417-2021	Knoll, Alois/0000-0003-4840-076X	Vermesan, O; John, R; DeLuca, C; Coppola, M				Optimizing Trajectories in Simulations with Deep Reinforcement Learning for Industrial Robots in Automotive Manufacturing								ARTIFICIAL INTELLIGENCE FOR DIGITISING INDUSTRY: Applications		River Publishers Series in Communications						35	45											Article; Book Chapter	2021	2021	This paper outlines the concept of optimizing trajectories for industrial robots by applying deep reinforcement learning in simulations. An application of high technical relevance is considered in a production line of an autmotive manufacturer (AUDI AG), where industrial manipulators apply sealant on a car body to prevent water intrusion and hence corrosion. A methodology is proposed that supports the human expert in the tedious task of programming the robot trajectories. A deep reinforcement learning agent generates trajectories in virtual instances where the use case is simulated. By making use of the automatically generated trajectories, the expert's task is reduced to minor changes instead of developing the trajectory from scratch. This paper describes an appropriate way to model the agent in the context of Markov decision processes and gives an overview of the employed technologies. The use case outlined in this paper is a proof of concept to demonstrate the applicability of reinforcement learning for industrial robotics.									0	0	0	0	0	0	0			2445-4842		978-87-7022-663-9; 978-87-7022-664-6									Tech Univ Munich, Munich, GermanyAUDI AG, Ingolstadt, Germany				2021-01-01	WOS:000893595200005		
P	NOTZ D; WINDELEN J; SUNG G; WERLING M; PONNAIYAN V; MIRCHEVSKA B										Method for generating reinforcement learning-based            machine-learning model, involves providing reward            function for machine-learning model, and performing            reinforcement learning on machine-learning model using            reward function					EP3751465-A1	BAYERISCHE MOTOREN WERKE AG																									NOVELTY - The method involves providing (110) a reward function for the machine-learning model. The reward function is based on an operating environment the vehicle is to be operated in. The operating environment is based on a country the vehicle is to be operated in, a legal framework under which the vehicle is to be operated and a vehicle model of the vehicle. The reinforcement learning is performed (120) on the machine-learning model using the reward function. The reinforcement learning is based on a simulation of the vehicle in a vehicular environment. The reward function comprises multiple reward function components categorized in multiple categories comprising categories of the group of legal constraints, safety preferences, speed preferences, comfort preferences and energy use preferences. USE - Method for generating reinforcement learning-based machine-learning model used in automotive development. ADVANTAGE - The more natural behavior of the vehicle increases comfort, safety of the vehicle, and satisfaction of customers. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for generating reinforcement learning-based machine-learning model;(2) a computer program product for generating reinforcement learning-based machine-learning model; and(3) an apparatus for generating reinforcement learning-based machine-learning model. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating a method for generating reinforcement learning-based machine-learning model.Step for providing reward function for the machine-learning model (110)Step for performing reinforcement learning on the machine-learning model using the reward function (120)															0																		2020-12-29	DIIDW:2020C5283V		
C	Kerbel, Lindsey; Ayalew, Beshah; Ivanco, Andrej; Loiselle, Keith			IEEE							Driver Assistance Eco-driving and Transmission Control with Deep Reinforcement Learning								2022 AMERICAN CONTROL CONFERENCE (ACC)								2409	2415											Proceedings Paper	2022	2022	With the growing need to reduce energy consumption and greenhouse gas emissions, Eco-driving strategies provide a significant opportunity for additional fuel savings on top of other technological solutions being pursued in the transportation sector. In this paper, a model-free deep reinforcement learning (RL) control agent is proposed for active Eco-driving assistance that trades-off fuel consumption against other driver-accommodation objectives, and learns optimal traction torque and transmission shifting policies from experience. The training scheme for the proposed RL agent uses an off-policy actor-critic architecture that iteratively does policy evaluation with a multi-step return and policy improvement with the maximum posteriori policy optimization algorithm for hybrid action spaces. The proposed Eco-driving RL agent is implemented on a commercial vehicle in car following traffic. It shows superior performance in minimizing fuel consumption compared to a baseline controller that has full knowledge of fuel-efficiency tables.					American Control Conference (ACC)American Control Conference (ACC)	JUN 08-10, 2022JUN 08-10, 2022	Amer Automat Control CouncilAmer Automat Control Council	Atlanta, GAAtlanta, GA	1	0	0	0	0	0	1					978-1-6654-5196-3									Clemson Univ, Dept Automot Engn, Greenville, SC 29607 USAAllison Transmission Inc, 1 Allison Way, Indianapolis, IN 46222 USA	Allison Transmission Inc			2022-11-09	WOS:000865458702041		
B	Karunakaran, D.; Berrio, J.S.; Worrall, S.; Nebot, E.										Critical Concrete Scenario Generation using Scenario-Based Falsification								2022 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE)								1	8				10.1109/RASSE54974.2022.9989690							Conference Paper	2022	2022	Autonomous vehicles have the potential to lower the accident rate when compared to human driving. Moreover, it has been the driving force of automated vehicles' rapid development over the last few years. In the higher Society of Automotive Engineers (SAE) automation level, the vehicle's and passengers' safety responsibility is transferred from the driver to the automated system, so thoroughly validating such a system is essential. Recently, academia and industry have embraced scenario-based evaluation as the complementary approach to road testing, reducing the overall testing effort required. It is essential to determine the system's flaws before deploying it on public roads as there is no safety driver to guarantee the reliability of such a system. This paper proposes a Reinforcement Learning (RL) based scenario-based falsification method to search for a high-risk scenario in a pedestrian crossing traffic situation. We define a scenario as risky when a system under test (SUT) does not satisfy the requirement. The reward function for our RL approach is based on Intel's Responsibility Sensitive Safety(RSS), Euclidean distance, and distance to a potential collision. Code and videos are available online at https://github.com/dkarunakaran/scenario_based_falsification.					2022 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE)2022 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE)	7-10 Nov. 20227-10 Nov. 2022	IEEEIEEE	Tainan, TaiwanTainan, Taiwan	0	0	0	0	0	0	0					978-1-6654-9491-5									Australian Centre for Field Robotics, Univ. of Sydney, Sydney, Australia				2023-03-22	INSPEC:22413045		
P	QIU T; PU Z; LIU Z; YI J; CHANG H										Enhanced A-star algorithm and depth enhanced            learning based unmanned vehicle path planning method,            involves combining global planning path and enforcing            trained local planning network model to unmanned            vehicle navigation					CN111780777-A; CN111780777-B	JIANGSU CAS INTELLIGENT MFG RES INST CO; CHINESE ACAD SCI AUTOMATION INST																									NOVELTY - The method involves establishing an initializing grid cost map according to environment information. Environment is mapped by using SALM technology. Obstacle information is extracted. An obstacle type is calibrated to evaluate threat of a surrounding grid of the obstacle by a cost model. A global path is planned by using an enhanced A-start algorithm. A sliding window is designed based on a global path and a laser radar sensor performance. A local planning network is designed based on deep reinforcement learning process by using an Actor-Critic architecture. A global planning path is combined. A trained local planning network model is enforced to unmanned vehicle navigation. USE - Enhanced A-star algorithm and depth enhanced learning based unmanned vehicle path planning method. ADVANTAGE - The method enables combining knowledge and data process to quickly plan and obtain the optimal path, so that higher autonomy of the unmanned vehicles can be achieved. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an enhanced A-star algorithm and depth enhanced learning based unmanned vehicle path planning method. (Drawing includes non-English language text).															0																		2020-11-17	DIIDW:2020A3324Q		
C	Bin Al Islam, S. M. A.; Aziz, H. M. Abdul; Wang, Hong; Young, Stanley E.			IEEE		Wang, Hong/0000-0002-9876-0176; Aziz, HM Abdul/0000-0002-8135-4577					Minimizing energy consumption from connected signalized intersections by reinforcement learning								2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		IEEE International Conference on Intelligent Transportation Systems-ITSC						1870	1875											Proceedings Paper	2018	2018	Explicit energy minimization objectives are often discouraged in signal optimization algorithms due to its negative impact on mobility performance. One potential direction to solve this problem is to provide a balanced objective function to achieve desired mobility with minimized energy consumption. This research developed a reinforcement learning (RL) based control with reward functions considering energy and mobility in a joint manner-a penalty function is introduced for number of stops. Further, we proposed a clustering-based technique to make the state-space finite which is critical for a tractable implementation of the RL algorithm. We implemented the algorithm in a calibrated NG-SIM network within a traffic micro-simulator-PTV VISSIM. With sole focus on energy, we report 47% reduction in energy consumption when compared with existing signal control schemes, however causing a 65.6% increase in system travel time. In contrast, the control strategy focusing on energy minimization with penalty for stops yields 6.7% reduction in energy consumption with 27% increase in system travel time. The developed RL algorithm with a flexible penalty function in the reward will achieve desired energy goals for a network of signalized intersections without compromising on the mobility performance.Disclaimer: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan(http://energy.gov/downloads/doe-public-access-plan					21st IEEE International Conference on Intelligent Transportation Systems (ITSC)21st IEEE International Conference on Intelligent Transportation Systems (ITSC)	NOV 04-07, 2018NOV 04-07, 2018	IEEE; IEEE Intelligent Transportat Syst SocIEEE; IEEE Intelligent Transportat Syst Soc	Maui, HIMaui, HI	7	0	0	0	0	0	9			2153-0009		978-1-7281-0323-5									Washington State Univ, Pullman, WA 99164 USAOak Ridge Natl Lab, Oak Ridge, TN 37830 USAPacific Northwest Natl Lab, Richland, WA USANatl Renewable Energy Lab, Golden, CO USA				2019-02-19	WOS:000457881301133		
C	Ikai, Yusaku; Yamamoto, Hidehiko; Yamada, Takayoshi						Jia, Y; Ito, T; Lee, JJ; Sugisaka, M				Unit Layout Design Supporting System of Cell Assembly Machine Using Two Robots by Reinforcement Learning								PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON ARTIFICIAL LIFE AND ROBOTICS (ICAROB 2016)								450	453											Proceedings Paper	2016	2016	In this study, we explain the development of Design Supporting System for Cell Assembly Machine System (CAMS) which systemizes the decision of the unit layout that composes the assembly machine using two robots. CAMS uses Profit Sharing which is one of the Reinforcement Learning methods, determining each units layout. We apply CAMS to the assembly of the differential gear box in automotive parts to verify its validity.					International Conference on Artificial Life and Robotics (ICAROB)International Conference on Artificial Life and Robotics (ICAROB)	JAN 29-31, 2016JAN 29-31, 2016	ALife Robot Corp Ltd; Int Conf Artificial Life & Robot, Int Steering Comm; IEEE Fukuoka Sect; IEEE Robot & Automat Soc; Chinese Assoc Artificial IntelligenceALife Robot Corp Ltd; Int Conf Artificial Life & Robot, Int Steering Comm; IEEE Fukuoka Sect; IEEE Robot & Automat Soc; Chinese Assoc Artificial Intelligence	Okinawa, JAPANOkinawa, JAPAN	0	0	0	0	0	0	0					978-4-9908350-1-9									Gifu Univ, Dept Human Informat Syst, Yanagido 1-1, Gifu, Gifu 5011194, JapanGifu Univ, Dept Mech Engn, Yanagido 1-1, Gifu, Gifu 5011194, Japan				2017-05-31	WOS:000400192600104		
P	WANG H; CHANG P; HUANG J; HSU C										Real-time obstacle avoidance system has            contrastive learning unit that maximizes relationship            between environmental sensing signal and environmental            data through cross-modal representation contrast            learning mechanism agreement					TW757999-B1; TW202223571-A	UNIV YANG MING CHIAO TUNG NAT																									NOVELTY - A real-time obstacle avoidance system, a real-time obstacle avoidance method and an unmanned vehicle with real-time obstacle avoidance function are disclosed. In the real-time obstacle avoidance system, the unmanned vehicle is in a specific environment and is set with an environmental sensing module, a data collection module and an operating module. The environmental sensing module is used to sense the specific environment to provide an environmental sensing signal. The data collection module is used to collect an environmental data related to the specific environment. The operating module is coupled to the environmental sensing module and the data collection module and used to receive the environmental sensing signal and the environmental data respectively and generate an autonomous navigation signal through a sim-to-real deep reinforcement learning mechanism. When the unmanned vehicle is driving in the specific environment, the unmanned vehicle avoids obstacles in real time according to the autonomous navigation signal.															0																		2022-05-19	DIIDW:202265156J		
J	Shen, L.-H.; Feng, K.-T.; Lee, T.-S.; Lin, Y.-C.; Lin, S.-C.; Chang, C.-C.; Chang, S.-F.										AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities [arXiv]								AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities [arXiv]																				Preprint	2024	2024	The requirement of wireless data demands is increasingly high as the sixth-generation (6G) technology evolves. Reconfigurable intelligent surface (RIS) is promisingly deemed to be one of 6G techniques for extending service coverage, reducing power consumption, and enhancing spectral efficiency. In this article, we have provided some fundamentals of RIS deployment in theory and hardware perspectives as well as utilization of artificial intelligence (AI) and machine learning. We conducted an intelligent deployment of RIS (i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs associated with an mmWave base station (BS) and a receiver. The RISs are deployed on the AGV with configured incident/reflection angles. While, both the mmWave BS and receiver are associated with an edge server monitoring downlink packets for obtaining system throughput. We have designed a federated multi-agent reinforcement learning scheme associated with several AGV-RIS agents and sub-agents per AGV-RIS consisting of the deployment of position, height, orientation and elevation angles. The experimental results presented the stationary measurement in different aspects and scenarios. The i-Dris can reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with comparably low complexity as well as rapid deployment, which outperforms the other existing works. At last, we highlight some opportunities and future issues in leveraging RIS-empowered wireless communication networks.									0	0	0	0	0	0	0																		2024-02-15	INSPEC:24511554		
B	[Anonymous]										2010 49th IEEE Conference on Decision and Control (CDC 2010)								2010 49th IEEE Conference on Decision and Control (CDC 2010)																				Conference Proceedings	2010	2010	The following topics are dealt with: tall transfer functions; singular spectra and econometric modelling; distributed parameter systems; adaptive control; sliding mode control; decentralized control; delays systems; control theory; electric smart grids; robotics; identification; aerospace; sensor networks; switched systems; filtering and estimation; optimisation; autonomous systems; stochastic control; robust control; network analysis and control; hybrid systems; stability; fault diagnosis; computational methods; biological and biomedical systems; model-based systems engineering; Markov processes; dynamic resource allocation and optimization in networks; energy efficient infrastructures; game theory; air traffic control systems theory; Kalman filtering; agents and autonomous systems; visual servo control; variational methods; energy systems; output feedback and observers; communication networks; plug and play control; image analysis; biology; networked control systems; randomization in systems and control; stochastic systems; decentralised network analysis; nonlinear systems; Petri nets; queueing systems; subspace methods; system identification and estimation; integrated vehicle dynamics and control; sparsity and compressive sensing in system identification; fluid flow systems; quantum information and control; adaptive dynamic programming; reinforcement learning; feedback control; cooperative control; mean field stochastic systems and control; H-infinity control; networked nonlinear dynamical systems; event-based control; Lyapunov methods; process control; LMI; medicine; optimal control; stochastic hybrid systems; switched systems; positivity constraints; electrical and power systems; networked control systems; automotive and aerospace systems; linear systems; constrained control; predictive control; model-controller reduction; geometric control on nonlinear manifolds; robot-assisted exploration; scalar potential fields; automotive control; mobile sensor networks; cooperative control; large-scale systems; formal methods in systems and control; secure control systems; modelling and control of bio mechanical systems; stochastic model predictive control; nonlinear predictive control; stochastic systems; algebraic methods; geometric methods; autonomous robots; nonlinear system identification; control of communication systems; robust stability; large-scale interconnections; information dynamics in social and economic networks; discrete event systems; analysis and design of alarm systems; systems with uncertainty; iterative learning control; distributed control; switched systems stability; joint spectral radius; dynamic networked multi-agent systems; sampled-data control; randomized algorithms; and behavioral systems.					2010 49th IEEE Conference on Decision and Control (CDC 2010)2010 49th IEEE Conference on Decision and Control (CDC 2010)	15-17 Dec. 201015-17 Dec. 2010		Atlanta, GA, USAAtlanta, GA, USA	0	0	0	0	0	0	0					978-1-4244-7745-6													2010-01-01	INSPEC:11848853		
C	Wang, Guohua; Siddhartha; Mishra, Kumar Vijay			IEEE							STAP in Automotive MIMO Radar with Transmitter Scheduling								2020 IEEE RADAR CONFERENCE (RADARCONF20)		IEEE Radar Conference																		Proceedings Paper	2020	2020	Automotive radars often employ multiple-input multiple-output (MIMO) array to attain high angular resolution with few antenna elements. The diversity gain is generally achieved by time-division multiplexing (TDM) during the transmission of frequency-modulated continuous-wave (FMCW) signals. However, TDM mode leads to longer pulse repetition intervals and, therefore, inherently and severely limits the maximum unambiguous Doppler velocity that a radar is able to detect. In this paper, we address the Doppler ambiguity problem in TDM MIMO automotive radars through a space-time adaptive processing (STAP) approach. A direct application of STAP may lead to a high antenna sidelobe level that hampers the detection performance. We mitigate this through optimal transmitter scheduling. We formulate the problem as combinatorial optimization and solve it via reinforcement learning. Numerical and experimental results demonstrate the efficacy of our method when compared with conventional techniques.					IEEE Radar Conference (RadarConf)IEEE Radar Conference (RadarConf)	SEP 21-25, 2020SEP 21-25, 2020	IEEEIEEE	Florence, ITALYFlorence, ITALY	1	0	0	0	0	0	1			1097-5764		978-1-7281-8942-0									Hertzwell Pte Ltd, Singapore 138565, Singapore	Hertzwell Pte Ltd			2021-02-22	WOS:000612224900275		
J	Leyendecker, Lars; Schmitz, Markus; Zhou, Hans Aoyang; Samsonov, Vladimir; Rittstieg, Marius; Lutticke, Daniel										Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach								INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING				16	03			381	402				10.1142/S1793351X22430024							Article; Proceedings Paper	SEP 2022	2022	For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing. Yet it still struggles to find widespread implementation in industrial environments. Traditional programming has so far proven to be insufficient in providing the required flexibility and dexterity to solve complex assembly tasks. Research in robotic control using deep reinforcement learning (DRL) advances quickly, however, the transfer to real-world applications in industrial settings is lagging behind. In this study, we apply DRL for robotic motion control to a multi-body contact automotive assembly task. Our focus lies on optimizing the final performance on the real-world setup. We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability of the controller's performance. We train the agent exclusively in simulation and successfully perform the Sim-to-Real transfer. Finally, we evaluate the controller's performance and robustness on an industrial setup and reflect its adherence to the high standards of automotive production.					5th IEEE International Conference on Robotic Computing5th IEEE International Conference on Robotic Computing	NOV 15-17, 2021NOV 15-17, 2021	IEEEIEEE	Taichung, TAIWANTaichung, TAIWAN	1	0	0	0	0	0	1			1793-351X	1793-7108										Fraunhofer Inst Prod Technol IPT, Steinbachstr 17, D-52074 Aachen, GermanyUniv Erlangen Nurnberg, Chair IT Management, Lange Gasse 20, D-90403 Nurnberg, GermanyRhein Westfal TH Aachen, Inst Informat Management Mech Engn, Dennewartstr 27, D-52068 Aachen, GermanyBMW AG, Dept Innovat & Digitalizat Data Analyt, Petuelring 130, D-80807 Munich, Germany				2022-08-30	WOS:000842125600006		
B	Xu, R.; Kuhl, M.; Von Hasseln, H.; Nowotka, D.										Reducing Overall Path Latency in Automotive Logical Execution Time Scheduling via Reinforcement Learning								RTNS2023: Proceedings of the 31st International Conference on Real-Time Networks and Systems								212	23				10.1145/3575757.3593658							Conference Paper	2023	2023	The Logical Execution Time paradigm is a promising approach for achieving time-deterministic communication on multi-core CPUs. Task scheduling under this paradigm is a variant of the Multi-Row Facility Layout Problem, which is known to be NP-hard. In this paper, we propose using reinforcement learning to reduce the overall path latency among all scheduled runnables while adhering to other constraints, such as schedulability, load balance, and data contention control. The neural networks, also known as agents, are trained on a real-world automotive powertrain project. We compare two schedules generated by the agents to the current one and one produced by a genetic algorithm. The agent trained with the Proximal Policy Optimization algorithm demonstrated the best performance. Additionally, we investigate the generalization ability of the agents against software updates, and the results show that our agents are well-generalized.					RTNS 2023: The 31st International Conference on Real-Time Networks and SystemsRTNS 2023: The 31st International Conference on Real-Time Networks and Systems	20232023		Dortmund, GermanyDortmund, Germany	0	0	0	0	0	0	0					978-1-4503-9983-8									Dependable Syst. Group, Kiel Univ., Kiel, GermanyMercedes-Benz AG, Germany				2023-09-14	INSPEC:23663420		
B	Thornton, C.E.; Howard, W.W.; Buehrer, R.M.										Online Learning-Based Waveform Selection for Improved Vehicle Recognition in Automotive Radar								ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)								1	5				10.1109/ICASSP49357.2023.10096969							Conference Paper	2023	2023	This paper describes important considerations and challenges associated with online reinforcement-learning based waveform selection for target identification in frequency modulated continuous wave (FMCW) automotive radar systems. We present a novel learning approach based on satisficing Thompson sampling, which quickly identifies a waveform expected to yield satisfactory classification performance. We demonstrate through measurement-level simulations that effective waveform selection strategies can be quickly learned, even in cases where the radar must select from a large catalog of candidate waveforms. The radar learns to adaptively select a bandwidth for appropriate resolution and a slow-time unimodular code for interference mitigation in the scene of interest by optimizing an expected classification metric.					ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	20232023		Rhodes, GreeceRhodes, Greece	0	0	0	0	0	0	0					978-1-7281-6327-7									Dept. of Electr. & Comput. Eng., Virginia Tech, Blacksburg, USA				2023-11-09	INSPEC:23945986		
C	Pan, Xinlei; Seita, Daniel; Gao, Yang; Canny, John			IEEE			Howard, A; Althoefer, K; Arai, F; Arrichiello, F; Caputo, B; Castellanos, J; Hauser, K; Isler, V; Kim, J; Liu, H; Oh, P; Santos, V; Scaramuzza, D; Ude, A; Voyles, R; Yamane, K; Okamura, A				Risk Averse Robust Adversarial Reinforcement Learning								2019 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)		IEEE International Conference on Robotics and Automation ICRA						8522	8528				10.1109/icra.2019.8794293							Proceedings Paper	2019	2019	Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary. Supplementary materials are available at https://sites.google.com/view/rararl.					IEEE International Conference on Robotics and Automation (ICRA)IEEE International Conference on Robotics and Automation (ICRA)	MAY 20-24, 2019MAY 20-24, 2019	Bosch; DJI; Kinova; Mercedes Benz; Samsung; Argo AI; Clearpath Robot; Element AI; Fetch Robot; Huawei; iRobot; KUKA; Quanser; SICK; Toyota Res Inst; Uber; Waymo; Zhejiang Lab; Amazon; Applanix; Cloudminds; Honda Res Inst; MathWorks; OusterBosch; DJI; Kinova; Mercedes Benz; Samsung; Argo AI; Clearpath Robot; Element AI; Fetch Robot; Huawei; iRobot; KUKA; Quanser; SICK; Toyota Res Inst; Uber; Waymo; Zhejiang Lab; Amazon; Applanix; Cloudminds; Honda Res Inst; MathWorks; Ouster	Montreal, CANADAMontreal, CANADA	33	2	0	0	0	0	35			1050-4729	2577-087X	978-1-5386-6026-3									Univ Calif Berkeley, Berkeley, CA 94720 USA				2019-12-06	WOS:000494942306035		
C	Leyendecker, Lars; Schmitz, Markus; Zhou, Hans Aoyang; Samsonov, Vladimir; Rittstieg, Marius; Lutticke, Daniel			IEEE Comp Soc							Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach								2021 FIFTH IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC 2021)								35	42				10.1109/IRC52146.2021.00012							Proceedings Paper	2021	2021	For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing. Yet it still struggles to find widespread implementation in industrial environments. Traditional programming has so far proven to be insufficient to provide the required flexibility and dexterity to solve complex assembly tasks. Although research in robotic control using deep reinforcement learning (DRL) advances quickly, the transfer to real-world applications in industrial settings is lagging behind. In this study, we apply DRL for robotic motion control at the use-case of a multi-body contact automotive assembly task and focus on optimizing the final performance on the real-world setup. We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability. We train an agent exclusively in simulation and successfully perform the Sim-to-Real transfer. Finally, we evaluate the controller's performance and robustness on an industrial setup and reflect its adherence to the high automotive production standards.					5th IEEE International Conference on Robotic Computing (IRC)5th IEEE International Conference on Robotic Computing (IRC)	NOV 15-17, 2021NOV 15-17, 2021	IEEE; IEEE Comp SocIEEE; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	4	0	0	0	0	0	5					978-1-6654-3416-4									Fraunhofer Inst Prod Technol IPT, Steinbachstr 17, D-52074 Aachen, GermanyDigitalizat BMW Grp, Dept Data Analyt & Innovat, Petuelring 130, D-80807 Munich, GermanyRhein Westfal TH Aachen, Inst Informat Management Mech Engn, Dennewartstr 27, D-52068 Aachen, Germany	Digitalizat BMW Grp			2022-05-04	WOS:000783796500005		
P	ZHANG R; GAO H; MA Q; FANG Q; ZHANG X; XU X; DU K; ZHOU X										Intelligent tracking control method for unmanned            vehicle, involves using apprentice learning method to            carry out lifting learning to obtain optimal strategy            according to initial strategy, and tracking and            controlling unmanned vehicle by using optimal            strategy					CN115688557-A	UNIV NAT DEFENSE TECHNOLOGY																									NOVELTY - The method involves collecting track data tracked by expert teaching. The track data is input into a deep neural network for deep learning to obtain initial strategy of expert teaching tracking. Apprentice learning method is used to carry out lifting learning to obtain optimal strategy according to the initial strategy. An unmanned vehicle is tracked and controlled by using the optimal strategy. An expert data set for expert teaching tracking is collected and stored. Data of the expert data set is generated by utilizing a built vehicle running high-simulation system to carry out multiple times of expert tracking driving, where the data of the expert data set comprises position data of the unmanned vehicle, attitude data, vehicle speed data, yaw angle data, direction angle data and vehicle distance lane line data. USE - Intelligent tracking control method for an unmanned vehicle. ADVANTAGE - The method enables improving driving stability of unmanned vehicle tracking driving, vehicle distance control and obstacle crossing to improve tracking driving performance. The method enables combining the artificial intelligence machine learning method and the apprentice learning method to obtain the initial strategy of the class person from the expert data by deep learning to finish reconstruction process of the reward function, thus obtaining the optimal strategy closer to the expert teaching tracking by reinforcement learning. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) an intelligent unmanned vehicle tracking control system;(2) a computer device comprising a memory stored with computer program for executing an intelligent tracking control method for an unmanned vehicle;(3) a computer readable storage medium storing computer program for executing an intelligent tracking control method for an unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an unmanned vehicle intelligent tracking control method. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2023191101		
J	Orlowski, Mateusz; Skruch, Pawel					Skruch, Pawel/0000-0002-8290-8375					Multiagent Manuvering with the Use of Reinforcement Learning								ELECTRONICS				12	8					1894			10.3390/electronics12081894							Article	APR 2023	2023	This paper presents an approach for defining, solving, and implementing dynamic cooperative maneuver problems in autonomous driving applications. The formulation of these problems considers a set of cooperating cars as part of a multiagent system. A reinforcement learning technique is applied to find a suboptimal policy. The key role in the presented approach is a multiagent maneuvering environment that allows for the simulation of car-like agents within an obstacle-constrained space. Each of the agents is tasked with reaching an individual goal, defined as a specific location in space. The policy is determined during the reinforcement learning process to reach a predetermined goal position for each of the simulated cars. In the experiments, three road scenarios-zipper, bottleneck, and crossroads-were used. The trained policy has been successful in solving the cooperation problem in all scenarios and the positive effects of applying shared rewards between agents have been presented and studied. The results obtained in this work provide a window of opportunity for various automotive applications.									0	0	0	0	0	0	0				2079-9292										Aptiv Serv Poland SA, Ul Podgorki Tynieckie 2, PL-30399 Krakow, PolandAGH Univ Sci & Technol, Dept Automat Control & Robot, Adam Mickiewicz Ave 30-B1, PL-30059 Krakow, Poland	Aptiv Serv Poland SA			2023-05-11	WOS:000977127300001		
P	DI K; CHEN W; JIANG Y										Layered reinforcement learning method for            uncertainty auxiliary task under confrontational scene,            involves performing task execution stage of lower layer            strengthening learning for solving task execution            problem caused by influence of undetermined auxiliary            task to single agent					CN116776963-A	UNIV SOUTHEAST																									NOVELTY - The method involves performing task distribution stage of an upper layer reinforcement learning. Global environment information is obtained by an intelligent agent. Auxiliary task information is extracted. Important main task information based on the auxiliary task information. Task distribution strategy is learned by other intelligent bodies. Task execution stage of a lower layer strengthening learning is performed. A sub-environment is constructed by the intelligent agent according to a distribution result. The task execution sequence is learnt in the sub-environment. The specific action is finally executed, where the upper layer strengthening learning is used for solving the task distribution problem caused by the influence of the undetermined auxiliary task to the multi-agents system, and the lower layer strengthening learning is used for solving the task execution problem caused by the influence of the undetermined auxiliary task to the single agent. USE - Layered reinforcement learning method for uncertainty auxiliary task under confrontational scene for autonomous unmanned system such as unmanned aerial vehicle, unmanned vehicle and bionic robot used in military, anti-terrorism and commerce. ADVANTAGE - The method enables efficiently helping the multi-agents system to learn hierarchical reinforcement learning algorithm how to execute the undetermined auxiliary task by firstly learning the multi-axial undefined auxiliary task distribution strategy of the upper layer, and learning the single-agent un-danced auxiliary task execution strategy of a lower layer. The influence of the determined auxiliary task to the group and the individual are solved, which can effectively reduce the complexity of the problem. The layered reinforcement learning process aims at uncertainty auxiliary task under confrontational scene. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a task allocation result calculated by an upper layer reinforcement learning for full intelligent agent cluster (Drawing includes non-English language text).															0																		2023-10-15	DIIDW:2023A08639		
J	Huang, Zhengrui; Wang, Shujie					Huang, Zhengrui/0000-0001-9445-2435; Wang, Shujie/0000-0002-8055-0234					Multilink and AUV-Assisted Energy-Efficient Underwater Emergency Communications								IEEE INTERNET OF THINGS JOURNAL				10	9			8068	8082				10.1109/JIOT.2022.3230322							Article	MAY 1 2023	2023	Recent development in wireless communications has provided many reliable solutions to emergency response issues, especially in scenarios with dysfunctional or congested base stations. Prior studies on underwater emergency communications, however, remain understudied, which poses a need for combining the merits of different underwater communication links (UCLs) and the manipulability of unmanned vehicles. To realize energy-efficient underwater emergency communications, we develop a novel underwater emergency communication network (UECN) assisted by multiple links, including underwater light, acoustic, and radio-frequency links, and autonomous underwater vehicles (AUVs) for collecting and transmitting underwater emergency data. First, we determine the optimal emergency response mode for an underwater sensor node (USN) using greedy search and reinforcement learning (RL), so that isolated USNs (I-USNs) can be identified. Second, according to the distribution of I-USNs, we dispatch AUVs to assist I-USNs in data transmission, i.e., jointly optimizing the locations and controls of AUVs to minimize the time for data collection and underwater movement. Finally, an adaptive clustering-based multiobjective evolutionary algorithm is proposed to jointly optimize the number of AUVs and the transmit power of I-USNs, subject to a given set of constraints on transmit power, signal-to-interference-plus-noise ratios (SINRs), outage probabilities, and energy, achieving the best tradeoff between the maximum emergency response time (ERT) and the total energy consumption (EC). Simulation results indicate that our proposed approach outperforms benchmark schemes in terms of energy efficiency (EE), contributing to underwater emergency communications.									3	0	0	0	0	0	3			2327-4662											Penn State Univ, Dept Geog, University Pk, PA 16802 USAPenn State Univ, Earth & Environm Syst Inst, University Pk, PA 16802 USAPenn State Univ, Inst Computat & Data Sci, University Pk, PA 16802 USA				2023-05-31	WOS:000976244700049		
B	Chu, A.; Xie, X.; Hermann, C.M.; Stork, W.; Roth-Stielow, J.						He, J.; Palpanas, T.; Hu, X.; Cuzzocrea, A.; Dou, D.; Slezak, D.; Wang, W.; Gruco, A.; Lin, J.C.-W.; Agrawal, R.				Towards Predictive Lifetime-Oriented Temperature Control of Power Electronics in E-vehicles via Reinforcement Learning								2023 IEEE International Conference on Big Data (BigData)								1667	76				10.1109/BigData59044.2023.10386292							Conference Paper	2023	2023	As the electric vehicle (EV) industry rapidly grows, the reliability of EVs is an ongoing challenge to the automotive industry. Among them, due to the introduction of electric motors, the aging and degradation of power electronics in EVs have a direct influence on the overall system safety and may lead to total failure. Therefore, extending the lifetime of power electronics has been a focus in the last decades, where reducing the temperature swings is the key to achieving the goal. However, temperature optimization usually requires future information on power loads, which is not available in classic approaches. Therefore, in this paper, we propose a baseline framework for lifetime-oriented temperature control with reinforcement learning (RL). Focusing on long-term prediction, the framework integrates various physical modelings of EV modules (sensors, actuators, vehicle dynamics and temperature management) and utilizes real-time route information to train an agent for driving behavior prediction. Based on further interaction with the EV model, future temperature development can be estimated in advance, thus enabling better swing optimization. Experiments demonstrate the effectiveness of our approach and the lifetime of power electronics can be extended by up to 63% on a representative test route. Compared to the classic approach, our predictive temperature control shows impressive energy efficiency by achieving up to 2.8x power loss reduction with better lifetime optimization.					2023 IEEE International Conference on Big Data (BigData)2023 IEEE International Conference on Big Data (BigData)	20232023	Ankura; IEEE DataportAnkura; IEEE Dataport	Sorrento, ItalySorrento, Italy	0	0	0	0	0	0	0					979-8-3503-2445-7									Karlsruhe Inst. of Technol., Karlsruhe, GermanyUniv. of Stuttgart, Stuttgart, Germany				2024-02-15	INSPEC:24502622		
P	ZHANG K; LI Z; HU J; DONG Y										Unmanned aerial vehicle automatic trajectory            planning method based on wireless optical            communication, involves using depth to reinforcement            learning plan flight path of unmanned aerial vehicle to            realize optimization target					CN116339370-A	TSINGHUA SHENZHEN INT GRADUATE SCHOOL																									NOVELTY - The method involves establishing an unmanned aerial vehicle auxiliary wireless optical communication channel model according to a visible light communication link channel model. Channel gain and channel capacity are calculated. Mobile space of an unmanned vehicle and a ground user is limited according to an actual application environment and the channel capacity. A minimum capacity threshold value of a communication link is determined. An optimization target is determined according to the mobile space of the unmanned vehicle. A depth to reinforcement learning plan flight path of unmanned aerial vehicle is used to realize optimization target. USE - Unmanned aerial vehicle automatic trajectory planning method based on wireless optical communication. ADVANTAGE - The method can dynamically adjust the degree of fairness of the unmanned aerial vehicle route planning, while improving the ground user communication capacity, at the same time, fully considering the communication fairness problem, and uses machine learning technology to optimize different target function to output the flight path planning route of the optimum. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer readable storage medium, has a set of instruction for unmanned aerial vehicle automatic trajectory planning method based on wireless optical communication. DESCRIPTION Of DRAWING(S) - The drawing shows an architecture diagram of the unmanned aerial vehicle automatic trajectory planning method based on wireless optical communication (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202371433U		
B	Venayagamoorthy, G.K.										Tutorial CICA-T. Computing with intelligence for identification and control of nonlinear systems								2009 IEEE Symposium on Computational Intelligence in Control and Automation. CICA 2009								1 pp.	1 pp.				10.1109/CICA.2009.4982774							Conference Paper	2009	2009	System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.					2009 IEEE Symposium on Computational Intelligence in Control and Automation. CICA 20092009 IEEE Symposium on Computational Intelligence in Control and Automation. CICA 2009	30 March-2 April 200930 March-2 April 2009		Nashville, TN, USANashville, TN, USA	0	0	0	0	0	0	0					978-1-4244-2752-9									Real-Time Power & Intell. Syst. Lab., Missouri Univ. of Sci. & Technol., Rolla, MO, USA				2009-01-01	INSPEC:10665748		
B	Biswas, A.; Anselma, P.G.; Emadi, A.										Real-Time Optimal Energy Management of Electrified Powertrains with Reinforcement Learning								2019 IEEE Transportation Electrification Conference and Expo (ITEC)								6 pp.	6 pp.				10.1109/ITEC.2019.8790482							Conference Paper	2019	2019	Reinforcement learning (RL)algorithm is employed in solving energy management problem for electrified powertrain in real-world driving scenarios and the application process is streamlined. A near-global optimal control policy is articulated for the energy management system (EMS) using Q-learning algorithm which is real-time implementable. The core of the EMS is an updating optimal control policy in the form of a changing look-up table comprising near-global optimal action value function (Q-values) corresponding to all feasible state-action combinations. Using the updating control policy, the EMS can optimally decide power-split between electric machines (EMs) and internal combustion engine (ICE) in real-world driving situations.					2019 IEEE Transportation Electrification Conference and Expo (ITEC)2019 IEEE Transportation Electrification Conference and Expo (ITEC)	19-21 June 201919-21 June 2019		Detroit, MI, USADetroit, MI, USA	0	0	0	0	0	0	0					978-1-5386-9310-0									McMaster Automotive Resource Centre, McMaster Univ., Hamilton, ON, CanadaDept. of Mech. & Aerosp. Eng., Politec. di Torino, Turin, Italy				2020-03-02	INSPEC:18888867		
J	Gaiselmann, Gerd; Altenburg, Stefan; Studer, Stefan; Peters, Steven				Peters, Steven/J-8422-2014	Peters, Steven/0000-0003-3131-1664					Deep reinforcement learning for gearshift controllers in automatic transmissions								ARRAY				15						100235			10.1016/j.array.2022.100235							Article	SEP 2022	2022	Control design for gearshifts in modern automotive automatic transmissions constitutes a challenging, time consuming task performed by highly trained experts. This is due to the fact that a variety of non-linear and partially observable systems need to be actuated, such that a comfortable shifting behavior is achieved within an sufficiently low shifting time. The presented approach leverages deep reinforcement learning (DRL) to control gear shifts, outperforming current state of the art controller performance. This requires formulating the shifting task as a Markov decision process by designing suitable action and observation spaces as well as a meaningful reward function. Due to the sample complexity of DRL methods, the control agents are trained in simulation and are subsequently transferred to a real transmission on a test bench. To successfully transfer DRL agents from simulation to reality, methods such as domain randomization and domain adaption leveraging evolutionary optimization are applied. To the best of the authors' knowledge, this work is the first to successfully apply DRL for the closed loop control of a real world automotive automatic transmission of realistic complexity.									2	0	0	0	0	0	2			2590-0056											Mercedes Benz Grp AG, Stuttgart, GermanyTech Univ Darmstadt, Inst Automot Engn FZD, Darmstadt, Germany	Mercedes Benz Grp AG			2022-09-01	WOS:001141399800002		
P	GONG H; ZHEN Z; CAO H; YIN H; ZHAO Q										Method for establishing and controlling unmanned            aerial vehicle formation environment based on deep            reinforcement learning, involves performing instruction            conversion and reward function, training designed            environment, converting action output by controller            into specific instruction					CN113885576-A	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - The method involves configuring a long aircraft to use a first-order speed holder and a second-order course holder automatic driving instrument model. The unmanned vehicle formation relative kinematics model is obtained according to small disturbance principle. A state space of a formation environment is designed. A staff maneuvering is configured as a library. An instruction conversion and a reward function are performed. A designed environment is trained. An action output by a controller is converted into a specific instruction and then input to a wing plane. USE - Method for establishing and controlling unmanned aerial vehicle formation environment based on deep reinforcement learning used in civil and military fields. ADVANTAGE - The unmanned aerial vehicle formation environment building and formation controller design based on deep reinforcement learning, the following aircraft can learn speed following the long machine and maintain the desired formation distance. The controller can make the wing plane strategy for self-learning learning, outputting the optimal action by the controller, at last, making the controller to control the speed and the course at the same time, wing plane the following long machine, and maintaining desired formation. In the actual application, the controller can form the corresponding aircraft instruction according to the characteristic of the unmanned aerial vehicles, so as to meet the requirement of unmanned aerial Vehicles precise formation control. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the unmanned aerial vehicle formation environment establishment and control based on deep reinforcement learning. (Drawing includes non-English language text)															0																		2023-08-10	DIIDW:2022113919		
B	Hoffmann, P.; Gorelik, K.; Ivanov, V.										Applicability Study of Model-Free Reinforcement Learning Towards an Automated Design Space Exploration Framework								2023 IEEE Symposium Series on Computational Intelligence (SSCI)								525	32				10.1109/SSCI52147.2023.10371864							Conference Paper	2023	2023	Design space exploration is a crucial aspect of engineering and optimization, focused on identifying optimal design configurations for complex systems with a high degree of freedom in the actor set. It involves systematic exploration while considering various constraints and requirements. One of the key challenges in design space exploration is the need for a control strategy tailored to the particular design. In this context, reinforcement learning has emerged as a promising solution approach for automatically inferring control strategies, thereby enabling efficient comparison of different designs. However, learning the optimal policy is computationally intensive, as the agent determines the optimal policy through trial and error. The focus of this study is on learning a single strategy for a given design and scenario, enabling the evaluation of numerous architectures within a limited time frame. The study also highlights the importance of plant modeling considering different modeling approaches to effectively capture the system complexity on the example of vehicle dynamics. In addition, a careful selection of an appropriate hyperparameter set for the reinforcement learning algorithm is emphasized to improve the overall performance and optimization process.					2023 IEEE Symposium Series on Computational Intelligence (SSCI)2023 IEEE Symposium Series on Computational Intelligence (SSCI)	20232023		Mexico City, MexicoMexico City, Mexico	0	0	0	0	0	0	0					978-1-6654-3065-4									Robert Bosch GmbH, Renningen, GermanyDept. of Mech. Eng., Tech. Universitiat Ilmenau, Ilmenau, Germany				2024-01-25	INSPEC:24350229		
B	Sharifi, Pouya										A Novel Reinforcement Learning-Optimization Approach for Integrating Wind Energy to Power System with Vehicle-To-Grid Technology																												Dissertation/Thesis	Jan 01 2020	2020										0	0	0	0	0	0	0					9798438740780									Texas A&M University, Industrial and Systems Engineering, Texas, United States	Texas A&M University				PQDT:49014150		
J	Sharif, M. Z.; Azmi, W. H.; Ghazali, M. F.; Zawawi, N. N. M.; Ali, H. M.				Azmi, WH/K-2563-2016; ghazali, mohd fairusham/K-2938-2016	Azmi, WH/0000-0003-0251-4024; ghazali, mohd fairusham/0000-0003-0178-2174; Mohd Zawawi, Nurul Nadia/0000-0001-8205-8895					Numerical and thermo-energy analysis of cycling in automotive air-conditioning operating with hybrid nanolubricants and R1234yf								NUMERICAL HEAT TRANSFER PART A-APPLICATIONS				83	9			935	957				10.1080/10407782.2022.2155277					DEC 2022		Article; Early Access		2023	Studies on automotive air-conditioning (AAC) systems involving compressor on-off cycling are still limited. This study focuses on improving the cycling of the AAC system using hybrid nanolubricants and hydrofluoroolefin-1234yf refrigerant. A dynamic model for an AAC system with a thermostatic switch that controls the on-off compressor was developed. The model was built in MATLAB Simulink and based on the state-space model using the fundamental conservation principles at the condenser, evaporator, and expansion valve. The experimental data were used to calculate the AAC system pressure, compressor, heat transfer coefficient of the condenser-evaporator, and expansion valve setting. The validation of the experimental data and the predicted data by the simulation suggested that the dynamic model could predict the AAC system's performance within +/- 5% deviation. The AAC system operating with Al2O3-SiO2/PAG nanolubricants has a lower temperature cycling frequency than the AAC system with the original PAG lubricant, representing less energy consumed. In addition, the AAC system with hybrid nanolubricants was performed with lower power consumption and significantly higher cooling capacity than the original system. The present simulation confirmed the feasibility of hybrid nanolubricants for application in an AAC system with a thermostatic switch.									6	0	0	0	0	0	6			1040-7782	1521-0634										Ctr Res Adv Fluid & Proc, Lebuhraya Tun Razak, Kuantan, Pahang, MalaysiaUniv Teknikal Malaysia Melaka, Fac Mech & Mfg Engn Technol, Durian Tunggal, MalaysiaUniv Malaysia Pahang, Fac Mech & Automot Engn Technol, Pekan, MalaysiaKing Fahd Univ Petr & Minerals, Mech Engn Dept, Dhahran, Saudi Arabia	Ctr Res Adv Fluid & Proc			2023-01-19	WOS:000907532400001		
P	WANG H; MENG Y; ZHANG X; MA Y; HAN L; ZHANG J; ZHANG W; XU S; JIN B										Smart training method for unmanned vehicles based            on simulation learning in virtual environment, involves            inputting optimal population individual and            low-dimensional simulation data set into neural network            to obtain driving behavior					CN111581887-A; CN111581887-B	UNIV ZHENGZHOU LIGHT IND																									NOVELTY - The method involves building a virtual scene. An unmanned vehicle motion model is established. The simulation is performed in the virtual scene built according to the unmanned vehicle motion model. The motion information of the unmanned vehicle is obtained as a simulation data set. The simulation data set is compressed, encoded and calculated to obtain a low-dimensional simulation data set. The low-dimensional simulation data set is fit to obtain the fitted value. The fitted value is input into the covariance matrix adaptation evolution strategy (CMA-ES) algorithm for reinforcement learning training to obtain the optimal population individual. The optimal population individual and low-dimensional simulation data set are input into the neural network to obtain driving behavior. USE - Smart training method for unmanned vehicles based on simulation learning in virtual environment. ADVANTAGE - The basis of the fitting value is optimized. The obtained optimal solution is added to the smart training process of the unmanned vehicle to guide the vehicle to learn good behavior. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a smart training method for unmanned vehicles based on simulation learning in virtual environment. (Drawing includes non-English language text)															0																		2023-08-10	DIIDW:2020846122		
C	Hamilton, Oliver K.; Breckon, Toby P.; Bai, Xuejiao; Kamata, Sei-ichiro			IEEE	Breckon, Toby/ABD-1451-2020	Breckon, Toby/0000-0003-1666-7590					A FOREGROUND OBJECT BASED QUANTITATIVE ASSESSMENT OF DENSE STEREO APPROACHES FOR USE IN AUTOMOTIVE ENVIRONMENTS								2013 20TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP 2013)		IEEE International Conference on Image Processing ICIP						418	422											Proceedings Paper	2013	2013	There has been significant recent interest in stereo correspondence algorithms for use in the urban automotive environment [1, 2, 3]. In this paper we evaluate a range of dense stereo algorithms, using a unique evaluation criterion which provides quantitative analysis of accuracy against range, based on ground truth 3D annotated object information. The results show that while some algorithms provide greater scene coverage, we see little differentiation in accuracy over short ranges, while the converse is shown over longer ranges. Within our long range accuracy analysis we see a distinct separation of relative algorithm performance. This study extends prior work on dense stereo evaluation of Block Matching (BM)[4], Semi-Global Block Matching (SGBM)[5], No Maximal Disparity (NoMD)[6], Cross[7], Adaptive Dynamic Programming (AdptDP)[8], Efficient Large Scale (ELAS)[9], Minimum Spanning Forest (MSF)[10] and Non-Local Aggregation (NLA)[11] using a novel quantitative metric relative to object range.					20th IEEE International Conference on Image Processing (ICIP)20th IEEE International Conference on Image Processing (ICIP)	SEP 15-18, 2013SEP 15-18, 2013	Inst Elect & Elect Engineers; IEEE Signal Proc SocInst Elect & Elect Engineers; IEEE Signal Proc Soc	Melbourne, AUSTRALIAMelbourne, AUSTRALIA	11	0	0	0	0	0	11			1522-4880		978-1-4799-2341-0									Cranfield Univ, Sch Engn, Cranfield MK43 0AL, Beds, EnglandWaseda Univ, Grad Sch Inf Prod & Syst, Kitakyushu, Fukuoka, Japan				2013-01-01	WOS:000351597600085		
P	XU R; ZHAO Q										Unmanned vehicle trajectory planning method based            on improved pruning neural network, involves inputting            vehicle surrounding state information collected by            unmanned vehicle into small parameter quantity depth            neural network model					CN115437378-A	UNIV NANJING SCI & TECHNOLOGY																									NOVELTY - The method involves designing a super-large parameter depth neural network model based on a CNN model. A deep neural network is trained by using a server cluster carrying multiple graphics processors. The deep neural networks are trained using a robust distillation method to perform robust compression on the trained deep network model to obtain a million-order parameter scale model. The vehicle surrounding state information collected by an unmanned vehicle is inputted into a small parameter quantity depth network model. The track planning process is performed by using the large parameter quantity network model according to the current vehicle surrounding status information. USE - Unmanned vehicle trajectory planning method based on improved pruning neural network. ADVANTAGE - The unmanned vehicle on the autonomous track planning stability is improved, reducing the possible presence of countermeasure attack, cannot avoid the sensor error, light change and other factors caused by the traditional vehicle-mounted neural network model cannot accurately predict the unstable condition of the path. The method uses a new neural network pruning method, by calculating the importance of the convolution filter in the hierarchical correlation propagation (LRP) algorithm neural network cutting the small convocation filter neural network function. The performance is better than the existing method. The robust distillation technology, and introduces confrontation learning, firstly recovering the robustness of the depth reinforcement learning the pruning strategy model after pruning. DESCRIPTION Of DRAWING(S) - The drawing shows a main flow chart of an unmanned vehicle trajectory planning method. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:2022F72084		
J	Lu Chao; Lu Hongliang; Yu Yang; Wang Haoyang; Wu Shaobin							吕超; 鲁洪良; 于洋; 王昊阳; 吴绍斌			Autonomous Overtaking Decision Making System Based on Hierarchical Reinforcement Learning and Social Preferences			基于分层强化学习和社会偏好的自主超车决策系统				中国公路学报	China Journal of Highway and Transport				35	3			115	126	1001-7372(2022)35:3<115:JYFCQH>2.0.TX;2-3										Article	2022	2022	To describe the interaction between the host vehicle (HV) and the overtaken vehicle (OV) in overtaking scenarios, the psychological term 'social preference' was introduced to describe the longitudinal behavioral pattern of OV, and a data-driven classification method was adopted to extract the social preference and incorporate it into the design of reinforcement learning based autonomous overtaking decision-making system (RL-based AODMS). By analyzing social preferences of overtaken vehicles based on the realistic overtaking data, this method was able to generate proper overtaking decisions in response to different preferences. First, the state transition probability of an overtaken vehicle during the overtaking interaction was calculated from a large number of realistic overtaking data and divided into three types: altruistic, egoistic, and prosocial. Then, a semi-model-based advanced Q-learning algorithm was proposed to integrate three preferences into decision model training. Meanwhile, an online classifier of social preference was built to determine the real-time preference of the overtaken vehicle. Combined with our previous study on lane-changing controllers, a hierarchical reinforcement learning based autonomous overtaking system (HRL-based AOS) was constructed. Finally, the joint validation on autonomous overtaking was done by collected realistic data and simulation. The results showed that the AODMS considering social preferences can predict the social preference of OV in real time and make reasonable decisions in complicated overtaking scenarios. Meanwhile, compared to the traditional AOS without considering social preference, the complete AOS constructed in this study showed better comfort and stability. To conclude, this study innovatively operationalizes data-driven social preference in overtaking decision making and improving the adaptability and rationality of decisions, which will contribute to the development of safe and reliable AOS.			针对超车过程中主车和被超越车之间的交互行为,引入心理学中的"社会偏好"来描述被超越车辆的纵向行为特征,并通过数据驱动的分类方法,提取社会偏好并将其融合在基于强化学习的自主超车决策系统设计中,通过分析大量真实超车数据中被超越车辆的社会偏好,认为该方法能够根据不同偏好情况产生合理的决策指令。首先,从大量真实超车数据中计算超车交互过程中被超越车辆的状态转移概率,并将其分成了3类(利他型、利己型和互惠型),并设计一种半基于模型改进的Q-learning算法,将3种社会偏好的概率考虑在决策模型训练中;然后,搭建实时的社会偏好分类器用于对被超越车辆社会偏好的实时分类;再结合换道控制器的研究结果,组合构建完整的分层强化学习自主超车系统;最后,通过实车采集数据与仿真环境进行了自主超车的联合验证。研究结果表明:考虑了社会偏好的自主超车决策系统能够对被超越车辆的社会偏好进行实时预测,进而在更加复杂的超车情况中做出合理的决策;相比于不考虑社会偏好的传统自主超车系统,构建的自主超车系统展现了更好的舒适性和稳定性。研究结果创新性地将数据驱动的社会偏好作用于超车决策过程,提升了决策的自适应性和合理性,将有助于发展安全可靠的自主超车系统。						0	0	0	0	0	0	0			1001-7372											北京理工大学机械与车辆学院, 北京 100081, 中国School of Mechanical Engineering, Beijing Institute of Technology, Beijing 100081, China	北京理工大学机械与车辆学院School of Mechanical Engineering, Beijing Institute of Technology			2022-07-15	CSCD:7208608		
P	ZHANG C; WANG S; HUANG Z										Unmanned lane change decision control method based            on near-end policy optimization algorithm involves            using trained unmanned lane-change decision control            model based on near-end policy optimization algorithm            to perform lane change to verify reliability of            unmanned lane-change decision control model					CN115973156-A	UNIV BEIJING TECHNOLOGY																									NOVELTY - The method involves constructing an unmanned lane-change decision control model based on a near-end policy optimization algorithm. An input sensor obtains a current environment information of an unmanned vehicle. A current environment information is inputted. A data buffer area of the unmanned lane-change decision control model is established. The unmanned lane-change decision control model is trained based on the near-end policy optimization algorithm. A speed of the unmanned vehicle and a target driving state are comprehensively considered. The unmanned lane-change decision control model is tested based on the near-end policy optimization algorithm. The trained unmanned lane-change decision control model is used based on the near-end policy optimization algorithm to perform a lane change in an environment to verify a reliability of the unmanned lane-change decision control model. USE - Unmanned lane change decision control method based on a near-end policy optimization algorithm. ADVANTAGE - The learning rate in the deep reinforcement learning difficult to set, at the same time, it improves the convergence speed of the model training. The unmanned vehicle interacts with the environment to obtain the current environment information, action space, the logarithm probability of the action space, the reward value obtained by executing the action at environment at the next time, and the back-closing mark is a terminal. The method uses PPO algorithm, which can better solve the problem that the learning rate of the deep reinforcement learning is difficult to be set, and it is possible to reduce the convergence time of the unmanned lane-free decision control model. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the unmanned lane change decision control method based on a near-end policy optimization algorithm (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202347327Q		
R	Kaichen, Ying										DATA of Fuzzy_Q-learning_Interaction_Controller_Design_for_Collaborative_Robot								Figshare													https://doi.org/10.6084/m9.figshare.20747437.v2							Data set	2022-09-08	2022	DATA of Fuzzy_Q-learning_Interaction_Controller_Design_for_Collaborative_Robot Copyright: CC BY 4.0									0	0	0	0	0	0	0																		2022-10-08	DRCI:DATA2022157025063952		
J	Drechsler, M. Funk; Fiorentin, T. A.; Goellinger, H.					Gollinger, Harald/0000-0003-1593-7800; Funk Drechsler, Maikol/0000-0002-7360-7829; Fiorentin, Thiago Antonio/0000-0002-2559-7574					Actor-Critic Traction Control Based on Reinforcement Learning with Open-Loop Training								MODELLING AND SIMULATION IN ENGINEERING				2021						4641450			10.1155/2021/4641450							Article	DEC 7 2021	2021	The use of actor-critic algorithms can improve the controllers currently implemented in automotive applications. This method combines reinforcement learning (RL) and neural networks to achieve the possibility of controlling nonlinear systems with real-time capabilities. Actor-critic algorithms were already applied with success in different controllers including autonomous driving, antilock braking system (ABS), and electronic stability control (ESC). However, in the current researches, virtual environments are implemented for the training process instead of using real plants to obtain the datasets. This limitation is given by trial and error methods implemented for the training process, which generates considerable risks in case the controller directly acts on the real plant. In this way, the present research proposes and evaluates an open-loop training process, which permits the data acquisition without the control interaction and an open-loop training of the neural networks. The performance of the trained controllers is evaluated by a design of experiments (DOE) to understand how it is affected by the generated dataset. The results present a successful application of open-loop training architecture. The controller can maintain the slip ratio under adequate levels during maneuvers on different floors, including grounds that are not applied during the training process. The actor neural network is also able to identify the different floors and change the acceleration profile according to the characteristics of each ground.									1	0	0	0	0	0	1			1687-5591	1687-5605										TH Ingolstadt, Ingolstadt, GermanyUniv Fed Santa Catarina, Ctr Tecnol Joinville, Joinville, Brazil	TH Ingolstadt			2022-05-29	WOS:000796912900001		
P	ZHANG B; ZHANG P										Multi-target cooperative tracking control method            of multi-unmanned aerial vehicle based on Multi-Agent            PPO (MAPPO) algorithm used in automatic driving,            involves controlling unmanned aerial vehicle to work            according to each action control quantity					CN115509251-A	UNIV SOUTH CHINA TECHNOLOGY																									NOVELTY - The method involves modeling a multi-unmanned aerial vehicle target tracking process. An unmanned aerial vehicle six-degree freedom kinematics model, a moving target constant turning rate and a speed model are established. A state value function, an action value function and a reward return function are constructed. A depth neural network structure is constructed, where the strategy network structure and a value network structure are provided with strategy network structures and value network structures. A multi-target cooperative tracking controller of multiple unmanned aerial vehicles is obtained based on a MAPPO algorithm. A local observation state of each unmanned aerialvehicle is inputted into the multi-objective tracking controller. An action control quantity of the unmanned vehicle is obtained. The unmanned vehicle to work is controlled according to the action control amount. USE - Multi-unmanned aerial vehicle multi-target cooperative tracking control method based on depth-reinforcement learning MAPPO algorithm used in automatic driving, computer vision, medical diagnosis and robot control. ADVANTAGE - The method enables utilizing a distributed frame to reduce requirement of the unmanned aerial vehicle to communication and calculation ability, thus effectively solving problem that the traditional multi-target tracking method of multiple unmanned aerial vehicles is large in calculation amount, and avoiding single intelligent body depth reinforcement learning algorithm processing multiple intelligent body problem caused by centralized control dimension. DESCRIPTION Of DRAWING(S) - The drawing shows a training flowchart of the multi-unmanned aerial vehicle multi-target cooperative tracking control method based on the Multi-Agent PPO (MAPPO) algorithm. (Drawing includes non-English language text)															0																		2023-01-12	DIIDW:202301957Y		
J	Luthra, S.; Mangla, S.K.; Kumar, S.; Garg, D.; Haleem, A.				Luthra, Sunil/D-4135-2014	Luthra, Sunil/0000-0001-7571-1331; Haleem, Abid/0000-0002-3487-0229					Identify and prioritise the critical factors in implementing the reverse logistics practices: a case of Indian auto component manufacturer								International Journal of Business and Systems Research				11	1-2			42	61											Journal Paper	2017	2017	In recent years, reverse logistics (RL) practices have been perceived a great recognition among researchers/practitioners. In this paper, we intend to identify and prioritise the critical factors (CFs) in implementing the RL practices, from the industrial viewpoint. There are 13 CFs crucial in accomplishing the RL practices were recognised on the basis of critical review of literature and experts opinion. These finalised 13 CFs were then analysed to determine their priority by means of analytical hierarchy process (AHP) technique. The AHP technique assists in determining the relative importance of the identified RL implementation critical factors. The findings of the work may help managers to address the related issues in RL implementation. Inputs needed to carry out this research work are taken from an Indian automotive components manufacturing company. The results of the study may help researchers/practitioners to prioritise their efforts to implement RL practices in effective manner. In the end, sensitivity analysis is carried out to examine the proposed RL implementation CFs stability.									15	0	0	0	3	0	15			1751-200X											Dept. of Mech. Eng., Gov. Polytech., Jhajjar, Jhajjar, IndiaDept. of Mech. Eng., Graphic Era Univ., Dehradun, IndiaDept. of Mech. Eng., Int. Inst. of Technol. & Manage., Murthal, IndiaDept. of Mech. Eng., Nat. Inst. of Technol., Kurukshetra, Kurukshetra, IndiaDept. of Mech. Eng., Jamia Millia Islamia Univ., New Delhi, India				2017-04-13	INSPEC:16771097		
B	El Mazgualdi, C.; Masrour, T.; El Hassani, I.; Khdoudi, A.						Masrour, T.; Cherrafi, A.; El Hassani, I.				A deep reinforcement learning (DRL) decision model for heating process parameters identification in automotive glass manufacturing								Artificial Intelligence and Industrial Applications. Smart Operation Management. Advances in Intelligent Systems and Computing (AISC 1193)								77	87				10.1007/978-3-030-51186-9_6							Conference Paper	2021	2021	This research investigates the applicability of Deep Reinforcement Learning (DRL) to control the heating process parameters of tempered glass in industrial electric furnace. In most cases, these heating process parameters, also called recipe, are given by a trial and error procedure according to the expert process experience. In order to optimize the time and the cost associated to this recipe choice, we developed an offline decision system which consists of a deep reinforcement learning framework, using Deep Q-Network (DQN) algorithm, and a self-prediction artificial neural network model. This decision system is used to define the main heating parameters (the glass transfer speed and the zone temperature) based on the desired outlet temperature of the glass, and it has the capacity to improve its performance without further human assistance. The results show that our DQN algorithm converges to the optimal policy, and our decision system provides good recipe for the heating process with deviation not exceeding process limits. To our knowledge, it is the first demonstrated usage of deep reinforcement learning for heating process of tempered glass specifically and tempering process in general. This work also provides the basis for dealing with the problem of energy consumption during the tempering process in electric furnace.					Artificial Intelligence and Industrial Applications. Smart Operation ManagementArtificial Intelligence and Industrial Applications. Smart Operation Management	19-20 March 202019-20 March 2020		Meknes, MoroccoMeknes, Morocco	1	0	0	0	0	0	1					978-3-030-51185-2									Lab. of Math. Modeling, Simulation & Smart Syst., Moulay Ismail Univ., Meknes, MoroccoDept. of Math. & Comput. Sci., Univ. Moulay Ismail Nat. Sch. of Arts & Crafts Meknes, Meknes, MoroccoDept. of Ind. & Manuf. Eng., Modeling & Optimization of Ind. Syst. Team, Univ. Moulay Ismail Nat. Sch. of Arts & Crafts Meknes, Meknes, MoroccoDept. of Ind. & Manuf. Eng., Artificial Intell. for Eng. Sci. Team (IASI), Univ. Moulay Ismail Nat. Sch. of Arts & Crafts Meknes, Meknes, Morocco				2020-10-09	INSPEC:19952827		
P	LIU Y; ZOU Y; FAN J; ZHANG X										Method for planning unknown environment search            path, involves planning collision-free path from            current position of unmanned vehicle at current moment            to target edge point according to grid map at current            moment and target edge point					CN116203972-A; CN116203972-B	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves obtaining (S1) a vehicle pose data and laser radar data at a current moment, where the vehicle pose data is the pose data of an unmanned vehicle used to explore an unknown environment. The grid map at the previous moment is updated (S2) by using the vehicle pose data and laser radar data at the current moment. The grid map at the current moment is obtained. The edge point is detected (S3) on the grid map at the current moment to obtain multiple edge points by using an edge detection algorithm. The grid map at the current moment is taken (S4) as input. A potential target point is selected as a target edge point based on a reinforcement learning algorithm. A collision-free path from the current position of unmanned vehicle at the current moment to the target edge point is planned (S5) according to the grid map at the current moment and the target edge point. Determination is made to check whether the exploration of the unknown environment is completed. USE - Method for planning unknown environment search path. ADVANTAGE - The method selects target edge points through a reinforcement learning algorithm, can solve the situation of long planning path and early termination of exploration, improves the exploration efficiency of unknown environment, and can realize full exploration of unknown environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a system for planning unknown environment search path;a device for planning unknown environment search path; anda computer readable storage medium has a set of instructions for planning unknown environment search path. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for planning unknown environment search path (Drawing includes non-English language text).S1Step for obtaining vehicle pose data and laser radar data at current momentS2Step for updating grid map at previous moment by using vehicle pose data and laser radar data at current momentS3Step for detecting edge point on grid map at current moment to obtain multiple edge points by using edge detection algorithmS4Step for taking grid map at current moment as inputS5Step for planning collision-free path from current position of unmanned vehicle at current moment to target edge point according to grid map at current moment and target edge point															0																		2023-07-15	DIIDW:202361684T		
P	SU B; HE X										Wing parachute motion            proportional-integral-derivative self-adaptive            controlling method, involves selecting certain state            sub-space similar to current state of wing parachute to            synthesize proportional-integral-derivative control            parameter for finishing motion control of wing            parachute system					CN116088295-A	AEROSPACE LIFE SUPPORT IND LTD; UNIV WUHAN																									NOVELTY - The method involves selecting key characteristic factors that affect dynamic performance of wing parachute to generate state space and dividing the state space into multiple state sub-spaces. Effective proportional-integral-derivative (PID) parameter of each state sub-space is obtained by performing deep reinforcement learning process. Certain state sub-space similar to a current state of the wing parachute is selected to synthesize PID control parameter under a current flying state of the wing parachute by utilizing effective PID parameter for finishing motion control of a wing parachute system. USE - Wing parachute motion PID self-adaptive controlling method for training control strategies of various unmanned equipment e.g. unmanned aerial vehicle, unmanned vehicle and robot. ADVANTAGE - The method enables synthesizing the effective PID control parameter under the current flying state of the wing parachute according to deep reinforcement learning PID control strategy, so that the motion control of the wing parachute system can be completed in an efficient manner, thus realizing complementary advantages of the deep reinforcement learning process and a PID controller. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating a wing parachute motion proportional-integral-derivative self-adaptive controlling method. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202352812W		
J	Turlej, Wojciech										High-Level Sensor Models for the Reinforcement Learning Driving Policy Training								ELECTRONICS				12	1					71			10.3390/electronics12010071							Article	JAN 2023	2023	Performance limitations of automotive sensors and the resulting perception errors are one of the most critical limitations in the design of Advanced Driver Assistance Systems and Autonomous Driving Systems. Ability to efficiently recreate realistic error patterns in a traffic simulation setup not only helps to ensure that such systems operate correctly in presence of perception errors, but also fulfills a key role in the training of Machine-Learning-based algorithms often utilized in them. This paper proposes a set of efficient sensor models for detecting road users and static road features. Applicability of the models is presented on an example of Reinforcement-Learning-based driving policy training. Experimental results demonstrate a significant increase in the policy's robustness to perception errors, alleviating issues caused by the differences between the virtual traffic environment used in the policy's training and the realistic conditions.									0	0	0	0	0	0	0				2079-9292										Aptiv Serv Poland SA, Ul Podgorki Tynieckie 2, PL-30399 Krakow, PolandAGH Univ Sci & Technol, Al Mickiweicza 30, PL-30059 Krakow, Poland	Aptiv Serv Poland SA			2023-01-28	WOS:000909162700001		
J	Pang Ke; Zhang Yanxin; Yin Chenkun										A Decision-making Method for Self-driving Based on Deep Reinforcement Learning								Journal of Physics: Conference Series				1576				012025 (8 pp.)	012025 (8 pp.)				10.1088/1742-6596/1576/1/012025							Conference Paper; Journal Paper	2020	2020	L5-level autonomous driving is the development trend of the future in the automotive industry, and the realization of autonomous driving through deep reinforcement learning algorithms are one of the research directions. Soft Actor-Critic the algorithm adds the maximum entropy term to the original deep reinforcement learning the objective function, and it shows great advantages in continuous control problems. Here, based on the open-source platform TORCS, this algorithm will be used to conduct automatic driving simulation experiments, design a reasonable reward function, add relevant constraints, use vehicle radar sensor information to make automatic driving decisions, and compare experiments with the Deep Deterministic Policy Gradient Algorithm. SAC can effectively extend training time, improve stability, and improve generalization ability.					4th International Conference on Artificial Intelligence, Automation and Control Technologies (AIACT 2020)4th International Conference on Artificial Intelligence, Automation and Control Technologies (AIACT 2020)	24-26 April 202024-26 April 2020		Hangzhou, ChinaHangzhou, China	0	0	0	0	0	0	0			1742-6596											Beijing Jiaotong Univ., Beijing, China				2020-08-07	INSPEC:19769227		
P	ZHANG Y; NING N; WU J; SHI H; JIN Z; ZHOU Y										Air-ground mobile network carrying fair            communication method, involves modeling unmanned aerial            vehicle energy carrying communication problem as            multi-target integer non-convex optimization problem of            fair throughput					CN115802313-A	UNIV HENAN																									NOVELTY - The method involves establishing (S1) an air-ground mobile network architecture based on multiple unmanned aerial vehicle and an intelligent reflecting surface. A wireless communication model is established (S2) by using the intelligent reflective surface to reconstruct a channel state between the unmanned aircraft and a ground user. A fair communication model for considering the communication efficiency and fairness of the user is established (S5). A judging matrix of fair throughput and energy consumption is constructed (S6) according to the user service quality level. The unmanned vehicle energy carrying communication problem is modeled (S7) as a multi-target integer non-convex optimization problem of fair bandwidth and unmanned machine residual energy maximization. A position of unmanned aircraft is updated (S8) by multi-intelligent housing depth reinforcement learning. USE - Air-ground mobile network carrying fair communication method for performing fair carrying communication based on unmanned aerial vehicle and intelligent reflecting surface under limited communication resource. ADVANTAGE - The network optimizes the phase of unmanned aerial vehicle position and intelligent reflecting surface based on multi-intelligent depth reinforcement learning provides fair communication for the ground user and wirelessly charges the unmanned aerial vehicles. The communication efficiency and fairness of the user establishing fair communication model, maximizing the system throughput under the premise of ensuring the user fairness. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating an unmanned aerial vehicle auxiliary communication. (Drawing includes non-English language text).S1Step for establishing an air-ground mobile network architecture based on multiple unmanned aerial vehicle and an intelligent reflecting surfaceS2Step for establishing a wireless communication model by using the intelligent reflective surface to reconstruct a channel state between the unmanned aircraft and a ground userS5Step for establishing a fair communication model for considering the communication efficiency and fairness of the userS6Step for constructing a judging matrix of fair throughput and energy consumption according to the user service quality levelS7Step for modeling the unmanned vehicle energy carrying communication problem as a multi-target integer non-convex optimization problem of fair bandwidth and unmanned machine residual energy maximizationS8Step for updating a position of unmanned aircraft by multi-intelligent body depth reinforcement learning															0																		2023-08-10	DIIDW:202331529S		
J	Lian, Renzong; Tan, Huachun; Peng, Jiankun; Li, Qin; Wu, Yuankai					Yuan-Kai, Wu/0000-0003-4435-9413; lian, ren zong/0000-0001-9804-6370					Cross-Type Transfer for Deep Reinforcement Learning Based Hybrid Electric Vehicle Energy Management								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				69	8			8367	8380				10.1109/TVT.2020.2999263							Article	AUG 2020	2020	Developing energy management strategies (EMSs) for different types of hybrid electric vehicles (HEVs) is a time-consuming and laborious task for automotive engineers. Experienced engineers can reduce the developing cycle by exploiting the commonalities between different types of HEV EMSs. Aiming at improving the efficiency of HEV EMSs development automatically, this paper proposes a transfer learning based method to achieve the cross-type knowledge transfer between deep reinforcement learning (DRL) based EMSs. Specifically, knowledge transfer among four significantly different types of HEVs is studied. We first use massive driving cycles to train a DRL-based EMS for Prius. Then the parameters of its deep neural networks, wherein the common knowledge of energy management is captured, are transferred into EMSs of a power-split bus, a series vehicle and a series-parallel bus. Finally, the parameters of 3 different HEV EMSs are fine-tuned in a small dataset. Simulation results indicate that, by incorporating transfer learning (TL) into DRL-based EMS for HEVs, an average 70% gap from the baseline in respect of convergence efficiency has been achieved. Our study also shows that TL can transfer knowledge between two HEVs that have significantly different structures. Overall, TL is conducive to boost the development process for HEV EMS.									64	5	0	0	0	0	69			0018-9545	1939-9359										Beijing Inst Technol, Sch Mech Engn, Beijing 100081, Peoples R ChinaSoutheast Univ, Sch Transportat, Nanjing 211189, Peoples R ChinaMcGill Univ, Dept Civil Engn & Appl Mech, Montreal, PQ H3A 0C3, Canada				2020-09-01	WOS:000560695700026		
C	Schoeman, Marlize; Marchand, Renier; van Tonder, Johann; Jakobus, Ulrich; Aguilar, Andres; Longtin, Kitty; Vogel, Martin; Alwajeeh, Taha			IEEE							New Features in Feko/WinProp 2019								2020 INTERNATIONAL APPLIED COMPUTATIONAL ELECTROMAGNETICS SOCIETY SYMPOSIUM (2020 ACES-MONTEREY)																				Proceedings Paper	2020	2020	This paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp). These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.					International Symposium of Applied-Computational-Electromagnetics-Society (ACES)International Symposium of Applied-Computational-Electromagnetics-Society (ACES)	JUL 27-31, 2020JUL 27-31, 2020	Appl Computat Electromagnet SocAppl Computat Electromagnet Soc	ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0					978-1-7335096-0-2									Altair Dev SA Pty Ltd, Stellenbosch, South AfricaAltair Engn GmbH, Boblingen, GermanyAltair Engn Inc, Hampton, VA USAAltair Engn France, Meylan, France	Altair Dev SA Pty LtdAltair Engn GmbHAltair Engn IncAltair Engn France			2021-04-28	WOS:000634983400034		
J	Wang, Yong; Tan, Huachun; Wu, Yuankai; Peng, Jiankun					Yuan-Kai, Wu/0000-0003-4435-9413					Hybrid Electric Vehicle Energy Management With Computer Vision and Deep Reinforcement Learning								IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS				17	6			3857	3868				10.1109/TII.2020.3015748							Article	JUN 2021	2021	Modern automotive systems have been equipped with a highly increasing number of onboard computer vision hardware and software, which are considered to be beneficial for achieving eco-driving. This article combines computer vision and deep reinforcement learning (DRL) to improve the fuel economy of hybrid electric vehicles. The proposed method is capable of autonomously learning the optimal control policy from visual inputs. The state-of-the-art convolutional neural networks-based object detection method is utilized to extract available visual information from onboard cameras. The detected visual information is used as a state input for a continuous DRL model to output energy management strategies. To evaluate the proposed method, we construct 100 km real city and highway driving cycles, in which visual information is incorporated. The results show that the DRL-based system with visual information consumes 4.3-8.8% less fuel compared with the one without visual information, and the proposed method achieves 96.5% fuel economy of the global optimum-dynamic programming.									49	3	0	0	0	0	51			1551-3203	1941-0050										Beijing Inst Technol, Dept Civil Engn & Appl Mech, Sch Mech Engn, Beijing 100081, Peoples R ChinaSoutheast Univ, Sch Transportat, Nanjing 211189, Peoples R China				2021-04-19	WOS:000626556300014		
J	Schoeman, Marlize; Marchand, Renier; van Tonder, Johann; Jakobus, Ulrich; Aguilar, Andres; Longtin, Kitty; Vogel, Martin; Alwajeeh, Taha										New Features in Feko and WinProp 2019								APPLIED COMPUTATIONAL ELECTROMAGNETICS SOCIETY JOURNAL				35	11			1354	1355				10.47037/2020.ACES.J.351146							Article	NOV 2020	2020	paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp). These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.									0	0	0	0	0	0	0			1054-4887	1943-5711										Altair Dev SA Pty Ltd, Stellenbosch, South AfricaAltair Engn GmbH, Boblingen, GermanyAltair Engn Inc, Hampton, VA USAAltair Engn France, Meylan, France	Altair Dev SA Pty LtdAltair Engn GmbHAltair Engn IncAltair Engn France			2021-10-21	WOS:000705547200012		
J	Leng, Jinling; Wang, Xingyuan; Wu, Shiping; Jin, Chun; Tang, Meng; Liu, Rui; Vogl, Alexander; Liu, Huiyu					Jin, Chun/0000-0003-4067-6817					A multi-objective reinforcement learning approach for resequencing scheduling problems in automotive manufacturing systems								INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH				61	15			5156	5175				10.1080/00207543.2022.2098871					AUG 2022		Article; Early Access		2023	This study investigated a multi-objective resequencing scheduling problem in the automotive manufacturing systems due to operational requirements on the color-batching of the paint shop and sequential requirements on the sequence adherence of an assembly shop. Resequencing cars as color-oriented batches reduced the costs of color changes and operational costs for paint shops. Also, assembly shops required paint shops to complete cars with fewer delays so that high sequence adherence with its demand was assured. Based on real-world applications, we investigated two contradictory objectives-color change costs and sequence tardiness-in a single-machine flowshop scheduling environment. A multi-objective-deep-Q-network algorithm was developed to determine the Pareto frontier. Reward shaping was designed to improve the convergence of the neural network. The 2D-folded-normal distribution was designed to sample the preference, which made the exploration and exploitation of the neural network more comprehensive and improved the training efficiency. Two experiments were conducted and showed that the proposed approach outperformed the meta-heuristic algorithm and the envelope Q-learning algorithm in solving time, performance, the convergence of the neural network, and the diversity of the Pareto frontier. Therefore, the proposed approach can be used in automotive paint shops to improve scheduling efficiency and reduce operational costs.									3	0	0	0	0	0	3			0020-7543	1366-588X										Dalian Univ Technol, Sch Econ & Management, Dalian, Peoples R ChinaBMW Brilliance Automot Ltd, Shenyang, Peoples R ChinaNortheastern Univ, Sch Mech Engn & Automat, Shenyang, Peoples R China				2022-08-16	WOS:000836975500001		
P	COENEN O; SINYAVSKIY O; POLONICHKO V										Non-transitory computer-readable storage medium            for performing robotic control of exploration using            e.g. automotive robots for internet traffic routing            application, has instructions for adjusting            stochasticity level to effectuate control					US9189730-B1	BRAIN CORP																									NOVELTY - The medium has a set of instructions for operating spiking neuron in accordance with a reinforcement learning process. A performance metric of a reinforcement learning process is evaluated (1042) by comparing the performance metric to a performance value. A stochasticity level is adjusted to effectuate control of exploration based on the performance metric by decreasing in the stochasticity level when the performance metric is above the performance value by processors, where the performance metric comprises a distance measure between the present process outcome and the target outcome. USE - Non-transitory computer-readable storage medium for performing robotic control of an exploration by a spiking neuron using robotic devices e.g. automotive robots, military devices and surgical robots, for learning applications e.g. internet traffic routing application, visual, auditory or tactile recognition application, assisted air-traffic controller application, robust airplane controller application, adaptive electronic assistant application for mobile and adaptive toys for humans or animals. Can also be used for signal processing systems e.g. machine vision systems, pattern detection and pattern recognition systems, object classification systems, signal filtering systems, data segmentation systems, data compression systems, data mining systems, optimization and scheduling systems and complex mapping systems. ADVANTAGE - The medium enables providing subsequent lowering of stochasticity to reduce energy use and spiking noise when target performance is attained and adaptive stochasticity control increases learning speed of a network. The medium allows a robotic controller to employ a lower-performance version of controller hardware to reduce controller cost, size and energy use. The medium enables combining sensory and state encoding in high dimension by a multitude of linear and nonlinear kernel functions and operators with a reinforcement learning controller, which can efficiently learn to make use of distributed and parallel input signals to construct a controller realizing an optimization of a performance function. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a computer-implemented method for reducing energy use by a computerized spiking neuron network apparatus(2) a reconfigurable robotic controller apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating reinforcement signal generation.Method for performing control of exploration by spiking neuron (1040)Step for evaluating performance metric of reinforcement learning process (1042)Step for comparing performance measure to criterion (1044)Step for generating reinforcement signal mode (1046)Step for generating another reinforcement signal mode (1048)															0																		2015-01-02	DIIDW:201571099S		
J	Loffredo, Alberto; May, Marvin Carl; Schaefer, Louis; Matta, Andrea; Lanza, Gisela										Reinforcement learning for energy-efficient control of parallel and identical machines								CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY				44				91	103				10.1016/j.cirpj.2023.05.007					JUN 2023		Article	SEP 2023	2023	Nowadays, the growing interest in industry for enhancing the sustainability of manufacturing processes is becoming a major trend. Energy consumption can be lowered by controlling machine states with energy -efficient control policies that switch off/on the device. Recent studies have shown that Reinforcement Learning algorithms can effectively control manufacturing systems without the requirement of prior knowledge about system parameters. This is a significant factor since full information on system dynamics is difficult to obtain in real-world applications. This work proposes a new Reinforcement Learning-based algorithm to apply energy-efficient control strategies to a single workstation consisting of identical parallel machines. The model goal is to achieve the optimum trade-off between system productivity and energy demand without relying on full knowledge of the system dynamics. Numerical experiments confirm ef-fectiveness, applicability, and generality of the proposed approach, even when applied to a real-world in-dustrial system from the automotive sector.& COPY; 2023 CIRP.									1	0	0	0	0	0	1			1755-5817	1878-0016										Politecn Milan, Mech Engn Dept, Via G Masa 1, I-20156 Milan, ItalyKarlsruhe Inst Technol KIT, Wbk Inst Prod Sci, Kaiserstr 12, D-76131 Karlsruhe, Germany				2023-07-14	WOS:001017730000001		
B	Zhu, Zhaoxuan										Reinforcement Learning in Eco-Driving for Connected and Automated Vehicles																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					9798834019756									The Ohio State University, Mechanical Engineering, Ohio, United States	The Ohio State University				PQDT:68437218		
J	XUE Jinlin; ZHANG Weigong; GONG Zongyang							薛金林; 张为公; 龚宗洋			Velocity Tracking Control Based on Reinforcement Learning Neural Network			基于强化学习神经网络的车速跟踪控制				测控技术	Measurement & Control Technology				26	7			36	38	1000-8829(2007)26:7<36:JYQHXX>2.0.TX;2-M										Article	2007	2007	A new approach for tracking vehicle speeds by robotic driver during emission testing is presented. Based on neural network and combined with adaptive capability of reinforcement learning, it can execute velocity tracking control through on-line learning of neural network. Using the data obtained from a teat vehicle, a neural network model of automotive for velocity tracking is developed at first. A neural network controller is designed based on reinforcement learning neural network framework to achieve adaptive control of velocity tracking. During simulation study, the velocity control neural network model is used to train primary controller rather than the actual test vehicle, and the developed and well-trained self-learning neural network controller is applied to velocity tracking control. Results show that the developed neural network controller has good performance of velocity tracking, and control efficacy is obvious.			提出一种用于汽车排放试验中驾驶机器人对车速跟踪控制的新方法.该控制方法基于神经网络并结合强化学习的自适应能力,通过神经网络的在线学习对车速进行跟踪控制.利用试验汽车所获得的数据,首先开发出用于车速控制的神经网络模型.然后基于强化学习神经网络结构设计神经网络控制器以取得车速跟踪的自适应控制.在仿真研究中,使用神经网络车速控制模型替代实际汽车来训练初始控制器,并用开发与训练好的自学习神经网络控制器用于汽车车速跟踪控制.结果表明,所开发的神经网络控制器具有良好的车速跟踪性能,控制效果明显.						1	2	0	0	0	0	3			1000-8829											东南大学仪器科学与工程系, 南京, 江苏 210096, 中国Department of Instrument Science and Engineering, Southeast University, Nanjing, Jiangsu 210096, China	东南大学仪器科学与工程系Department of Instrument Science and Engineering, Southeast University			2007-01-01	CSCD:2849224		
C	Crincoli, Giuseppe; Fierro, Fabiana; Iadarola, Giacomo; La Rocca, Piera Elena; Martinelli, Fabio; Mercaldo, Francesco; Santone, Antonella						DiVimercati, SDC; Samarati, P				A Method for Road Accident Prevention in Smart Cities based on Deep Reinforcement Learning								SECRYPT : PROCEEDINGS OF THE 19TH INTERNATIONAL CONFERENCE ON SECURITY AND CRYPTOGRAPHY								513	518				10.5220/0011146500003283							Proceedings Paper	2022	2022	Autonomous vehicles play a key role in the smart cities vision: they bring benefits and innovation, but also safety threats, especially if they suffer from vulnerabilities that can be easily exploited. In this paper, we propose a method that exploits Deep Reinforcement Learning to train autonomous vehicles with the purpose of preventing road accidents. The experimental results demonstrated that a single self-driving vehicle can help to optimise traffic flows and mitigate the number of collisions that would occur if there were no self-driving vehicles in the road network. Our results proved that the training progress is able to reduce the collision frequency from 1 collision every 32.40 hours to 1 collision every 53.55 hours, demonstrating the effectiveness of deep reinforcement learning in road accident prevention in smart cities.					19th International Conference on Security and Cryptography (SECRYPT)19th International Conference on Security and Cryptography (SECRYPT)	JUL 11-13, 2022JUL 11-13, 2022		Lisbon, PORTUGALLisbon, PORTUGAL	0	0	0	0	0	0	0					978-989-758-590-6									Natl Res Council Italy CNR, Inst Informat & Telemat, Pisa, ItalySpike Reply, Milan, ItalyUniv Molise, Campobasso, Italy	Spike Reply			2022-09-21	WOS:000853004900052		
P	ZHOU Y; WANG Z; LU P; GENG S; HE B										Algorithm based on a digital twin-based robot            large-scale long-term inspection task coverage            strategy, includes constructing virtual scene that            corresponds to real scene through digital twin            technology, and completing navigation task of robot            from current position to selected local target            point					CN115790600-A	UNIV TONGJI																									NOVELTY - The algorithm includes constructing a virtual scene that corresponds to the real scene one-to-one through digital twin technology. The surrounding environment information is obtained through the robot sensor in the virtual scene and a local map is built. A global target point in an unexplored area is regularly selected according to the local map, and a local target point is selected on the path of the global target point. The deep reinforcement learning algorithm is used to complete the navigation task of the robot from the current position to the selected local target point. The robot completes the inspection and coverage task in the virtual scene. The algorithm is applied to the robot in the real scene, and the data synchronization is completed in the virtual scene and the real scene, so that the robot completes the inspection and coverage task in a certain area, and specifies the target point of the inspection in the virtual scene and guides the robot to go directly. USE - Algorithm based on a digital twin-based robot's large-scale long-term inspection task coverage strategy. ADVANTAGE - The algorithm combines the digital twinning and depth reinforcement learning training the unmanned vehicle to realize autonomous navigation algorithm, at the same time, the information obtained by unmanned vehicle assembling sensor is used for updating the virtual scene, and the position and speed of unmanned vehicle in real scene can be monitored online in real time through virtual scene. The robot finishes the high-efficiency exploration of certain area without prior map. The local navigation based on depth reinforcement training algorithm avoids dynamic pedestrian and vehicle, based on the working mode of virtual-real fusion of digital twinning, thus ensures that the exploration is more efficient. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of an algorithm based on a digital twin-based robot's large-scale long-term inspection task coverage strategy. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202329471N		
C	Cao, Yushi; Zheng, Yan; Lin, Shang-Wei; Liu, Yang; Teo, Yon Shin; Toh, Yuxuan; Adiga, Vinay Vishnumurthy			IEEE	yang, liu/GVU-8760-2022; Liu, Yang/HNJ-6693-2023; Liu, Yang/D-2306-2013; yang, liu/HTN-9175-2023; liu, yang/HIU-0559-2022; LIU, YANG/HWQ-4615-2023; liu, yang/HQY-7531-2023; liu, yang/HHY-8583-2022	Liu, Yang/0000-0001-7300-9215; 					Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning								2021 36TH IEEE/ACM INTERNATIONAL CONFERENCE ON AUTOMATED SOFTWARE ENGINEERING ASE 2021								1151	1155				10.1109/ASE51524.2021.9678703							Proceedings Paper	2021	2021	Discovering the underlying structure of HMI software efficiently and sufficiently for the purpose of testing without any prior knowledge on the software logic remains a difficult problem. The key challenge lies in the complexity of the HMI software and the high variance in the coverage of current methods. In this paper, we introduce the PathFinder, an effective and automatic HMI software exploration framework. PathFinder adopts a curiosity-based reinforcement learning framework to choose actions that lead to the discovery of more unknown states. Additionally, PathFinder progressively builds a navigation model during the exploration to further improve state coverage. We have conducted experiments on both simulations and real-world HMI software testing environment, which comprise a full tool chain of automobile dashboard instrument cluster. The exploration coverage outperforms manual and fuzzing methods which are the current industrial standards.					36th IEEE/ACM International Conference on Automated Software Engineering (ASE)36th IEEE/ACM International Conference on Automated Software Engineering (ASE)	NOV 14-20, 2021NOV 14-20, 2021	IEEE; Assoc Comp Machinery; Monash Univ; Facebook; NASA Ames Res Ctr; Amazon Web Servi; Huawei; IEEE; IEEE Comp Soc; IEEE Tech Council Software Engn; ACM SIGSOFT; ACM SIGAI; Deakin UnivIEEE; Assoc Comp Machinery; Monash Univ; Facebook; NASA Ames Res Ctr; Amazon Web Servi; Huawei; IEEE; IEEE Comp Soc; IEEE Tech Council Software Engn; ACM SIGSOFT; ACM SIGAI; Deakin Univ	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1					978-1-6654-0337-5									Nanyang Technol Univ, Continental NTU Corp Lab, Singapore, SingaporeContinental Automot Singapore Pte Ltd, Singapore, SingaporeTianjin Univ, Tianjin, Peoples R China	Continental Automot Singapore Pte Ltd			2022-04-22	WOS:000779309000113		
B	Sanghun Yun; Jahyun Kim; Hyogon Kim										Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks								2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall). Proceedings								6 pp.	6 pp.				10.1109/VTCFall.2019.8891225							Conference Paper	2019	2019	In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications. For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs). However, the in- vehicle network environment can be starkly different from the Internet where TCP has been optimized. The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments. In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.					2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall)2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall)	22-25 Sept. 201922-25 Sept. 2019		Honolulu, HI, USAHonolulu, HI, USA	0	0	0	0	0	0	0					978-1-7281-1220-6									Korea Univ., Seoul, South Korea				2019-11-29	INSPEC:19134835		
J	Yang, Bin; Wu, Bin; You, Yuwen; Guo, Chunmei; Qiao, Liang; Lv, Zhihan				Lv, Zhihan/GLU-4458-2022; Yang, Bin/GQA-9324-2022; Lv, Zhihan/AHD-0875-2022; wu, bin/IUM-4802-2023; Lyu, Zhihan/I-3187-2014; Lv, Zhihan/GLR-6000-2022	Yang, Bin/0000-0002-1579-7978; Lyu, Zhihan/0000-0003-2525-3074; Lv, Zhihan/0000-0003-2525-3074					Edge intelligence based digital twins for internet of autonomous unmanned vehicles								SOFTWARE-PRACTICE & EXPERIENCE													10.1002/spe.3080					MAR 2022		Article; Early Access		2022	It aims to explore the efficient and reliable wireless transmission and cooperative communication mechanism of Internet of Vehicles (IoV) based on edge intelligence technology. It first proposes an intelligent network architecture for IoV services by combining network slicing and deep learning (DL) technology, and then began to study the key technologies needed to achieve the architecture. It designs the cooperative control mechanism of unmanned vehicle network based on the full study of wireless resource allocation algorithm from the micro level. Second, in order to improve the safety of vehicle driving, deep reinforcement learning is used to configure the wireless resources of IoV network to meet the needs of various IoV services. The research results show that the accuracy rate of the improved AlexNet algorithm model can reach 99.64%, the accuracy rate is more than 80%, the data transmission delay is less than 0.02 ms, and the data transmission packet loss rate is less than 0.05. The algorithm model has practical application value for solving the data transmission related problems of vehicular internet communication, providing an important reference value for the intelligent development of unmanned vehicle internet.									3	0	0	0	0	0	3			0038-0644	1097-024X										Tianjin Chengjian Univ, Sch Energy & Safety Engn, Tianjin, Peoples R ChinaZhejiang A&F Univ, Informat & Educ Technol Ctr, Hangzhou, Zhejiang, Peoples R ChinaQingdao Univ, Coll Comp Sci & Technol, Qingdao, Peoples R ChinaUppsala Univ, Fac Arts, Dept Game Design, Uppsala, Sweden				2022-03-16	WOS:000765613500001		
J	Yuqi Han										Adaptive adaptive obstacle avoidance algorithm of collaborative unmanned vehicles in dynamic scenes with monocular cameras								Proceedings of SPIE				11897				1189705 (8 pp.)	1189705 (8 pp.)				10.1117/12.2602756							Conference Paper; Journal Paper	2021	2021	The monocular camera is widely used in robots and unmanned vehicles system because it is low cost and easy to calibrate and identify. However, the depth lack of the monocular camera hinders positioning and determining the real size of obstacles in the unmanned vehicle system. To solve the problem, we propose a collaborative structure to accurately acquire the position of static or dynamic obstacles based on the partially observing information from multiple monocular cameras. After that, a reinforcement learning based obstacle avoidance algorithm is proposed for unmanned vehicles under an unknown environment. Specifically, we discuss the influence of obstacles' moving orientations on the performance of obstacles adaptive avoidance. Simulation results verify the feasibility of the proposed algorithm.					Optoelectronic Imaging and Multimedia Technology VIIIOptoelectronic Imaging and Multimedia Technology VIII	10-12 Oct. 202110-12 Oct. 2021		Nantong, ChinaNantong, China	0	0	0	0	0	0	0			0277-786X															2022-04-15	INSPEC:21575881		
B	Durr, S.; Lamprecht, R.; Kauffmann, M.; Huber, M.F.										Reinforcement Learning based Optimization of Bayesian Networks for Generating Feasible Vehicle Configuration Suggestions								2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)								16	22				10.1109/CASE49439.2021.9551428							Conference Paper	2021	2021	A promising method in the automotive industry to anticipate future customer demands is the concept of planned orders. Due to multi-variant products, changing customer demands, and dynamic environments the process of generating planned orders is challenging. This paper introduces an approach using graphical models to generate planned order suggestions in a multi-variant order management process. Bayesian networks are modelled by learning the structure from different data sources, which enable the possibility to directly sample configuration suggestions. To find an optimized graph structure, a method using hierarchical correlation clustering and reinforcement learning is applied, taking into account technical and sales-operated feasibility constraints. The method has high potential in practical usage and is evaluated by a realworld use case of the Dr. Ing. h.c. F. Porsche AG.					2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)	23-27 Aug. 202123-27 Aug. 2021		Lyon, FranceLyon, France	0	0	0	0	0	0	0					978-1-6654-1873-7									Inst. of Ind. Manuf. & Manage. IFF, Stuttgart, GermanyCenter for Cyber Cognitive Intelligence, Fraunhofer Inst. for Manuf. Eng. & Autom. IPA, Stuttgart, Germany				2021-12-10	INSPEC:21202013		
B	Johansson, Tobias; Morteza; Devdatt										Machine Learning Based Methods for Virtual Validation of Autonomous Driving																												Dissertation/Thesis	Jan 01 2021	2021										0	0	0	0	0	0	0					979-8-209-79089-1									Chalmers Tekniska Hogskola (Sweden), Sweden	Chalmers Tekniska Hogskola (Sweden)				PQDT:50731286		
C	Starace, Luigi Libero Lucio; Romdhana, Andrea; Di Martino, Sergio			Assoc Comp Machinery	Starace, Luigi Libero Lucio/ADG-3293-2022; Di Martino, Sergio/F-2602-2012	Starace, Luigi Libero Lucio/0000-0001-7945-9014; Di Martino, Sergio/0000-0002-1019-9004					GenRL at the SBST 2022 Tool Competition								15TH SEARCH-BASED SOFTWARE TESTING WORKSHOP (SBST 2022)								49	50				10.1145/3526072.3527533							Proceedings Paper	2022	2022	GenRL is a Deep Reinforcement Learning-based tool designed to generate test cases for Lane-Keeping Assist Systems. In this paper, we briefly presents GenRL, and summarize the results of its participation in the Cyber-Physical Systems (CPS) tool competition at SBST 2022.					15th Search-Based Software Testing Workshop (SBST)15th Search-Based Software Testing Workshop (SBST)	MAY09, 2022MAY09, 2022	Assoc Comp Machinery; IEEE Comp SocAssoc Comp Machinery; IEEE Comp Soc	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1					978-1-4503-9318-8									Univ Napoli Federico II, Naples, ItalyUniv Genoa, Genoa, ItalyFBK ICT, Secur & Trust Unit, Trento, Italy				2022-09-17	WOS:000850536700013		
J	Xinlei Pan; Seita, D.; Yang Gao; Canny, J.										Risk Averse Robust Adversarial Reinforcement Learning [arXiv]								arXiv								7 pp.	7 pp.											Journal Paper	31 March 2019	2019	Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary.									0	0	0	0	0	0	0														Univ. of California, Berkeley, Berkeley, CA, USA				2020-03-16	INSPEC:18842361		
C	Rahman, S. M. Mizanoor			IEEE							Performance Metrics for Human-Robot Collaboration: An Automotive Manufacturing Case								2021 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR AUTOMOTIVE (METROAUTOMOTIVE)								260	265				10.1109/MetroAutomotive50197.2021.9502881							Proceedings Paper	2021	2021	A human-robot collaborative system in the form of a power and skill assist robotic system was developed where a human and a robot could collaborate to perform object manipulation for targeted assembly tasks in automotive manufacturing. We assumed such assembly tasks as the representative assembly tasks in automotive manufacturing. We reflected human's weight perception in the dynamics and control of the power and skill assist system following a psychophysical method using a reinforcement learning scheme. We recruited 20 human subjects who separately performed assembly tasks with the system in human-robot collaboration (HRC). We then observed the collaborative assembly tasks, conducted extensive literature reviews, reviewed our previous and ongoing related works and brainstormed with the subjects and other relevant researchers, and then proposed HRC performance assessment metrics and methods for collaborative automotive manufacturing. The proposed metrics comprised of assessment criteria and methods related to both human-robot interaction (HRI) and manufacturing performance. We then verified the proposed performance metrics in pilot studies in the laboratory environment using the same collaborative system and subjects. The verification results proved the effectiveness of the assessment metrics and methods in terms of usability, practicability and reliability. We then proposed to apply classification and regression type machine learning approaches under supervised and reinforcement learning setups to learn different classes and decision-making rules respectively regarding HRC performance. The proposed performance metrics and methods can serve as the preliminary efforts towards developing comprehensive assessment metrics for HRC in general and for human-robot collaborative automotive manufacturing in particular.					1st IEEE International Workshop on Metrology for Automotive (MetroAutomotive)1st IEEE International Workshop on Metrology for Automotive (MetroAutomotive)	JUL 01-02, 2021JUL 01-02, 2021	IEEE; Motor Vehicle Univ Emilia Romagna; Alma Mater Studiorum Univ Bologna; Univ Modena Reggio Emilia; Univ Parma; Univ Ferrara; AthenaIEEE; Motor Vehicle Univ Emilia Romagna; Alma Mater Studiorum Univ Bologna; Univ Modena Reggio Emilia; Univ Parma; Univ Ferrara; Athena	ELECTR NETWORKELECTR NETWORK	3	0	0	0	0	0	3					978-1-6654-3906-0									Univ West Florida, Hal Marcus Coll Sci & Engn, Dept Intelligent Syst & Robot, Pensacola, FL 32514 USA				2022-05-25	WOS:000793864300047		
P	YANG K; YU Q; LI Y; HU J										Multi-unmanned autonomous navigation and task            allocation algorithm of wireless self-powered            communication network, involves solving optimization            problem according to asynchronous multi-intelligent            depth reinforcement learning algorithm					CN113776531-A	UNIV CHINA ELECTRONIC SCI & TECHNOLOGY; UNIV CHINA YANGTZE DELTA REGION ELECTRON																									NOVELTY - Multi-unmanned autonomous navigation and task allocation algorithm of wireless self-powered communication network comprises (i) determining network model, communication mode and channel model, (ii) modeling the downlink wireless power transmission and uplink wireless information transmission, and determining the optimized target expression and the constraint condition thereof, (iii) analyzing and optimizing the problem, where the optimization problem modeling is Markov process, (iv) determining the network communication protocol and the unmanned aerial vehicle flight decision model, and (v) defining the neural network input state of each unmanned aerial Vehicle, output action, reward function and input and output of the public neural network, and solving optimization problem according to the asynchronous multi-intelligent deep reinforcement learning algorithm. USE - Multi-unmanned autonomous navigation and task allocation algorithm of wireless self-powered communication network of an unmanned aerial vehicle i.e. aerial base station. ADVANTAGE - The multi-unmanned aerial vehicle autonomous navigation and task allocation algorithm combines the flight paths of multiple unmanned aerial vehicles in the wireless self-powered communication network, and jointly designs user scheduling, flight path of each unmanned aerial vehicle, flight speed, communication mode, and task distribution and track optimization between the unmanned vehicle, finishing the navigation task without collision of unmanned vehicle in the predetermined flight time, and furthest improving the system user average uplink transmission data quantity. The optimization problem modeling is Markov process, and claims an asynchronous multi-intelligent body depth enhanced learning algorithm based on shared neural network, solving the optimization problem, gradually training neural network and finally realizing the target of maximum system uplink total data quantity, thus solving the problem. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the multi-unmanned autonomous navigation and task allocation algorithm of wireless self-powered communication network (Drawing includes non-English language text).															0																		2022-03-01	DIIDW:2022257820		
J	Kaviani, Mohamad Amin; Tavana, Madjid; Kumar, Anil; Michnik, Jerzy; Niknam, Raziyeh; Campos, Elaine Aparecida Regiani de				Niknam, Raziyeh/AAZ-5566-2021; Tavana, Madjid/K-5230-2012; Kumar, Anil/HJB-2850-2022; Regiani de Campos, Elaine Aparecida/M-1830-2016; Boothapati, Anil Kumar/HHS-1813-2022; Kumar, Anil/A-2657-2013	Kumar, Anil/0000-0002-5817-5829; Regiani de Campos, Elaine Aparecida/0000-0002-9606-2593; Kumar, Anil/0000-0002-1691-0098; Michnik, Jerzy/0000-0002-6266-3040					An integrated framework for evaluating the barriers to successful implementation of reverse logistics in the automotive industry								JOURNAL OF CLEANER PRODUCTION				272						122714			10.1016/j.jclepro.2020.122714							Article	NOV 1 2020	2020	Reverse logistics (RL) strategy can have a positive impact on productivity, and the diminishing resources, along with the strict environmental regulations, have strengthened the need for this strategy. The purpose of this study is to develop an integrated framework for identifying: (1) the critical barriers to the successful implementation of RL in the automotive industry; (2) the importance and implementation priorities of these barriers; and (3) the causal relations among them. The proposed framework is composed of the Delphi method to identify the most relevant barriers, the best-worst method (BWM) to determine their importance, and the weighted influence non-linear gauge system (WINGS) to analyze their causal relationships. The proposed framework is applied to a case study in the automotive industry. The results indicate the economic barriers are the most important, and the knowledge barriers are the least important barriers to the successful implementation of RL in the automotive industry. (c) 2020 Elsevier Ltd. All rights reserved.									53	0	0	0	4	0	53			0959-6526	1879-1786										Univ Cyprus, KIOS Res & Innovat Ctr Excellence, Dept Elect & Comp Engn, Nicosia, CyprusLa Salle Univ, Business Syst & Analyt Dept, Philadelphia, PA 19141 USAUniv Paderborn, Business Informat Syst Dept, Paderborn, GermanyLondon Metropolitan Univ, Guildhall Sch Business & Law, London, EnglandUniv Econ Katowice, Dept Operat Res, Katowice, PolandIslamic Azad Univ, Dept Ind Engn, Shiraz Branch, Shiraz, IranUniv Fed Rio Grande do Sul, Dept Ind Engn, Porto Alegre, RS, Brazil				2020-10-13	WOS:000570238600012		
C	Hao, Shengang; Zheng, Jun; Yang, Jie; Ni, Ziwei; Zhang, Quanxin; Zhang, Li						Zhou, J; Adepu, S; Alcaraz, C; Batina, L; Casalicchio, E; Chattopadhyay, S; Jin, C; Lin, J; Losiouk, E; Majumdar, S; Meng, W; Picek, S; Shao, J; Su, C; Wang, C; Zhauniarovich, Y; Zonouz, S				A Multi-agent Deep Reinforcement Learning-Based Collaborative Willingness Network for Automobile Maintenance Service								APPLIED CRYPTOGRAPHY AND NETWORK SECURITY WORKSHOPS, ACNS 2022		Lecture Notes in Computer Science		13285				84	103				10.1007/978-3-031-16815-4_6							Proceedings Paper	2022	2022	With the growth of maintenance market scale of automobile manufacturing enterprises, simple information technology is not enough to solve the problem of uneven resource allocation and low customer satisfaction in maintenance chain services. To solve this problem, this paper abstracts the automotive maintenance collaborative service into a multi-agent collaborative model based on the decentralized partially observable Markov decision progress (Dec-POMDP). Based on this model, a multi-agent deep reinforcement learning algorithm based on collaborative willingness network (CWN-MADRL) is presented. The algorithm uses a value decomposition based MADRL framework, adds a collaborative willingness network based on the original action value network of the agent, and uses the attention mechanism to improve the impact of the collaboration between agents on the action decision-making, while saving computing resources. The evaluation results show that, our CWN-MADRL algorithm can converge quickly, learn effective task recommendation strategies, and achieve better system performance compared with other benchmark algorithms.					20th International Conference on Applied Cryptography and Network Security (ACNS)20th International Conference on Applied Cryptography and Network Security (ACNS)	JUN 20-23, 2022JUN 20-23, 2022		ELECTR NETWORKELECTR NETWORK	0	0	0	0	0	0	0			0302-9743	1611-3349	978-3-031-16815-4; 978-3-031-16814-7									Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R ChinaCommun Univ Zhejiang, Dept Media Engn, Hangzhou 310018, Peoples R China				2022-11-02	WOS:000869767400006		
P	ZHENG L; ZHANG P										Method for facilitating multi-machine            collaboration simultaneous localization and mapping            (SLAM) for robot, involves transmitting pose            information and actual distance information by robot,            and performing rear-end optimization of SLAM tracks by            using deep deterministic policy gradient (TD3)            algorithm					CN116721154-A	UNIV SOUTH CHINA TECHNOLOGY																									NOVELTY - The method involves running an ORB-SLAM2 program on a robot. The images are obtained through a camera to perform pose estimation to obtain a multi-machine initial motion track pose diagram. Training process is performed by using a depth reinforcement learning algorithm to optimize the track to obtain a more accurate pose based on the obtained robot motion track pose diagram. An active sensing strategy introduced and the poses of multiple machines are optimized on the basis of a reinforcement learning algorithm. A corresponding robot is selected to optimize pose information by a TD3 algorithm according to a real-time SLAM estimated probability value. The pose information and the actual distance information are mutually transmitted by the robots. The TD3 algorithm is used for performing rear-end optimization of SLAM tracks, so that the effect of eliminating accumulated errors is achieved. USE - Method for facilitating multi-machine collaboration SLAM i.e. group mobile robot based on active deep reinforcement learning for use in the fields of unmanned vehicle and robot. ADVANTAGE - The TD3 algorithm is used for carrying out the rear-end optimization of the SLAM track so as to achieve the effect of eliminating accumulated errors. The method effectively eliminates the error accumulation condition in the SLAM system, improves the positioning and mapping precision of the SLAM, has no restriction of loop, and increases the robustness of the SLAM system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the multi-machine collaborative visual SLAM system. (Drawing includes non-English language text)															0																		2023-09-29	DIIDW:202397191K		
P	YANG J; ZHU K										Scheduling method for charging drones to charge            task drones in the air, involves charging UAV to obtain            current state by training obtained network decision,            and charging task UAV according to charging scheduling            instruction output by actor network					CN114548663-A	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - The method involves determining a charging mode according to charging requirement information of a task unmanned aerial vehicle. Charging scheduling problem model is established according to the charging requirement and a task type of the task unmanned air vehicle. A charging scheduling strategy of multiple charging unmanned air vehicles is optimized based on the charging scheduling task. A task of the shortest time is used as a target. The charging scheduling strategies of the charging unmanned aerial vehicles are learned based on deep reinforcement learning. The unmanned air vehicle is charged according to a charging scheduling instruction output by an actor network. USE - Scheduling method for charging drones to charge task drones in the air ADVANTAGE - The method enables realizing shortest task completion time of the task unmanned aerial vehicle, and optimizing the charging scheduling strategy of the vehicle charging and charging unmanned aerial vehicles. The method allows a training actor network to realize charging unmanned vehicle scheduling strategy optimization, based on depth reinforcement learning multi-intelligent body reinforcement learning charging the vehicle according to the optimized strategy and the current environment to make decision, thus realizing minimizing the task time, minimizing the energy consumption of the target, with charging time and charging place can be flexibly deployed, and minimizing the effect of distribution. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a scheduling method for charging drones to charge task drones in the air. (Drawing includes non-English language text).															0																		2022-07-12	DIIDW:202275564L		
C	Lin, M; Malec, J						Maranzana, M				Timing analysis of RL programs								REAL TIME PROGRAMMING 1997: (WRTP 97)								99	104											Proceedings Paper	1998	1998	Predictability is a very important feature of complex real-time systems. For a complex system involving a number of subsystems, the timing properties of each embedded system play an important role since the total system's response depends on temporal behaviour of all the subsystems.In this paper, we analyze the timing properties of reactive programs written in a rule-based language RL. RL is a relatively simple rule language for programming discrete-response part of embedded applications in a layered architecture. It has been already used in a complex automotive application.We provide the upper timing bounds for RL programs executed using either of the two evaluation strategies developed earlier. In conclusion we provide some comments on the temporal behaviour of a layered system consisting of a reactive RL program combined with a set of periodic tasks.					IFAC/IFIP Workshop on Real-Time Programming (WRTP 97)IFAC/IFIP Workshop on Real-Time Programming (WRTP 97)	SEP 15-17, 1997SEP 15-17, 1997	Int Federat Automat Control, Tech Comm Real Time Softw Engn; Int Federat Informat Proc, Working Grp 54 Ind Software Qual & Certificat; Assoc Francaise Cyberet Tech; Lab Ingenierie Informat Ind; Inst Natl Sci Appliquees, LyonInt Federat Automat Control, Tech Comm Real Time Softw Engn; Int Federat Informat Proc, Working Grp 54 Ind Software Qual & Certificat; Assoc Francaise Cyberet Tech; Lab Ingenierie Informat Ind; Inst Natl Sci Appliquees, Lyon	LYON, FRANCELYON, FRANCE	0	0	0	0	0	0	0					0-08-043045-7									Linkoping Univ, Dept Comp & Informat Sci, S-58183 Linkoping, Sweden				1998-01-01	WOS:000074660300017		
J	Zhang, Xumei; Yu, Jiaxuan; Yan, Wei; Wang, Yan; Subramanian, Nachiappan				Yu, Jiaxuan/JJC-8507-2023; Subramanian, Nachiappan/K-1563-2014	Subramanian, Nachiappan/0000-0003-4076-6433; Wang, Xingquan/0000-0001-6086-0481					A Comprehensive Review of Reverse Logistics in the Automotive Industry								IEEE ACCESS				11				47112	47128				10.1109/ACCESS.2023.3273591							Review	2023	2023	Reverse logistics (RL) of automobiles has received wide attention in recent years with the innovation of resource utilization and the increase of environmental awareness. Many research papers have been published in the RL discipline focusing on the automotive industry. However, no review article is available on product-specific issues. To bridge this gap, 91 papers published in the Web of Science (WOS) database between January 2013 and March 2023 were selected and analyzed using content analysis to classify the articles into survey, evaluation, decision making, framework, modeling and review. The main findings of this paper are as follows: (1) Research on RL activities has mainly focused on recycling and remanufacturing, and insufficient research has been conducted on activities such as dismantling and waste management. (2) In terms of research objects, End-of-Life Vehicles (ELVs) and automotive batteries have received a lot of attention, while less research has been done on automotive tires, engines, waste oil, etc., which need further attention. (3) Integrated economic, environmental and social considerations are research opportunities for future evaluation and decision making. (4) The establishment of multi-objective problems and the innovation of solution methods may be the future research direction. (5) Green and sustainability themes are the main trends in the development of RL in the automotive industry in the future.									0	0	0	0	0	0	0			2169-3536											Wuhan Univ Sci & Technol, Sch Automobile & Traff Engn, Wuhan 430065, Peoples R ChinaWuhan Univ Sci & Technol, Hubei Prov Key Lab Mech Transmiss & Mfg Engn, Wuhan 430081, Peoples R ChinaUniv Brighton, Sch Comp Engn & Math, Brighton BN2 4GJ, EnglandUniv Sussex, Business Sch, Brighton BN1 9RH, England				2023-07-02	WOS:001010378400001		
C	Yun, Sanghun; Kim, Jahyun; Kim, Hyogon			IEEE							Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks								2019 IEEE 90TH VEHICULAR TECHNOLOGY CONFERENCE (VTC2019-FALL)		IEEE Vehicular Technology Conference Proceedings											10.1109/vtcfall.2019.8891225							Proceedings Paper	2019	2019	In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications. For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs). However, the in-vehicle network environment can be starkly different from the Internet where TCP has been optimized. The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments. In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.					90th IEEE Vehicular Technology Conference (IEEE VTC-Fall)90th IEEE Vehicular Technology Conference (IEEE VTC-Fall)	SEP 22-25, 2019SEP 22-25, 2019	IEEEIEEE	Honolulu, HIHonolulu, HI	0	0	0	0	0	0	0			2577-2465		978-1-7281-1220-6									Korea Univ, Seoul, South Korea				2019-01-01	WOS:000610542200171		
P	ZHANG H										Intelligent planning device for urban low-altitude            logistics unmanned aerial vehicle route, has task path            planning device that is provided for planning task path            for preventing danger and avoiding obstacles					CN114706424-A	UNIV SOUTHWEST JIAOTONG																									NOVELTY - The device has a storage device for storing task data, communication data, terrain data, task path planning data and navigation capacity planning data. A pre-processing device pre-processes operation data of a relay station in a range. A processor obtains a target address and delivery aging data and matching according to a received delivery order. A communication device realizes information transmission between a processor and an unmanned aerial vehicle. A task management device classifies unmanned vehicle flight task and transmits the unmanned vehicle to the unmanned aircraft. A transmission distance determining device determines distance constraint condition two adjacent path points on an unmanned vehicle transportation path. A transportation condition determining unit determines flight restriction condition of logistics unmanned vehicle single cargo transportation. USE - Intelligent planning device for urban low-altitude logistics unmanned aerial vehicle (UAV) route used in logistics enterprises. Can also be used in aircraft. ADVANTAGE - The device combines the unmanned aerial vehicle flight path planning process based on depth reinforcement learning the route planning method of the UAV logistics distribution at the same time, thus optimizing path planning problem of two dimensions in logistics of UAV, and hence effectively ensuring the safety and high efficiency of the UPSU logistics path obtained by optimization. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the whole frame of an intelligent planning device for urban low-altitude logistics unmanned aerial vehicle route. (Drawing includes non-English language text).															0																		2023-08-10	DIIDW:202290877G		
B	Zheng, H.; Wu, F.; Yang, S.; Ju, X.; Liu, Y.; Zhu, K.										Design and Research of Vehicle Platoon Formation Algorithm Based on Multi-Agent Reinforcement Learning								2023 8th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)								451	5				10.1109/IC-NIDC59918.2023.10390616							Conference Paper	2023	2023	The field of artificial intelligence is advancing rapidly, with reinforcement learning making significant strides in solving various sequential decision problems in machine learning. As research progresses, multi-agent reinforcement learning has emerged in the field of reinforcement learning and has been applied to numerous domains. Vehicle formation is an important means of transportation for reducing vehicle energy consumption and improving air quality. Controlling sparse vehicles on the road to form formations is a fascinating research topic. In this paper, we propose a planning framework for formation control based on federated learning and multi-agent reinforcement learning to address this problem. We model vehicle energy consumption to accurately assess energy usage during vehicle formation. Additionally, we incorporate reinforcement learning algorithms into the vehicle formation process to enable multi-vehicle asynchronous decision-making and save formation time. We also introduce federated learning into the training process to significantly reduce overall system communication.					2023 8th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)2023 8th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)	20232023		Beijing, ChinaBeijing, China	0	0	0	0	0	0	0					979-8-3503-1792-3									Sch. of Inf. & Commun. Eng., BISTU, Beijing, ChinaSch. of Artificial Intelligence, BUPT, Beijing, ChinaLiuzhou Dongke Intelligent City I&D Co., Ltd., Liuzhou, ChinaSch. of Vehicle & Mobility, THU, Beijing, China				2024-02-15	INSPEC:24503611		
P	ZHANG W										Humidity sensor for humidity and air quality            measurement in e.g. automotive and truck, has            resistor-inductor (RL) network with inductor and            resistor that is coupled to capacitor with high            relative temperature coefficient of resistance            (TCR)					US2009308155-A1; US8024970-B2	HONEYWELL INT INC																									NOVELTY - The humidity sensor (100) has a resistor-inductor-capacitor (RLC) resonant circuit formed in or on a substrate (101). The resonant circuit has a capacitor (105) with electrically conductive plate (106,107) and a moisture sensitive dielectric interposed between the plates. An RL network has an inductor (115) and a resistor (119) coupled to capacitor having a high relative TCR portion formed from a first material coupled to a low relative TCR portion formed from a second material different from the first material. USE - Humidity sensor for humidity and air quality measurement in automotive and truck for comfort and safety. Can also be used in powertrain, home appliance for moisture and temperature control, energy efficiency, humidity switches, heating, ventilating, and air conditioning, reprography for inkjet and laser, for weather stations, humidity displays and air quality measurement. ADVANTAGE - Enables to adjust and correct the measured relative humidity value based on the temperature value determined from the measured quality factor value of the resonant circuit. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of temperature compensated humidity sensing. DESCRIPTION Of DRAWING(S) - The drawing shows a cross sectional view of an RLC-based humidity sensor having temperature sensing.Humidity sensor (100)Substrate (101)Capacitor (105)Electrically conductive plate (106,107)Inductor (115)Resistor (119)															0																		2009-01-02	DIIDW:2009S51458		
P	LI B; YUAN X; WANG B; HUANG M										Cooperative competition method for intelligent            body, involves training initial reinforcement learning            model according to virtual air war scene, action space            information, state space information and reward            value					CN113893539-A; CN113893539-B	CHINA ELECTRONICS TECHNOLOGY GROUP CORP																									NOVELTY - The method involves determining a virtual air war scene of an intelligent body. Action space information and state space information of the intelligent body are determined according to the virtual air War scenario. The action space information is provided with an action value. The state Space information is contained with a state value. A reward value of the action is determined corresponding to the state value according to a state of the value. An initial reinforcement learning model is trained based on the virtual Air War scenario, the action spatial information, the state spatial information and the reward value. Target reinforcement learning models are obtained when the initial reinforcement training model is in a convergent state. The intelligent body is used to fight. USE - Method for cooperative competition of intelligent body such as unmanned aerial vehicle (UAV) and unmanned ground vehicle (UGV) used in air war autonomous maneuvering decision. Can also be used in intelligent robot, game game and unmanned vehicle. ADVANTAGE - The problem of difficult and unstable target reward is overcome, when the strategy of the multi-intelligent body is changed, it will not influence the strengthening learning of the multidimensional intelligent body. The cooperative competition method of intelligent body provides cooperative competition of the intelligent body, device, terminal device and storage medium, through determining the intelligent structure to the virtual empty warfare scene of the war, according to virtual air war scene, and determining the action space information and state space information of one or more intelligent structures according to the state value, determining the reward value of the action corresponding to state value and training the initial reinforcement learning model, and using the target strengthening learning model and the rule intelligent body to fight, overcoming the problem that the target reward. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a cooperative competition device of intelligent body;(b) a computer-readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method. (Drawing includes non-English language text).															0																		2022-02-19	DIIDW:202213534N		
J	Mayer, Sebastian; Classen, Tobias; Endisch, Christian					Mayer, Sebastian/0000-0002-2669-5897; Endisch, Christian/0000-0003-1557-2944					Modular production control using deep reinforcement learning: proximal policy optimization								JOURNAL OF INTELLIGENT MANUFACTURING				32	8			2335	2351				10.1007/s10845-021-01778-z					MAY 2021		Article; Early Access		2021	EU regulations on CO2 limits and the trend of individualization are pushing the automotive industry towards greater flexibility and robustness in production. One approach to address these challenges is modular production, where workstations are decoupled by automated guided vehicles, requiring new control concepts. Modular production control aims at throughput-optimal coordination of products, workstations, and vehicles. For this np-hard problem, conventional control approaches lack in computing efficiency, do not find optimal solutions, or are not generalizable. In contrast, Deep Reinforcement Learning offers powerful and generalizable algorithms, able to deal with varying environments and high complexity. One of these algorithms is Proximal Policy Optimization, which is used in this article to address modular production control. Experiments in several modular production control settings demonstrate stable, reliable, optimal, and generalizable learning behavior. The agent successfully adapts its strategies with respect to the given problem configuration. We explain how to get to this learning behavior, especially focusing on the agent's action, state, and reward design.									10	0	0	0	0	0	10			0956-5515	1572-8145										TH Ingolstadt, Inst Innovat Mobil, Esplanade 10, D-85049 Ingolstadt, GermanyTech Univ Munich, Arcisstr 21, D-80333 Munich, Germany	TH Ingolstadt			2021-06-01	WOS:000652959200001		
P	LIU Y; WEN Z; LI J; JIN X; NIU Y										Method for performing small unmanned aerial            vehicle anti-control mixed decision based on deep            reinforcement learning and rule drive, involves            training and optimizing decision model, and updating            control rule model according to decision model					CN113625569-A; CN113625569-B	PLA NO 32802 TROOPS																									NOVELTY - The method involves obtaining position motion information of a small unmanned aerial vehicle. A three-degree-of-freedom particle motion model of the small unmanned air vehicle is constructed. A small unmanned vehicle control rule model is constructed to describe small unmanned aircraft control steps. A state space, an action space and a reward function are constructed according to a Markov decision process. A D3QN network is established based on a dueling structure. An anti-control decision model is trained and optimized. The small unmanned helicopter control rules model is updated according to the control decision model. USE - Small unmanned aerial vehicle anti-control mixed decision method based on deep reinforcement learning and rule drive for use in civil field, military field and local conflict. ADVANTAGE - The automation level of the small unmanned aerial vehicle control system for preventing and controlling task is effectively improved. The decision speed in the existing small aerial vehicle anti-control command decision is slow, difficult to process complex scene and so on, meeting the command decision requirement of the control small airborne vehicle.															0																		2023-08-10	DIIDW:2021D1238X		
J	Sowmya, Karri; Dhabu, Meera M.										Model free Reinforcement Learning to determine pricing policy for car parking lots								EXPERT SYSTEMS WITH APPLICATIONS				230						120532			10.1016/j.eswa.2023.120532					JUN 2023		Article	NOV 15 2023	2023	Finding a parking space has not only become painful but also costs a lot in most of the metropolitan cities. With the increase in number of vehicles and limited resources such as manpower and space, the need for effective management of parking lots has increased. Improper management of parking lots can have negative consequences such as traffic congestion, wastage of time in search of parking spaces, air pollution and even loss of revenue for the parking lot managers. Dynamic pricing is a powerful tool to control the behavior of drivers by diverting them towards the unoccupied and cheaper parking lots. Though there are several existing dynamic pricing strategies, determining the right prices is quite challenging due to lack of knowledge of drivers' behavior and several uncertainties like harsh weather and special days. In this paper Reinforcement Learning(RL) technique called Q-learning is used to calculate the dynamic prices for parking lots on hourly basis without the need of prior information about the system. Crucial factors like distance of the parking lots from the city centers, weather and holidays are considered in the proposed algorithm to achieve better accuracy. Price Elasticity of Demand (PED) is used in the proposed work to calculate the new state(occupancy) when an action(dynamic price charged by the parking lot owner) is taken place. Hourly prices are estimated using the proposed algorithm and simulation results show that the calculated prices can efficiently manage parking occupancy during peak and off peak hours. The simulation output also shows that the proposed algorithm can successfully increase the revenue of the parking lot owners.									2	0	0	0	0	0	2			0957-4174	1873-6793										Visvesvaraya Natl Inst Technol, Dept Comp Sci & Engn, Nagpur 440010, Maharashtra, India				2023-07-19	WOS:001023334400001		
J	Koch, Lucas; Picerno, Mario; Badalian, Kevin; Lee, Sung-Yong; Andert, Jakob				Picerno, Mario/ABE-5779-2021; Andert, Jakob Lukas/W-4495-2017	Picerno, Mario/0000-0002-8835-3040; Andert, Jakob Lukas/0000-0002-6754-1907; Koch, Lucas/0000-0002-7368-8833; Lee, Sung Yong/0000-0002-9246-8895; Badalian, Kevin/0000-0002-5593-0227					Automated function development for emission control with deep reinforcement learning								ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE				117						105477			10.1016/j.engappai.2022.105477					OCT 2022		Article; Early Access		2023	The conventional automotive development process for embedded systems today is still time-and data -inefficient, and requires highly experienced software developers and calibration engineers. Consequently, it is cost-intensive and at the same time prone to sub-optimal solutions. Reinforcement Learning offers a promising approach to address these challenges. The evolved agents have proven their ability to master complex control tasks in a close-to-optimal manner without any human intervention, but the training procedures are hardly compatible with current development processes. As a result, Reinforcement Learning has rarely been used in powertrain development until now. This work describes an integration of Reinforcement Learning in the embedded system development process to automatically train and deploy agents in transient driving cycles. Using the example of exhaust gas re-circulation control for a Diesel engine, an agent is successfully trained in a fully virtualized environment, achieving emission reductions of up to 10% in comparison to a state-of-the-art controller. Further investigations are carried out to quantify the impact of the driving cycle and ambient conditions on the agent's performance. To demonstrate the transferability between different levels of virtualization, the experienced agent is then tested in closed-loop with a real hardware controller to operate the physical actuator. By confirming the reproducibility of the learned strategy on real hardware, this article serves as proof-of-concept for a sustainable, Reinforcement Learning based path to automatically develop embedded controllers for complex control problems.									2	0	0	0	0	0	2			0952-1976	1873-6769										Teaching & Res Area Mechatron Mobile Prop, Forckenbeckstr 4, D-52074 Aachen, Germany	Teaching & Res Area Mechatron Mobile Prop			2022-12-07	WOS:000886931000009		
C	Liu, Qiang; Zhang, Yuru; Wang, Haoxin			IEEE	Liu, Qiang/HHZ-3181-2022; luo, yuan/JLS-6416-2023	Liu, Qiang/0000-0002-4307-2990					<i>EdgeMap</i>: CrowdSourcing High Definition Map in Automotive Edge Computing								IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2022)		IEEE International Conference on Communications						4300	4305				10.1109/ICC45855.2022.9838617							Proceedings Paper	2022	2022	High definition (HD) map needs to be updated frequently to capture road changes, which is constrained by limited specialized collection vehicles. To maintain an up-to-date map, we explore crowdsourcing data from connected vehicles. Updating the map collaboratively is, however, challenging under constrained transmission and computation resources in dynamic networks. In this paper, we propose EdgeMap, a crowdsourcing HD map to minimize the usage of network resources while maintaining the latency requirements. We design a DATE algorithm to adaptively offload vehicular data on a small time scale and reserve network resources on a large time scale, by leveraging the multi-agent deep reinforcement learning and Gaussian process regression. We evaluate the performance of EdgeMap with extensive network simulations in a time-driven end-to-end simulator. The results show that EdgeMap reduces more than 30% resource usage as compared to state-of-the-art solutions.					IEEE International Conference on Communications (ICC)IEEE International Conference on Communications (ICC)	MAY 16-20, 2022MAY 16-20, 2022	IEEE; Samsung; LG; Huawei; Qualcomm; Technol Innovat Inst; ZTE; Elect & Telecommunicat Res Inst; KMW; SOLiD; ERICSSON LG; NIIEEE; Samsung; LG; Huawei; Qualcomm; Technol Innovat Inst; ZTE; Elect & Telecommunicat Res Inst; KMW; SOLiD; ERICSSON LG; NI	Seoul, SOUTH KOREASeoul, SOUTH KOREA	0	0	0	0	0	0	0			1550-3607		978-1-5386-8347-7									Univ Nebraska Lincoln, Lincoln, NE 68588 USAXidian Univ, Xian, Peoples R ChinaUniv North Carolina Charlotte, Charlotte, NC USA				2022-12-07	WOS:000864709904080		
J	Gan, Jiongpeng; Li, Shen; Wei, Chongfeng; Deng, Lei; Tang, Xiaolin					Gan, Jiongpeng/0009-0007-3287-7201; Wei, Chongfeng/0000-0002-4565-509X; Li, Shen/0000-0002-7111-8861					Intelligent Learning Algorithm and Intelligent Transportation-Based Energy Management Strategies for Hybrid Electric Vehicles: A Review								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				24	10			10345	10361				10.1109/TITS.2023.3283010					JUN 2023		Review; Early Access		2023	As one of the alternatives to conventional fuel vehicles, hybrid electric vehicles (HEV) offer lower fuel consumption and fewer exhaust emissions. To improve the performance of the HEV, the energy management strategy (EMS) is one of the most critical technologies. Classic EMS can be broadly classified into rule-based and optimization-based. With the development of machine learning technology, the deep reinforcement learning (DRL) algorithm of intelligent learning algorithms has been applied to the EMS. This paper mainly reviews the research progress of the EMS based on DRL from two aspects of the algorithm and training environment, and the EMS research involving combining the intelligent transportation system (ITS) is reviewed. In addition, the experimental test progress situations of DRL-based EMS research are discussed. Finally, the challenge of DRL-based EMSs is analyzed and some solutions are provided. In particular, it also involves some discussion about automotive cyber security in the intelligent transportation environment.									3	0	0	0	0	0	3			1524-9050	1558-0016										Chongqing Univ, Coll Mech & Vehicle Engn, Chongqing 400044, Peoples R ChinaTsinghua Univ, Dept Civil Engn, Beijing 100000, Peoples R ChinaQueens Univ Belfast, Sch Mech & Aerosp Engn, Belfast BT7 1NN, North Ireland				2023-07-03	WOS:001012379100001		
J	Li, Zhaojian; Chu, Tianshu; Kolmanovsky, Ilya V.; Yin, Xiang; Yin, Xunyuan				Yin, Xiang/ADO-0292-2022; Chu, Tianshu/Z-4133-2019	Chu, Tianshu/0000-0002-3519-8737; Kolmanovsky, Ilya/0000-0002-7225-4160					Cloud resource allocation for cloud-based automotive applications								MECHATRONICS				50				356	365				10.1016/j.mechatronics.2017.10.010							Article	APR 2018	2018	There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive tasks. Efficient utilization of on-demand cloud resources holds a significant potential to improve future vehicle safety, comfort, and fuel economy. In the meanwhile, issues like cyber security and resource allocation pose great challenges. In this paper, we treat the resource allocation problem for cloud-based automotive systems. Both private and public cloud paradigms are considered where a private cloud provides an internal, company-owned Internet service dedicated to its own vehicles while a public cloud serves all subscribed vehicles. This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-based automotive systems. Complications such as stochastic communication delays and task deadlines are explicitly considered. In particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited to utilize the cloud resources for best Quality of Services. On the other hand, a decentralized auction-based model is developed for public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a "selfish" agent. Numerical examples are presented to illustrate the effectiveness of the developed techniques.									13	0	0	0	0	0	13			0957-4158											Michigan State Univ, Dept Mech Engn, E Lansing, MI 48824 USAStanford Univ, Dept Civil & Environm Engn, Stanford, CA 94305 USAUniv Michigan, Dept Aerosp Engn, Ann Arbor, MI 48109 USAShanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R ChinaUniv Alberta, Dept Chem & Mat Engn, Edmonton, AB T6G 1H9, Canada				2018-12-28	WOS:000430520800030		
P	SHAO C; ZHANG C; LI T; FAN W; ZHENG X										Unmanned vehicle path tracking and anti-collision            control strategy, involves calculating discount factor            according to time sequence difference method error to            obtain control quantity of improved control            strategy					CN116360408-A	BEIJING SUANFENG ZHENGTU TECHNOLOGY CO																									NOVELTY - The strategy involves analyzing vehicle kinematics and characteristics to establish a dynamic model of a vehicle. A controller input output quantity is determined. A vehicle running track is smoothed and improved to increase an environment constraint condition. A yaw rate stability and lateral speed stability calculation method is introduced to determine a vehicle stable operation range. A neural network controller model is constructed based on a control logic of a DDPG algorithm. A model input data matrix, an experience pool data storage mode and model output control quantity are determined. Reinforcement learning sparsity problem is determined based on vehicle running environment characteristics. A discount factor is calculated according to time sequence difference process error to obtain control quantity of an improved control strategy. USE - Unmanned vehicle path tracking and anti-collision control strategy based on DDPG algorithm. ADVANTAGE - The control policy can autonomously select path tracking and execution priority of obstacle avoidance task, improve the path tracking, anti-collision capability in the vehicle driving process, and can effectively improve the vehicle control precision. The DDPG algorithm has stronger learning ability and can continuously optimize the control effect. DESCRIPTION Of DRAWING(S) - The drawing shows a structural diagram of the neural network controller for unmanned vehicle path tracking and anti-collision based on DDPG algorithm. (Drawing includes non-English language text)															0																		2023-07-29	DIIDW:202373539W		
P	PENG H; ZHU J; LI S; CAI Y; CHEN T; CHEN Y; GENG H; CHAI X										Unmanned cooperative control method based on depth            reinforcement learning between human and machine for            auxiliary robot, involves establishing machine            intelligent body action strategy model by deep            Q-learning algorithm of the loop and constructing            function description of human action intention            model					CN115032900-A	NO 54 CHINA ELECTRONICS TECHNOL RES INST																									NOVELTY - The method involves constructing a function description of a human action intention model. A machine intelligent body action strategy model of function description is constructed. A random optimal strategy function is used to describe the machine intelligent body action strategy model. The input of the model is the environment state s. The output of the human action and human action intent is the optimal action. The machine intelligent body action strategy model is established using the deep Q-learning algorithm of the loop and is a multi-layer neural network to achieve the end-to-end mapping from the state to the action. The action of the human operator is selected when the action of human operators is close to the optimal action output by the machine intelligent body action strategy model. The optimal action output is chosen by the machine intelligent body action strategy model. The optimal action is output by the machine intelligent body action strategy model. USE - Unmanned cooperative control method e.g. unmanned aerial vehicle and unmanned vehicle based on depth reinforcement learning for auxiliary robot and remote operation robot. ADVANTAGE - The method avoids the control authority distribution problem in the human-machine cooperative control by depth reinforcement learning can achieve the better without human cooperative control. The safety of the whole human- machine cooperative system, stability and task execution performance are improved. The cooperative policy has strong target performance, and the synergistic efficiency is higher. The method uses the deep Q-learning algorithm of the human in-the-loop based on the deep reinforcement learning based algorithm frame, increases the human machine control strategy design, and achieves the effective human machine-machine control authority adjustment. The large amount of interactive data needed by training is avoided. The real data lack problem is solved in practical application. The control authority is dynamically adjusted and distributed to the human or machine intelligent body in the task execution process, to improve the safety. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram for unmanned cooperative control method based on depth reinforcement learning (Drawing includes non-English language text).															0																		2022-10-06	DIIDW:2022C1644A		
C	Gankin, Dennis; Mayer, Sebastian; Zinn, Jonas; Vogel-Heuser, Birgit; Endisch, Christian			IEEE	Vogel-Heuser, Birgit/M-1585-2017						Modular Production Control with Multi-Agent Deep Q-Learning								2021 26TH IEEE INTERNATIONAL CONFERENCE ON EMERGING TECHNOLOGIES AND FACTORY AUTOMATION (ETFA)		IEEE International Conference on Emerging Technologies and Factory Automation-ETFA											10.1109/ETFA45728.2021.9613177							Proceedings Paper	2021	2021	The automotive industry is increasingly focusing on product customization. The concept of Modular Production addresses this issue by providing more flexibility in production with Automated Guided Vehicles transporting products between modular workstations. The added complexity of Modular Production Control calls for approaches that can handle the scheduling complexity while also minimizing production costs. As a result, literature has focused on two promising approaches: Deep Reinforcement Learning and Multi-Agent Systems. Both approaches have their advantages. Especially in complex, large-scale production environments with random breakdowns, those two fields have been seldomly combined, though. As a result, this article aims to fill that research gap by introducing a Deep Reinforcement Learning Multi-Agent System approach for Modular Production Control. We introduce a reward design incentivizing agents to achieve maximal throughput. In addition, we show that the method learns optimal behavior even in a large-scale production environment with random machine breakdowns. Lastly, we compare the Multi-Agent System to a single-agent implementation of the Deep Reinforcement Learning approach and conclude that the Multi-Agent Deep Reinforcement Learning method learns and solves the Modular Production Control problem with the same solution quality as the single agent. Hence, the approach allows to foster MAS benefits such as robustness without losses in the solution quality.					26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)	SEP 07-10, 2021SEP 07-10, 2021	Inst Elect & Elect Engineers; Malardalen Univ; IEEE Ind Elect SocInst Elect & Elect Engineers; Malardalen Univ; IEEE Ind Elect Soc	ELECTR NETWORKELECTR NETWORK	1	0	0	0	0	0	1			1946-0740	1946-0759	978-1-7281-2989-1									Tech Univ Munich, Inst Automat & Informat Syst, D-80333 Munich, GermanyTH Ingolstadt, Inst Innovat Mobil, D-85049 Ingolstadt, Germany	TH Ingolstadt			2022-04-07	WOS:000766992600014		
J	Anzalone, Luca; Barra, Paola; Barra, Silvio; Castiglione, Aniello; Nappi, Michele				Castiglione, Aniello/F-1034-2011; Anzalone, Luca/JQU-2341-2023; Barra, Silvio/J-8577-2019	Castiglione, Aniello/0000-0003-0571-1074; Anzalone, Luca/0000-0002-0399-8836; Barra, Paola/0000-0002-7692-0626; Barra, Silvio/0000-0003-4042-3000; Nappi, Michele/0000-0002-2517-2867					An End-to-End Curriculum Learning Approach for Autonomous Driving Scenarios								IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				23	10			19817	19826				10.1109/TITS.2022.3160673					MAY 2022		Article; Early Access		2022	In this work, we combine Curriculum Learning with Deep Reinforcement Learning to learn without any prior domain knowledge, an end-to-end competitive driving policy for the CARLA autonomous driving simulator. To our knowledge, we are the first to provide consistent results of our driving policy on all towns available in CARLA. Our approach divides the reinforcement learning phase into multiple stages of increasing difficulty, such that our agent is guided towards learning an increasingly better driving policy. The agent architecture comprises various neural networks that complements the main convolutional backbone, represented by a ShuffleNet V2. Further contributions are given by (i) the proposal of a novel value decomposition scheme for learning the value function in a stable way and (ii) an ad-hoc function for normalizing the growth in size of the gradients. We show both quantitative and qualitative results of the learned driving policy.									5	0	0	0	0	0	6			1524-9050	1558-0016										Univ Bologna, Dept Phys & Astron DIFA, I-40127 Bologna, ItalySapienza Univ Rome, Dept Comp Sci, I-00185 Rome, ItalyUniv Naples Federico II, Dept Elect & Informat Technol Engn DIETI, I-80138 Naples, ItalyUniv Naples Parthenope, Dept Sci & Technol DIST, I-80133 Naples, ItalyUniv Salerno, Dept Comp Sci, I-84084 Salerno, Italy				2022-06-06	WOS:000800779000001		
C	Victor Miguel, Velazquez Espitia; Jose Angel, Gonzalez Gonzalez; Juarez Omar, Mata; Pedro, Ponce; Arturo, Molina			IEEE							Deep Q-learning for Control: Technique and Implementation Considerations on a Physical System: Active Automotive Rear Spoiler Case								2021 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2021)								818	824				10.1109/ICMA52036.2021.9512669							Proceedings Paper	2021	2021	Deep Q-learning is the combination of artificial neural networks advantages (ANNs) with Q-learning. ANNs have expanded the possibilities on a variety of algorithms by enhancing their capabilities and surpassing their limitations. This is the case of reinforcement learning. Nowadays, Deep Q-learning is used in a variety of applications in different fields, including the development of intelligent algorithms to control physical systems. Deep Q-learning has demonstrated the possibility of achieving effective results by solving specific tasks that are highly complex to model through classical approaches. An important drawback is that these models require an elaborated implementation process, and several design decisions must be taken in order to achieve reliable results. Often, developers might find the design process mostly experimental rather than ruled-based. Addressing this problem, the present work describes in detail the implementation process of Deep Q-learning to control a physical system, proposes considerations and analysis parameters for each of the main steps. Demonstrated in the development of the "Active automotive rear spoiler", the results present a methodology that successfully guides towards a proper implementation of Deep Q-learning. The knowledge of this paper should not be taken as a recipe, but rather as an evaluation reference to equip the reinforcement learning developers with tools for the development of projects.					IEEE International Conference on Mechatronics and Automation (IEEE ICMA)IEEE International Conference on Mechatronics and Automation (IEEE ICMA)	AUG 08-11, 2021AUG 08-11, 2021	IEEE; IEEE Robot & Automat Soc; Tianjin Univ Technol; UEC Tokyo; Japan Soc Mech Engineers; Japan Soc Precis Engn; Univ Elect Sci & Technol China; Kagawa Univ; Beijing Inst Technol; Robot Soc Japan; Soc Instrument & Control Engineers; Natl Nat Sci Fdn China; Chinese Mech Engn Soc; Chinese Assoc Automat; State Key Lab Robot & Syst; Beijing Inst Technol, Inst Adv Biomed Engn Syst; Minist Ind & Informat Technol Beijing Inst Technol, Key Lab Convergence Med Engn Syst & Healthcare Technol; Guangxi Univ Sci & Technol; Tianjin Key Lab Control Theory & Applicat Complicated Syst; Tianjin Key Lab Adv Mech Syst Design & Intelligent Control; Tianjin Univ Technol, Tianjin Int Joint Res & Dev Ctr; Harbin Engn Univ; Univ Electro Commun; Opt & Precis EngnIEEE; IEEE Robot & Automat Soc; Tianjin Univ Technol; UEC Tokyo; Japan Soc Mech Engineers; Japan Soc Precis Engn; Univ Elect Sci & Technol China; Kagawa Univ; Beijing Inst Technol; Robot Soc Japan; Soc Instrument & Control Engineers; Natl Nat Sci Fdn China; Chinese Mech Engn Soc; Chinese Assoc Automat; State Key Lab Robot & Syst; Beijing Inst Technol, Inst Adv Biomed Engn Syst; Minist Ind & Informat Technol Beijing Inst Technol, Key Lab Convergence Med Engn Syst & Healthcare Technol; Guangxi Univ Sci & Technol; Tianjin Key Lab Control Theory & Applicat Complicated Syst; Tianjin Key Lab Adv Mech Syst Design & Intelligent Control; Tianjin Univ Technol, Tianjin Int Joint Res & Dev Ctr; Harbin Engn Univ; Univ Electro Commun; Opt & Precis Engn	Takamatsu, JAPANTakamatsu, JAPAN	1	0	0	0	0	0	1					978-1-6654-4101-8									Tecnol Monterrey, Sch Engn & Sci, Calle Puente 222, Mexico City 14380, DF, Mexico				2022-06-09	WOS:000792713400142		
P	WANG C; TIAN W; QIAN X; WANG S; HU H; LIU Q										Processing self-adaptive assembling of radio            frequency connector used as automotive industry,            comprises e.g. obtaining initial position information            of electric connector socket by photographing, driving            high precision three-axis module platform to move, and            assembling electric connector					CN114976802-A; CN114976802-B	UNIV NANJING AERONAUTICS & ASTRONAUTICS																									NOVELTY - Processing self-adaptive assembling of radio frequency connector comprises obtaining initial position information of an electric connector socket by photographing using a binocular camera, driving high precision three-axis module platform to move, completing initial pose alignment of the electric connector and the electrical connector socket, executing electric connector-socket assembly strategy according to a relative pose error between sensor information and off-line database, controlling contact pose error by a variable impedance controller based on reinforcement learning, and assembling electric connector. USE - The method is useful for processing self-adaptive assembling of radio frequency connector, which is used as key component of signal transmission in array antenna of robot flexible control field, and used as automotive industry. ADVANTAGE - The method realizes precise force control of an electric connector assembly, improves assembling effect of the electric connector, effectively reduces attitude error between the connector and socket, allows variable impedance control method based on reinforcement learning, realizes accurate control of contact, reduces tracking error of the contact, and optimizes contact connector in inserting process of the control. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:elf-adaptive assembling system of the radio frequency connector; andA readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for processing self-adaptive assembling of radio frequency connector (Drawing includes non-English language text).															0																		2022-10-20	DIIDW:2022B5221N		
C	Ye, Yiming; Xu, Bin; Zhang, Jiangfeng; Lawler, Benjamin; Ayalew, Beshah			IEEE	Ye, Yiming/HLG-9017-2023	Ye, Yiming/0000-0001-7737-6100					Reinforcement Learning-Based Energy Management System Enhancement Using Digital Twin for Electric Vehicles								2022 IEEE VEHICLE POWER AND PROPULSION CONFERENCE (VPPC)		IEEE Vehicle Power and Propulsion Conference											10.1109/VPPC55846.2022.10003411							Proceedings Paper	2022	2022	Compared to conventional engine-based powertrains, electrified powertrain exhibit increased energy efficiency and reduced emissions, making electrification a key goal for the automotive industry. For a vehicle with hybrid energy storage system, its performance and lifespan are substantially affected by the energy management system. Reinforcement learning-based methods are gaining popularity in vehicle energy management, but most of the literature in this area focus on pure simulation while hardware implementation is still limited. This paper introduces the digital twin methodology to enhance the Q-learning-based energy management system for battery and ultracapacitor electric vehicles. The digital twin model can exploit the bilateral interdependency between the virtual model and the actual system, which improves the control performance of the energy management system. The physical model is established based on a hardware-in-the-loop simulation platform. In addition, battery degradation is also considered for prolonging the battery lifespan to reduce the operating cost. The validation results of the trained reinforcement learning agent illustrate that the digital twin-enhanced Q-learning energy management system improves the energy efficiency by 4.36% and the battery degradation is reduced by 25.28%.					IEEE Vehicle Power and Propulsion Conference (VPPC)IEEE Vehicle Power and Propulsion Conference (VPPC)	NOV 01-04, 2022NOV 01-04, 2022	IEEEIEEE	Merced, CAMerced, CA	3	0	0	0	0	0	3			1938-8756		978-1-6654-7587-7									Clemson Univ, Dept Automot Engn, Greenville, SC 29601 USAUniv Oklahoma, Sch Aerosp & Mech Engn, Norman, OK 73019 USA				2023-03-15	WOS:000925907200099		
B	Hua, R.; Tao, F.; Fu, Z.; Zhu, L.						Sun, F.; Meng, Q.; Fu, Z.; Fang, B.				A Deep Reinforcement Learning-Based Energy Management Optimization for Fuel Cell Hybrid Electric Vehicle Considering Recent Experience								Cognitive Systems and Information Processing: 8th International Conference, ICCSIP 2023, Revised Selected Papers. Communications in Computer and Information Science (1918)								374	86				10.1007/978-981-99-8018-5_28							Conference Paper	2024	2024	This study emphasizes a recent experience sampling method in conjunction with Deep Deterministic Policy Gradient (DDPG) to enhance the training speed and improve the training outcomes. Firstly, to ensure the safe operation of the battery and energy storage system under peak power, a power demand decoupling method based on frequency domain is proposed to achieve power stratification. Subsequently, a multi-objective equivalent consumption minimization strategy model is established based on the data types of the experimental platform, and the improved DDPG algorithm is employed to solve it. Finally, simulation results demonstrate that compared to conventional DDPG algorithms, the improved DDPG algorithm can enhance efficiency by an average of 2.02%.					International Conference on Cognitive Systems and Signal ProcessingInternational Conference on Cognitive Systems and Signal Processing	20232023		Luoyang, ChinaLuoyang, China	0	0	0	0	0	0	0					978-981-99-8018-5									Sch. of Inf. Eng., Henan Univ. of Sci. & Technol., Luoyang, ChinaLongmen Lab., Hengyang, ChinaTsinghua Univ., Beijing, ChinaSouthern Univ. of Sci. & Technol., Shenzhen, ChinaHenan Univ. of Sci. & Technol., Luoyang, China				2024-02-01	INSPEC:24410510		
J	Lee, Joash; Niyato, Dusit; Guan, Yong Liang; Kim, Dong In				Guan, Yong Liang/A-5090-2011; Lee, Joash/GPX-5469-2022; Lee, Joash/ACN-7777-2022	Guan, Yong Liang/0000-0002-9757-630X; Lee, Joash/0000-0002-8837-5290; 					Learning to Schedule Joint Radar-Communication With Deep Multi-Agent Reinforcement Learning								IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY				71	1			406	422				10.1109/TVT.2021.3124810							Article	JAN 2022	2022	Radar detection and communication are two essential sub-tasks for the operation of next-generation autonomous vehicles (AVs). The forthcoming proliferation of faster 5G networks utilizing mmWave has raised concerns on interference with automotive radar sensors, which has led to a body of research on Joint Radar-Communication (JRC). This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV. We first formulate the problem as a Markov Decision Process (MDP). We then propose a more general multi-agent system, with an appropriate medium access control (MAC) protocol, which is formulated as a partially observed Markov game (POMG). To solve the POMG, we propose a multi-agent extension of the Proximal Policy Optimization (PPO) algorithm, along with algorithmic features to enhance learning from raw observations. Simulations are run with a range of environmental parameters to mimic variations in real-world operation. The results show that the chosen deep reinforcement learning methods allow the agents to obtain strong performance with minimal a priori knowledge about the environment.									6	0	0	0	0	0	6			0018-9545	1939-9359										Nanyang Technol Univ, Energy Res Inst NTU, Singapore 639798, SingaporeNanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, SingaporeNanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, SingaporeSungkyunkwan Univ, Dept Elect & Comp Engn, Suwon 16419, South Korea				2022-02-01	WOS:000745533700036		
B	Hu, Anran										Learning in Mean-Field Games and Continuous-Time Stochastic Control Problems																												Dissertation/Thesis	Jan 01 2022	2022										0	0	0	0	0	0	0					9798352951149									University of California, Berkeley, Industrial Engineering & Operations Research, California, United States	University of California, Berkeley				PQDT:68567980		
P	WANG Y; ZHAO J; WEI J; CHI P										Unmanned aerial vehicle cluster cooperative attack            and defense decision making method, involves            constructing capture strategy by action space of            vehicle cluster, and using training to obtain action            strategy to attack and defend against vehicle					CN115454136-A	UNIV BEIHANG																									NOVELTY - The method involves establishing an unmanned aerial vehicle cluster attack resistance task model. An unmanned aerial vehicle cluster is sent to perform intercepting and intercepting of an enemy unmanned aerial vehicle task. A movement model of an unmanned vehicle and an enemy unmanned aerial vehicle is established. A motion model of the unmanned aerial vehicle group and the enemy unmanned aerial vehicle are established. An action mode is selected according to a state of the enemy unmanned aerial vehicle. Multi-intelligent body reinforcement learning training process is performed to train the unmanned aerial vehicle group strategy network to obtain an action strategy. The Multi-intelligent body reinforcement learning training process is used to obtain the action strategy to attack and defend the enemy unmanned aerial vehicle. USE - Unmanned aerial vehicle cluster cooperative attack and defense decision making method. ADVANTAGE - The method enables avoiding dimensional disaster problem caused by directly training large-scale unmanned aerial vehicle cluster so as to improve expansibility of a cluster number and success rate of an attacking and defending against task. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a unmanned aerial vehicle cluster cooperative attack and defense decision making method. (Drawing includes non-English language text).															0																		2022-12-29	DIIDW:2022F47814		
C	Feher, Arpad; Aradi, Szilard; Becsi, Tamas				Aradi, Szilárd/AAE-2623-2019; Bécsi, Tamás/H-9818-2012	Aradi, Szilárd/0000-0001-6811-2584; Bécsi, Tamás/0000-0002-1487-9672	Szakal, A				Q-learning based Reinforcement Learning Approach for Lane Keeping								2018 18TH IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND INFORMATICS (CINTI)		International Symposium on Computational Intelligence and Informatics						31	35											Proceedings Paper	2018	2018	The paper presents the application of a Q-learning reinforcement learning method in the area of vehicle control. The purpose of the research presented is to design an end-to-end behavior control of a kinematic vehicle model placed in a simulated race track environment, by using reinforcement learning approach. The varied trajectory of the track to provide different situations for the agent. The environment sensing model is based on high-level sensor information. Track curvature, lateral position, and relative yaw angle can be reached from the current automotive sensors, such as camera or IMU based systems. The objectives were reached through the definition of a rewarding system, with subrewards and penalties enforcing lane keeping. After the description of the theoretical basis the environment model with the reward function is detailed. Finally the experiments with the learning process are presented and the results of the evaluation are given from some aspects of control quality and safety.					18th IEEE International Symposium on Computational Intelligence and Informatics (CINTI)18th IEEE International Symposium on Computational Intelligence and Informatics (CINTI)	NOV 21-22, 2018NOV 21-22, 2018	IEEE; Hungarian Fuzzy Assoc; Obuda UnivIEEE; Hungarian Fuzzy Assoc; Obuda Univ	Budapest, HUNGARYBudapest, HUNGARY	5	0	0	0	0	0	5			2380-8586		978-1-7281-1117-9									Budapest Univ Technol & Econ, Dept Control Transportat & Vehicle Syst, Stoczek U-2, H-1111 Budapest, Hungary				2018-01-01	WOS:000522225400006		
P	DAI Z; LIU C; YE Y										Space-ground cooperative sensing method for            exploring personality and cooperative characteristic of            unmanned population, involves using self-supervised            classifier to define internal reward based on            characteristic characteristic					CN115525965-A	BEIJING INST TECHNOLOGY																									NOVELTY - The method involves collecting data from each interest point under a space cooperation group sensing scene by an unmanned group. The total available spectrum of all interest points is divided into Z sub-channels based on space-based non-orthogonal multiple access technology. The data collecting event of each sub-channel z is represented by a tuple (u, g, i, i, i, and i, z, t, where i and i' represent the interest point accessed by the unmanned aerial vehicle and the unmanned vehicle. The local observation is obtained from an environment outside the reward and the next time slot from the environment according to the movement condition of the current time slot and the data collection condition. The simulation platform calculates the value function in the multi-agent actor-evaluation depth reinforcement learning method. USE - Space-ground cooperative sensing method for exploring personality and cooperative characteristic of unmanned population e.g. unmanned aerial vehicle and ground unmanned population e.g. unmanned vehicle. ADVANTAGE - The method uses the self-supervised classifier to define the internal reward, based on the characteristic characteristic generation of the reward, thus effectively making the unmanned aerial vehicle and the unmanned vehicle to reach better space division, and hence greatly improving the task efficiency. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a space-ground cooperative sensing method (Drawing includes Non-English language text).															0																		2023-08-10	DIIDW:202304119H		
C	Elmalaki, Salma			IEEE							MAConAuto: Framework for Mobile-Assisted Human-in-the-Loop Automotive System								2022 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)		IEEE Intelligent Vehicles Symposium						740	749				10.1109/IV51971.2022.9827415							Proceedings Paper	2022	2022	Automotive is becoming more and more sensor-equipped. Collision avoidance, lane departure warning, and self-parking are examples of applications becoming possible with the adoption of more sensors in the automotive industry. Moreover, the driver is now equipped with sensory systems like wearables and mobile phones. This rich sensory environment and the real-time streaming of contextual data from the vehicle make the human factor integral in the loop of computation. By integrating the human's behavior and reaction into the advanced driver-assistance systems (ADAS), the vehicles become a more contextaware entity. Hence, we propose MAConAuto, a framework that helps design human-in-the-loop automotive systems by providing a common platform to engage the rich sensory systems in wearables and mobile to have context-aware applications. By personalizing the context adaptation in automotive applications, MAConAuto learns the behavior and reactions of the human to adapt to the personalized preference where interventions are continuously tuned using Reinforcement Learning. Our general framework satisfies three main design properties, adaptability, generalizability, and conflict resolution. We show how MAConAuto can be used as a framework to design two applications as human-centric applications, forward collision warning, and vehicle HVAC system with negligible time overhead to the average human response time.					33rd IEEE Intelligent Vehicles Symposium (IEEE IV)33rd IEEE Intelligent Vehicles Symposium (IEEE IV)	JUN 05-09, 2022JUN 05-09, 2022	IEEE; Bosch; IEEE Intelligent Transport Syst Soc; RWTH Aachen Univ, IKA; Mercedes BenzIEEE; Bosch; IEEE Intelligent Transport Syst Soc; RWTH Aachen Univ, IKA; Mercedes Benz	Aachen, GERMANYAachen, GERMANY	1	0	0	0	0	0	1			1931-0587		978-1-6654-8821-1									Univ Calif Irvine, Irvine, CA 92717 USA				2022-10-13	WOS:000854106700104		
P	LEE S										Vehicle manufacturing process monitoring system, has artificial neural network model for configuring component and assembly unit based on big data, and failure analysis unit for updating threshold value based on reinforcement learning					KR2120128-B1	LEE S																									NOVELTY - The system (100) has a defect analysis unit for detecting defect type based on process data. A display unit outputs status of a production line in which defect type is detected, where the defect type includes injection defect, assembly defect, stain defect, crack defect, paint defect and foreign substance defect. A management server (200) builds big data based on the process data. An artificial neural network model configures component and assembly unit based on the big data. A failure analysis unit updates threshold value based on reinforcement learning. USE - Vehicle manufacturing process monitoring system. ADVANTAGE - The system detects defective parts by monitoring production status of automotive components. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a vehicle manufacturing process monitoring system. (Drawing includes non-English language text).System (100)Management server (200)															0																		2023-08-10	DIIDW:2020502718		
