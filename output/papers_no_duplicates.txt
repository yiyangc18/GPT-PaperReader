PT	AU	TI	PY	SO	SE	BP	EP	AB	DT	PD	DI	methods	domains	IF	Cite	partition
C	Ma, Zibo; Liu, Xudong; Zhang, Liguo	Dissipation of Stop-and-Go Waves of Mixed Autonomous Vehicle Flow with Reinforcement Learning	2021	2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)	Chinese Control Conference	6064	6069	"Traffic congestion is a common phenomenon in cities, and improving traffic efficiency has become an urgent problem to be solved. Many researchers have proposed feasible solutions from different perspectives, such as traffic flow, signal lights, etc. This paper proposes a method of dissipating stop-and-go wave based on reinforcement learning (RL). Specifically, we take the mixed autonomous vehicle flow as the research object, using RL to train the driving strategy of connected autonomous vehicle (CAV), and dissipating the stop-and-go waves in vehicle flow by adjusting the CAV's driving behavior. We propose the concept of ""Equivalent Density Difference"" as a model to describe the difference of traffic flow dynamics before and after a specific vehicle within a certain range, and use this index to design RL model. The proposed method combines the advantages of data-driven and model-driven, improving the training efficiency of RL. Experimental results show that this method can increase the system-level speed and improve the stability of the mixed autonomous vehicle flow."	Proceedings Paper	2021		[]	[]	-1	-1	-1
C	Xia, Yuyang; Liu, Shuncheng; Chen, Xu; Xu, Zhi; Zheng, Kai; Su, Han	RISE: A Velocity Control Framework with Minimal Impacts based on Reinforcement Learning	2022	PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022		2210	2219	Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade. However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes. In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle). In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles. To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle. Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles. To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph. Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.	Proceedings Paper	2022	10.1145/3511808.3557435	[]	[]	-1	-1	-1
C	Naruse, K; Kakazu, Y; Leu, MC	Modular reinforcement learning for autonomous vehicle navigation in an unknown workspace	1998	1998 JAPAN-U.S.A. SYMPOSIUM ON FLEXIBLE AUTOMATION - PROCEEDINGS, VOLS I AND II		577	583	This paper presents the development of a planning system using modular reinforcement learning for autonomous vehicle navigation in an unknown workspace. The system consists of modules that learn subtasks independently: Each module carries out a subtask of the navigation such as obstacle avoidance or goal seeking. Computational simulations demonstrate that the proposed planning system solves the navigation problem efficiently.	Proceedings Paper	1998		[]	[]	-1	-1	-1
J	Xu Zezhou; Qu Dayi; Hong Jiale; Song Xiaochen	Research on Decision-making Method for Autonomous Driving Behavior of Connected and Automated Vehicle	2021	Complex Systems and Complexity Science		88	94	Aiming at the problem of direct conflict between autonomous vehicles and other humandriven vehicles at intersections,an autonomous vehicle behavior decision model is built,and deep reinforcement learning is used to train autonomous vehicles when passing road intersections,allowing autonomous vehicles to make autonomous decisions and achieve fast control of complex scenarios, and the comparison with the non-dominated sorting genetic algorithm-â…¡verifies the stability of the autonomous vehicle.The simulation results show that the autonomous vehicle behavior decision-making method using the depth deterministic strategy gradient algorithm has better output speed to ensure the smooth changes of the throttle and brake values,and effectively solve the safety and comfort problems of autonomous vehicles.	Article	2021		[]	[]	-1	-1	-1
J	Quang-Duy Tran; Bae, Sang-Hoon	An Efficiency Enhancing Methodology for Multiple Autonomous Vehicles in an Urban Network Adopting Deep Reinforcement Learning	2021	APPLIED SCIENCES-BASEL				To reduce the impact of congestion, it is necessary to improve our overall understanding of the influence of the autonomous vehicle. Recently, deep reinforcement learning has become an effective means of solving complex control tasks. Accordingly, we show an advanced deep reinforcement learning that investigates how the leading autonomous vehicles affect the urban network under a mixed-traffic environment. We also suggest a set of hyperparameters for achieving better performance. Firstly, we feed a set of hyperparameters into our deep reinforcement learning agents. Secondly, we investigate the leading autonomous vehicle experiment in the urban network with different autonomous vehicle penetration rates. Thirdly, the advantage of leading autonomous vehicles is evaluated using entire manual vehicle and leading manual vehicle experiments. Finally, the proximal policy optimization with a clipped objective is compared to the proximal policy optimization with an adaptive Kullback-Leibler penalty to verify the superiority of the proposed hyperparameter. We demonstrate that full automation traffic increased the average speed 1.27 times greater compared with the entire manual vehicle experiment. Our proposed method becomes significantly more effective at a higher autonomous vehicle penetration rate. Furthermore, the leading autonomous vehicles could help to mitigate traffic congestion.	Article	FEB 2021	10.3390/app11041514	[]	[]	-1	-1	-1
C	Corso, Anthony; Du, Peter; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.	Adaptive Stress Testing with Reward Augmentation for Autonomous Vehicle Validation	2019	2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	163	168	Determining possible failure scenarios is a critical step in the evaluation of autonomous vehicle systems. Real world vehicle testing is commonly employed for autonomous vehicle validation, but the costs and time requirements are high. Consequently, simulation driven methods such as Adaptive Stress Testing (AST) have been proposed to aid in validation. AST formulates the problem of finding the most likely failure scenarios as a Markov decision process, which can be solved using reinforcement learning. In practice, AST tends to find scenarios where failure is unavoidable and tends to repeatedly discover the same types of failures of a system. This work addresses these issues by encoding domain relevant information into the search procedure. With this modification, the AST method discovers a larger and more expressive subset of the failure space when compared to the original AST formulation. We show that our approach is able to identify useful failure scenarios of an autonomous vehicle policy.	Proceedings Paper	2019		[]	[]	-1	-1	-1
C	Arvind, C. S.; Senthilnath, J.	Autonomous RL: Autonomous Vehicle Obstacle Avoidance in a Dynamic Environment using MLP-SARSA Reinforcement Learning	2019	2019 IEEE 5TH INTERNATIONAL CONFERENCE ON MECHATRONICS SYSTEM AND ROBOTS (ICMSR 2019)		120	124	This paper presents a Multi-Layer Perceptron-State Action Reward State Action (MLP-SARSA) based reinforcement learning methodology for dynamic obstacle detection and avoidance for autonomous vehicle navigation. MLP-SARSA is an on-policy reinforcement learning approach, which gains information and rewards from the environment and helps the autonomous vehicle to avoid dynamic moving obstacles. MLP with SARSA provides a significant advantage over dynamic environment compared to other traditional reinforcement algorithms. In this study, a MLP-SARSA model is trained in a complex urban simulation environment with dynamic obstacles using the pygame library. Experimental results show that the trained MLP-SARSA can navigate the autonomous vehicle in a dynamic environment with more confidences than traditional Q-learning and SARSA reinforcement algorithms.	Proceedings Paper	2019	10.1109/icmsr.2019.8835462	[]	[]	-1	-1	-1
B	Xinchen Zhang; Jun Zhang; Yuansheng Lin; Longyang Xie	Research on Decision Model of Autonomous Vehicle Based on Deep Reinforcement Learning	2021	Proceedings of 2021 IEEE 11th International Conference on Electronics Information and Emergency Communication (ICEIEC)		136	40	The Deep Q Network (DQN) model has been widely used in autonomous vehicle lane change decision in highway scenes, but the traditional DQN has the problems of overestimation and slow convergence speed. Aiming at these problems, an autonomous vehicle lane changing decision model based on the improved DQN is proposed. First, the obtained state values are input into two neural networks with the same structure and different parameter update frequencies to reduce the correlation between empirical samples, and then the hybrid strategy based on e-greedy and Boltzmann is used to make the vehicles explore the environment. Finally, the model is trained and tested in the experimental scene built by the NGSIM dataset. The experimental results show that the Double Deep Q Network (DDQN) model based on the hybrid strategy improves the success rate of the autonomous vehicle's lane-changing decision and the convergence speed of the network.	Conference Paper	2021	10.1109/ICEIEC51955.2021.9463817	[]	[]	-1	-1	-1
B	Xu, G.; Chen, B.; Li, G.; He, X.	Connected Autonomous Vehicle Platoon Control Through Multi-agent Deep Reinforcement Learning	2022	Broadband Communications, Networks, and Systems: 12th EAI International Conference, BROADNETS 2021, Proceedings. Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering (413)		239	48	The rise of the artificial intelligence (AI) brings golden opportunity to accelerate the development of the intelligent transportation system (ITS). The platoon control of connected autonomous vehicle (CAV) as the key technology exhibits superior for improving traffic system. However, there still exist some challenges in multi-objective platoon control and multi-agent interaction. Therefore, this paper proposed a connected autonomous vehicle latoon control approach with multi-agent deep reinforcement learning (MADRL). Finally, the results in stochastic mixed traffic flow based on SUMO (simulation of urban mobility) platform demonstrate that the proposed method is feasible, effective and advanced.	Conference Paper	2022	10.1007/978-3-030-93479-8_16	[]	[]	-1	-1	-1
C	Kuhnert, KD; KrÃ¶del, M	Evaluative feedback as the basis for behavior optimization in the area of autonomous vehicle steering	2005	2005 IEEE Intelligent Transportation Systems Conference (ITSC)		671	675	Steering an autonomous vehicle requires the permanent adaptation of behavior in relationship to the various situations the vehicle is in. This paper describes a research which implements such adaptation and optimization based on Reinforcement Learning (RL) which in detail purely learns from evaluative feedback in contrast to instructive feedback. In this way it self-explores and self-optimises actions for situations in a defined environment. The target of this research is to determine to what extent RL-based Systems serve as an enhancement or even an alternative to classical concepts of autonomous intelligent vehicles such as modelling or neural nets.	Proceedings Paper	2005		[]	[]	-1	-1	-1
J	Rasheed, Iftikhar; Hu, Fei; Zhang, Lin	Deep reinforcement learning approach for autonomous vehicle systems for maintaining security and safety using LSTM-GAN	2020	VEHICULAR COMMUNICATIONS				The success of autonomous vehicles (AV hs) depends upon the effectiveness of sensors being used and the accuracy of communication links and technologies being employed. But these sensors and communication links have great security and safety concerns as they can be attacked by an adversary to take the control of an autonomous vehicle by influencing their data. Especially during the state estimation process for monitoring of autonomous vehicles' dynamics system, these concerns require immediate and effective solution. In this paper we present a new adversarial deep reinforcement learning algorithm (NDRL) that can be used to maximize the robustness of autonomous vehicle dynamics in the presence of these attacks. In this approach the adversary tries to insert defective data to the autonomous vehicle's sensor readings so that it can disrupt the safe and optimal distance between the autonomous vehicles traveling on the road. The attacker tries to make sure that there is no more safe and optimal distance between the autonomous vehicles, thus it may lead to the road accidents. Further attacker can also add fake data in such a way that it leads to reduced traffic flow on the road. On the other hand, autonomous vehicle will try to defend itself from these types of attacks by maintaining the safe and optimal distance i.e. by minimizing the deviation so that adversary does not succeed in its mission. This attacker-autonomous vehicle action reaction can be studied through the game theory formulation with incorporating the deep learning tools. Each autonomous vehicle will use Long-Short-Term-Memory (LSTM)-Generative Adversarial Network (GAN) models to find out the anticipated distance variation resulting from its actions and input this to the new deep reinforcement learning algorithm (NDRL) which attempts to reduce the variation in distance. Whereas attacker also chooses deep reinforcement learning algorithm (NDRL) and wants to maximize the distance variation between the autonomous vehicles. (c) 2020 Elsevier Inc. All rights reserved.	Article	DEC 2020	10.1016/j.vehcom.2020.100266	[]	[]	-1	-1	-1
J	Fu Yihao; Bao Hong; Liang Tianjiao; Fu Dongpu; Pan Feng	Research on decision algorithm for autonomous vehicle lane change based on vision DQN	2023	Transducer and Microsystem Technology		52	55	Unmanned driving technology is current research hotspot in the field of artificial intelligence(AI). Traditional deep Q network(DQN)has problems such as slow convergence speed in the vision-based lane change decision. Aiming at this problem,a vision-based DQN autonomous vehicle lane-changing decision model is proposed. Visual perception based on the attention mechanism is fused with DQN to focus the network on important image features,and introduces the Q-Masking mechanism to reduce the complexity of decision-making and speed up the convergence speed of DQN training. Finally,a DQN-based speed decision algorithm is proposed to form a complete lane changing decision model,which is trained and tested in a simulation environment. The experimental results show that the proposed model can realize a faster lane change decision-making strategy,and accelerate the convergence speed of DQN,at the same time.	Article	2023		[]	[]	-1	-1	-1
C	Mokhtari, Kasra; Wagner, Alan R.	Don't Get into Trouble! Risk-aware Decision-Making for Autonomous Vehicles	2022	2022 31ST IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (IEEE RO-MAN 2022)		1570	1577	Risk is traditionally described as the expected likelihood of an undesirable outcome, such as a collision for an autonomous vehicle. Accurately predicting risk or potentially risky situations is critical for the safe operation of an autonomous vehicle. This work combines use of a controller trained to navigate around individuals in a crowd and a risk-based decision-making framework for an autonomous vehicle that integrates high-level risk-based path planning with a reinforcement learning-based lowlevel control. We evaluated our method using a high-fidelity simulation environment. We show our method results in zero collisions with pedestrians and predicted the least risky path, time to travel, or day to travel in approximately 72% of traversals. This work can improve safety by allowing an autonomous vehicle to one day avoid and react to risky situations.	Proceedings Paper	2022	10.1109/RO-MAN53752.2022.9900795	[]	[]	-1	-1	-1
J	Bin Issa, Razin; Das, Modhumonty; Rahman, Md. Saferi; Barua, Monika; Rhaman, Md. Khalilur; Ripon, Kazi Shah Nawaz; Alam, Md. Golam Rabiul	Double Deep Q-Learning and Faster R-CNN-Based Autonomous Vehicle Navigation and Obstacle Avoidance in Dynamic Environment	2021	SENSORS				Autonomous vehicle navigation in an unknown dynamic environment is crucial for both supervised- and Reinforcement Learning-based autonomous maneuvering. The cooperative fusion of these two learning approaches has the potential to be an effective mechanism to tackle indefinite environmental dynamics. Most of the state-of-the-art autonomous vehicle navigation systems are trained on a specific mapped model with familiar environmental dynamics. However, this research focuses on the cooperative fusion of supervised and Reinforcement Learning technologies for autonomous navigation of land vehicles in a dynamic and unknown environment. The Faster R-CNN, a supervised learning approach, identifies the ambient environmental obstacles for untroubled maneuver of the autonomous vehicle. Whereas, the training policies of Double Deep Q-Learning, a Reinforcement Learning approach, enable the autonomous agent to learn effective navigation decisions form the dynamic environment. The proposed model is primarily tested in a gaming environment similar to the real-world. It exhibits the overall efficiency and effectiveness in the maneuver of autonomous land vehicles.	Article	FEB 2021	10.3390/s21041468	[]	[]	-1	-1	-1
C	Du, Guodong; Zou, Yuan; Zhang, Xudong; Dong, Guoshun; Yin, Xin	Heuristic Reinforcement Learning Based Overtaking Decision for an Autonomous Vehicle	2021	IFAC PAPERSONLINE		59	66	This paper proposes an intelligent overtaking decision based on the heuristic reinforcement learning method for an autonomous vehicle. The proposed overtaking control focuses on the safety and efficiency of the autonomous vehicle driving. Firstly, the overtaking problem is modeled and the adaptive safe driving area is constructed. Then, a heuristic reinforcement learning method called Heu-Dyna is developed to derive the optimal overtaking decision, which introduces the heuristic planning function. Besides, the generalized correlation coefficient is designed to evaluate the training perfection of the control strategy. The simulation results show that the performance of the proposed method on the rapidity and optimality is superior to the Q-learning method and the Dyna method. Furthermore, the adaptability of the proposed method is validated by applying different driving conditions. Copyright (C) 2021 The Authors.	Proceedings Paper; Early Access		10.1016/j.ifacol.2021.10.141	[]	[]	-1	-1	-1
B	Hsiao-Ting Tseng; Chen-Chiung Hsieh; Wei-Ting Lin; Jyun-Ting Lin	Deep Reinforcement Learning for Collision Avoidance of Autonomous Vehicle	2020	2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)		2 pp.	2 pp.	To save training efforts, reinforcement learning approach is applied to the autonomous vehicle for obstacle avoidance. Therefore, this study is aimed to let the autonomous vehicle to learn from mistakes and readdress its movement accuracy for collision avoidance in working environment. An enhanced learning method Q-learning is used to record and update the Q values for different movement through a table that the autonomous vehicle can use it to determine how and where to move. The Q table is learned through the deep learning neural network which may encounter innumerable situations from the environments and the different actions performed by the autonomous vehicle. In the experiments, the depth camera is adopted as the input device to be not affected by light intensity and road color. The Q table is ready to use after 9000 epochs or about 3.5 hours training. Let the autonomous vehicle run for 3 minutes at a time in three different environments with lights on and off 10 times each. The success rate of obstacle avoidance is as high as 95% which proves the feasibility of proposed approach.	Conference Paper	2020	10.1109/ICCE-Taiwan49838.2020.9258199	[]	[]	-1	-1	-1
C	Pan, Kai; Kong, Weibo; Guo, Xing	A Route Planning for Autonomous Vehicle in 5G and Edge Computing Environment	2022	2022 8TH INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING AND COMMUNICATIONS, BIGCOM		294	303	Route planning have a huge impact on the safe driving and energy efficiency of autonomous vehicle, which is an inevitable part of Internet of Vehicle. In the 5G edge environment, autonomous vehicle needs to communicate with other vehicles and base stations, route planning needs to consider the communication overhead of autonomous vehicle in addition to the driving distance. However, the current route planning methods only consider the shortest path, which cannot comprehensively consider the distance overhead and time cost. To cope with these challenges, this paper proposes an algorithm based on reinforcement learning to solve autonomous vehicle route planning through 5G networks and edge computing(RLVRP), which can obtain the goal of minimizing the driving distance based on minimizing the task processing time and server response time. In addition, we update our route planning strategy according to the dynamic change of network resources. Extensive experimental results show that the proposed algorithm greatly reduces service delay compared with state-of-the-art baselines.	Proceedings Paper	2022	10.1109/BigCom57025.2022.00044	[]	[]	-1	-1	-1
J	Szoke, Laszlo; Aradi, Szilard; Becsi, Tamas; Gaspar, Peter	Skills to Drive: Successor Features for Autonomous Highway Pilot	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		18707	18718	Reinforcement learning applications are spreading among different domains, including autonomous vehicle control. The diverse situations that can happen during, for instance, at a highway commute are infinite, and with labeled data, the perfect coverage of all use-cases sounds ambitious. However, with the complex tasks and complicated scenarios faced during an autonomous vehicle system design, the credit assignment problem arises. How to construct appropriate objectives for the artificial intelligence to learn and the preferences between the different goals also matter of the designer's choice. This work attempts to tackle the problem by utilizing successor features and providing a possible decomposition of the reward functions, guiding the agent's actions. This method makes the training easier for the agent and enables immediate, profound performance on new combined tasks. Furthermore, with the optimal composition, the desired behavior can be fine-tuned, and as an auxiliary gain, the decomposition empowers different driving styles and makes driving preferences rapidly changeable. We introduce the adaptation of FastRL algorithm to autonomous vehicle domain, meanwhile developing a stabilizing way of using Successor Features, namely DoubleFastRL. We compare our solution for a highway driving scenario with basic agents such as Q-learning having multi-objective training.	Article; Early Access		10.1109/TITS.2022.3150493	[]	[]	-1	-1	-1
C	Mo, Shuojie; Pei, Xiaofei; Chen, Zhenfu	Decision-Making for Oncoming Traffic Overtaking Scenario using Double DQN	2019	2019 3RD CONFERENCE ON VEHICLE CONTROL AND INTELLIGENCE (CVCI)		230	233	Great progress has been made in the field of machine learning in recent years. And learning-based methods have been widely utilized for developing highly autonomous vehicle. To this end, we introduce a reinforcement learning based intelligent autonomous vehicle decision making method for oncoming overtaking scenario. The goal of reinforcement learning is to learn how to take optimal decision in corresponding observations through interactions with the environment using a reward function to estimate whether the decision is good or not. A Double Deep Q-learning (Double DQN) agent was used to learn policies (control strategies) for both longitudinal speed and lane change decision. Prioritized Experience Replay (PER) was used to accelerate convergence of the policies. A two-way 3-car scenario with oncoming traffic was established in SUMO (Simulation of Urban Mobility) to train and test the policies.	Proceedings Paper	2019	10.1109/cvci47823.2019.8951626	[]	[]	-1	-1	-1
C	You, Changxi; Lu, Jianbo; Filev, Dimitar; Tsiotras, Panagiotis	Highway Traffic Modeling and Decision Making for Autonomous Vehicle Using Reinforcement Learning	2018	2018 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	1227	1232	This paper studies the decision making problem of autonomous vehicles in traffic. We model the interaction between an autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an experienced driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. By designing the reward function of the MDP, the desired, driving behavior of the autonomous vehicle is obtained using reinforcement learning. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Jacinto, Edwar; Martinez, Fernando; Martinez, Fredy	Navigation of Autonomous Vehicles using Reinforcement Learning with Generalized Advantage Estimation	2023	INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS		954	959	This study proposes a reinforcement learning ap-proach using Generalized Advantage Estimation (GAE) for autonomous vehicle navigation in complex environments. The method is based on the actor-critic framework, where the actor network predicts actions and the critic network estimates state values. GAE is used to compute the advantage of each action, which is then used to update the actor and critic networks. The approach was evaluated in a simulation of an autonomous vehicle navigating through challenging environments and it was found to effectively learn and improve navigation performance over time. The results suggest GAE as a promising direction for further research in autonomous vehicle navigation in complex environments.	Article	JAN 2023		[]	[]	-1	-1	-1
J	Lu, Chengjie; Shi, Yize; Zhang, Huihui; Zhang, Man; Wang, Tiexin; Yue, Tao; Ali, Shaukat	Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions	2023	IEEE TRANSACTIONS ON SOFTWARE ENGINEERING		384	402	Autonomous vehicles must operate safely in their dynamic and continuously-changing environment. However, the operating environment of an autonomous vehicle is complicated and full of various types of uncertainties. Additionally, the operating environment has many configurations, including static and dynamic obstacles with which an autonomous vehicle must avoid collisions. Though various approaches targeting environment configuration for autonomous vehicles have shown promising results, their effectiveness in dealing with a continuous-changing environment is limited. Thus, it is essential to learn realistic environment configurations of continuously-changing environment, under which an autonomous vehicle should be tested regarding its ability to avoid collisions. Featured with agents dynamically interacting with the environment, Reinforcement Learning (RL) has shown great potential in dealing with complicated problems requiring adapting to the environment. To this end, we present an RL-based environment configuration learning approach, i.e., DeepCollision, which intelligently learns environment configurations that lead an autonomous vehicle to crash. DeepCollision employs Deep Q-Learning as the RL solution, and selects collision probability as the safety measure, to construct the reward function. We trained four DeepCollision models and conducted an experiment to compare them with two baselines, i.e., random and greedy. Results show that DeepCollision demonstrated significantly better effectiveness in generating collisions compared with the baselines. We also provide recommendations on configuring DeepCollision with the most suitable time interval based on different road structures.	Article	JAN 1 2023	10.1109/TSE.2022.3150788	[]	[]	-1	-1	-1
J	Zhiqian Qiao; Tyree, Z.; Mudalige, P.; Schneider, J.; Dolan, J.M.	Hierarchical reinforcement learning method for autonomous vehicle behavior planning [arXiv]	2019	arXiv		8 pp.	8 pp.	In this work, we propose a hierarchical reinforcement learning (HRL) structure which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals. In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other similar tasks with the same sub-goals. The states are defined as processed observations which are transmitted from the perception system of the autonomous vehicle. A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure. Compared to traditional RL methods, our algorithm is more sample-efficient since its modular design allows reusing the policies of sub-goals across similar tasks. The results show that the proposed method converges to an optimal policy faster than traditional RL methods.	Journal Paper	9 Nov. 2019		[]	[]	-1	-1	-1
B	Sahal, M.; Hidayat, Z.; Saputra, F.D.; Rizqifadiilah, M.A.; Putra, R.A.	Obstacle Avoidance System on Autonomous Car Using D3QN	2023	2023 14th International Conference on Information & Communication Technology and System (ICTS)		199	204	An autonomous vehicle's triumphant and safe navigation, which can circumvent obstacles, necessitates a skill set encompassing steering wheel control, sometimes called obstacle avoidance. One potential approach to address this issue is using a simulation framework wherein an automobile is subjected to various barriers. During this simulation, the sensory input of the car, as well as its corresponding actions, are recorded and analyzed. An alternative approach involves allowing the vehicle to autonomously acquire knowledge to optimize its performance towards the desired objective. The Dueling Deep Double QNetworks (D3QN) approach is a strategy that enables the model to autonomously learn and optimize its performance to attain the most favorable conclusion. The D3QN architecture is a computational framework incorporating Dueling and double-Q processes. The implementation of D3QN is anticipated to result in a reduction in the training time required for an autonomous vehicle. This study is expected to substitute for training an autonomous vehicle.	Conference Paper	2023	10.1109/ICTS58770.2023.10330873	[]	[]	-1	-1	-1
B	Cheng, Y.; Li, W.; Duan, F.; Li, S.E.	Discrete-time Finite Horizon Adaptive Dynamic Programming for Autonomous Vehicle Control	2022	2022 IEEE International Conference on Unmanned Systems (ICUS)		424	9	Nowadays, the autonomous vehicle control has become more and more critical, especially for the trajectory tracking problem. The existing model predictive control method applied to autonomous vehicle can reduce the tracking error. However, it requires online calculation for the model recurrence in long horizon, which causes heavy computational burden. This paper presents a novel discrete-time model predictive control solving framework. A finite horizon adaptive dynamic programming method was proposed to estimate the control policy for trajectory tracking. Under the constraint of self consistent condition, the control policy is fitted through deep neural network as a parameterized function to obtain faster solution. The performance of the proposed method is tested and verified under double lane change conditions via CarSim. The experimental results show that the system can accurately track given reference trajectory with 5000Hz frequency, which is three times faster than that solved by least squares estimation method.	Conference Paper	2022	10.1109/ICUS55513.2022.9987042	[]	[]	-1	-1	-1
C	Tseng, Hsiao-Ting; Hsieh, Chen-Chiung; Lin, Wei-Ting; Lin, Jyun-Ting	Deep Reinforcement Learning for Collision Avoidance of Autonomous Vehicle	2020	2020 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN (ICCE-TAIWAN)	IEEE International Conference on Consumer Electronics-Taiwan			To save training efforts, reinforcement learning approach is applied to the autonomous vehicle for obstacle avoidance. Therefore, this study is aimed to let the autonomous vehicle to learn from mistakes and readdress its movement accuracy for collision avoidance in working environment. An enhanced learning method Q-learning is used to record and update the Q values for different movement through a table that the autonomous vehicle can use it to determine how and where to move. The Q table is learned through the deep learning neural network which may encounter innumerable situations from the environments and the different actions performed by the autonomous vehicle. In the experiments, the depth camera is adopted as the input device to be not affected by light intensity and road color. The Q table is ready to use after 9000 epochs or about 3.5 hours training. Let the autonomous vehicle run for 3 minutes at a time in three different environments with lights on and off 10 times each. The success rate of obstacle avoidance is as high as 95% which proves the feasibility of proposed approach.	Proceedings Paper	2020	10.1109/icce-taiwan49838.2020.9258199	[]	[]	-1	-1	-1
J	Amini, Alexander; Gilitschenski, Igor; Phillips, Jacob; Moseyko, Julia; Banerjee, Rohan; Karaman, Sertac; Rus, Daniela	Learning Robust Control Policies for End-to-End Autonomous Driving From Data-Driven Simulation	2020	IEEE ROBOTICS AND AUTOMATION LETTERS		1143	1150	In this work, we present a data-driven simulation and training engine capable of learning end-to-end autonomous vehicle control policies using only sparse rewards. By leveraging real, human-collected trajectories through an environment, we render novel training data that allows virtual agents to drive along a continuum of new local trajectories consistent with the road appearance and semantics, each with a different view of the scene. We demonstrate the ability of policies learned within our simulator to generalize to and navigate in previously unseen real-world roads, without access to any human control labels during training. Our results validate the learned policy onboard a full-scale autonomous vehicle, including in previously un-encountered scenarios, such as new roads and novel, complex, near-crash situations. Our methods are scalable, leverage reinforcement learning, and apply broadly to situations requiring effective perception and robust operation in the physical world.	Article	APR 2020	10.1109/LRA.2020.2966414	[]	[]	-1	-1	-1
J	Huang, Kaisi; Wen, Mingyun; Park, Jisun; Sung, Yunsick; Park, Jong Hyuk; Cho, Kyungeun	Enhanced Image Preprocessing Method for an Autonomous Vehicle Agent System	2021	COMPUTER SCIENCE AND INFORMATION SYSTEMS		461	479	Excessive training time is a major issue face when training autonomous vehicle agents with neural networks by using images as input. This paper proposes a deep time-economical Q network (DQN) input image preprocessing method to train an autonomous vehicle agent in a virtual environment. The environmental information is extracted from the virtual environment. A top-view image of the entire environment is then redrawn according to the environmental information. During training of the DQN model, the top-view image is cropped to place the vehicle agent at the center of the cropped image. The current frame top-view image is combined with the images from the previous two training iterations. The DQN model use this combined image as input. The experimental results indicate higher performance and shorter training time for the DQN model trained with the preprocessed images compared with that trained without preprocessing.	Article	APR 2021	10.2298/CSIS200212005H	[]	[]	-1	-1	-1
J	Likas, A; Blekas, K	A reinforcement learning approach based on the fuzzy min-max neural network	1996	NEURAL PROCESSING LETTERS		167	172	The fuzzy min-max neural network constitutes a neural architecture that is based on hyperbox fuzzy sets and can be incrementally trained by appropriately adjusting the number of hyperboxes and their corresponding volumes. Two versions have been proposed: for supervised and unsupervised learning. In this paper a modified approach is presented that is appropriate for reinforcement learning problems with discrete action space and is applied to the difficult task of autonomous vehicle navigation when no a priori knowledge of the enivronment is available. Experimental results indicate that the proposed reinforcement learning network exhibits superior learning behavior compared to conventional reinforcement schemes.	Article	DEC 1996	10.1007/BF00426025	[]	[]	-1	-1	-1
J	Li, Nan; Oyler, Dave W.; Zhang, Mengxuan; Yildiz, Yildiray; Kolmanovsky, Ilya; Girard, Anouck R.	Game Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems	2018	IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY		1782	1797	Autonomous driving has been the subject of increased interest in recent years both in industry and in academia. Serious efforts are being pursued to address legal, technical, and logistical problems and make autonomous cars a viable option for everyday transportation. One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience. Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where interactions among multiple drivers and vehicles occur simultaneously. Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help to decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests. In this paper, we present a game theoretic traffic model that can be used to: 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system. We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.	Article	SEP 2018	10.1109/TCST.2017.2723574	[]	[]	-1	-1	-1
B	Liu, Z.; Gao, H.; Ma, H.; Cai, S.; Hu, Y.; Qu, T.; Chen, H.; Gong, X.	Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation	2023	2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)		1609	14	Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.	Conference Paper	2023	10.1109/IROS55552.2023.10341750	[]	[]	-1	-1	-1
J	Kontoravdis, D.; Likas, A.; Stafylopatis, A.	Collision-free movement of an autonomous vehicle using reinforcement learning	1992	ECAI 92. 10th European Conference on Artificial Intelligence Proceedings		666	70	Explores the potential use of reinforcement learning in control applications. Reinforcement learning systems are of particular interest because they require as training feedback only a scalar signal provided to the entire neural network and they admit a simple on-line implementation. The authors demonstrate the ability of a reinforcement learning adaptive controller to drive an autonomous vehicle through simulated paths comprising left and right turns. The neural network is responsible for providing the proper control commands so that the vehicle stays on the road and avoids collision.	Conference Paper	1992		[]	[]	-1	-1	-1
B	Jie Xu; Xiaofei Pei; Kexuan Lv	Decision-Making for Complex Scenario using Safe Reinforcement Learning	2020	Proceedings of the 2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	6	In recent years, machine learning is widely used in many fields. Compared with the rule-based method, machine learning plays a more excellent role in the decision-making of the autonomous vehicle. Some complex situations are often met in our daily life. To this end, Safe reinforcement learning(RL) is introduced to ensure that safer actions are selected. Constant Turn Rate and Acceleration(CTRA) model is first used to predict the future trajectories of surrounding vehicles. Then Double Deep Q-Learning(DDQN) method is used to make decisions and ensure the autonomous vehicle can move at the desired speed as much as possible. In order to achieve a safer decision-making, some safety rules are introduced. Finally, the algorithm is demonstrated in Simulation of Urban Mobility(SUMO) and has been proved to have an outstanding performance on such a complex scenario.	Conference Paper	2020	10.1109/CVCI51460.2020.9338584	[]	[]	-1	-1	-1
J	Hyun Jae Cho; Behl, M.	Towards Automated Safety Coverage and Testing for Autonomous Vehicles with Reinforcement Learning [arXiv]	2020	arXiv		16 pp.	16 pp.	The kind of closed-loop verification likely to be required for autonomous vehicle (AV) safety testing is beyond the reach of traditional test methodologies and discrete verification. Validation puts the autonomous vehicle system to the test in scenarios or situations that the system would likely encounter in everyday driving after its release. These scenarios can either be controlled directly in a physical (closed-course proving ground) or virtual (simulation of predefined scenarios) environment, or they can arise spontaneously during operation in the real world (open-road testing or simulation of randomly generated scenarios). In AV testing, simulation serves primarily two purposes: to assist the development of a robust autonomous vehicle and to test and validate the AV before release. A challenge arises from the sheer number of scenario variations that can be constructed from each of the above sources due to the high number of variables involved (most of which are continuous). Even with continuous variables discretized, the possible number of combinations becomes practically infeasible to test. To overcome this challenge we propose using reinforcement learning (RL) to generate failure examples and unexpected traffic situations for the AV software implementation. Although reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving.	Journal Paper	22 May 2020		[]	[]	-1	-1	-1
J	You, Changxi; Lu, Jianbo; Filev, Dimitar; Tsiotras, Panagiotis	Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning	2019	ROBOTICS AND AUTONOMOUS SYSTEMS		1	18	Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion. They represent the main trend in future intelligent transportation systems. This paper concentrates on the planning problem of autonomous vehicles in traffic. We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MOP) and consider the driving style of an expert driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques. Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning. The unknown reward function of the expert driver is approximated using a deep neural-network (DNN). We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques. (C) 2019 Elsevier B.V. All rights reserved.	Article	APR 2019	10.1016/j.robot.2019.01.003	[]	[]	-1	-1	-1
B	Xia, Y.; Liu, S.; Hu, R.; Yu, Q.; Feng, X.; Zheng, K.; Su, H.	SMART: A Decision-Making Framework with Multi-modality Fusion for Autonomous Driving Based on Reinforcement Learning	2023	Database Systems for Advanced Applications: 28th International Conference, DASFAA 2023, Proceedings. Lecture Notes in Computer Science (13946)		447	62	Decision-making in autonomous driving is an emerging technology that has rapid progress over the last decade. In single-lane scenarios, autonomous vehicles should simultaneously optimize their velocity decisions and steering angle decisions to achieve safety, efficiency, comfort, small impacts on rear vehicles, and small offsets to the lane center line. Previous studies, however, have typically optimized these two decisions separately, ignoring the potential relationship between them. In this work, we propose a decision-making framework, named SMART (deci S ion- M aking fr A mework based on R einforcemen T learning), to optimize the velocity and steering angle of the autonomous vehicle in parallel. In order for the autonomous vehicle to effectively perceive the curvature of the lane and interactions with other vehicles, we adopt a graph attention mechanism to extract and fuse the features from different modalities (i.e., sensor-collected vehicle states and camera-collected lane information). Then a hybrid reward function takes into account aspects of safety, efficiency, comfort, impact, and lane centering to instruct the autonomous vehicle to make optimal decisions. Furthermore, our framework enables the autonomous vehicle to adaptively choose the duration of an action, which helps the autonomous vehicle pursue higher reward values. Extensive experiments evidence that SMART significantly outperforms the existing methods in multiple metrics.	Conference Paper	2023	10.1007/978-3-031-30678-5_33	[]	[]	-1	-1	-1
J	Cabezas-Olivenza, Mireya; Zulueta, Ekaitz; Sanchez-Chica, Ander; Fernandez-Gamiz, Unai; Teso-Fz-Betono, Adrian	Stability Analysis for Autonomous Vehicle Navigation Trained over Deep Deterministic Policy Gradient	2023	MATHEMATICS				The Deep Deterministic Policy Gradient (DDPG) algorithm is a reinforcement learning algorithm that combines Q-learning with a policy. Nevertheless, this algorithm generates failures that are not well understood. Rather than looking for those errors, this study presents a way to evaluate the suitability of the results obtained. Using the purpose of autonomous vehicle navigation, the DDPG algorithm is applied, obtaining an agent capable of generating trajectories. This agent is evaluated in terms of stability through the Lyapunov function, verifying if the proposed navigation objectives are achieved. The reward function of the DDPG is used because it is unknown if the neural networks of the actor and the critic are correctly trained. Two agents are obtained, and a comparison is performed between them in terms of stability, demonstrating that the Lyapunov function can be used as an evaluation method for agents obtained by the DDPG algorithm. Verifying the stability at a fixed future horizon, it is possible to determine whether the obtained agent is valid and can be used as a vehicle controller, so a task-satisfaction assessment can be performed. Furthermore, the proposed analysis is an indication of which parts of the navigation area are insufficient in training terms.	Article	JAN 2023	10.3390/math11010132	[]	[]	-1	-1	-1
J	Chalaki, B.; Beaver, L.; Remer, B.; Jang, K.; Vinitsky, E.; Bayen, A.M.; Malikopoulos, A.A.	Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning [arXiv]	2019	arXiv		8 pp.	8 pp.	In this paper, we demonstrate the first successful zero-shot transfer of an autonomous driving policy directly from simulator to a scaled autonomous vehicle under stochastic disturbances. Using adversarial multi-agent reinforcement learning, in which an adversary perturbed both the inputs and outputs of the autonomous vehicle during training, we train an autonomous vehicle to manage a roundabout merge in the presence of an adversary in simulation. At test time in hardware, the adversarial policy successfully reproduces the simulated ramp metering behavior and outperforms both a human driving baseline and adversary-free trained policies. Finally, we demonstrate that the addition of adversarial training considerably improves the stability and robustness of policies being transferred to the real world. Supplementary information and videos can be found at: (https://sites.google.com/view/ud-ids-lab/arlv).	Journal Paper	12 March 2019		[]	[]	-1	-1	-1
B	Emamifar, M.; Ghoreishi, S.F.	Uncertainty-Aware Reinforcement Learning for Safe Control of Autonomous Vehicles in Signalized Intersections	2023	2023 IEEE Conference on Artificial Intelligence (CAI)		81	2	This paper proposes a reinforcement learning approach for the control of autonomous vehicles at signalized intersections. The proposed method is a modified version of the Q-learning approach that takes into account the risky scenarios that might arise in the control of an autonomous vehicle due to the inherent uncertainties in the system. The proposed algorithm enables robust and risk-aware decision-making in uncertain and sensitive environments. The proposed algorithm is evaluated in a simulated autonomous vehicle scenario, where it outperforms the standard Q-learning in terms of safety.	Conference Paper	2023	10.1109/CAI54212.2023.00042	[]	[]	-1	-1	-1
R	De Abreu, Ricardo	Anti-lock braking systems	2022	Figshare				The data provided summarises the model used to describe the dynamics of the anti-lock braking system. This includes the specifics of the model-free control method. The results of the systems are provided as well. Copyright: CC BY 4.0	Data set	2022-09-01	https://doi.org/10.25403/UPresearchdata.20363601.v1	[]	[]	-1	-1	-1
C	Qiao, Zhiqian; Tyree, Zachariah; Mudalige, Priyantha; Schneider, Jeff; Dolan, John M.	Hierarchical Reinforcement Learning Method for Autonomous Vehicle Behavior Planning	2020	2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	6084	6089	Behavioral decision making is an important aspect of autonomous vehicles (AV). In this work, we propose a behavior planning structure based on hierarchical reinforcement learning (HRL) which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals. In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other tasks with the same sub-goals. A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure. Compared to traditional RL methods, our algorithm is more sample-efficient, since its modular design allows reusing the policies of sub-goals across similar tasks for various transportation scenarios. The results show that the proposed method converges to an optimal policy faster than traditional RL methods.	Proceedings Paper	2020	10.1109/IROS45743.2020.9341496	[]	[]	-1	-1	-1
J	Crewe, Jacob; Humnabadkar, Aditya; Liu, Yonghuai; Ahmed, Amr; Behera, Ardhendu	SLAV-Sim: A Framework for Self-Learning Autonomous Vehicle Simulation	2023	SENSORS				With the advent of autonomous vehicles, sensors and algorithm testing have become crucial parts of the autonomous vehicle development cycle. Having access to real-world sensors and vehicles is a dream for researchers and small-scale original equipment manufacturers (OEMs) due to the software and hardware development life-cycle duration and high costs. Therefore, simulator-based virtual testing has gained traction over the years as the preferred testing method due to its low cost, efficiency, and effectiveness in executing a wide range of testing scenarios. Companies like ANSYS and NVIDIA have come up with robust simulators, and open-source simulators such as CARLA have also populated the market. However, there is a lack of lightweight and simple simulators catering to specific test cases. In this paper, we introduce the SLAV-Sim, a lightweight simulator that specifically trains the behaviour of a self-learning autonomous vehicle. This simulator has been created using the Unity engine and provides an end-to-end virtual testing framework for different reinforcement learning (RL) algorithms in a variety of scenarios using camera sensors and raycasts.	Article	OCT 2023	10.3390/s23208649	[]	[]	-1	-1	-1
B	Liu, S.; Chang, P.; Chen, H.; Chakraborty, N.; Driggs-Campbell, K.	Learning to Navigate Intersections with Unsupervised Driver Trait Inference	2022	2022 International Conference on Robotics and Automation (ICRA)		3576	82	Navigation through uncontrolled intersections is one of the key challenges for autonomous vehicles. Identifying the subtle differences in hidden traits of other drivers can bring significant benefits when navigating in such environments. We propose an unsupervised method for inferring driver traits such as driving styles from observed vehicle trajectories. We use a variational autoencoder with recurrent neural networks to learn a latent representation of traits without any ground truth trait labels. Then, we use this trait representation to learn a policy for an autonomous vehicle to navigate through a T-intersection with deep reinforcement learning. Our pipeline enables the autonomous vehicle to adjust its actions when dealing with drivers of different traits to ensure safety and efficiency. Our method demonstrates promising performance and outperforms state-of-the-art baselines in the T-intersection scenario.	Conference Paper	2022	10.1109/ICRA46639.2022.9811635	[]	[]	-1	-1	-1
C	Ni, Jiayang; Ma, Rubing; Zhong, Hua; Wang, Bo	Autonomous Vehicles Roundup Strategy by Reinforcement Learning with Prediction Trajectory	2022	2022 41ST CHINESE CONTROL CONFERENCE (CCC)	Chinese Control Conference	3370	3375	Autonomous vehicles are increasingly applied on many situations, but their autonomous decision-making ability needs to be improved. Multi-Agent Deep Deterministic Policy Gradient(MADDPG) adopts the method of centralized evaluation and decentralized execution, so that the autonomous vehicle can obtain the whole-field status information and make decisions through the companion information. In the process of autonomous vehicle training, we introduce artificial potential field, action guidance and other methods to alleviate the problem of sparse rewards. At the same time, we add a repulsion function to consider the relationship between team vehicles. Extended Kalman Filter(EKF) is also applied to predict the autonomous vehicle trajectory, changing the training network state input information. At the same time, secondary correction of the predicted autonomous vehicle trajectory is made to change the prediction range with the training time, and improve the training convergence speed while the speed of opposite agents increases. Simulation experiments show that the convergence speed and win rate ofMADDPG algorithm based on trajectory prediction and artificial potential field is significantly improved, and it also has strong adaptability to various task scenarios.	Proceedings Paper	2022		[]	[]	-1	-1	-1
C	Cabaneros, Alex; Angulo, Cecilio	A Comparison of Autonomous Vehicle Navigation Simulators Under Regulatory and Reinforcement Learning Constraints	2019	ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT	Frontiers in Artificial Intelligence and Applications	115	124	The transition from conventional vehicles to autonomous vehicles is regulated thorough ADAS (Advanced Driver Assistance Systems) functionalities. The combination of different ADAS functions allows vehicles navigate on a highway autonomously, but at the same time, following the traffic rules and regulations requirements, and also guaranteeing safety on the road. The practical objective in this article is to implement a Reinforcement Learning method whose actions are based in these regulated functions for autonomous vehicles navigation. With this aim, a study of the state-of-the-art of autonomous vehicles simulators has been completed. Hence, the algorithm will be tested using a five-lane highway simulator, previously selected. Results and performance of the model through experimentation will be presented and evaluated using the simulator for different network architectures.	Proceedings Paper	2019	10.3233/FAIA190114	[]	[]	-1	-1	-1
J	He, Xiangkun; Yang, Haohan; Hu, Zhongxu; Lv, Chen	Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach	2023	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		184	193	Reinforcementlearning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties. Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.	Article	JAN 2023	10.1109/TIV.2022.3165178	[]	[]	-1	-1	-1
B	Sinha, A.; White, D.; Cao, Y.	Deep Reinforcement Learning-based Optimal Time-constrained Intercept Guidance	2024	AIAA SCITECH 2024 Forum		2206	2206	In this work, we design the guidance strategy for an autonomous vehicle to rendezvous with a stationary target at a time specified a priori. We first design the suitable guidance law for the autonomous vehicle to steer it on the desired trajectory towards the target. In particular, our aim is to drive the necessary error variable to zero in an optimal fashion by minimizing a meaningful cost function. In essence, the proposed method provides an optimal transient performance for the autonomous vehicle to rendezvous with the stationary target precisely at the desired time. As the optimal transient behavior depends on the design parameters, especially the controller gain, we leverage the proximal policy optimization in the reinforcement learning framework to obtain the required design parameter. We show through simulations that the proposed technique is energy-efficient when compared to other finite/fixed-time convergent guidance laws.	Conference Paper	2024	10.2514/6.2024-2206	[]	[]	-1	-1	-1
C	Tiong, Teckchai; Saad, Ismail; Teo, Kenneth Tze Kin; bin Lago, Herwansyah	Autonomous Vehicle Driving Path Control with Deep Reinforcement Learning	2023	2023 IEEE 13TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE, CCWC		84	92	Autonomous vehicle (AV) uses the artificial intelligence (AI) technologies to control the vehicle without human intervention. The implementation of AV has the advantages over the human- driven vehicle such as reducing the road traffic deaths that caused by human errors, increasing the traffic efficiency and minimizing the carbon emission to save the environment. The main objective of this paper is to develop an AV that keeps a safe distance while following the lead car and remains at the centerline of road. The proposed Deep Reinforcement Learning (DRL) algorithm for the autonomous driving simulation is Deep Deterministic Policy Gradient (DDPG). In this paper, the DDPG model for the path following control, reward function, actor network and critic network are created. The DDPG agent has been trained until 1650-episode rewards have been received. After the training, the proposed DDPG agent has been simulated to verify the performance. Then, the values of the two hyperparameters, which are mini-batch size and actor learning rate, are tuned to obtain the shortest training time.	Proceedings Paper	2023	10.1109/CCWC57344.2023.10099122	[]	[]	-1	-1	-1
C	Parras, Juan; Zazo, Santiago	ROBUST DEEP REINFORCEMENT LEARNING FOR UNDERWATER NAVIGATION WITH UNKNOWN DISTURBANCES	2021	2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021)		3440	3444	We study an underwater navigation problem, where an Underwater Autonomous Vehicle must reach a target position in the presence of a disturbance that may be unknown. In order to deal with this problem, we make use of Deep Reinforcement Learning tools, and more concretely, we make use of robust control ideas, which allow training an agent in the presence of uncertainty. We propose a robust Proximal Policy Optimization agent and train it using simulations of an underwater medium: this agent shows an excellent performance when facing unknown disturbances, being able to approach the performance of the optimal agent which had an exact knowledge of the underwater disturbance.	Proceedings Paper	2021	10.1109/ICASSP39728.2021.9414937	[]	[]	-1	-1	-1
B	Bautista-Montesano, R.; Galluzzi, R.; Di, X.; Bustamante-Bello, R.	Reinforcement Learning-Based Navigation Approach for a Downscaled Autonomous Vehicle in Simplified Urban Scenarios	2023	2023 International Symposium on Electromobility (ISEM)		1	7	This paper presents the implementation of a reinforcement learning based navigation architecture for autonomous vehicles in urban scenarios. These types of scenarios represent a challenging task due to the presence of dynamic and static road elements. This work validates the use and feasibility of high-level reinforcement learning controllers in the autonomous vehicle software pipeline. Tests are performed using a 1:10 downscaled autonomous prototype on a track with one main and two secondary roads. The platform is equipped with a LIDAR, inertial measurement units, a stereo camera and motor drives for steering and propulsion. Experiments yield favorable outcomes in terms of collision avoidance, lane keeping and navigational comfort.	Conference Paper	2023	10.1109/ISEM59023.2023.10334911	[]	[]	-1	-1	-1
J	Pan, Huihui; Zhang, Chi; Sun, Weichao	Fault-Tolerant Multiplayer Tracking Control for Autonomous Vehicle via Model-Free Adaptive Dynamic Programming	2023	IEEE TRANSACTIONS ON RELIABILITY		1395	1406	This article investigates the completely unknown autonomous vehicle tracking issues with actuator faults through model-free adaptive dynamic programming (MFADP) approaches. Because partial parameters are measured difficultly or inaccurately, the model-based control theories are imperfect for the vehicles. Therefore, the proposed multiplayer optimal control method in this work, which is not necessary to know the prior system knowledge, achieves the purpose of unknown vehicle tracking control via a novel MFADP theory. Besides, the control strategies are robust, which contain adaptive regulators to eliminate the disturbance of the vehicle systems caused by actuator faults, modeling errors, and curvature interference. To reduce the computational burden of control, a single neural network (NN) architecture is constructed with minimal computational cost and fast response speed. In addition, the convergence analysis of the NN structure, the stability and robustness analysis of identification, and the control schemes in this work are supplied. Finally, two driving scenario simulations are shown to prove the effectiveness of the established controller.	Article; Early Access		10.1109/TR.2022.3208467	[]	[]	-1	-1	-1
J	Isele, D.; Nakhaei, A.; Fujimura, K.	Safe Reinforcement Learning on Autonomous Vehicles [arXiv]	2019	arXiv		6 pp.	6 pp.	There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications. Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems. We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle. [IROS 2018].	Journal Paper	27 Sept. 2019		[]	[]	-1	-1	-1
C	Leung, Karen; Veer, Sushant; Schmerling, Edward; Pavone, Marco	Learning Autonomous Vehicle Safety Concepts from Demonstrations	2023	2023 AMERICAN CONTROL CONFERENCE, ACC	Proceedings of the American Control Conference	3193	3200	"Evaluating the safety of an autonomous vehicle (AV) depends on the behavior of surrounding agents which can be heavily influenced by factors such as environmental context and informally-defined driving etiquette. A key challenge is in determining a minimum set of assumptions on what constitutes reasonable foreseeable behaviors of other road users for the development of AV safety models and techniques. In this paper, we propose a data-driven AV safety design methodology that first learns ""reasonable"" behavioral assumptions from data, and then synthesizes an AV safety concept using these learned behavioral assumptions. We borrow techniques from control theory, namely high-order control barrier functions and Hamilton-Jacobi reachability, to provide inductive bias to aid interpretability, verifiability, and tractability of our approach. In our experiments, we learn an AV safety concept using demonstrations collected from a highway traffic-weaving scenario, compare our learned concept to existing baselines, and showcase its efficacy in evaluating real-world driving logs."	Proceedings Paper	2023		[]	[]	-1	-1	-1
B	Wang, Y.; Dai, X.; Wang, K.; Ali, H.; Zhu, F.	Embed Trajectory Imitation in Reinforcement Learning: A Hybrid Method for Autonomous Vehicle Planning	2023	2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)		1	6	Learning-based autonomous vehicle trajectory planning methods have shown excellent performance in a variety of complex traffic scenarios. However, the existing imitation learning (IL) and reinforcement learning (RL) algorithms still have their limitations, such as poor safety and generalizability for IL, and low data efficiency for RL. To leverage their respective advantages and mitigate the limitations, this paper proposes a novel hybrid RL algorithm for autonomous vehicle planning, where IL is embedded in it to guide its exploration with expert knowledge. Different from existing approaches, we use multi-step trajectory prediction instead of behavior cloning as the IL method integrated with online RL. Through such design, we make a further step in the research about how expert demonstration can be helpful to RL. Moreover, we conduct parallel training and testing of the algorithm based on real-world driving data. Experimental results demonstrate that our proposed approach outperforms standalone IL and RL methods, and performs better than RL methods enhanced by behavior cloning.	Conference Paper	2023	10.1109/DTPI59677.2023.10365415	[]	[]	-1	-1	-1
C	Cui, Leilei; Ozbay, Kaan; Jiang, Zhong-Ping	Combined Longitudinal and Lateral Control of Autonomous Vehicles based on Reinforcement Learning	2021	2021 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	1929	1934	In this paper, in order for the autonomous vehicle to keep a desired distance from the preceding vehicle and stay in the lane, a data-driven optimal control approach is proposed. Firstly, the dynamics of the autonomous vehicle is derived. In order to overcome the cutting-edge limitation, a virtual preceding vehicle is defined which is perpendicular to the preceding vehicle. The tracking error is defined as the deviation between the look ahead point of the autonomous vehicle and the virtual preceding vehicle. Then, the error system is derived. Secondly, based on the error system, in order to minimize the cost determined by the tracking error and the energy consumption, the Hamilton-Jacobi-Bellman (HJB) equation is established. A model-based policy iteration technique is proposed to solve the HJB equation. Thirdly, a two-phase data-driven policy iteration algorithm is proposed and implemented by using adaptive dynamic programming (ADP). The efficacy of the proposed data-driven optimal control approach is validated by computer simulations.	Proceedings Paper	2021	10.23919/ACC50511.2021.9483388	[]	[]	-1	-1	-1
J	Kolat, Mate; Becsi, Tamas	Multi-Agent Reinforcement Learning for Highway Platooning	2023	ELECTRONICS				The advent of autonomous vehicles has opened new horizons for transportation efficiency and safety. Platooning, a strategy where vehicles travel closely together in a synchronized manner, holds promise for reducing traffic congestion, lowering fuel consumption, and enhancing overall road safety. This article explores the application of Multi-Agent Reinforcement Learning (MARL) combined with Proximal Policy Optimization (PPO) to optimize autonomous vehicle platooning. We delve into the world of MARL, which empowers vehicles to communicate and collaborate, enabling real-time decision making in complex traffic scenarios. PPO, a cutting-edge reinforcement learning algorithm, ensures stable and efficient training for platooning agents. The synergy between MARL and PPO enables the development of intelligent platooning strategies that adapt dynamically to changing traffic conditions, minimize inter-vehicle gaps, and maximize road capacity. In addition to these insights, this article introduces a cooperative approach to Multi-Agent Reinforcement Learning (MARL), leveraging Proximal Policy Optimization (PPO) to further optimize autonomous vehicle platooning. This cooperative framework enhances the adaptability and efficiency of platooning strategies, marking a significant advancement in the pursuit of intelligent and responsive autonomous vehicle systems.	Article	DEC 2023	10.3390/electronics12244963	[]	[]	-1	-1	-1
J	Quek, Yang Thee; Koh, Li Ling; Koh, Ngiap Tiam; Tso, Wai Ann; Woo, Wai Lok	Deep Q-network implementation for simulated autonomous vehicle control	2021	IET INTELLIGENT TRANSPORT SYSTEMS		875	885	Deep reinforcement learning is poised to be a revolutionised step towards newer possibilities in solving navigation and autonomous vehicle control tasks. Deep Q-network (DQN) is one of the more popular methods of deep reinforcement learning that allows the agent that controls the vehicle to learn through its mistakes based on its actions and interactions with the environment. This paper presents the implementation of DQN to an autonomous self-driving vehicle control in two different simulated environments; first environment is in Python which is a simple 2D environment and then advanced to Unity software separately which is a 3D environment. Based on the scores and pixel inputs, the agent in the vehicle learns and adapts to its surrounding. It develops the best solution strategy to direct itself in the environment where its task is to manoeuvre the vehicle from point to point on a simulated highway scenario. The implemented DQN technique approximates the action value function with convolutional neural network. This evaluates the Q-function for the Q-learning architecture and updates the action value function. This paper shows that DQN is an effective learning method for the agent of an autonomous vehicle. In both simulated environments, the autonomous vehicle gradually learnt the manoeuvre operations and progressively gained the ability to successfully navigate itself and avoid obstacles without prior information of the surrounding.	Article; Early Access		10.1049/itr2.12067	[]	[]	-1	-1	-1
J	Duy Quang Tran; Bae, Sang-Hoon	Proximal Policy Optimization Through a Deep Reinforcement Learning Framework for Multiple Autonomous Vehicles at a Non-Signalized Intersection	2020	APPLIED SCIENCES-BASEL				Advanced deep reinforcement learning shows promise as an approach to addressing continuous control tasks, especially in mixed-autonomy traffic. In this study, we present a deep reinforcement-learning-based model that considers the effectiveness of leading autonomous vehicles in mixed-autonomy traffic at a non-signalized intersection. This model integrates the Flow framework, the simulation of urban mobility simulator, and a reinforcement learning library. We also propose a set of proximal policy optimization hyperparameters to obtain reliable simulation performance. First, the leading autonomous vehicles at the non-signalized intersection are considered with varying autonomous vehicle penetration rates that range from 10% to 100% in 10% increments. Second, the proximal policy optimization hyperparameters are input into the multiple perceptron algorithm for the leading autonomous vehicle experiment. Finally, the superiority of the proposed model is evaluated using all human-driven vehicle and leading human-driven vehicle experiments. We demonstrate that full-autonomy traffic can improve the average speed and delay time by 1.38 times and 2.55 times, respectively, compared with all human-driven vehicle experiments. Our proposed method generates more positive effects when the autonomous vehicle penetration rate increases. Additionally, the leading autonomous vehicle experiment can be used to dissipate the stop-and-go waves at a non-signalized intersection.	Article	AUG 2020	10.3390/app10165722	[]	[]	-1	-1	-1
J	Pei Xiaofei; Mo Shuojie; Chen Zhenfu; Yang Bo	Lane Changing of Autonomous Vehicle Based on TD3Algorithm in Human-machine Hybrid Driving Environment	2021	China Journal of Highway and Transport		246	254	Improving the human acceptance of autonomous vehicles in the future is important,and deep reinforcement learning is a key technology for their acceptance.To solve the lane-changing decision problem in a human-machine hybrid driving traffic flow,this study used the deep reinforcement-learning algorithm Twin Delayed Deep Deterministic Policy Gradient (TD3) to realize the free-lane-changing behavior of an autonomous vehicle.First,the theoretical framework of reinforcement learning based on the Markov decision process was introduced.Then,according to the driving data from actual conditions in the NGSIM dataset,a six-lane moderate traffic-congestion simulation scene was established using the autonomous driving simulator NGSIM-ENV.Other non-autonomous vehicles were controlled according to the recorded data in NGSIM.For lane-changing decision making in a continuous action space,the TD3 algorithm was used to develop a lane-changing model to control the driving behavior of the autonomous vehicle.In the proposed lane-changing model,the state space that contained the selfvehicle and environment information and the action space,which included the vehicle acceleration and heading angle,were established.Simultaneously,a reward function in the reinforcement learning was designed to consider factors such as safety,driving efficiency,and comfort.Finally,in the NGSIM-ENV simulation platform,the lane-changing behavior of the autonomous vehicles based on the TD3 algorithm was compared with that in the driving data of human drivers.The average driving velocity was found to increase by 4.8%,and the driving safety and comfort were improved.The simulation results verify the effectiveness of the lane-changing model after the model training is completed.Furthermore,safe,comfortable,and reasonable lane-changing behavior in a complex traffic environment can be realized.	Article	2021		[]	[]	-1	-1	-1
C	Lee, Jaeyoung; Balakrishnan, Aravind; Gaurav, Ashish; Czarnecki, Krzysztof; Sedwards, Sean	WISEMOVE: A Framework to Investigate Safe Deep Reinforcement Learning for Autonomous Driving	2019	QUANTITATIVE EVALUATION OF SYSTEMS (QEST 2019)	Lecture Notes in Computer Science	350	354	WISEMOVE is a platform to investigate safe deep reinforcement learning (DRL) in the context of motion planning for autonomous driving. It adopts a modular architecture that mirrors our autonomous vehicle software stack and can interleave learned and programmed components. Our initial investigation focuses on a state-of-the-art DRL approach from the literature, to quantify its safety and scalability in simulation, and thus evaluate its potential use on our vehicle.	Proceedings Paper	2019	10.1007/978-3-030-30281-8_20	[]	[]	-1	-1	-1
R	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano	Testing autonomous vehicle lane keeping assist system in BeamNG simulator	2023	Zenodo				In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator. Road topologies were generated by our tool RIGAA (available at:Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: Creative Commons Attribution 4.0 International Open Access	Data set	2023-09-26	https://doi.org/10.5281/ZENODO.8252677	[]	[]	-1	-1	-1
C	Jiang, Zhenyu; Wang, Zhongli; Cui, Xin; Zheng, Chaochao	Intelligent Safety Decision-Making for Autonomous Vehicle in Highway Environment	2021	INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2021, PT IV	Lecture Notes in Artificial Intelligence	702	713	Safe driving policies is the key technology to realize the adaptive cruise control of autonomous vehicle in highway environment. In this paper, the reinforcement learning method is applied to autonomous driving's decision-making. To solve the problem that present reinforcement learning methods are difficult to deal with the randomness and uncertainty in driving environment, a model-free method for analyzing the Lyapunov stability and H infinity performance is applied to Actor-Critic algorithm to improve the stability and robustness of reinforcement learning. The safety of taking an action is judged by setting a safety threshold, thus improving the safety of behavioral decisions. Our method also designs a set of reward functions to better meet the safety and efficiency of driving decisions in the highway environment. The results show that the method can provide safe driving strategies for driverless vehicles in both normal road conditions and environments with unexpected situations, enabling the vehicles to drive safely.	Proceedings Paper	2021	10.1007/978-3-030-89092-6_64	[]	[]	-1	-1	-1
C	Arvind, C. S.; Senthilnath, J.	Autonomous Vehicle for Obstacle Detection and Avoidance Using Reinforcement Learning	2020	SOFT COMPUTING FOR PROBLEM SOLVING, SOCPROS 2018, VOL 1	Advances in Intelligent Systems and Computing	55	66	Obstacle detection and avoidance during navigation of an autonomous vehicle is one of the challenging problems. Different sensors like RGB camera, Radar, and Lidar are presently used to analyze the environment around the vehicle for obstacle detection. Analyzing the environment using supervised learning techniques has proven to be an expensive process due to the training of different obstacle for different scenarios. In order to overcome such difficulty, in this paper Reinforcement Learning (RL) techniques are used to understand the uncertain environment based on sensor information to make the decision. Policy free, model-free Q-learning based RL algorithm with the multilayer perceptron neural network (MLP-NN) is applied and trained to predict optimal vehicle future action based on the current state of the vehicle. Further, the proposed Q-Learning with MLP-NN based approach is compared with the state of the art, namely, Q-learning. A simulated urban area obstacles scenario is considered with the different number of ultrasonic radar sensors in detecting obstacles. The experimental result shows that Q-learning with MLP-NN along with the ultrasonic sensors is proven to be more accurate than conventional Q-learning technique with the ultrasonic sensors. Hence it is demonstrated that combining Qlearning with MLP-NN will improve in predicting obstacles for autonomous vehicle navigation.	Proceedings Paper	2020	10.1007/978-981-15-0035-0_5	[]	[]	-1	-1	-1
B	Vartika, V.; Singh, S.; Das, S.; Mishra, S.K.; Sahu, S.S.	A Review on Intelligent PID Controllers in Autonomous Vehicle	2021	Advances in Smart Grid Automation and Industry 4.0. Select Proceedings of ICETSGAI4.0. Lecture Notes in Electrical Engineering (LNEE 693)		391	9	In recent times, autonomous vehicle is gaining popularity in several of applications ranging from conventional transport system to nuclear reactor. The importance of autonomous vehicle increases manifold for those applications where it is not possible or very much risky for human to reach out. Many Artificial Intelligence techniques have been successfully applied in this field. In this paper, an adequate number of recent machine learning-based techniques to design intelligent PID controllers in autonomous vehicles have been reviewed and suitably compared with some other competitive approaches. Particularly, the parameters tuning ofKp,Ki,Kd as well as the rise time characteristics by two methods, namely Ziegler-Nichols and genetic algorithm (GA), have been discussed. Also, we have compared the properties of rise time and percentage overshoot by conventional PID controllers and fuzzy-based PID controllers.	Conference Paper	2021	10.1007/978-981-15-7675-1_39	[]	[]	-1	-1	-1
S	Kuhnert, KD; KrÃ¶del, M	Autonomous vehicle steering based on evaluative feedback by reinforcement learning	2005	MACHINE LEARNING AND DATA MINING IN PATTERN RECOGNITION, PROCEEDINDS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE	405	414	Steering an autonomous vehicle requires the permanent adaptation of behavior in relation to the various situations the vehicle is in. This paper describes a research which implements such adaptation and optimization based on Reinforcement Learning (RL) which in detail purely learns from evaluative feedback in contrast to instructive feedback. Convergence of the learning process has been achieved at various experimental results revealing the impact of the different RL parameters. While using RL for autonomous steering is in itself already a novelty, additional attention has been given to new proposals for post-processing and interpreting the experimental data.	Article; Proceedings Paper	2005		[]	[]	-1	-1	-1
J	Cheng, Yanqiu; Chen, Chenxi; Hu, Xianbiao; Chen, Kuanmin; Tang, Qing; Song, Yang	Enhancing Mixed Traffic Flow Safety via Connected and Autonomous Vehicle Trajectory Planning with a Reinforcement Learning Approach	2021	JOURNAL OF ADVANCED TRANSPORTATION				The longitudinal trajectory planning of connected and autonomous vehicle (CAV) has been widely studied in the literature to reduce travel time or fuel consumptions. The safety impact of CAV trajectory planning to the mixed traffic flow with both CAV and human-driven vehicle (HDV), however, is not well understood yet. This study presents a reinforcement learning modeling approach, named Monte Carlo tree search-based autonomous vehicle safety algorithm, or MCTS-AVS, to optimize the safety of mixed traffic flow, on a one-lane roadway with signalized intersection control. Crash potential index (CPI) is defined to quantitively measure the safety performance of the mixed traffic flow. The CAV trajectory planning problem is firstly formulated as an optimization model; then, the solution procedure based on reinforcement learning is proposed. The tree-expansion determination module and rollout termination module are developed to identify and reduce the unnecessary tree expansion, so as to train the model more efficiently towards the desired direction. The case study results showed that the proposed algorithm was able to reduce the CPI by 76.56%, when compared with a benchmark model without any intelligence, and 12.08%, when compared with another benchmark model that the team developed earlier. These results demonstrated the satisfactory performance of the proposed algorithm in enhancing the safety of the mixed traffic flow.	Article	JUN 14 2021	10.1155/2021/6117890	[]	[]	-1	-1	-1
J	Jebamikyous, Hrag-Harout; Kashef, Rasha	Autonomous Vehicles Perception (AVP) Using Deep Learning: Modeling, Assessment, and Challenges	2022	IEEE ACCESS		10523	10535	Perception is the fundamental task of any autonomous driving system, which gathers all the necessary information about the surrounding environment of the moving vehicle. The decision-making system takes the perception data as input and makes the optimum decision given that scenario, which maximizes the safety of the passengers. This paper surveyed recent literature on autonomous vehicle perception (AVP) by focusing on two primary tasks: Semantic Segmentation and Object Detection. Both tasks play an important role as a vital component of the vehicle's navigation system. A comprehensive overview of deep learning for perception and its decision-making process based on images and LiDAR point clouds is discussed. We discussed the sensors, benchmark datasets, and simulation tools widely used in semantic segmentation and object detection tasks, especially for autonomous driving. This paper acts as a road map for current and future research in AVP, focusing on models, assessment, and challenges in the field.	Article	2022	10.1109/ACCESS.2022.3144407	[]	[]	-1	-1	-1
C	Mostafizi, Alireza; Siam, Mohammad Rayeedul Kalam; Wang, Haizhong	Autonomous Vehicle Routing Optimization in a Competitive Environment: A Reinforcement Learning Application	2018	INTERNATIONAL CONFERENCE ON TRANSPORTATION AND DEVELOPMENT 2018: CONNECTED AND AUTONOMOUS VEHICLES AND TRANSPORTATION SAFETY		109	118	This paper presents a multiagent approach to identify the shortest path for the intelligent agents (i.e., autonomous vehicles) traveling through the transportation network using a Q-learning algorithm. In addition, this paper discusses how a machine achieves an optimal solution in the case that there are a large number of intelligent agents trying to minimize their travel times simultaneously. The Q-learning algorithm with a reverse order Q matrix is updated to reach the optimal path within a grid network, during which different coefficients have been evaluated and analyzed based on their impact on the algorithm's performance. The reinforcement learning algorithm is then applied to the multiagent system with different market penetration of autonomous vehicle agents. The results revealed that as the percentage of intelligent agents increases, it is more difficult to converge to an optimal solution. A physical interpretation is that when all of the agents are trying to maximize their utility, and if the intelligent agents have conflicted interests with each other, the system does not converge to a global optimum. We found that the final converged total travel time reaches its minimum when 75% of the agents are intelligent autonomous vehicles. The critical threshold lies between 50% and 75%, below which the system performance improves marginally over time.	Proceedings Paper	2018		[]	[]	-1	-1	-1
C	Naruse, K; Leu, MC	Autonomous vehicle navigation by reinforcement learning with problem structure identification	1998	INTELLIGENT AUTONOMOUS SYSTEMS: IAS-5		226	233	This paper presents a mechanism of an efficient reinforcement learning algorithm for autonomous vehicle navigation. The efficiency is achieved by identifying the structure of a given problem, and it is represented as a set of behaviors, which are efficient sequences of actions to solve the problem. Computational simulations are carried out and demonstrate the proposed mechanism successfully.	Proceedings Paper	1998		[]	[]	-1	-1	-1
J	Ayimba, Constantine; Cislaghi, Valerio; Quadri, Christian; Casari, Paolo; Mancuso, Vincenzo	Copy-CAV: V2X-enabled wireless towing for emergency transport	2023	COMPUTER COMMUNICATIONS		87	96	As smart connected vehicles become increasingly common and pave the way for the autonomous vehicles of the future, their ability to provide enhanced safety and assistance services has improved. One such service is the emergency transport of drivers in medical distress: as a positive solution of the distress is typically more likely after timely response, an autonomous vehicle could cut on emergency response times, and thus play a key role in saving the life of its driver.In this paper, we show how such an autonomous emergency transport service can be run from a wireless cellular network, and discuss the importance of having a human in the loop in order to expedite driving. We present a Monte-Carlo-based driver assessment system that the network can use when selecting the most suitable candidate to wirelessly tow an autonomous vehicle with an incapacitated driver. We show that this mechanism results in a selection policy that ensures better cohesion between the vehicles, thereby significantly improving service reliability by reducing the chances of disruptions by intervening traffic.	Article; Early Access		10.1016/j.comcom.2023.04.009	[]	[]	-1	-1	-1
C	Anh Huynh; Ba-Tung Nguyen; Hoai-Thu Nguyen; Sang Vu; Hien Nguyen	A Method of Deep Reinforcement Learning for Simulation of Autonomous Vehicle Control	2021	ENASE: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON EVALUATION OF NOVEL APPROACHES TO SOFTWARE ENGINEERING		372	379	Nowadays autonomous driving is expected to revolutionize the transportation sector. Carmakers, researchers, and administrators have been working on this field for years and significant progress has been made. However, the doubts and challenges to overcome are still huge, regarding not only complex technologies but also human awareness, culture, current traffic infrastructure. In terms of technical perspective, the accurate detection of obstacles, avoiding adjacent obstacles, and automatic navigation through the environment are some of the difficult problems. In this paper, an approach for solving those problems is proposed by using of Policy Gradient to control a simulated car via reinforcement learning. The proposed method is worked effectively to train an agent to control the simulated car in Unity ML-agents Highway, which is a simulating environment. This environment is chosen from some criteria of an environment simulating autonomous vehicle. The testing of the proposed method got positive results. Beside the average speed was w ell, the agent successfully learned the turning operation, progressively gaining the ability to navigate larger sections of the simulated raceway without crashing.	Proceedings Paper	2021	10.5220/0010478903720379	[]	[]	-1	-1	-1
C	Zarrouki, Baha; Kloes, Verena; Heppner, Nikolas; Schwan, Simon; Ritschel, Robert; Vosswinkel, Rick	Weights-varying MPC for Autonomous Vehicle Guidance: a Deep Reinforcement Learning Approach	2021	2021 EUROPEAN CONTROL CONFERENCE (ECC)		119	125	Model Predictive Control (MPC) can achieve excellent results for complex control tasks like path-following of autonomous vehicles. However, its performance depends on the right choice of a cost function for its internal optimization problem. Optimizing the cost function to different objectives is challenging and time-consuming. In this paper, we propose to automatically learn context-dependent optimal weights for the cost function with Deep Reinforcement Learning and to adapt the weights online. We show that our approach outperforms the results of a human expert.	Proceedings Paper	2021		[]	[]	-1	-1	-1
B	Priisalu, M.; Paduraru, C.; Smichisescu, C.	Varied Realistic Autonomous Vehicle Collision Scenario Generation	2023	Image Analysis: 23rd Scandinavian Conference, SCIA 2023, Proceedings. Lecture Notes in Computer Science (13886)		354	72	Recently there has been an increase in the number of available autonomous vehicle (AV) models. To evaluate and compare the safety of the various models the AVs need to be tested in several diverse safety-critical scenarios. We propose the Adversarial Test Case Generator (ATCG) that differently from previous test case generators allows for the generation of realistic collision scenarios with varied AV and pedestrian behaviour models, on varied scenes and with varied traffic density. Given a top-view image and the semantic segmentation of a traffic scene, the ATCG learns to place multiple AVs and goal-reaching pedestrians in the scene such that collisions occur. Pedestrians in previous multi-agent traffic scenario generation works are confined to unrealistic behaviours such as seeking collisions with the AV or ignoring the AV. Although such scenarios with multiple suicidal pedestrians are collision prone it is unlikely in reality that all pedestrians act abnormally. In realistic collision scenarios the generated pedestrians' behaviours must resemble real pedestrians. The ATCG is a team of Reinforcement Learning (RL) agents and can be easily extended with additional RL agents to produce more complex scenes allowing for advanced AVs to be tested.	Conference Paper	2023	10.1007/978-3-031-31438-4_24	[]	[]	-1	-1	-1
C	KrÃ¶del, M; Kuhnert, KD	Optimising situation-based behaviour of autonomous vehicles	2004	2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM		380	385	Reinforcement Learning (RL) is a method which provides true learning capabilities regarding situation-based actions. RL-systems explore and self-optimise actions for situations in a defined environment. This paper describes the research of a driver (assistance) system based on pure Reinforcement Learning in the framework of an autonomous vehicle. The target of this research is to determine to what extend RL-based Systems serve as an enhancement or even an alternative to classical concepts of autonomous intelligent vehicles such as modelling or neural nets.	Proceedings Paper	2004		[]	[]	-1	-1	-1
B	Shuojie Mo; Xiaofei Pei; Zhenfu Chen	Decision-Making for Oncoming Traffic Overtaking Scenario using Double DQN	2019	2019 3rd Conference on Vehicle Control and Intelligence (CVCI)		4 pp.	4 pp.	Great progress has been made in the field of machine learning in recent years. And learning-based methods have been widely utilized for developing highly autonomous vehicle. To this end, we introduce a reinforcement learning based intelligent autonomous vehicle decision making method for oncoming overtaking scenario. The goal of reinforcement learning is to learn how to take optimal decision in corresponding observations through interactions with the environment using a reward function to estimate whether the decision is good or not. A Double Deep Q-learning (Double DQN) agent was used to learn policies (control strategies) for both longitudinal speed and lane change decision. Prioritized Experience Replay (PER) was used to accelerate convergence of the policies. A two-way 3-car scenario with oncoming traffic was established in SUMO (Simulation of Urban Mobility) to train and test the policies.	Conference Paper	2019	10.1109/CVCI47823.2019.8951626	[]	[]	-1	-1	-1
B	Liu, S.; Xia, Y.; Chen, X.; Xie, J.; Su, H.; Zheng, K.	Impact-aware Maneuver Decision with Enhanced Perception for Autonomous Vehicle	2023	2023 IEEE 39th International Conference on Data Engineering (ICDE)		3255	68	Autonomous driving is an emerging technology that has developed rapidly over the last decade. There have been numerous interdisciplinary challenges imposed on the current transportation system by autonomous vehicles. In this paper, we conduct an algorithmic study on the autonomous vehicle decision-making process, which is a fundamental problem in the vehicle automation field and the root cause of most traffic congestion. We propose a perception-and-decision framework, called HEAD, which consists of an enHanced pErception module and a mAneuver Decision module. HEAD aims to enable the autonomous vehicle to perform safe, efficient, and comfortable maneuvers with minimal impact on other vehicles. In the enhanced perception module, a graph-based state prediction model with a strategy of phantom vehicle construction is proposed to predict the one-step future states for multiple surrounding vehicles in parallel, which deals with sensor limitations such as limited detection range and poor detection accuracy under occlusions. Then in the maneuver decision module, a deep reinforcement learning-based model is designed to learn a policy for the autonomous vehicle to perform maneuvers in continuous action space w.r.t. a parameterized action Markov decision process. A hybrid reward function takes into account aspects of safety, efficiency, comfort, and impact to guide the autonomous vehicle to make optimal maneuver decisions. Extensive experiments offer evidence that HEAD can advance the state of the art in terms of both macroscopic and microscopic effectiveness.	Conference Paper	2023	10.1109/ICDE55515.2023.00250	[]	[]	-1	-1	-1
J	Rasouli, A.; Goebel, R.; Taylor, M.E.; Kotseruba, I.; Alizadeh, S.; Yang, T.; Alban, M.; Shkurti, F.; Zhuang, Y.; Scibior, A.; Rezaee, K.; Garg, A.; Meger, D.; Luo, J.; Paull, L.; Zhang, W.; Wang, X.; Chen, X.	NeurIPS 2022 Competition: Driving SMARTS [arXiv]	2022	arXiv		10 pp.	10 pp.	Driving SMARTS is a regular competition designed to tackle problems caused by the distribution shift in dynamic interaction contexts that are prevalent in real-world autonomous driving (AD). The proposed competition supports methodologically diverse solutions, such as reinforcement learning (RL) and offline learning methods, trained on a combination of naturalistic AD data and open-source simulation platform SMARTS. The two-track structure allows focusing on different aspects of the distribution shift. Track 1 is open to any method and will give ML researchers with different backgrounds an opportunity to solve a real-world autonomous driving challenge. Track 2 is designed for strictly offline learning methods. Therefore, direct comparisons can be made between different methods with the aim to identify new promising research directions. The proposed setup consists of 1) realistic traffic generated using real-world data and micro simulators to ensure fidelity of the scenarios, 2) framework accommodating diverse methods for solving the problem, and 3) baseline method. As such it provides a unique opportunity for the principled investigation into various aspects of autonomous vehicle deployment.	Journal Paper	14 Nov. 2022		[]	[]	-1	-1	-1
C	Kim, Yujin; Pae, Dong-Sung; Jang, Sun-Ho; Kang, Seong-Woo; Lim, Myo-Taeg	Reinforcement Learning for Autonomous Vehicle using MPC in Highway Situation	2022	2022 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION (ICEIC)				Path planning for Autonomous Vehicle(AV) is a challenging problem, as the vehicle is required to obey the traffic rules while avoiding the collision with the other vehicles. Model Predictive Control(MPC) is one of the popular approach for proposing a feasible and stable path by reflecting vehicle dynamics in solving objective function and constraining the expected future control input. However, one of the drawbacks with this approach is that the demanded computational power increases proportionally to the number of considered future inputs. This paper presents a path planning algorithm using Reinforcement Learning(RL). RL is similar to MPC in finding the optimal solution that maximizes the reward function which can be seen as intrinsic objective function. In that respect, adequate employment of MPC path in training resulted in improved efficiency and performance. Through the simulations, proposed method showed 98% of similarity with path of MPC and reduced computation time by 91.13% on average, thus it is qualified for real-time path planning.	Proceedings Paper	2022	10.1109/ICEIC54506.2022.9748810	[]	[]	-1	-1	-1
J	Peixoto, Maria J. P.; Azim, Akramul	Improving environmental awareness for autonomous vehicles	2023	APPLIED INTELLIGENCE		1842	1854	Autonomous vehicles (AVs) have multiple tasks with different priorities and safety levels where classic supervised learning techniques are no longer applicable. Thus, reinforcement learning (RL) algorithms become increasingly appropriate for this domain as the RL algorithms can act on complex problems and adapt their responses in the face of unforeseen situations and environments. The RL agent aims to perform the action that guarantees the optimal reward with the best score. The problem with this approach is if the agent finds a possible optimal action with a reasonable premium and gets stuck in this mediocre strategy, which at the same time is neither the best nor the worst solution. Therefore, the agent avoids performing a more extensive exploration to find new paths and learn alternatives to generate a higher reward. To alleviate this problem, we research the behavior of two types of noise in AVs training. We analyze the results and point out the noise method that most stimulates exploration. A vast exploration of the environment is highly relevant to AVs because they know more about the environment and learn alternative ways of acting in the face of uncertainties. With that, AVs can expect more reliable actions in front of sudden changes in the environment. According to our experiments' results in a simulator, we can see that noise allows the autonomous vehicle to improve its exploration and increase the reward.	Article; Early Access		10.1007/s10489-022-03468-6	[]	[]	-1	-1	-1
J	Liu, Z.; Gao, H.; Ma, H.; Cai, S.; Hu, Y.; Qu, T.; Chen, H.; Gong, X.	Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation [arXiv]	2023	Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation [arXiv]				Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.	Preprint	2023		[]	[]	-1	-1	-1
C	Soleimanitaleb, Zahra; Keyvanrad, Mohammad Ali; Jafari, Ali	Object Tracking Methods:A Review	2019	2019 9TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE 2019)		282	288	Object tracking is one of the most important tasks in computer vision that has many practical applications such as traffic monitoring, robotics, autonomous vehicle tracking, and so on. Different researches have been done in recent years, but because of different challenges such as occlusion, illumination variations, fast motion, etc. researches in this area continues. In this paper, various methods of tracking objects are examined and a comprehensive classification is presented that classified tracking methods into four main categories of feature-based, segmentation-based, estimation-based, and learning-based methods that each of which has its own sub-categories. The main focus of this paper is on learning-based methods, which are classified into three categories of generative methods, discriminative methods, and reinforcement learning. One of the sub-categories of the discriminative model is deep learning. Because of high-performance, deep learning has recently been very much considered.yyyyyy	Proceedings Paper	2019	10.1109/iccke48569.2019.8964761	[]	[]	-1	-1	-1
J	Zhang, Kun; Su, Rong; Zhang, Huaguang; Tian, Yunlin	Adaptive Resilient Event-Triggered Control Design of Autonomous Vehicles With an Iterative Single Critic Learning Framework	2021	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS		5502	5511	This article investigates the adaptive resilient event-triggered control for rear-wheel-drive autonomous (RWDA) vehicles based on an iterative single critic learning framework, which can effectively balance the frequency/changes in adjusting the vehicle's control during the running process. According to the kinematic equation of RWDA vehicles and the desired trajectory, the tracking error system during the autonomous driving process is first built, where the denial-of-service (DoS) attacking signals are injected into the networked communication and transmission. Combining the event-triggered sampling mechanism and iterative single critic learning framework, a new event-triggered condition is developed for the adaptive resilient control algorithm, and the novel utility function design is considered for driving the autonomous vehicle, where the control input can be guaranteed into an applicable saturated bound. Finally, we apply the new adaptive resilient control scheme to a case of driving the RWDA vehicles, and the simulation results illustrate the effectiveness and practicality successfully.	Article	DEC 2021	10.1109/TNNLS.2021.3053269	[]	[]	-1	-1	-1
B	Deb Raha, A.; Shirajum Munir, M.; Adhikary, A.; Qiao, Y.; Park, S.-B.; Seon Hong, C.	An Artificial Intelligent-Driven Semantic Communication Framework for Connected Autonomous Vehicular Network	2023	2023 International Conference on Information Networking (ICOIN)		352	7	Semantic communication will considerably enhance transmission efficiency by exploring and only transmitting semantic information. However, most of the previous work in this field is limited to particular applications such as text, audio, or images and does not consider task-oriented communications, where the effectiveness of the transmitted information must be taken into account for completing a specific task. This paper focuses on developing a semantic communication framework for a high altitude platform (HAP)-supported fully connected autonomous vehicle network. A system model is proposed in which the traffic infrastructure (TI) transmits its semantic information to the macro base station (MBS) whenever it observes a connected and autonomous vehicle (CAV). The semantic information has been extracted using a convolutional autoencoder (CAE) as the encoder of CAE gives a smaller representation of the input data. Then, after receiving the semantic concept, the MBS decides on an appropriate action for the CAVs. A proximal policy optimization (PPO) algorithm in the MBS for interpreting and making a decision for the semantic concepts. Simulation results show that the proposed method can reduce up to 63.26% of the communication cost.	Conference Paper	2023	10.1109/ICOIN56518.2023.10049005	[]	[]	-1	-1	-1
J	Cai, Yingfeng; Zhou, Rong; Wang, Hai; Sun, Xiaoqiang; Chen, Long; Li, Yicheng; Liu, Qingchao; He, Youguo	Rule-constrained reinforcement learning control for autonomous vehicle left turn at unsignalized intersection	2023	IET INTELLIGENT TRANSPORT SYSTEMS		2143	2153	Controlling an autonomous vehicle's unprotected left turn at an intersection is a challenging task. Traditional rule-based autonomous driving decision and control algorithms struggle to construct accurate and trustworthy mathematical models for such circumstances, owing to their considerable uncertainty and unpredictability. To overcome this problem, a rule-constrained reinforcement learning (RCRL) control method is proposed in this work for autonomous driving. To train a reinforcement learning controller with rule constraints, outcomes of the path planning module are used as a goal condition in the reinforcement learning framework. Since they include vehicle dynamics, the proposed approach is safer and more reliable compared to end-to-end learning, thereby ensuring that the generated trajectories are locally optimal while adjusting to unpredictable situations. In the experiments, a highly randomized two-way four-lane intersection is established based on the CARLA simulator to verify the effectiveness of the proposed RCRL control method. Accordingly, the results show that the proposed method can provide real-time safe planning and ensure high passing efficiency for autonomous vehicles in the unprotected left turn task.	Article; Early Access		10.1049/itr2.12336	[]	[]	-1	-1	-1
B	Capasso, A.P.; Maramotti, P.; Dell'Eva, A.; Broggi, A.	End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning	2021	2021 IEEE Intelligent Vehicles Symposium (IV)		443	50	Navigating through intersections is one of the main challenging tasks for an autonomous vehicle. However, for the majority of intersections regulated by traffic lights, the problem could be solved by a simple rule-based method in which the autonomous vehicle behavior is closely related to the traffic light states. In this work, we focus on the implementation of a system able to navigate through intersections where only traffic signs are provided. We propose a multi-agent system using a continuous, model-free Deep Reinforcement Learning algorithm used to train a neural network for predicting both the acceleration and the steering angle at each time step. We demonstrate that agents learn both the basic rules needed to handle intersections by understanding the priorities of other learners inside the environment, and to drive safely along their paths. Moreover, a comparison between our system and a rule-based method proves that our model achieves better results especially with dense traffic conditions. Finally, we test our system on real world scenarios using real recorded traffic data, proving that our module is able to generalize both to unseen environments and to different traffic conditions.	Conference Paper	2021	10.1109/IV48863.2021.9575135	[]	[]	-1	-1	-1
B	Zhang, C.; Pan, H.; Sun, W.	Dual-Loop Adaptive Dynamic Programming for Autonomous Vehicle Trajectory Following Control Against Actuator Faults	2022	Advances in Applied Nonlinear Dynamics, Vibration and Control -2021: The proceedings of 2021 International Conference on Applied Nonlinear Dynamics, Vibration and Control (ICANDVC2021). Lecture Notes in Electrical Engineering (799)		199	211	This article presents a novel control strategy based on dual-loop adaptive dynamic programming (ADP) to optimize the tracking performance and ensure the security of autonomous vehicles when actuator faults occur. The proposed dual-loop ADP of controller, composed of two online policy iteration algorithms and an adaptive observer for fault-tolerant control (FTC), guarantees the online tracking accuracy of underactuated systems with uncertain faults, which is difficult to control by single ADP. In addition to actuator faults, the proposed controller can also address the external disturbances such as icy roads and uncertainty of wheel cornering stiffness, which means the controller possesses better robustness. The autonomous vehicle is simulated in multiple driving scenarios with different types of faults, demonstrating the effectiveness of the control strategy.	Conference Paper	2022	10.1007/978-981-16-5912-6_15	[]	[]	-1	-1	-1
C	Wang, Yongshuai; Zheng, Chen; Sun, Mingwei; Chen, Zengqiang; Sun, Qinglin	Reinforcement-learning-aided adaptive control for autonomous driving with combined lateral and longitudinal dynamics	2023	2023 IEEE 12TH DATA DRIVEN CONTROL AND LEARNING SYSTEMS CONFERENCE, DDCLS	Data Driven Control and Learning Systems	840	845	This paper presents a deep reinforcement learning-aided controller for a 3-DOF autonomous vehicle with combined lateral and longitudinal dynamics. In this scheme, the active disturbance rejection control (ADRC) gives full play to its advantages of being model-free and being able to estimate and compensate for internal uncertainties and external disturbances in real-time, and deep deterministic policy gradient (DDPG) fully considers safety, comfort, economy, and combines driving demand with state, action, reward to achieve real-time adaptive adjustment of control parameters. Thus, the adaptive controller can better deal with uncertainties from modeling, parameters, and driving environment, and self-learning and adaptation ability is obtained simultaneously. Moreover, simulation results illustrate that the adaptive controller performs satisfactorily for different driving operations and environments due to the online tuning and optimization of control parameters.	Proceedings Paper	2023	10.1109/DDCLS58216.2023.10166569	[]	[]	-1	-1	-1
C	Zhang, Songan; Peng, Huei; Nageshrao, Subramanya; Tseng, H. Eric	Generating Socially Acceptable Perturbations for Efficient Evaluation of Autonomous Vehicles	2020	2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW 2020)	IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	1341	1347	Deep reinforcement learning methods have been considered and implemented for autonomous vehicle's decision-making in recent years. A key issue is that deep neural networks can be fragile to adversarial attacks through unseen inputs, and thus the reinforcement learning policy, that uses deep neural network would be also fragile to malicious attacks or benign but out of distribution perturbations. In this paper, we address the latter issue: we focus on generating socially acceptable perturbations (SAP), so that the autonomous vehicle (AV agent under evaluation), instead of the challenging vehicle (challenger), is primarily responsible for the crash. In our process, one challenger is added to the environment and trained by deep reinforcement learning to generate the desired perturbation. The reward is designed so that the challenger aims to fail the AV agent in a socially acceptable way. After training the challenger, the AV agent policy is evaluated in both the original naturalistic environment and the environment with one challenger. The results show that the AV agent policy which is safe in the naturalistic environment has many crashes in the perturbed environment.	Proceedings Paper	2020	10.1109/CVPRW50498.2020.00173	[]	[]	-1	-1	-1
J	Kundu, M.; Kumar, D.J.N.	Route Optimization of Unmanned Aerial Vehicle by using Reinforcement Learning	2021	Journal of Physics: Conference Series		012076 (6 pp.)	012076 (6 pp.)	The study proposes the machine learning based algorithm for autonomous vehicle. The dynamic characteristic of unmanned aerial vehicle and real time disturbances such as wind current, obstacles are considered. The novelty of the work lies in the introduction of reinforcement learning for achieving optimized path which can be followed by the unmanned aerial vehicles to complete the tour from initial to destination point. The feasible optimal route may be found by incorporating the L algorithm with a reasonable optimality and computational cost when map and current field data are given.	Conference Paper; Journal Paper	2021	10.1088/1742-6596/1921/1/012076	[]	[]	-1	-1	-1
C	Pal, Mayank K.; Bhati, Rupali; Sharma, Anil; Kaul, Sanjit K.; Anand, Saket; Sujit, P. B.	A Reinforcement Learning Approach to Jointly Adapt Vehicular Communications and Planning for Optimized Driving	2018	2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	3287	3293	Our premise is that autonomous vehicles must optimize communications and motion planning jointly. Specifically, a vehicle must adapt its motion plan staying cognizant of communications rate related constraints and adapt the use of communications while being cognizant of motion planning related restrictions that may be imposed by the on-road environment. To this end, we formulate a reinforcement learning problem wherein an autonomous vehicle jointly chooses (a) a motion planning action that executes on-road and (hi a communications action of querying sensed information from the infrastructure. The goal is to optimize the driving utility of the autonomous vehicle. We apply the Q-learning algorithm to make the vehicle learn the optimal policy, which makes the optimal choice of planning and communications actions at any given time. We demonstrate the ability of the optimal policy to smartly adapt communications and planning actions, while achieving large driving utilities, using simulations.	Proceedings Paper	2018		[]	[]	-1	-1	-1
B	Saqib, N.; Yousuf, M.M.	Design and Implementation of Shortest Path Line Follower Autonomous Rover Using Decision Making Algorithms	2021	2021 Asian Conference on Innovation in Technology (ASIANCON)		6 pp.	6 pp.	"For an autonomous rover, navigating through an unknown environment achieving the target location is the key feature of the rover. Also, path finding is the main problem in the navigation of autonomous vehicles. This is generally done via a dynamic control paradigm, which requires local translation from defined states to actions. Reinforcement learning can be used to develop a control strategy that has learning capabilities in an uncertain environment. The result analysis shows that an autonomous vehicle that is trained by reinforcement can roam freely in a useful or ""reasonably"" manner, and therefore its training process is increased significantly."	Conference Paper	2021	10.1109/ASIANCON51346.2021.9544672	[]	[]	-1	-1	-1
B	Gotad, Siddhesh Vivek	Application of Neural Networks for Design and Development of Low-cost Autonomous Vehicles	2019						Dissertation/Thesis	Jan 01 2019		[]	[]	-1	-1	-1
C	Chen, Jun; Meng, Xiangyu; Li, Zhaojian	Reinforcement Learning-based Event-Triggered Model Predictive Control for Autonomous Vehicle Path Following	2022	2022 AMERICAN CONTROL CONFERENCE (ACC)		3342	3347	Event-triggered model predictive control (MPC) has been proposed in literature to alleviate the high computational requirement of MPC. Compared to conventional time-triggered MPC, event-triggered MPC solves the optimal control problem only when an event is triggered. Several event-trigger policies have been studied in literature, typically requiring a prior knowledge of the MPC closed-loop system behavior. This paper addresses such limitation by investigating the use of model-free reinforcement learning (RL) to trigger MPC. Specifically, the optimal event-trigger policy is learnt by an RL agent through interactions with the MPC closed-loop system, whose dynamical behavior is assumed to be unknown to the RL agent. A reward function is defined to balance the closed-loop control performance and event frequency. As an illustrative example, the autonomous vehicle path following problem is used to demonstrate the applicability of using RL to learn and execute trigger policy for event-triggered MPC.	Proceedings Paper	2022		[]	[]	-1	-1	-1
J	Garcia Cuenca, Laura; Puertas, Enrique; Fernandez Andres, Javier; Aliane, Nourdine	Autonomous Driving in Roundabout Maneuvers Using Reinforcement Learning with Q-Learning	2019	ELECTRONICS				Navigating roundabouts is a complex driving scenario for both manual and autonomous vehicles. This paper proposes an approach based on the use of the Q-learning algorithm to train an autonomous vehicle agent to learn how to appropriately navigate roundabouts. The proposed learning algorithm is implemented using the CARLA simulation environment. Several simulations are performed to train the algorithm in two scenarios: navigating a roundabout with and without surrounding traffic. The results illustrate that the Q-learning-algorithm-based vehicle agent is able to learn smooth and efficient driving to perform maneuvers within roundabouts.	Article	DEC 2019	10.3390/electronics8121536	[]	[]	-1	-1	-1
C	Balas, Cristian; Karlsen, Robert; Muench, Paul; Mikulski, Dariusz; Mohammed, Utayba; Al-Holou, Nizar	Collective trust estimation in multi-agent systems	2019	UNMANNED SYSTEMS TECHNOLOGY XXI	Proceedings of SPIE			In previous work, a multi-layered neural network trust model, dubbed NeuroTrust, was introduced. This trust model was also implemented in an autonomous vehicles convoy simulation, in which speed and gap distance depended on trust. It has been shown that, in time, through on-line reinforcement learning, this trust model produces better results for significant performance metrics in the respective autonomous vehicle convoy when compared to a baseline trust algorithm. In this paper, the NeuroTrust model is expanded to leverage the experience of multiple decision-making agents. A trust aggregation method is proposed for NeuroTrust and is simulated for multiple autonomous vehicle convoy scenarios. It is shown that the NeuroTrust model tends to optimize faster by leveraging each agent's experience.	Proceedings Paper	2019	10.1117/12.2518751	[]	[]	-1	-1	-1
B	Harsha gullapalli, V.S.; M, P.	Machine Learning for Vehicle Behavioural Control	2022	2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)		1453	8	With the advancement of Artificial Intelligence, a revolution in the automotive field has taken place in last few decades. With the development of Advanced Driver Assistance Systems (ADAS), various sensors were available, which paved theway for fully- autonomous vehicles. The first autonomous car has appeared in 1980's, since then a lot of research has taken place in order to develop an autonomous vehicle, which can drive the passengers all by itself without any human-driver intervention. This project work is aimed at developing a simulation model of an Autonomous Vehicle. The so developed Autonomous Vehicle is able to navigate through a pre-defined path avoiding obstacles, walkers, and other vehicles autonomously without any human control. For attaining this driving autonomy, a Reinforcement Learning Algorithm was developed and trained using Deep Q- Learning technique and in-loop-simulations on CARLA-simulator. Upon training the agent for 20,000 episodes of simulation, it is overt from simulation results that the learning accuracy of the agent aggrandized, the average reward nabbed by the agent amplified, the agent's training loss subsided, which reinforce that the agent started to acquire more positive average-reward than the negative average-reward. At the 17,150th, episode of training, the agent concomitantly set forth peak learning accuracy, diminutive training loss and best average-reward. This tacit validate that the agent is adept in maneuvering without colliding with other vehicles, walkers, and with no lane invasions, autonomously from its experience earned from episodic in-loop-simulations.	Conference Paper	2022	10.1109/ICACCS54159.2022.9785076	[]	[]	-1	-1	-1
C	Isele, David; Nakhaei, Alireza; Fujimura, Kikuo	Safe Reinforcement Learning on Autonomous Vehicles	2018	2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	6162	6167	There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications. Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems. We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle.	Proceedings Paper	2018		[]	[]	-1	-1	-1
R	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano	Testing autonomous vehicle lane keeping assist system in BeamNG simulator	2023	Zenodo				In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator. Road topologies were generated by our tool RIGAA (available at:Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: Creative Commons Attribution 4.0 International Open Access	Data set	2023-09-26	https://doi.org/10.5281/ZENODO.8252678	[]	[]	-1	-1	-1
J	KIM, Hyun; Seonhyung, YOOï¼Œ; Jinwoo, LEEï¼Œ; Beomyeol, BAEKï¼Œ; Junghee, SHINï¼Œ	Real-time Dynamic Route Generation Algorithm for Demand-responsive Driverless Transit Operation (DRDTO) Applied to Corridors to Consider U-Turns	2022	Korean Society of Transportation		260	276	Recently, autonomous vehicle is receiving much attention in various sectors including transportation and public transportation. This study presents an adaptive routing algorithm for the real-time demand response service of autonomous transit vehicles. The experimental setting includes the real-time demand occurring randomly over time within the study area. Each demand (service call) request the transit service from designated origin to destination. The routing algorithm is designed to make U-turn and skip stops to improve service and reduce the wait time of users. This study adapts Reinforcement Learning, one of the machine learning techniques, namely reinforcement learning, which can precede a complex calculation process in an offline process. Simulation experiments conducted on a testbed of the Chungju campus of the Korea National University of Transportation. The simulation results show that the proposed routing algorithm can improve the adaptive transit service over the fixed operation in selected performance indicators.	research-article	2022		[]	[]	-1	-1	-1
J	Stafylopatis, A; Blekas, K	Autonomous vehicle navigation using evolutionary reinforcement learning	1998	EUROPEAN JOURNAL OF OPERATIONAL RESEARCH		306	318	Reinforcement learning schemes perform direct on-line search in control space. This makes them appropriate for modifying control rules to obtain improvements in the performance of a system. The effectiveness of a reinforcement learning strategy is studied here through the training of a learning classifier system (LCS) that controls the movement of an autonomous vehicle in simulated paths including left and right turns. The LCS comprises a set of condition-action rules (classifiers) that compete to control the system and evolve by means of a genetic algorithm (GA). Evolution and operation of classifiers depend upon an appropriate credit assignment mechanism based on reinforcement learning. Different design options and the role of various parameters have been investigated experimentally. The performance of vehicle movement under the proposed evolutionary approach is superior compared with that of other (neural) approaches based on reinforcement learning that have been applied previously to the same benchmark problem. (C) 1998 Elsevier Science B.V.	Article; Proceedings Paper	JUL 16 1998	10.1016/S0377-2217(97)00372-X	[]	[]	-1	-1	-1
C	Lee, Minkyung; Hong, Choong Seon	Service Chaining Offloading Decision in the EdgeAI: A Deep Reinforcement Learning Approach	2020	APNOMS 2020: 2020 21ST ASIA-PACIFIC NETWORK OPERATIONS AND MANAGEMENT SYMPOSIUM (APNOMS)	Asia-Pacific Network Operations and Management Symposium-APNOMS	393	396	Many mission critical devices are increasing with upcoming 56 network to fulfill a low latency for a real time network service on smart factory, autonomous vehicle, etc. Distributed cloud computing system also has a key role to execute the various mobile devices, because, an edge computing is the nearest from the mobile devices to provide low latency and computation energy consumption. In this paper, we consider the autonomous vehicles with video live streaming services. Especially, the vehicles require a low transmission delay as within 10 ms. To reduce a latency with low energy consumption, we propose a service chaining offloading decision with a deep reinforcement learning. We split tasks of the vehicle per service function blocks which have their own role. So it can do partial offloading and user association in a On-Device Edge of the vehicle and in the SBS at the same time. We can get results that service chaining offloading decision gives more optimal energy consumption with low-latency to autonomous vehicle users.	Proceedings Paper	2020	10.23919/apnoms50412.2020.9237048	[]	[]	-1	-1	-1
C	Kolomanski, Michal; Sakhai, Mustafa; Nowak, Jakub; Wielgosz, Maciej	Towards End-to-End Chase in Urban Autonomous Driving Using Reinforcement Learning	2023	INTELLIGENT SYSTEMS AND APPLICATIONS, VOL 3	Lecture Notes in Networks and Systems	408	426	This paper addresses the challenging task of developing an autonomous chase protocol. First, training of an autonomous vehicle capable of driving autonomously from point A to B was developed to proceed with a chase protocol as a second step. A dedicated driving setup, based on a discrete action space and a single RGB camera, was developed through a series of experiments. A dedicated curriculum learning agenda allowed to train the model capable of performing all fundamental road maneuvers. Several reward functions were proposed, which enabled effective training of the agent. In the subsequent experiments, we selected the reward function and model that produced the most significant outcome, guaranteeing that the chasing car was within 25 m of a runaway car for 63% of the episode duration. To the best of our knowledge, this work is the first one that addressed the task of the chase in urban driving using the Reinforcement Learning approach.	Proceedings Paper	2023	10.1007/978-3-031-16075-2_29	[]	[]	-1	-1	-1
C	El Hamdani, Sara; Loudari, Salaheddine; Novotny, Stanislav; Bouchner, Petr; Benamar, Nabil	A Markov Decision Process Model for a Reinforcement Learning-based Autonomous Pedestrian Crossing Protocol	2021	2021 3RD IEEE MIDDLE EAST AND NORTH AFRICA COMMUNICATIONS CONFERENCE (MENACOMM)		147	151	"Autonomous Traffic Management (ATM) systems empowered with Machine Learning (ML) technics are a promising solution for eliminating traffic light and decreasing traffic congestion in the future. However, few efforts have focused on integrating pedestrians in ATM, namely the static programming-based cooperative protocol called Autonomous Pedestrian Crossing (APC). In this paper, we model a Markov Decision Process ( MDP) to enable a Deep Reinforcement Learning (DRL)-based version of APC protocol that is able to dynamically achieve the same objectives (i.e. decreasing traffic delay at the crossing area). Using concrete state space, action set and reward functions, our model forces the Autonomous Vehicle (AV) to ""think"" and behave according to APC architecture. Compared to the traditional programming APC system, our approach permits the AV to learn from its previous experiences in non-signalized crossing and optimize the distance and the velocity parameters accordingly."	Proceedings Paper	2021	10.1109/MENACOMM50742.2021.9678310	[]	[]	-1	-1	-1
J	Kontoravdis, D.; Likas, A.; Blekas, K.; Stafylopatis, A.	A fuzzy neural network approach to autonomous vehicle navigation	1994	Proceedings EURISCON '94. European Robotics and Intelligent Systems Conference		243	52 vol.1	The paper presents an application of the GARIC architecture to autonomous vehicle navigation. This method constitutes a fuzzy neural approach that maps fuzzy control rules into the architecture of a neural network and uses a reinforcement learning scheme employing an adaptive heuristic critic to adjust the parameters of the fuzzy variables. Since the original GARIC formulation assumes continuous valued control variables, the authors have developed a modified scheme that can be applied to problems assuming discrete control variables (as is the case with their motion control problem). Experimental tests on difficult grounds comprising left and right turns justify the effectiveness of the proposed method, since the vehicle learns to navigate almost perfectly in only a few steps and exhibits very steep learning curves compared to the pure reinforcement case.	Conference Paper	1994		[]	[]	-1	-1	-1
C	Zhang, Yuxiang; Gao, Bingzhao; Zhou, Jinghua; Guo, Lulu; Chen, Hong	Velocity control in a right-turn across traffic scenario for autonomous vehicles using kernel-based reinforcement learning	2017	2017 CHINESE AUTOMATION CONGRESS (CAC)	Chinese Automation Congress	6211	6216	Recently, advanced control methods like machine leaning are increasingly applied to autonomous vehicle. This paper focuses on velocity control in a right-turn traffic scenario. A Markov Decision Processes(MDPs) is modeled and the actor-critic reinforcement learning architecture is employed. Then the kernel-based least squares policy iteration algorithm(KLSPI) is applied. Simulation results show that the proposed method can perform different policy in different cases, which preliminarily verify the rationality.	Proceedings Paper	2017		[]	[]	-1	-1	-1
J	Havenstrom, Simen Theie; Rasheed, Adil; San, Omer	Deep Reinforcement Learning Controller for 3D Path Following and Collision Avoidance by Autonomous Underwater Vehicles	2021	FRONTIERS IN ROBOTICS AND AI				Control theory provides engineers with a multitude of tools to design controllers that manipulate the closed-loop behavior and stability of dynamical systems. These methods rely heavily on insights into the mathematical model governing the physical system. However, in complex systems, such as autonomous underwater vehicles performing the dual objective of path following and collision avoidance, decision making becomes nontrivial. We propose a solution using state-of-the-art Deep Reinforcement Learning (DRL) techniques to develop autonomous agents capable of achieving this hybrid objective without having a priori knowledge about the goal or the environment. Our results demonstrate the viability of DRL in path following and avoiding collisions towards achieving human-level decision making in autonomous vehicle systems within extreme obstacle configurations.	Article	JAN 25 2021	10.3389/frobt.2020.566037	[]	[]	-1	-1	-1
C	Fleicher, Michael; Musicant, Oren; Azaria, Amos	Using Physiological Metrics to Improve Reinforcement Learning for Autonomous Vehicles	2022	2022 IEEE 34TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI	Proceedings-International Conference on Tools With Artificial Intelligence	1223	1230	"Thanks to recent technological advances Autonomous Vehicles (AVs) are becoming available at some locations. Safety impacts of these devices have, however, been difficult to assess. In this paper we utilize physiological metrics to improve the performance of a reinforcement learning agent attempting to drive an autonomous vehicle in simulation. We measure the performance of our reinforcement learner in several aspects, including the amount of stress imposed on potential passengers, the number of training episodes required, and a score measuring the vehicle's speed as well as the distance successfully traveled by the vehicle, without traveling off-track or hitting a different vehicle. To that end, we compose a human model, which is based on a dataset of physiological metrics of passengers in an autonomous vehicle. We embed this model in a reinforcement learning agent by providing negative reward to the agent for actions that cause the human model an increase in heart rate. We show that such a ""passenger-aware"" reinforcement learner agent does not only reduce the stress imposed on hypothetical passengers, but, quite surprisingly, also drives safer and its learning process is more effective than an agent that does not obtain rewards from a human model."	Proceedings Paper	2022	10.1109/ICTAI56018.2022.00186	[]	[]	-1	-1	-1
J	Ashwin, S.H.; Naveen Raj, R.	Deep reinforcement learning for autonomous vehicles: lane keep and overtaking scenarios with collision avoidance	2023	International Journal of Information Technology		3541	53	Numerous accidents and fatalities occur every year across the world as a result of the reckless driving of drivers and the ever-increasing number of vehicles on the road. Due to these factors, autonomous cars have attracted enormous attention as a potentially game-changing technology to address a number of persistent problems in the transportation industry. Autonomous vehicles need to be modeled as intelligent agents with the capacity to observe, and perceive the complex and dynamic environment on the road, and decide an action with the highest priority to the lives of people in every scenarios. The proposed deep deterministic policy gradient-based sequential decision algorithm models the autonomous vehicle as a learning agent and trains it to drive on a lane, overtake a static and a moving vehicle, and avoid collisions with obstacles on the front and right side. The proposed work is simulated using a TORC simulator and has shown the expected performance under the above-said scenarios.	Journal Paper	2023	10.1007/s41870-023-01412-6	[]	[]	-1	-1	-1
C	Liu, Yuqi; Zhang, Qichao; Zhao, Dongbin	A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios	2021	2021 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2021)				In recent years, control under urban intersection scenarios has become an emerging research topic. In such scenarios, the autonomous vehicle confronts complicated situations since it must deal with the interaction with social vehicles timely while obeying the traffic rules. Generally, the autonomous vehicle is supposed to avoid collisions while pursuing better efficiency. The existing work fails to provide a framework that emphasizes the integrity of the scenarios while deploying and testing reinforcement learning(RL) methods. Specifically, we propose a benchmark for training and testing RL-based autonomous driving agents in complex intersection scenarios, which is called RL-CIS. Then, a set of baselines consisting various algorithms are deployed. The test benchmark and baselines provide a fair and comprehensive training and testing platform for the study of RL for autonomous driving in the intersection scenario, advancing RL-based methods for autonomous driving control. The code of our proposed framework can be found at https://github.com/liuyuqi123/ComplexUrbanScenarios.	Proceedings Paper	2021	10.1109/SSCI50451.2021.9660172	[]	[]	-1	-1	-1
J	Yildiz, A.; Yel, E.; Corso, A.L.; Wray, K.H.; Witwicki, S.J.; Kochenderfer, M.J.	Experience Filter: Using Past Experiences on Unseen Tasks or Environments [arXiv]	2023	arXiv				One of the bottlenecks of training autonomous vehicle (AV) agents is the variability of training environments. Since learning optimal policies for unseen environments is often very costly and requires substantial data collection, it becomes computationally intractable to train the agent on every possible environment or task the AV may encounter. This paper introduces a zero-shot filtering approach to interpolate learned policies of past experiences to generalize to unseen ones. We use an experience kernel to correlate environments. These correlations are then exploited to produce policies for new tasks or environments from learned policies. We demonstrate our methods on an autonomous vehicle driving through T-intersections with different characteristics, where its behavior is modeled as a partially observable Markov decision process (POMDP). We first construct compact representations of learned policies for POMDPs with unknown transition functions given a dataset of sequential actions and observations. Then, we filter parameterized policies of previously visited environments to generate policies to new, unseen environments. We demonstrate our approaches on both an actual AV and a high-fidelity simulator. Results indicate that our experience filter offers a fast, low-effort, and near-optimal solution to create policies for tasks or environments never seen before. Furthermore, the generated new policies outperform the policy learned using the entire data collected from past environments, suggesting that the correlation among different environments can be exploited and irrelevant ones can be filtered out.	Journal Paper	29 May 2023		[]	[]	-1	-1	-1
J	Kuutti, Sampo; Bowden, Richard; Fallah, Saber	Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages	2021	SENSORS				The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control. However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles. In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent. By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy. This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle. We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance. Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone.	Article	MAR 2021	10.3390/s21062032	[]	[]	-1	-1	-1
J	Du, Guodong; Zou, Yuan; Zhang, Xudong; Li, Zirui; Liu, Qi	Hierarchical Motion Planning and Tracking for Autonomous Vehicles Using Global Heuristic Based Potential Field and Reinforcement Learning Based Predictive Control	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		8304	8323	The autonomous vehicle is widely applied in various ground operations, in which motion planning and tracking control are becoming the key technologies to achieve autonomous driving. In order to further improve the performance of motion planning and tracking control, an efficient hierarchical framework containing motion planning and tracking control for the autonomous vehicles is constructed in this paper. Firstly, the problems of planning and control are modeled and formulated for the autonomous vehicle. Then, the logical structure of the hierarchical framework is described in detail, which contains several algorithmic improvements and logical associations. The global heuristic planning based artificial potential field method is developed to generate the real-time optimal motion sequence, and the prioritized Q-learning based forward predictive control method is proposed to further optimize the effectiveness of tracking control. The hierarchical framework is evaluated and validated by the numerical simulation, virtual driving environment simulation and real-world scenario. The results show that both the motion planning layer and the tracking control layer of the hierarchical framework perform better than other previous methods. Finally, the adaptability of the proposed framework is verified by applying another driving scenario. Furthermore, the hierarchical framework also has the ability for the real-time application.	Article; Early Access		10.1109/TITS.2023.3266195	[]	[]	-1	-1	-1
C	Masmoudi, Mehdi; Ghazzai, Hakim; Frikha, Mounir; Massoud, Yehia	Autonomous Car-Following Approach Based on Real-time Video Frames Processing	2019	2019 IEEE INTERNATIONAL CONFERENCE OF VEHICULAR ELECTRONICS AND SAFETY (ICVES 19)				Car-following theory has received considerable attention from the transportation fields in the last decade. Autonomous vehicles are designed to provide convenient and safe driving by avoiding accidents caused by driver errors. However, it is always important to enhance the recognition of driver's driving-style in our roads. With car following models, automated vehicles can imitate the human behavior driving and assure a high safety level on road. All the built-in technologies must be integrated and complemented to achieve these goals. Automated object detection and the following process are one of the main research tasks that must be undertaken for this purpose. In this paper, we design a car following framework for autonomous vehicles using the reinforcement learning technique. The objective is to follow one leader car based on image-frames. To this end, we propose to employ the YOLOv3 object detector to detect vehicles the Q-learning technique to train the follower vehicle to autonomously navigate and follow the leader. Simulation results show the convergence of the model and investigate the behavior of the image-based autonomous vehicle in following its leader.	Proceedings Paper	2019		[]	[]	-1	-1	-1
R	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano	Testing autonomous vehicle lane keeping assist system in BeamNG simulator	2023	Figshare				In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator. Road topologies were generated by our tool RIGAA (available at: Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: CC BY 4.0	Data set	2023-09-11	https://doi.org/10.6084/m9.figshare.23968398.v1	[]	[]	-1	-1	-1
C	Nageshrao, Subramanya; Tseng, H. Eric; Filev, Dimitar	Autonomous Highway Driving using Deep Reinforcement Learning	2019	2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)	IEEE International Conference on Systems Man and Cybernetics Conference Proceedings	2326	2331	The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. Due to this, formulating a rule based decision maker for selecting driving maneuvers may not be ideal. Similarly, it may not be efficient to solve optimal control problem in real-time for a predefined cost function. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with the simulated traffic. Here the decision maker is a deep neural network that provides an action choice for a given system state. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density.	Proceedings Paper	2019		[]	[]	-1	-1	-1
C	Moradipari, Ahmadreza; Bae, Sangjae; Alizadeh, Mahnoosh; Pari, Ehsan Moradi; Isele, David	Predicting Parameters for Modeling Traffic Participants	2022	2022 IEEE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	703	708	Accurately modeling the behavior of traffic participants is essential for safely and efficiently navigating an autonomous vehicle through heavy traffic. We propose a method, based on the intelligent driver model, that allows us to accurately model individual driver behaviors from only a small number of frames using easily observable features. On average, this method makes prediction errors that have less than 1 meter difference from an oracle with full-information when analyzed over a 10-second horizon of highway driving. We then validate the efficiency of our method through extensive analysis against a competitive data-driven method such as Reinforcement Learning that may be of independent interest.	Proceedings Paper	2022	10.1109/ITSC55140.2022.9922467	[]	[]	-1	-1	-1
C	Bin Issa, Razin; Rahman, Md. Saferi; Das, Modhumonty; Barua, Monika; Alam, Md. Golam Rabiul	Reinforcement Learning based Autonomous Vehicle for Exploration and Exploitation of Undiscovered Track	2020	2020 34TH INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2020)		276	281	This research focuses on autonomous traversal of land vehicles through exploring undiscovered tracks and overcoming environmental barriers. Most of the existing systems can only operate and traverse in a distinctive mapped model especially in a known area. However, the proposed system which is trained by Deep Reinforcement learning can learn by itself to operate autonomously in extreme conditions. The dynamic double deep Q-learning (DDQN) model enables the proposed system not be confined to only in known environments. The ambient environmental obstacles are identified through Faster R-CNN for smooth movement of the autonomous vehicle. The exploration and exploitation strategies of DDQN enables the autonomous agent to learn proper decisions for various dynamic environments and tracks. The proposed model is tested in a gaming environment. It shows the overall effectiveness in traversing of autonomous land vehicles. The goal is to integrate Deep Reinforcement learning and Faster R-CNN to make the system effective to traverse through undiscovered paths by detecting obstacles.	Proceedings Paper	2020	10.1109/icoin48656.2020.9016539	[]	[]	-1	-1	-1
B	Lopez Pulgarin, E.J.; Irmak, T.; Paul, J.V.; Meekul, A.; Herrmann, G.; Leonards, U.	Comparing Model-based and Data-driven Controllers for an Autonomous Vehicle Task	2018	Towards Autonomous Robotic Systems. 19th Annual Conference, TAROS 2018 Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10965)		170	82	The advent of autonomous vehicles comes with many questions from an ethical and technological point of view. The need for high performing controllers, which show transparency and predictability is crucial to generate trust in such systems. Popular data-driven, black box-like approaches such as deep learning and reinforcement learning are used more and more in robotics due to their ability to process large amounts of information, with outstanding performance, but raising concerns about their transparency and predictability. Model-based control approaches are still a reliable and predictable alternative, used extensively in industry but with restrictions of their own. Which of these approaches is preferable is difficult to assess as they are rarely directly compared with each other for the same task, especially for autonomous vehicles. Here we compare two popular approaches for control synthesis, model-based control i.e. Model Predictive Controller (MPC), and data-driven control i.e. Reinforcement Learning (RL) for a lane keeping task with speed limit for an autonomous vehicle; controllers were to take control after a human driver had departed lanes or gone above the speed limit. We report the differences between both control approaches from analysis, architecture, synthesis, tuning and deployment and compare performance, taking overall benefits and difficulties of each control approach into account.	Conference Paper	2018	10.1007/978-3-319-96728-8_15	[]	[]	-1	-1	-1
J	Hong Yun Chen; Yan Qiang Li; Zi Hui Zhang; Yong Wang	Test Method for Decision Planning of Autonomous Vehicles Based on DQN Algorithm	2021	E3S Web of Conferences		03022 (5 pp.)	03022 (5 pp.)	In February 2020, Beijing, China andCalifornia, USA respectively released road test reports of 2019 for autonomous vehicles. Beijing and California respectively represent the highest level of testing and application of autonomous vehicles in the two countries. This article will compare the test items, evaluation criteria and technical defects of each autonomous vehicle company in the road test reports of China and the United States, also analyze the existing problems, and propose an idea for the construction of a comprehensive test site for autonomous vehicles. This article aims to solve the prominently exposed problems in decision-making and planning in autonomous vehicles with DQN algorithm-base vehicle fleet, and to look forward to the future development trend of autonomous driving testing.	Conference Paper; Journal Paper	2021	10.1051/e3sconf/202125303022	[]	[]	-1	-1	-1
J	Nemeth, Balazs; Gaspar, Peter	The Design of Performance Guaranteed Autonomous Vehicle Control for Optimal Motion in Unsignalized Intersections	2021	APPLIED SCIENCES-BASEL				The design of the motion of autonomous vehicles in non-signalized intersections with the consideration of multiple criteria and safety constraints is a challenging problem with several tasks. In this paper, a learning-based control solution with guarantees for collision avoidance is proposed. The design problem is formed in a novel way through the division of the control problem, which leads to reduced complexity for achieving real-time computation. First, an environment model for the intersection was created based on a constrained quadratic optimization, with which guarantees on collision avoidance can be provided. A robust cruise controller for the autonomous vehicle was also designed. Second, the environment model was used in the training process, which was based on a reinforcement learning method. The goal of the training was to improve the economy of autonomous vehicles, while guaranteeing collision avoidance. The effectiveness of the method is presented through simulation examples in non-signalized intersection scenarios with varying numbers of vehicles.	Article	APR 2021	10.3390/app11083464	[]	[]	-1	-1	-1
C	Hu, Yeping; Nakhaei, Alireza; Tomizuka, Masayoshi; Fujimura, Kikuo	Interaction-aware Decision Making with Adaptive Strategies under Merging Scenarios	2019	2019 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	151	158	In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants. Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness. Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise. Many traditional methods have been proposed to solve decision making problems under merging scenarios. However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios. In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios. A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically. A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency. An exemplar merging scenario was used to implement and examine the proposed method.	Proceedings Paper	2019	10.1109/iros40897.2019.8968478	[]	[]	-1	-1	-1
C	Trasnea, Bogdan; Marina, Liviu A.; Vasilcoi, Andrei; Pozna, Claudiu R.; Grigorescu, Sorin M.	GridSim: A Vehicle Kinematics Engine for Deep Neuroevolutionary Control in Autonomous Driving	2019	2019 THIRD IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC 2019)		443	444	Current state of the art solutions in the control of an autonomous vehicle mainly use supervised end-to-end learning, or decoupled perception, planning and action pipelines. Another possible solution is deep reinforcement learning, but such a method requires that the agent interacts with its surroundings in a simulated environment. In this paper we introduce GridSim, which is an autonomous driving simulator engine running a car-like robot architecture to generate occupancy grids from simulated sensors. We use GridSim to study the performance of two deep learning approaches, deep reinforcement learning and driving behavioral learning through genetic algorithms. The deep network encodes the desired behavior in a two elements fitness function describing a maximum travel distance and a maximum forward speed, bounded to a specific interval. The algorithms are evaluated on simulated highways, curved roads and inner-city scenarios, all including different driving limitations.	Proceedings Paper	2019	10.1109/IRC.2019.00091	[]	[]	-1	-1	-1
C	Hester, Todd; Stone, Peter	The Open-Source TEXPLORE Code Release for Reinforcement Learning on Robots	2014	ROBOCUP 2013: ROBOT WORLD CUP XVII	Lecture Notes in Artificial Intelligence	536	543	The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations on-line. RL is a paradigm for learning sequential decision making tasks, usually formulated as a Markov Decision Process (MDP). For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. In addition, the algorithm must learn efficiently in the face of noise, sensor/actuator delays, and continuous state features. In this paper, we present the texplore ROS code release, which contains texplore, the first algorithm to address all of these challenges together. We demonstrate texplore learning to control the velocity of an autonomous vehicle in real-time. texplore has been released as an open-source ROS repository, enabling learning on a variety of robot tasks.	Proceedings Paper	2014		[]	[]	-1	-1	-1
J	Xiaobai Ma; Driggs-Campbell, K.; Kochenderfer, M.J.	Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning [arXiv]	2019	arXiv		7 pp.	7 pp.	To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods. [Intelligent Vehicles Symposium (IV), 2018 IEEE].	Journal Paper	8 March 2019		[]	[]	-1	-1	-1
J	Wang, Hongbo; Hu, Chenglei; Zhou, Juntao; Feng, Lizhao; Ye, Bin; Lu, Yongjie	Path tracking control of an autonomous vehicle with model-free adaptive dynamic programming and RBF neural network disturbance compensation	2022	PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING		825	841	The performance of the model-based controller is always affected by the uncertainty and nonlinearity of the model parameters in the vehicle path tracking process. To address this issue, a novel path tracking controller based on model-free adaptive dynamic programming (ADP) is proposed for autonomous vehicles in this paper. To be specific, the proposed controller obtains information from the online state and front-wheel angle input data which are repeatedly used to calculate the controller gain iteratively. So, this controller features not requiring accurate knowledge of vehicle model parameters for controller development. Meanwhile, the path tracking performance of the autonomous vehicle will be inevitably disturbed by unknown nonlinear external disturbance. To approximate this disturbance, the learning characteristics of Radial Basis Function Neural Network (RBFNN) are applied to generate compensation for the front-wheel angle. Afterward, the weight updating law of RBFNN is derived by Lyapunov function to ensure the stability and convergence of the whole system. Finally, Hardware in the loop (HIL) test results demonstrate that the proposed ADP-RBF controller can improve the comprehensive performance of the vehicle path tracking control system and achieve the balance between path tracking accuracy and minimum sideslip angle.	Article; Early Access		10.1177/09544070211033835	[]	[]	-1	-1	-1
J	Kuutti, S.; Bowden, R.; Fallah, S.	Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages [arXiv]	2021	arXiv		18 pp.	18 pp.	The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control. However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles. In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent. By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy. This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle. We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance. Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone. [Sensors 2021, 21, 2032 doi:10.3390/s21062032].	Journal Paper	17 March 2021		[]	[]	-1	-1	-1
J	Sur, Chiranjib	UCRLF: unified constrained reinforcement learning framework for phase-aware architectures for autonomous vehicle signaling and trajectory optimization	2019	EVOLUTIONARY INTELLIGENCE		689	712	Signaling and trajectory optimization work as contention and researchers have debated on what should be the best for the vehicle, but it seems that both components are complement to each other and there can be combined situations with bounds where maximum optimization can be achieved. This paper introduces a novel approach called Phase-Aware Deep Learning and Constrained Reinforcement Learning for optimization and constant improvement of signal and trajectory for autonomous vehicle operation modules for an intersection. It deals with all the components required for the signaling system to operate, communicate and also navigate the vehicle with proper trajectory so that it faces less waiting time and the overall system operates with minimum waiting time and comparable throughput rate. We have done analysis on the operating time and the vehicle movement as these are vital for pollution and energy consumption. Our methodologies are not only efficient in time and computation but also have incorporated highly optimized data representation to reduce the overhead of maintaining and accessing the data. This ensures very efficient time complexity and theoretical computation time and better lower bounds. Constrained Reinforcement Learning concept is the main contribution of this work and it helped in decreasing 84% of the waiting time for the vehicles.	Article	DEC 2019	10.1007/s12065-019-00278-7	[]	[]	-1	-1	-1
B	Ghnaya, I.; Mosbah, M.; Aniss, H.; Ahmed, T.	Multi-Agent Advantage Actor-Critic Learning For Message Content Selection in Cooperative Perception Networks	2023	NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium		1	9	Recent advancements in autonomous vehicle perception haveexposed limitations of onboard sensors such as radar, lidar, and cameras, which road obstacles and adverse weather conditions can impede. Connected and Autonomous Vehicles (CAVs) are leveraging wireless communications to share perception information through a process called Cooperative Perception (CP), aiming to provide a more comprehensive understanding of their environment. However, this can result in excessive redundant and useless information in the network, as the same road objects may be detected and exchanged simultaneously by multiple CAVs. This not only consumes more network resources but also may overload the communication channel, reducing the delivery of perception information to CAVs and ultimately decreasing the overall CP awareness in the network. This paper introduces MCORM, a multi-agent learning method based on the advantage actor-critic algorithm to maximize object usefulness and reduce redundancy in the network. Our evaluations demonstrate that through this method, CAVs learn optimal CP message content selection policies that maximize usefulness. Further more, our proposal proves to be more effective in mitigating object redundancy and improving network reliability in comparison to existing approaches.	Conference Paper	2023	10.1109/NOMS56928.2023.10154436	[]	[]	-1	-1	-1
B	Zeng, X.; Yu, Q.; Liu, S.; Xia, Y.; Su, H.; Zheng, K.	Target-Oriented Maneuver Decision for Autonomous Vehicle: A Rule-Aided Reinforcement Learning Framework	2023	CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management		3124	33	Autonomous driving systems (ADSs) have the potential to revolutionize transportation by improving traffic safety and efficiency. As the core component of ADSs, maneuver decision aims to make tactical decisions to accomplish road following, obstacle avoidance, and efficient driving. In this work, we consider a typical but rarely studied task, called Target-Lane-Entering (TLE), where an autonomous vehicle should enter a target lane before reaching an intersection to ensure a smooth transition to another road. For navigation-assisted autonomous driving, a maneuver decision module chooses the optimal timing to enter the target lane in each road section, thus avoiding rerouting and reducing travel time. To achieve the TLE task, we propose a ruLe-aided reINforcement lEarning framework, called LINE, which combines the advantages of RL-based policy and rule-based strategy, allowing the autonomous vehicle to make target-oriented maneuver decisions. Specifically, an RL-based policy with a hybrid reward function is able to make safe, efficient, and comfortable decisions while considering the factors of target lanes. Then a strategy of rule revision aims to help the policy learn from intervention and block the risk of missing target lanes. Extensive experiments based on the SUMO simulator confirm the effectiveness of our framework. The results show that LINE achieves state-of-the-art driving performance with over 95% task success rate.	Conference Paper	2023	10.1145/3583780.3615072	[]	[]	-1	-1	-1
B	Saxena, D.M.; Sangjae Bae; Nakhaei, A.; Fujimura, K.; Likhachev, M.	Driving in Dense Traffic with Model-Free Reinforcement Learning	2020	2020 IEEE International Conference on Robotics and Automation (ICRA)		5385	92	Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.	Conference Paper	2020	10.1109/ICRA40945.2020.9197132	[]	[]	-1	-1	-1
J	Mokhtari, K.; Wagner, A.R.	Pedestrian Collision Avoidance for Autonomous Vehicles at Unsignalized Intersection Using Deep Q-Network [arXiv]	2021	arXiv		8 pp.	8 pp.	Prior research has extensively explored Autonomous Vehicle (AV) navigation in the presence of other vehicles, however, navigation among pedestrians, who are the most vulnerable element in urban environments, has been less examined. This paper explores AV navigation in crowded, unsignalized intersections. We compare the performance of different deep reinforcement learning methods trained on our reward function and state representation. The performance of these methods and a standard rule-based approach were evaluated in two ways, first at the unsignalized intersection on which the methods were trained, and secondly at an unknown unsignalized intersection with a different topology. For both scenarios, the rule-based method achieves less than 40\% collision-free episodes, whereas our methods result in a performance of approximately 100\%. Of the three methods used, DDQN/PER outperforms the other two methods while it also shows the smallest average intersection crossing time, the greatest average speed, and the greatest distance from the closest pedestrian.	Journal Paper	30 April 2021		[]	[]	-1	-1	-1
C	Saxena, Dhruv Mauria; Bae, Sangjae; Nakhaei, Alireza; Fujimura, Kikuo; Likhachev, Maxim	Driving in Dense Traffic with Model-Free Reinforcement Learning	2020	2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	5385	5392	Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.	Proceedings Paper	2020	10.1109/icra40945.2020.9197132	[]	[]	-1	-1	-1
J	Wright, M.A.; Horowitz, R.	Attentional policies for cross-context multi-agent reinforcement learning [arXiv]	2019	arXiv		11 pp.	11 pp.	Many potential applications of reinforcement learning in the real world involve interacting with other agents whose numbers vary over time. We propose new neural policy architectures for these multi-agent problems. In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we follow the spirit of recent work on the power of relational inductive biases in deep networks by learning multi-agent relationships at the policy level via an attentional architecture. In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents' state information when selecting their next action. The structure of our architectures allow them to be applied on environments with varying numbers of agents. We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and significantly outperforming it when scaling to large numbers of agents.	Journal Paper	31 May 2019		[]	[]	-1	-1	-1
J	Wang, Yongshuai; Chen, Zengqiang; Sun, Mingwei; Sun, Qinglin	Enhancing active disturbance rejection design via deep reinforcement learning and its application to autonomous vehicle	2024	EXPERT SYSTEMS WITH APPLICATIONS				Taking the velocity regulation of autonomous driving as an example, this paper developed an enhancing active disturbance rejection design employing the deep reinforcement learning-deep deterministic policy gradient (DDPG). In this scheme, the active disturbance rejection control (ADRC) is adopted to online estimate and compensate disturbances and uncertainties, and feasible regions of control parameters are obtained through the Lyapunov method. Then DDPG is combined with ADRC to online adaptively tune control parameters in response to the changing environments, where safety, comfort, and energy-saving are considered in the reward design, and mapping relation between the defined action and state is constructed for the maximal reward. Besides, numerical simulations demonstrate the better performance and stronger robustness of the enhancing design when facing uncertainties, sensor noise, and mechanical faults, and comfort and energy consumption have also been improved to some extent compared with the general ADRC and model predictive control (MPC).	Article; Early Access		10.1016/j.eswa.2023.122433	[]	[]	-1	-1	-1
B	Kadam, R.; Vidhani, V.; Valecha, B.; Bane, A.; Giri, N.	Autonomous Vehicle Simulation Using Deep Reinforcement Learning	2021	Machine Learning for Predictive Analysis. Proceedings of ICTIS 2020. Lecture Notes in Networks and Systems (LNNS 141)		541	50	The reinforcement learning algorithms have been proven to be extremely accurate in performing a variety of tasks. These algorithms have outperformed humans in traditional games. This paper proposes a reinforcement learning based approach to autonomous driving. The autonomous vehicles must be able to deal with all external situations to ensure safety and to avoid undesired circumstances such as collisions. Thus, we propose the use of deep deterministic policy gradient (DDPG) algorithm which is able to work in a complex and continuous domain. To avoid physical damage and reduce costs, we choose to use a simulator to test the proposed approach. The CARLA simulator would be used as the environment. To fit the DDPG algorithm to the CARLA environment, our network architecture consists of critic and actor networks. The performance would be evaluated based on rewards generated by the agent while driving in the simulated environment.	Conference Paper	2021	10.1007/978-981-15-7106-0_53	[]	[]	-1	-1	-1
J	Yu, Jiaxing; Arab, Aliasghar; Yi, Jingang; Pei, Xiaofei; Guo, Xuexun	Hierarchical framework integrating rapidly-exploring random tree with deep reinforcement learning for autonomous vehicle	2023	APPLIED INTELLIGENCE		16473	16486	This paper proposes a systematic driving framework where the decision making module of reinforcement learning (RL) is integrated with rapidly-exploring random tree (RRT) as motion planning. RL is used to generate local goals and semantic speed commands to control the longitudinal speed of a vehicle while rewards are designed for the driving safety and the traffic efficiency. Guaranteeing the driving comfort, RRT returns a feasible path to be followed by the vehicle with the speed commands. The scene decomposition approach is implemented to scale the deep neural network (DNN) to environments with multiple traffic participants and double deep Q-networks (DDQN) with prioritized experience replay (PER) is utilized to accelerate the training process. To handle the disturbance of the perception of the agent, we use an ensemble of neural networks to evaluate the uncertainty of decisions. It has shown that the proposed framework can tackle unexpected actions of traffic participants at an intersection yielding safe, comfort and efficient driving behaviors. Also, the ensemble of DDQN with PER is proved to be superior over standard DDQN in terms of learning efficiency and disturbance vulnerability.	Article; Early Access		10.1007/s10489-022-04358-7	[]	[]	-1	-1	-1
B	Cheng, Y.; Wang, H.; Chu, H.; Yu, L.; Gao, B.; Chen, H.	Lane-keeping Control of Autonomous Vehicles Using Reinforcement Learning and Predictive Safety Filter *	2023	2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	6	The urgent need for both safety and high-performance in the motion planning and decision-making system of the autonomous vehicle presents a significant challenge for learning-based technologies. In this paper, we introduces a lane-keeping control structure that combines reinforcement learning and a predictive safety filter to ensure the vehicle's safety motion. A vehicle agent under the racetrack environment is trained by the soft actor critic algorithm, to achieve high-performance continuous lane-keeping motion. Additionally, we establish the safety task for lane-keeping, which results in a learning-based safety filter, to avoid violation of driving on the outer lane. Through simulations, we demonstrate the effectiveness of our method in ensuring the safe and trustworthy control of autonomous vehicle lane-keeping.	Conference Paper	2023	10.1109/CVCI59596.2023.10397167	[]	[]	-1	-1	-1
J	Zheng, Haotian; Chen, Chaoyi; Li, Shuai; Zheng, Sifa; Li, Shengbo Eben; Xu, Qing; Wang, Jianqiang	Learning-Based Safe Control for Robot and Autonomous Vehicle Using Efficient Safety Certificate	2023	IEEE OPEN JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS		419	430	Energy-function-based safety certificates can provide demonstrable safety for complex automatic control systems used in safety control tasks. However, recent studies on learning-based energy function synthesis have only focused on feasibility, which can lead to over-conservatism and reduce controller efficiency. In this study, we propose using magnitude regularization techniques to enhance the efficiency of safe controllers by reducing conservativeness within the energy function while maintaining promising demonstrable safety guarantees. Specifically, we measure conservativeness by the magnitude of the energy function and reduce it by adding a magnitude regularization term to the integrated loss. We present the SafeMR algorithm, which is synthesized using reinforcement learning (RL) to unify the learning process of the safety controller and the energy function. To verify the effectiveness of the algorithm, we conducted two sets of experiments, one in a robot-based environment and the other in an autonomous vehicle environment. The experimental results demonstrate that the proposed approach reduces the conservativeness of the energy function and outperforms the baseline in terms of controller efficiency for the robot, while ensuring safety.	Article	2023	10.1109/OJITS.2023.3280573	[]	[]	-1	-1	-1
C	Humeniuk, Dmytro; Khomh, Foutse; Antoniol, Giuliano	RIGAA at the SBFT 2023 Tool Competition - Cyber-Physical Systems Track	2023	2023 IEEE/ACM INTERNATIONAL WORKSHOP ON SEARCH-BASED AND FUZZ TESTING, SBFT		49	50	Testing and verification of autonomous systems is critically important. In the context of SBFT 2023 CPS testing tool competition, we present our tool RIGAA for generating virtual roads to test an autonomous vehicle lane keeping assist system. RIGAA combines reinforcement learning as well as evolutionary search to generate test scenarios. It has achieved the second highest final score among 5 other submitted tools.	Proceedings Paper	2023	10.1109/SBFT59156.2023.00011	[]	[]	-1	-1	-1
J	Saxena, D.M.; Sangjae Bae; Nakhaei, A.; Fujimura, K.; Likhachev, M.	Driving in Dense Traffic with Model-Free Reinforcement Learning [arXiv]	2019	arXiv		8 pp.	8 pp.	Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation.	Journal Paper	14 Sept. 2019		[]	[]	-1	-1	-1
C	Giannini, Francesco; Fortino, Giancarlo; Franze, Giuseppe; Pupo, Francesco	A Deep Q Learning-Model Predictive Control Approach to vehicle routing and control with platoon constraints	2022	2022 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING (CASE)	IEEE International Conference on Automation Science and Engineering	563	568	In this paper we deal with the control of platoon of autonomous vehicles driving in Urban Road Networks (URNs). We exploit the idea of using Deep Reinforcement Learning (DRL) as the path planner of the proposed architecture. The advantage of this solution is the capability to deal with the actual traffic congestion while driving the autonomous vehicle to its destination. In particular, the high-level routing decisions are translated into manipulable set-points for Receding Horizon controllers by making more computational affordable and efficient the control action on the vehicle dynamics. Feasibility and asymptotic closed-loop stability are formally proved. Some simulations on a platoon, consisting of three agents described by double-integrator models, are provided to show the effectiveness of the overall architecture.	Proceedings Paper	2022	10.1109/CASE49997.2022.9926699	[]	[]	-1	-1	-1
J	Khalid, Muhammad; Wang, Liang; Wang, Kezhi; Aslam, Nauman; Pan, Cunhua; Cao, Yue	Deep reinforcement learning-based long-range autonomous valet parking for smart cities	2023	SUSTAINABLE CITIES AND SOCIETY				In this paper, to reduce the congestion rate at the city center and increase the traveling quality of experience (QoE) of each user, the framework of long-range autonomous valet parking is presented. Here, an Autonomous Vehicle (AV) is deployed to pick up, and drop off users at their required spots, and then drive to the car park around well-organized places of city autonomously. In this framework, we aim to minimize the overall distance of AV, while guarantee all users are served with great QoE, i.e., picking up, and dropping off users at their required spots through optimizing the path planning of the AV and number of serving time slots. To this end, we first present a learning-based algorithm, which is named as Double-Layer Ant Colony Optimization (DLACO) algorithm to solve the above problem in an iterative way. Then, to make the fast decision, while considers the dynamic environment (i.e., the AV may pick up and drop off users from different locations), we further present a deep reinforcement learning-based algorithm, i.e., Deep Q-learning Network (DQN) to solve this problem. Experimental results show that the DL-ACO and DQN-based algorithms both achieve the considerable performance.	Article; Early Access		10.1016/j.scs.2022.104311	[]	[]	-1	-1	-1
B	Chi Kit Ngai; Yung, N.H.C.	DAQL-enabled autonomous vehicle navigation in dynamically changing environment	2011	Advances in Reinforcement Learning		385	410	In this chapter we have presented a multiple goal reinforcement learning framework and illustrated on a two-goal problem in autonomous vehicle navigation. In general, DAQL can be applied in any goals that environmental response is available, whereas QL would suffice if environmental response is not available or can be ignored. A proportional goal fusion function was used to maintain balance between the two goals in this case. Extensive simulations have been carried out to evaluate its performance under different obstacle behaivors and sensing accuracy. The results showed that the proposed method is characterized by its ability to (1) deal with single obstacles at any speed and from any directions; (2) deal with two obstacles approaching from different directions; (3) cope with large sensor noise; (4) navigate in high obstacle density and high relative velocity environment. Detailed comparison of the proposed method with the R&G method reveals that improvements by the proposed method in path time and the number of collision-free episodes are substantial.	Book Chapter	2011		[]	[]	-1	-1	-1
J	Nie, Xiaotong; Liang, Yupeng; Ohkura, Kazuhiro	Autonomous highway driving using reinforcement learning with safety check system based on time-to-collision	2023	ARTIFICIAL LIFE AND ROBOTICS		158	165	Decision making is an essential component of autonomous vehicle technology and received significant attention from academic and industry organizations. One of the promising approaches in designing a decision-making method is Reinforcement Learning (RL). To apply an RL algorithm to an autonomous driving problem, a feature representation of the state must first be chosen. The most commonly used representation is the spatial-temporal state feature. However, if the number or order of the surrounding vehicle changes, the feature representation will be affected. In this paper, we utilize time-to-collision (TTC) as the feature representation and propose a TTC-based safety check system. The action output by the RL controller would be replaced with a safer action chosen by the safety check system when an agent detects a potential collision, i.e., the TTC is below the time threshold. A ramp merging task is used to illustrate the effect. Simulation results show that the proposed method can effectively improve the arrival rate and reduce the collision rate, even in the case of dense traffic situations. Furthermore, we also conducted experiments to examine the performance of the safety check system with different time thresholds.	Article; Early Access		10.1007/s10015-022-00846-8	[]	[]	-1	-1	-1
J	Lin, Bing; Lin, Kai; Lin, Changhang; Lu, Yu; Huang, Ziqing; Chen, Xinwei	Computation offloading strategy based on deep reinforcement learning for connected and autonomous vehicle in vehicular edge computing	2021	JOURNAL OF CLOUD COMPUTING-ADVANCES SYSTEMS AND APPLICATIONS				Connected and Automated Vehicle (CAV) is a transformative technology that has great potential to improve urban traffic and driving safety. Electric Vehicle (EV) is becoming the key subject of next-generation CAVs by virtue of its advantages in energy saving. Due to the limited endurance and computing capacity of EVs, it is challenging to meet the surging demand for computing-intensive and delay-sensitive in-vehicle intelligent applications. Therefore, computation offloading has been employed to extend a single vehicle's computing capacity. Although various offloading strategies have been proposed to achieve good computing performace in the Vehicular Edge Computing (VEC) environment, it remains challenging to jointly optimize the offloading failure rate and the total energy consumption of the offloading process. To address this challenge, in this paper, we establish a computation offloading model based on Markov Decision Process (MDP), taking into consideration task dependencies, vehicle mobility, and different computing resources for task offloading. We then design a computation offloading strategy based on deep reinforcement learning, and leverage the Deep Q-Network based on Simulated Annealing (SA-DQN) algorithm to optimize the joint objectives. Experimental results show that the proposed strategy effectively reduces the offloading failure rate and the total energy consumption for application offloading.	Article	JUN 8 2021	10.1186/s13677-021-00246-6	[]	[]	-1	-1	-1
B	Qi, H.; Hou, E.; Liu, G.; Ye, P.	Cognitive Reinforcement Learning For Autonomous Driving	2023	2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)		1	5	Most existing reinforcement learning methods are combined with deep neural networks in autonomous driving. There is a well-known trouble named 'black-box' of deep neural networks, which lacks of interpretability, occurring in the decision-making process. That will affect the safety of autonomous vehicle. In this work, we propose a cognitive reinforcement learning framework. This framework illustrates a cognitive model that transfers the state space to cognitive signals. Subsequencely, based on these signals, the model simulates the human decision-making process. As a result, the outcome of decision provides reward to the reinforcement learning. The computational experiments conducted in CARLA demonstrate that our framework performs equally well as the conventional reinforcement learning methods and provides interpretability under the same circumstance in the reinforcement learning.	Conference Paper	2023	10.1109/DTPI59677.2023.10365456	[]	[]	-1	-1	-1
J	Lee, Dongsu; Kwon, Minhae	Combating Stop-and-Go Wave Problem at a Ring Road Using Deep Reinforcement Learning Based Autonomous Vehicles	2021	The Journal of Korean Institute of Communications and Information Sciences		1667	1682	With the rapid development of artificial intelligence, autonomous driving has recently attracted considerable attention. This paper aims to use an autonomous vehicle to improve road flow by solving the stop-and-go-wave problem on a ring road. We design a special model of Markov decision process model to solve stop-and-go-wave and use three deep reinforcement learning algorithms to train autonomous vehicles: Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3). We then compare their driving patterns and performances. We confirmed that an autonomous vehicle on the ring road could control the flow of multiple non-autonomous vehicles with an extensive simulation study, thus successfully solving the stop-and-go wave problem.	research-article	2021	10.7840/kics.2021.46.10.1667	[]	[]	-1	-1	-1
J	Urrea, Claudio; Garrido, Felipe; Kern, John	Design and Implementation of Intelligent Agent Training Systems for Virtual Vehicles	2021	SENSORS				This paper presents the results of the design, simulation, and implementation of a virtual vehicle. Such a process employs the Unity videogame platform and its Machine Learning-Agents library. The virtual vehicle is implemented in Unity considering mechanisms that represent accurately the dynamics of a real automobile, such as motor torque curve, suspension system, differential, and anti-roll bar, among others. Intelligent agents are designed and implemented to drive the virtual automobile, and they are trained using imitation or reinforcement. In the former method, learning by imitation, a human expert interacts with an intelligent agent through a control interface that simulates a real vehicle; in this way, the human expert receives motion signals and has stereoscopic vision, among other capabilities. In learning by reinforcement, a reward function that stimulates the intelligent agent to exert a soft control over the virtual automobile is designed. In the training stage, the intelligent agents are introduced into a scenario that simulates a four-lane highway. In the test stage, instead, they are located in unknown roads created based on random spline curves. Finally, graphs of the telemetric variables are presented, which are obtained from the automobile dynamics when the vehicle is controlled by the intelligent agents and their human counterpart, both in the training and the test track.	Article	JAN 2021	10.3390/s21020492	[]	[]	-1	-1	-1
B	Brunoud, A.; Lombard, A.; Abbas-Turki, A.; Gaud, N.; Kang-Hyun, J.	Hybrid deep reinforcement learning model for safe and efficient autonomous vehicles at pedestrian crossings	2023	2023 International Workshop on Intelligent Systems (IWIS)		1	6	The autonomous vehicle is a particularly promising area of application for machine learning algorithms. Autonomous driving is challenging due to the complexity of the road network, the hardly predictable human behaviors, either pedestrians or drivers and the imperative need to ensure the safety of all road users. Deep Reinforcement Learning algorithms have garnered significant interest as a viable option for controlling autonomous vehicles. They allow defining a control policy by solely describing the observation space, the action space, and a reward function. Yet, the most common solutions face difficulties when the action space is hybrid (simultaneously continuous and discrete), and the reward is sparse, which is the case when a selection between choices must be made (like yielding or not), and the trajectory of the vehicle must be adapted according to this decision. This paper addresses these challenges by focusing on controlling an autonomous vehicle at pedestrian crossings. The vehicle adapts its speed (continuous decision) and the displayed signal (discrete decision) according to the pedestrian's state. The unique characteristic of the speed profile is to facilitate safe pedestrian crossings without requiring the vehicle to come to a complete stop. This raises the consistency problem between the speed profile and the signals issued by the vehicle. This paper introduces a new hybrid DRL model capable of making both decisions. It discusses the training method, the construction and necessary constraints on the reward, as well as the success of generalization to different speed limitations.	Conference Paper	2023	10.1109/IWIS58789.2023.10284662	[]	[]	-1	-1	-1
J	Chen, Yuanhang; Wu, Shaofang	Framework of Active Obstacle Avoidance for Autonomous Vehicle Based on Hybrid Soft Actor-Critic Algorithm	2023	JOURNAL OF TRANSPORTATION ENGINEERING PART A-SYSTEMS				In this paper, a framework of active obstacle avoidance for autonomous vehicles based on the hybrid soft actor-critic (SAC) algorithm is proposed. In the stage of local path planning, a comprehensive cost function considering collision risks, deviation from the global route, road lines crossing, and driving comfortability is developed to provide a local optimal path avoiding both static and dynamic obstacles considering multiple predicting timesteps. Then, a path tracking controller on the foundation of a hybrid SAC algorithm is designed to mitigate the problem of high sample complexity caused by random initialization of parameters in conventional reinforcement learning approaches. Model predictive control (MPC) plays a guiding role by applying its control action to combine with the action of SAC online to obtain a more effective state and reward information for training. The mechanism of the combination of MPC with SAC to balance the exploration and reliability is explained in detail. In order to improve the convergence rate and learning efficiency, a dual actor network structure for two different control actions is adopted. With considerations of various relevant factors influencing the control effect, the reward for the hybrid SAC algorithm is designed carefully. Finally, the results of simulation experiments illustrate that the proposed approach performs effectively with the assurance of safety and driving comfortability. In summary, the hybrid SAC algorithm with dual actor networks performs better than other algorithms for comparison in all test scenarios in this paper.	Article	APR 1 2023	10.1061/JTEPBS.0000772	[]	[]	-1	-1	-1
C	Fiedler, Julius; Gerwien, Maximilian; Knoll, Carsten	A Hybrid Tactical Decision-Making Approach in Automated Driving Combining Knowledge-Based Systems and Reinforcement Learning	2022	2022 IEEE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	3478	3483	Decision-making in automated driving is influenced both by objective traffic rules and subjective perceptions and goals of the driver. Thus, a suitable representation of the environment of the autonomous vehicle is required to model complex traffic situations and extract key features. To achieve this objective, this work uses an ontology-based situation interpretation (OBSI) to model traffic situations. The resulting semantic state representation is used to train models of vehiclecontrolling agents using reinforcement learning. Based on our simulations, it can be shown that the semantic preprocessing of traffic situations significantly improves the agent's performance regarding safety and driving style.	Proceedings Paper	2022	10.1109/ITSC55140.2022.9922505	[]	[]	-1	-1	-1
B	Rezgui, J.; Bisaillon, C.; O'leary, L.O.	A platform for sharing artificial intelligence algorithms in autonomous driving: an overview of enhanced LAOP	2020	2020 International Symposium on Networks, Computers and Communications (ISNCC)		6 pp.	6 pp.	To solve the Autonomous Vehicle (AV) problem, an artificial learning model that translates sensor data into controls must be built. This way, the vehicle can react to its changing environment. To the best of our knowledge, there are no platforms where researchers can both develop new machine learning models, train them and compare them directly to others. We believe that such a platform can greatly impact the field of artificial intelligence and AV. In this paper, we propose an enhanced version of the Learning Algorithm Optimization Platform LAOP, called LAOP 2.0. It helps researchers in the field of artificial intelligence develop their models by allowing them to easily test and compare them. We also introduce the Learning Algorithm Sharing Platform (LASP), which makes it easy to share deep learning algorithms. As a demonstration of the versatility of our platform, we compared two learning algorithms. The first one, Fully Connected Neural Network (FUCONN), uses reinforcement learning to train itself. The second one, Mimicking HUman Behaviour (MHUB), uses supervised learning to adjust its weights and learns from human input. We demonstrate through extensive simulations that FUCONN outperforms MHUB after being trained.	Conference Paper	2020	10.1109/ISNCC49221.2020.9297192	[]	[]	-1	-1	-1
J	Paravarzar, S.; Mohammad, B.	Motion Prediction on Self-driving Cars: A Review [arXiv]	2020	arXiv		9 pp.	9 pp.	The autonomous vehicle motion prediction literature is reviewed. Motion prediction is the most challenging task in autonomous vehicles and self-drive cars. These challenges have been discussed. Later on, the state-of-the art has reviewed based on the most recent literature and the current challenges are discussed. The state-of-the-art consists of classical and physical methods, deep learning networks, and reinforcement learning. prons and cons of the methods and gap of the research presented in this review. Finally, the literature surrounding object tracking and motion will be presented. As a result, deep reinforcement learning is the best candidate to tackle self-driving cars.	Journal Paper	6 Nov. 2020		[]	[]	-1	-1	-1
J	Lowe, Evan; Guvenc, Levent	Autonomous Vehicle Emergency Obstacle Avoidance Maneuver Framework at Highway Speeds	2023	ELECTRONICS				An autonomous vehicle (AV) uses high-level decision making and lower-level actuator controls, such as throttle (acceleration), braking (deceleration), and steering (change in lateral direction) to navigate through various types of road networks. Path planning and path following for highway driving are currently available in series-produced highly automated vehicles. In addition to these, emergency collision avoidance decision making and maneuvering are another key and essential feature that is needed in a series production AV at highway driving speeds. For reliability, low cost, and fast computation, such an emergency obstacle avoidance maneuvering system should use well-established conventional methods as opposed to data-driven neural networks or reinforcement learning methods, which are currently not suitable for use in highway AV driving. This paper presents a novel Emergency Obstacle Avoidance Maneuver (EOAM) methodology for AVs traveling at higher speeds and lower road surface friction, involving time-critical maneuver determination and control. The proposed EOAM framework offers usage of the AV's sensing, perception, control, and actuation system abilities as one cohesive system to avoid an on-road obstacle, based first on performance feasibility and second on passenger comfort, and it is designed to be well integrated within an AV's high-level control and decision-making system. To demonstrate the efficacy of the proposed method, co-simulation including the AV's EOAM logic in Simulink and a vehicle model in CarSim is conducted with speeds ranging from 55 to 165 km/h and on road surfaces with friction ranging from 1.0 to 0.1. The results are analyzed and interpreted in the context of an entire AV system, with implications for future work.	Article	DEC 2023	10.3390/electronics12234765	[]	[]	-1	-1	-1
B	Naruse, K.; Kakazu, Y.	Rule generation and generalization by inductive decision tree and reinforcement learning	1994	Distributed Autonomous Robotic Systems		91	8	In this paper, we attempt to construct a planning mechanism composed of distributed agents for autonomous vehicle navigation in an unknown workspace. Each agent decides a direction that the vehicle should move without any communication to the other agents, by only observing the workspace and the other agents. The agent includes the reinforcement learning mechanism for generating rules for the navigation. However, the rules depend on a state observation method. For generalizing the rules, an inductive decision tree is introduced to the agent. In a new workspace, the agent plans a path efficiently by learning a specific rule to the new workspace, and using the generalized rules. Some computational simulations have been carried out for verifying the proposed agent.	Conference Paper	1994		[]	[]	-1	-1	-1
C	Kang, Liuwang; Shen, Haiying	A Data-Driven Optimal Control Decision-Making System for Multiple Autonomous Vehicles	2021	2021 ACM/IEEE 6TH SYMPOSIUM ON EDGE COMPUTING (SEC 2021)		192	201	With the fast development and rising popularity of autonomous vehicle (AV) technology, multiple AVs may soon be driving on the same road simultaneously. Such multi-AV coexistence driving situations will lead to new and persistent challenges. Therefore, improvements on making control decisions for multiple AVs becomes necessary for continued driving safety. In this paper, we propose a multi-AV decision making system (MADM), which considers multi-AV coexistence driving situations during the decision-making process. In MADM, we first build a policy formation method to generate policies that learn the driving behaviors of an expert based on the expert's driving trajectory data. We then develop a multi-AV decision-making method, which adjusts the formed policies through multi-agent reinforcement learning. The adjusted policies make control decisions for multiple AVs with safety guarantee. We used a real-world traffic dataset to evaluate the decision making performance of MADM in comparison with several state-of-the-art methods. Experimental results show that MADM reduces emergency rate by as high as 51% when compared with existing methods.	Proceedings Paper	2021	10.1145/3453142.3493686	[]	[]	-1	-1	-1
J	Yuan, Yali; Tasik, Robert; Adhatarao, Sripriya Srikant; Yuan, Yachao; Liu, Zheli; Fu, Xiaoming	RACE: Reinforced Cooperative Autonomous Vehicle Collision Avoidance	2020	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		9279	9291	With the rapid development of autonomous driving, collision avoidance has attracted attention from both academia and industry. Many collision avoidance strategies have emerged in recent years, but the dynamic and complex nature of driving environment poses a challenge to develop robust collision avoidance algorithms. Therefore, in this paper, we propose a decentralized framework named RACE: Reinforced Cooperative Autonomous Vehicle Collision AvoidancE. Leveraging a hierarchical architecture we develop an algorithm named Co-DDPG to efficiently train autonomous vehicles. Through a security abiding channel, the autonomous vehicles distribute their driving policies. We use the relative distances obtained by the opponent sensors to build the VANET instead of locations, which ensures the vehicle's location privacy. With a leader-follower architecture and parameter distribution, RACE accelerates the learning of optimal policies and efficiently utilizes the remaining resources. We implement the RACE framework in the widely used TORCS simulator and conduct various experiments to measure the performance of RACE. Evaluations show that RACE quickly learns optimal driving policies and effectively avoids collisions. Moreover, RACE also scales smoothly with varying number of participating vehicles. We further compared RACE with existing autonomous driving systems and show that RACE outperforms them by experiencing 65% less collisions in the training process and exhibits improved performance under varying vehicle density.	Article	SEP 2020	10.1109/TVT.2020.2974133	[]	[]	-1	-1	-1
J	Wu, Jingda; Huang, Zhiyu; Lv, Chen	Uncertainty-Aware Model-Based Reinforcement Learning: Methodology and Application in Autonomous Driving	2023	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		194	203	To further improve learning efficiency and performance of reinforcement learning (RL), a novel uncertainty-aware model-based RL method is proposed and validated in autonomous driving scenarios in this paper. First, an action-conditioned ensemble model with the capability of uncertainty assessment is established as the environment model. Then, a novel uncertainty-aware model-based RL method is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL's learning efficiency and performance. The proposed method is then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios. Validation results suggest that the proposed method outperforms the model-free RL approach with respect to learning efficiency, and model-based approach with respect to both efficiency and performance, demonstrating its feasibility and effectiveness.	Article	JAN 2023	10.1109/TIV.2022.3185159	[]	[]	-1	-1	-1
C	Kasparaviciute, Gabriele; Nielsen, Stig Anton; Boruah, Dhruv; Nordin, Peter; Dancu, Alexandru	Plastic Grabber: Underwater Autonomous Vehicle Simulation for Plastic Objects Retrieval Using Genetic Programming	2019	BUSINESS INFORMATION SYSTEMS WORKSHOPS (BIS 2018)	Lecture Notes in Business Information Processing	527	533	We propose a path planning solution using genetic programming for an autonomous underwater vehicle. Developed in ROS Simulator that is able to roam in an environment, identify a plastic object, such as bottles, grab it and retrieve it to the home base. This involves the use of a multi-objective fitness function as well as reinforcement learning, both required for the genetic programming to assess the model's behaviour. The fitness function includes not only the objective of grabbing the object but also the efficient use of stored energy. Sensors used by the robot include a depth image camera, claw and range sensors that are all simulated in ROS.	Proceedings Paper	2019	10.1007/978-3-030-04849-5_46	[]	[]	-1	-1	-1
C	Mahmud, Shohaib; Shen, Haiying; Foutz, Ying Natasha Zhang; Anton, Joshua	Reinforcement Learning Based Route And Stop Planning For Autonomous Vehicle Shuttle Service	2022	2022 IEEE 19TH INTERNATIONAL CONFERENCE ON MOBILE AD HOC AND SMART SYSTEMS (MASS 2022)		668	674	AV shuttles can be a complement to the existing bus services to fill the gap between the mobility demand of a city population and the supply of the existing bus services. In contrast to traditional approaches, this problem requires the consideration of new factors due to new objectives of the AV shuttle service such as mobility service for elderly and physically disabled persons, first/last mile transits, lessening of parking woes of the commuters, small local shop connectivity, and etc. In this work, we propose methods for route and stop planning for AV shuttle service in an urban city. We utilize a large scale human mobility dataset collected from users' cellphones' GPS sensors in Richmond city (as a city example) to identify the different potential transportation demands of AV shuttles. We propose an optimization problem-based solution and a deep reinforcement learning (RL) based solution. We conduct comprehensive evaluation based on our human mobility dataset along with real-world road network information. Our evaluation shows the proposed RL based system outperforms the comparison methods in terms of addressing the transportation needs as mentioned above by a significant margin while also minimizing the travel time on roads.	Proceedings Paper	2022	10.1109/MASS56207.2022.00098	[]	[]	-1	-1	-1
J	Yeping Hu; Nakhaei, A.; Tomizuka, M.; Fujimura, K.	Interaction-aware decision making with adaptive strategies under merging scenarios [arXiv]	2019	arXiv		8 pp.	8 pp.	In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants. Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness. Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise. Many traditional methods have been proposed to solve decision making problems under merging scenarios. However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios. In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios. A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically. A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency. An exemplar merging scenario was used to implement and examine the proposed method.	Journal Paper	12 April 2019		[]	[]	-1	-1	-1
J	Liu, Yonggang; Liu, Gang; Wu, Yitao; He, Wen; Zhang, Yuanjian; Chen, Zheng	Reinforcement-Learning-Based Decision and Control for Autonomous Vehicle at Two-Way Single-Lane Unsignalized Intersection	2022	ELECTRONICS				Intersections have attracted wide attention owing to their complexity and high rate of traffic accidents. In the process of developing L3-and-above autonomous-driving techniques, it is necessary to solve problems in autonomous driving decisions and control at intersections. In this article, a decision-and-control method based on reinforcement learning and speed prediction is proposed to manage the conjunction of straight and turning vehicles at two-way single-lane unsignalized intersections. The key position of collision avoidance in the process of confluence is determined by establishing a road-geometry model, and on this basis, the expected speed of the straight vehicle that ensures passing safety is calculated. Then, a reinforcement-learning algorithm is employed to solve the decision-control problem of the straight vehicle, and the expected speed is optimized to direct the agent to learn and converge to the planned decision. Simulations were conducted to verify the performance of the proposed method, and the results show that the proposed method can generate proper decisions for the straight vehicle to pass the intersection while guaranteeing preferable safety and traffic efficiency.	Article	APR 2022	10.3390/electronics11081203	[]	[]	-1	-1	-1
B	Bharilya, V.; Kumar, N.	A Survey of the State-of-the-Art Reinforcement Learning-Based Techniques for Autonomous Vehicle Trajectory Prediction	2023	2023 International Conference on Electrical, Electronics, Communication and Computers (ELEXCOM)		1	6	Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future trajectories of nearby traffic participants, similar to the predictive driving abilities of human drivers. Reinforcement Learning (RL) has emerged as a promising approach for learning complex decision-making policies in dynamic environments. This survey explores the application of RL approaches in trajectory prediction, focusing on inverse reinforcement learning, deep reinforcement learning, and imitation learning. It provides an in-depth analysis of the underlying principles, algorithms, and architectures employed in these methods, highlighting their respective strengths and limitations. Moreover, the survey addresses the current challenges in the field and presents potential future research directions, offering valuable insights to readers.	Conference Paper	2023	10.1109/ELEXCOM58812.2023.10370504	[]	[]	-1	-1	-1
C	Muktadir, Golam Md; Whitehead, Jim	Adversarial jaywalker modeling for simulation-based testing of Autonomous Vehicle Systems	2022	2022 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	1697	1702	We present an approach for creating adversarial jaywalkers, autonomous pedestrian models which intentionally act to create unsafe situations involving other vehicles. An adversarial jaywalker employs a hybrid state-model with social forces and state transition rules. The parameters (for social forces and state transitions) of this model are tuned via reinforcement learning to create risky situations faster with synthetic yet plausible behavior. The resulting jaywalkers are capable of realistic behavior while still engaging in sufficiently risky actions to be useful for testing. These adversarial pedestrian models are useful in a wide range of scenario-based tests for autonomous vehicles.	Proceedings Paper	2022	10.1109/IV51971.2022.9827422	[]	[]	-1	-1	-1
J	Cao, Zhong; Li, Xiang; Jiang, Kun; Zhou, Weitao; Liu, Xiaoyu; Deng, Nanshan; Yang, Diange	Autonomous Driving Policy Continual Learning With One-Shot Disengagement Case	2023	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		1380	1391	Disengagement cases during naturalistic driving are rare or even one-shot, but valuable for autonomous driving. The autonomous vehicles are necessary to continually learn from these disengagement cases, to improve the policy for better performance when next time meeting these cases. Manually adjusting the policy or adding the rules to fix these disengagement cases may cause engineering burden and may contradict other driving functions. To this end, this work proposes a continually learning agent which can automatically get improved once encountering a disengagement case. The main idea is to establish a disengagement-imagination environment, and then train the policy using imagination data for performance improvement, named disengagement-case imagination augmented continual learning (DICL). In the imagination environment, the surrounding objects are designed to first follow the recorded trajectory, and then switch to the interactive models for the policy training. The switch point is carefully designed to make the imagination contain the disengagement reasons but avoid overfitting the collected driving case. This method is evaluated by the real autonomous driving disengagement data, collected from an open-road-testing autonomous vehicle. The results show that the DICL agent can automatically learn to handle the emerging disengagement case and similar cases. This work provides a possible way to make the AV agents automatically get improvement during road testing.	Article	FEB 2023	10.1109/TIV.2022.3184729	[]	[]	-1	-1	-1
J	Israelsen, B.W.; Ahmed, N.R.; Frew, E.; Lawrence, D.; Argrow, B.	Machine Self-confidence in Autonomous Systems via Meta-analysis of Decision Processes [arXiv]	2018	arXiv		11 pp.	11 pp.	Algorithmic assurances from advanced autonomous systems assist human users in understanding, trusting, and using such systems appropriately. Designing these systems with the capacity of assessing their own capabilities is one approach to creating an algorithmic assurance. The idea of `machine self-confidence' is introduced for autonomous systems. Using a factorization based framework for self-confidence assessment, one component of self-confidence, called `solver-quality', is discussed in the context of Markov decision processes for autonomous systems. Markov decision processes underlie much of the theory of reinforcement learning, and are commonly used for planning and decision making under uncertainty in robotics and autonomous systems. A `solver quality' metric is formally defined in the context of decision making algorithms based on Markov decision processes. A method for assessing solver quality is then derived, drawing inspiration from empirical hardness models. Finally, numerical experiments for an unmanned autonomous vehicle navigation problem under different solver, parameter and environment conditions indicate that the self-confidence metric exhibits the desired properties. Discussion of results, and avenues for future investigation are included.	Journal Paper	15 Oct. 2018		[]	[]	-1	-1	-1
C	Choi, Yunho; Lee, Kyungjae; Oh, Songhwai	Distributional Deep Reinforcement Learning with a Mixture of Gaussians	2019	2019 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	9791	9797	In this paper, we propose a novel distributional reinforcement learning (RL) method which models the distribution of the sum of rewards using a mixture density network. Recently, it has been shown that modeling the randomness of the return distribution leads to better performance in Atari games and control tasks. Despite the success of the prior work, it has limitations which come from the use of a discrete distribution. First, it needs a projection step and softmax parametrization for the distribution, since it minimizes the KL divergence loss. Secondly, its performance depends on discretization hyperparameters such as the number of atoms and bounds of the support which require domain knowledge. We mitigate these problems with the proposed parameterization, a mixture of Gaussians. Furthermore, we propose a new distance metric called the Jensen-Tsallis distance, which allows the computation of the distance between two mixtures of Gaussians in a closed form. We have conducted various experiments to validate the proposed method, including Atari games and autonomous vehicle driving.	Proceedings Paper	2019	10.1109/icra.2019.8793505	[]	[]	-1	-1	-1
B	Mekala, M.S.; Zhang, H.; Park, J.H.; Jung, H.-Y.	Quantum-based Offloading Strategy for Intelligent Vehicle Network	2023	2023 IEEE 20th Consumer Communications & Networking Conference (CCNC)		987	8	Role of RSUs become essential in enhancing the service reliability rate of the end vehicle to strengthen autonomous vehicle technology. Computation-intensive services are delay-sensitive, and most existingmethodsattempted comprehensively to meet the application deadline but have not reached the expectations due to classical computing. In this regard, we design a novel offloading decision-making method based on quantum theory through reinforcement learning. Grover's algorithm is employed to select a feasible device based on cost and energy usage probability ratio. Theoretical and mathematical validations and simulation outcomes confine the impact of novel decision-making methods on the statistical constraints of the heterogeneous framework.	Conference Paper	2023	10.1109/CCNC51644.2023.10059954	[]	[]	-1	-1	-1
B	Yuqi Zheng; Ruidong Yan; Bin Jia; Rui Jiang	Feedback Forecasting based Deep Deterministic Policy Gradient Algorithm for Car-Following of Autonomous Vehicle	2021	2021 IEEE International Conference on Unmanned Systems (ICUS)		396	401	Deep reinforcement learning has been applied to the car-following control of autonomous driving in recent years. However, the safety of car-following through reinforcement learning remains a problem. To address this problem, a feedback forecasting based deep reinforcement learning algorithm is proposed. A differential equation model is used to predict the collision of adjacent vehicles. The output action of reinforcement learning is supervised by the feedback single to achieve a safe distance. As a result, the performance of autonomous driving safety is improved while others are slightly affected. The simulation results, based on NGSIM data, show that the predict collision avoidance algorithm is feasible and effective in the car-following control of autonomous driving.	Conference Paper	2021	10.1109/ICUS52573.2021.9641292	[]	[]	-1	-1	-1
C	Bai, Haoyu; Hsu, David; Lee, Wee Sun	Planning How to Learn	2013	2013 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	2853	2859	When a robot uses an imperfect system model to plan its actions, a key challenge is the exploration-exploitation trade-off between two sometimes conflicting objectives: (i) learning and improving the model, and (ii) immediate progress towards the goal, according to the current model. To address model uncertainty systematically, we propose to use Bayesian reinforcement learning and cast it as a partially observable Markov decision process (POMDP). We present a simple algorithm for offline POMDP planning in the continuous state space. Offline planning produces a POMDP policy, which can be executed efficiently online as a finite-state controller. This approach seamlessly integrates planning and learning: it incorporates learning objectives in the computed plan, which then enables the robot to learn nearly optimally online and reach the goal. We evaluated the approach in simulations on two distinct tasks, acrobot swing-up and autonomous vehicle navigation amidst pedestrians, and obtained interesting preliminary results.	Proceedings Paper	2013		[]	[]	-1	-1	-1
B	Geng, J.; Li, W.; Duan, F.	Adaptive Dynamic Programming-Based Active Disturbance Rejection Control for Autonomous Vehicle Tracking	2023	2023 IEEE International Conference on Unmanned Systems (ICUS)		1334	9	Nowadays, autonomous driving technology is more closely connected with human society, but the complex urban road environment puts higher requirements on the autonomous vehicle's robustness and fast response ability. To solve the problems mentioned above, this paper proposes a trajectory tracking control method with anti-interference and high real-time performance. The active disturbance rejection control approach is utilized to reject external disturbance and internal model uncertainties with the model predictive control controller. To speed up the computation, parameterized adaptive dynamic programming is used to estimate the optimal control sequence. A double-lane change demonstration is used to test and verify the proposed method. The experimental results show that our method improves the anti-interference ability of the system while speeding up the control speed. The proposed method can maintain the lateral error within 0.09m even with imposing 0.05rad disturbance to the input, and the single-step control time is half the traditional MPC.	Conference Paper	2023	10.1109/ICUS58632.2023.10318320	[]	[]	-1	-1	-1
J	Merton, H.; Delamore, T.; Stol, K.; Williams, H.	Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle [arXiv]	2024	Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle [arXiv]				With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack. This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following. Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation. Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed. Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following. Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.	Preprint	2024		[]	[]	-1	-1	-1
C	Kim, Taewan; Kim, H. Jin	Path Tracking Control and Identification of Tire Parameters using On-line Model-based Reinforcement Learning	2016	2016 16TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS)	International Conference on Control Automation and Systems	215	219	Path tracking control for autonomous vehicle using model predictive control (MPC) algorithm maintains maneuverability by calculating a sequence of control input which minimizes a tracking error. The weakness of this method is that the performance of MPC may decrease significantly when the priori prediction model is not accurate. Therefore, it is important to keep the vehicle stable when MPC having model error. This paper uses an on-line model-based reinforcement learning (RL) to decrease the path error by learning unknown parameters and updating a prediction model. To validate, two kinds of path tracking simulation are conducted: one is the comparison the performance between on-line model-based RL and MPC with model error. The other one is about the test when the model used in MPC and the true dynamics, which actually received input, have different tire model. The model-based RL method succeeds to learn unknown tire parameters and maintains their maneuverability in both simulations.	Proceedings Paper	2016		[]	[]	-1	-1	-1
J	Gao, Hongbo; Shi, Guanya; Xie, Guotao; Cheng, Bo	Car-following method based on inverse reinforcement learning for autonomous vehicle decision-making	2018	INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS				There are still some problems need to be solved though there are a lot of achievements in the fields of automatic driving. One of those problems is the difficulty of designing a car-following decision-making system for complex traffic conditions. In recent years, reinforcement learning shows the potential in solving sequential decision optimization problems. In this article, we establish the reward function R of each driver data based on the inverse reinforcement learning algorithm, and r visualization is carried out, and then driving characteristics and following strategies are analyzed. At last, we show the efficiency of the proposed method by simulation in a highway environment.	Article	DEC 6 2018	10.1177/1729881418817162	[]	[]	-1	-1	-1
J	Gulino, C.; Fu, J.; Luo, W.; Tucker, G.; Bronstein, E.; Lu, Y.; Harb, J.; Pan, X.; Wang, Y.; Chen, X.; Co-Reyes, J.D.; Agarwal, R.; Roelofs, R.; Lu, Y.; Montali, N.; Mougin, P.; Yang, Z.; White, B.; Faust, A.; Mcallister, R.; Anguelov, D.; Sapp, B.	Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research [arXiv]	2023	Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research [arXiv]				Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simulation requires accurate modeling of nuanced and complex multi-agent interactive behaviors. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing. Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios. It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imitation and reinforcement learning algorithms with ablation studies on different design decisions, where we highlight the effectiveness of routes as guidance for planning agents and the ability of RL to overfit against simulated agents.	Preprint	2023		[]	[]	-1	-1	-1
J	Kaloudi, Nektaria; Li, Jingyue	AST-SafeSec: Adaptive Stress Testing for Safety and Security Co-Analysis of Cyber-Physical Systems	2023	IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY		5567	5579	Cyber-physical systems are becoming more intelligent with the adoption of heterogeneous sensor networks and machine learning capabilities that deal with an increasing amount of input data. While this complexity aims to solve problems in various domains, it adds new challenges for the system assurance. One issue is the rise in the number of abnormal behaviors that affect system performance due to possible sensor faults and attacks. The combination of safety risks, which are usually caused by random sensor faults and security risks that can happen during any random system state, makes the full coverage testing of the cyber-physical system challenging. Existing techniques are inadequate to deal with complex safety and security co-risks against cyber-physical systems. In this paper, we propose AST-SafeSec, an analysis methodology for both safety and security aspects that utilizes reinforcement learning to identify the most likely adversarial paths at various normal or failure states of a cyber-physical system that can influence system behavior through its sensor data. The methodology is evaluated using an autonomous vehicle scenario by incorporating a security attack into the stochastic sensor elements of a vehicle. Evaluation results show that the methodology analyzes the interaction of malicious attacks with random faults and identifies the incident caused by the interactions and the most likely path that leads to the incident.	Article	2023	10.1109/TIFS.2023.3309160	[]	[]	-1	-1	-1
B	Chandramohan, A.; Poel, M.; Meijerink, B.; Heijenk, G.	Machine learning for cooperative driving in a multi-lane highway environment	2019	2019 Wireless Days (WD)		4 pp.	4 pp.	Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them. In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment. A driving algorithm is designed using deep Q learning, a type of reinforcement learning. In order to train and test driving algorithms, we deploy a simulated traffic system, using SUMO (Simulation of Urban Mobility). The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles. Furthermore, the impact of limited communication range and random packet loss is investigated. Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high. We propose directions for additional research to improve the performance of the algorithm.	Conference Paper	2019	10.1109/WD.2019.8734192	[]	[]	-1	-1	-1
J	Cai, Peide; Mei, Xiaodong; Tai, Lei; Sun, Yuxiang; Liu, Ming	High-Speed Autonomous Drifting With Deep Reinforcement Learning	2020	IEEE ROBOTICS AND AUTOMATION LETTERS		1247	1254	Drifting is a complicated task for autonomous vehicle control. Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely. We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic. The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed. After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map. The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.	Article	APR 2020	10.1109/LRA.2020.2967299	[]	[]	-1	-1	-1
J	Gao, Weinan; Odekunle, Adedapo; Chen, Yunfeng; Jiang, Zhong-Ping	Predictive cruise control of connected and autonomous vehicles via reinforcement learning	2019	IET CONTROL THEORY AND APPLICATIONS		2849	2855	Predictive cruise control concerns designing controllers for autonomous vehicles using the broadcasted information from the traffic lights such that the idle time around the intersection can be reduced. This study proposes a novel adaptive optimal control approach based on reinforcement learning to solve the predictive cruise control problem of a platoon of connected and autonomous vehicles. First, the reference velocity is determined for each autonomous vehicle in the platoon. Second, a data-driven adaptive optimal control algorithm is developed to estimate the gains of the desired distributed optimal controllers without the exact knowledge of system dynamics. The obtained controller is able to regulate the headway, velocity, and acceleration of each vehicle in a suboptimal sense. The goal of trip time reduction is achieved without compromising vehicle safety and passenger comfort. Numerical simulations are presented to validate the efficacy of the proposed methodology.	Article	NOV 26 2019	10.1049/iet-cta.2018.6031	[]	[]	-1	-1	-1
B	Luyao Chen; Shaorong Xie; Tao Pang; Hang Yu; Xiangfeng Luo; Zhenyu Zhang	Learning from Suboptimal Demonstration via Trajectory-Ranked Adversarial Imitation	2022	2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)		486	93	Robots trained by Imitation Learning(IL) are used in many tasks(e.g., autonomous vehicle manipulation). Generative Adversarial Imitation Learning (GAIL) assumes that the demonstration set used for training is of high quality. However, such demonstrations are difficult and expensive to obtain. GAIL-related methods fail to learn effective strategies if non-high quality demonstrations are used because the performance of agents trained by this method is limited by the demonstrator's operations. Our idea is to enable the agent to learn strategy with better performance than the demonstrator from a suboptimal demonstration set, which contains non-high quality demonstrations that are easier to obtain. Inspired by this, we propose the Trajectory-Ranked Adversarial Imitation Learning (TRAIL) method. First, for demonstration set processing, we introduce a ranking process and define the concept of Performance Relative Advantage of suboptimal demonstrations to specify the ranking order. Second, for model training, we reconstruct the objective function of GAIL and use an experience replay buffer, enabling the agent to learn implicit features and ranking information from the ranked suboptimal demonstration set and possess the ability to outperform the demonstrator. Experiments show that in Mujoco's tasks, our method can learn from a suboptimal demonstration set and can achieve better performance than baseline methods.	Conference Paper	2022	10.1109/ICTAI56018.2022.00078	[]	[]	-1	-1	-1
C	Ma, Xiaobai; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.	Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning	2018	2018 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	1665	1671	To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Gao, Hongbo; Shi, Guanya; Wang, Kelong; Xie, Guotao; Liu, Yuchao	Research on decision-making of autonomous vehicle following based on reinforcement learning method	2019	INDUSTRIAL ROBOT-THE INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH AND APPLICATION		444	452	Purpose Over the past decades, there has been significant research effort dedicated to the development of autonomous vehicles. The decision-making system, which is responsible for driving safety, is one of the most important technologies for autonomous vehicles. The purpose of this study is the use of an intensive learning method combined with car-following data by a driving simulator to obtain an explanatory learning following algorithm and establish an anthropomorphic car-following model. Design/methodology/approach This paper proposed car-following method based on reinforcement learning for autonomous vehicles decision-making. An approximator is used to approximate the value function by determining state space, action space and state transition relationship. A gradient descent method is used to solve the parameter. Findings The effect of car-following on certain driving styles is initially achieved through the simulation of step conditions. The effect of car-following initially proves that the reinforcement learning system is more adaptive to car following and that it has certain explanatory and stability based on the explicit calculation of R. Originality/value The simulation results show that the car-following method based on reinforcement learning for autonomous vehicle decision-making realizes reliable car-following decision-making and has the advantages of simple sample, small amount of data, simple algorithm and good robustness.	Article	MAY 20 2019	10.1108/IR-07-2018-0154	[]	[]	-1	-1	-1
C	Afifi, Haitham; Ramaswamy, Arunselvan; Karl, Holger	Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks	2021	IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2021)	IEEE International Conference on Communications			In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs). In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength). Second, quality of service (QoS) which is used to measure the network's performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-Q-Networks (DQN). Additionally, we compare the performance of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.	Proceedings Paper	2021	10.1109/ICC42927.2021.9500318	[]	[]	-1	-1	-1
J	Valiente, R.; Razzaghpour, M.; Toghi, B.; Shah, G.; Fallah, Y.P.	Prediction-aware and Reinforcement Learning based Altruistic Cooperative Driving [arXiv]	2022	arXiv				Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.	Journal Paper	18 Nov. 2022		[]	[]	-1	-1	-1
J	Li, Zequn; Lokhandwala, Mustafa; Al-Abbasi, Abubakr O.; Aggarwal, Vaneet; Cai, Hua	Integrating reinforcement-learning-based vehicle dispatch algorithm into agent-based modeling of autonomous taxis	2023	TRANSPORTATION				While increasing number of studies are using agent-based models to study the potential environmental, economic, and social impacts of shared autonomous vehicles, the idle vehicle dispatching in these models is often simplified to heuristic rules. Because the system performance of an autonomous taxi fleet can be significantly affected by the vehicle dispatching algorithm, refining the vehicle dispatching can help us better evaluate the potential benefits and impacts of shared autonomous vehicle systems. The recent development of reinforcement-learning-based vehicle dispatching algorithms provides opportunities to improve autonomous vehicle system modeling with efficient and scalable vehicle dispatching. This study integrates a reinforcement learning algorithm into to an agent-based simulation model of a ride hailing system. Using an autonomous taxi fleet in New York City as a case study, we compared the system performance of using the proposed Deep Q-Network (DQN) method for dispatching decision with common rule-based and heuristic dispatch algorithms from relevant literatures. The results show that (1) DQN dispatches vehicles more conservatively (with less dispatching activities and distances) but achieved similar (slightly lower) rider service level with proactive dispatch methods; and (2) DQN outperformed all other dispatch methods evaluated in this study with significantly higher dispatch efficiency, as measured by the ratio of the number of extra riders served due to dispatching to the extra fleet dispatch distance.	Article; Early Access		10.1007/s11116-023-10433-w	[]	[]	-1	-1	-1
C	D'Orazio, T; Cicirelli, G; Distante, C	A vision-based approach for learning an elementary navigation behavior	1998	INTELLIGENT ROBOTS AND COMPUTER VISION XVII: ALGORITHMS, TECHNIQUES, AND ACTIVE VISION	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)	320	326	Developing elementary behavior is the starting point for the realization of complex systems. We present a learning algorithm that realizes a simple goal-reaching behavior for an autonomous vehicle when no a-priori knowledge of the environment is provided. Information coming from a visual sensor is used to detect a general state of the system. To each state an optimal action is associated using a Q-learning algorithm. As sets of states and actions are limited, a few training trials are sufficient in simulation to learn the optimal policy, During test trials (both in simulated and real environment) fuzzy sets with membership functions are introduced to compute the state of the system and the proper action at the extent of tackling errors in state estimation due to noise in vision measures. Experimental results both in simulated and real environment are shown.	Proceedings Paper	1998	10.1117/12.325778	[]	[]	-1	-1	-1
B	Gu, J.; Li, J.; Tei, K.	A Reinforcement Learning Approach for Adaptive Covariance Tuning in the Kalman Filter	2022	2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)		1569	74	State estimation and localization for the autonomous vehicle are essential for accurate navigation and safe maneuvers. The commonly used method is Kalman filtering, but its performance is affected by the noise covariance. An inappropriate set value will decrease the estimation accuracy and even makes the filter diverge. The noise covariance estimation problem has long been considered a tough issue because there is too much uncertainty in where the noise comes from and therefore unable to model it systematically. In recent years, Deep Reinforcement Learning (DRL) has made astonishing progress and is an excellent choice for tackling the problem that cannot be solved by conventional techniques, such as parameter estimation. By finely abstracting the problem as an MDP, we can use the DRL methods to solve it without too many prior assumptions. We propose an adaptive covariance tuning method applied to the Error State Extend Kalman Filter by taking advantage of DRL, called Reinforcement Learning Aided Covariance Tuning. The preliminary experiment result indicates that our method achieves a 14.73% estimation accuracy improvement on average compared with the vanilla fixed-covariance method and bound the estimation error within 0.4 m.	Conference Paper	2022	10.1109/IMCEC55388.2022.10020019	[]	[]	-1	-1	-1
C	Gao, Weinan; Jiang, Zhong-Ping; Ozbay, Kaan	Adaptive Optimal Control of Connected Vehicles	2015	2015 10TH INTERNATIONAL WORKSHOP ON ROBOT MOTION AND CONTROL (ROMOCO)		288	293	In this paper, a data-driven non-model-based approach is proposed for the adaptive optimal control of connected vehicles, comprised of n human-driven vehicles only transmitting motional data and an autonomous vehicle in the tail receiving the broadcasted data from preceding vehicles by wireless vehicle-to-vehicle (V2V) communication devices. An optimal control problem is formulated to minimize the errors of distance and velocity and to optimize the fuel usage. By employing adaptive dynamic programming (ADP) technique, optimal controllers are obtained by online approximation for the connected vehicles without knowing the system dynamics. The effectiveness of the proposed approach is demonstrated via online learning control of the connected vehicles in two scenarios.	Proceedings Paper	2015		[]	[]	-1	-1	-1
C	Wu, Jia; Li, ZiYan	Hierarchical Joint Control for Urban Mixed-Autonomy Traffic Optimization	2020	2020 IEEE 32ND INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE (ICTAI)	Proceedings-International Conference on Tools With Artificial Intelligence	700	705	With the fast development of autonomous vehicle technologies, the vehicle fleet will be made up of a mixture of human-driven vehicles and autonomous vehicles (AVs) in the coming 20-30 years. To efficiently utilize abundant data to deal with the mixed-autonomy traffic control problem, this paper formulates and approaches the problem using a deep reinforcement learning framework (DRL). DRL is a promising data-driven approach for traffic signal control and AVs control in a large-scale grid. However, traffic control based DRL is quite challenging since the complexity of control and the large search space of the policy. To deal with these issues, we propose a hierarchical joint control framework based on prior knowledge. Specifically, traffic signals at intersections and AVs are controlled by their local controllers, set according to well-adjusted policies; while the coordination of the traffic signals at intersections and the coordination among AVs are determined by two master controllers, respectively. Thus, the control of the whole grid is handled by two master controllers. In this way, the dimension of the action space is greatly decreased and the control operates much smoother. We verify our method by implementing a series of experiments in SUMO. The numerical experiments demonstrate the potential of the mixed-autonomy traffic control, compared with traditional traffic signal systems without AVs. We also demonstrate that our method is easy to train and operates robustly.	Proceedings Paper	2020	10.1109/ICTAI50040.2020.00112	[]	[]	-1	-1	-1
J	Cao, Y.; Ivanovic, B.; Xiao, C.; Pavone, M.	Reinforcement Learning with Human Feedback for Realistic Traffic Simulation [arXiv]	2023	Reinforcement Learning with Human Feedback for Realistic Traffic Simulation [arXiv]				In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.	Preprint	2023		[]	[]	-1	-1	-1
C	Kuutti, Sampo; Fallah, Saber; Bowden, Richard	Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies	2020	2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	108	114	Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.	Proceedings Paper	2020	10.1109/icra40945.2020.9197351	[]	[]	-1	-1	-1
B	Ma, N.; Zhang, Y.; Cai, W.; Qi, H.; Zhang, T.; Fu, C.	Reinforcement Learning-Based On-Ramp Merging Decision-Making for Autonomous Vehicles	2023	2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	8	On-ramp merging is a complex and high-accidents scenario. The complexity of on-ramp merging scenario is mainly reflected in the aspects of varied merging environment and uncertain driving behavior of the surrounding vehicles, so it is a challenging scenario for autonomous vehicles. In this paper, a deep Q-learning network (DQN)-based merging decision-making method is proposed for autonomous vehicles. First, the models of vehicle dynamics and on-ramp merging scenarios are built, and the merging motivation is quantitative for designing the reinforcement learning. Second, the actions of the autonomous vehicle are defined, and a value-based DQN network for the merging decision-making is designed by simultaneously considering the driving safety, efficiency and comfort. Finally, in order to evaluate the performance of DQN-based merging decision-making method, a dynamic programming (DP)-based merging decision-making method is selected as the benchmarked method. The validation results demonstrate the autonomous vehicle by the proposed DQN-based merging decision-making method possesses good performance in safety, efficiency and comfort at on-ramp merging scenarios.	Conference Paper	2023	10.1109/CVCI59596.2023.10397432	[]	[]	-1	-1	-1
J	Emuna, Ran; Duffney, Rotem; Borowsky, Avinoam; Biess, Armin	Example-guided learning of stochastic human driving policies using deep reinforcement learning	2023	NEURAL COMPUTING & APPLICATIONS		16791	16804	Deep reinforcement learning has been successfully applied to the generation of goal-directed behavior in artificial agents. However, existing algorithms are often not designed to reproduce human-like behavior, which may be desired in many environments, such as human-robot collaborations, social robotics and autonomous vehicles. Here we introduce a model-free and easy-to-implement deep reinforcement learning approach to mimic the stochastic behavior of a human expert by learning distributions of task variables from examples. As tractable use-cases, we study static and dynamic obstacle avoidance tasks for an autonomous vehicle on a highway road in simulation (Unity). Our control algorithm receives a feedback signal from two sources: a deterministic (handcrafted) part encoding basic task goals and a stochastic (data-driven) part that incorporates human expert knowledge. Gaussian processes are used to model human state distributions and to assess the similarity between machine and human behavior. Using this generic approach, we demonstrate that the learning agent acquires human-like driving skills and can generalize to new roads and obstacle distributions unseen during training.	Article; Early Access		10.1007/s00521-022-07947-2	[]	[]	-1	-1	-1
J	Jeong-Hoon Lee; Se-Young Oh; Doo-Hyun Choi	Lateral control of an autonomous vehicle using reinforcement learning	1998	Journal of the Institute of Electronics Engineers of Korea C		76	88	While most of the research on reinforcement learning assumed a discrete control space, many of the real world control problems need to have continuous output. This can be achieved by using continuous mapping functions for the value and action functions of the reinforcement learning architecture. Two questions arise here: 1) what sort of function representation to use? and 2) how to determine the amount of noise for search in action space? An ubiquitous neural network is used here to learn the value and policy functions. Next, the reinforcement predictor that is intended to predict the next reinforcement is introduced which also determines the amount of noise to add to the controller output. The proposed reinforcement learning architecture is found to have a sound online learning control performance especially at high-speed road following of high curvature road. Both computer simulation and actual experiments on a test vehicle have been performed and their efficiency and effectiveness has been verified.	Journal Paper	Nov. 1998		[]	[]	-1	-1	-1
B	Ziyan Chen	Analysis and Self-driving Algorithm Decision Mode Design	2022	2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA).		93	7	The countries around the world are closely monitoring the legislative progress of road traffic safety and autonomous vehicles. This paper aims to focus on the innovation of autonomous vehicle algorithm and technical progress, analyzing the ethical dilemmas and finding out the most proper data counting mode. The ethical choice of autonomous driving systems will be hugely controversial around the world for a long period. The behavior of intelligent robots in moral dilemmas can be simulated using a fairly simple value-based calculating mode, which is attributed by participants to the makers of intelligent operating systems. Using the fully deterministic model, all possible paths are searched and the best path is determined using the digital computation function in this paper. The algorithm design mode of this paper is that the matrix algorithm should be adopted as the core algorithm mode, regression algorithm, reinforcement algorithm as the auxiliary algorithm modes.	Conference Paper	2022	10.1109/ICPECA53709.2022.9719038	[]	[]	-1	-1	-1
C	Oyler, Dave W.; Yildiz, Yildiray; Girard, Anouck R.; Li, Nan I.; Kolmanovsky, Ilya V.	A Game Theoretical Model of Traffic with Multiple Interacting Drivers for Use in Autonomous Vehicle Development	2016	2016 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	1705	1710	This paper describes a game theoretical model of traffic where multiple drivers interact with each other. The model is developed using hierarchical reasoning, a game theoretical model of human behavior, and reinforcement learning. It is assumed that the drivers can observe only a partial state of the traffic they are in and therefore although the environment satisfies the Markov property, it appears as non-Markovian to the drivers. Hence, each driver implicitly has to find a policy, i.e. a mapping from observations to actions, for a Partially Observable Markov Decision Process. In this paper, a computationally tractable solution to this problem is provided by employing hierarchical reasoning together with a suitable reinforcement learning algorithm. Simulation results are reported, which demonstrate that the resulting driver models provide reasonable behavior for the given traffic scenarios.	Proceedings Paper	2016		[]	[]	-1	-1	-1
J	Liu, Teng; Huang, Bing; Deng, Zejian; Wang, Hong; Tang, Xiaolin; Wang, Xiao; Cao, Dongpu	Heuristics-oriented overtaking decision making for autonomous vehicles using reinforcement learning	2020	IET ELECTRICAL SYSTEMS IN TRANSPORTATION		417	424	This study presents a three-lane highway overtaking strategy for an automated vehicle, which is based on a heuristic planning reinforcement learning algorithm. The proposed decision-making controller focuses on keeping the autonomous vehicle operating safely and efficiently. First, the modelling of the overtaking driving scenario is introduced and the reference approaches named intelligent driver model and minimise overall braking induced by lane changes are formulated. Second, the Dyna-H algorithm, which combines the modified Q-learning algorithm with a heuristic planning policy, is utilised for highway overtaking decision-making. Three different heuristic strategies are formulated to improve learning efficiency and compare performance. This algorithm is applied to determine the lane change and speed selection for an ego vehicle in the environment with uncertainties. Finally, the performance of Dyna-H is estimated in the autonomous overtaking scenario by comparing it with the reference and traditional learning methods. Furthermore, the Dyna-H-enabled decision-making strategies are validated and analysed in an open-sourcing driving dataset. Results prove that the proposed decision-making strategy could produce superior performance in convergence rate and control.	Article; Proceedings Paper	DEC 2020	10.1049/iet-est.2020.0044	[]	[]	-1	-1	-1
C	Testi, Enrico; Favarelli, Elia; Giorgetti, Andrea	Reinforcement Learning for Connected Autonomous Vehicle Localization via UAVs	2020	PROCEEDINGS OF 2020 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR AGRICULTURE AND FORESTRY (METROAGRIFOR)		13	17	In precision farming, a very promising scenario is represented by a connected and autonomous vehicle (CAV) moving in a cultivated field and collecting high-resolution videos and hyperspectral images, requiring both localization and broadband communication. An effective approach to provide both localization and wideband communication exploits unmanned aerial vehicles (UAVs) that may act as relays to ensure seamless connectivity with a base station (BS). In this paper, we propose a reinforcement learning (RL)-based algorithm to find the best spatial configuration of a swarm of UAVs to localize a CAV in an unknown environment and assist the communication with a BS. The UAVs cooperate to estimate the position of the CAV exploiting only the received signal strength (RSS). A reward function, based on the distance between the UAVs and the CAV, and the estimated geometric diluition of precision (GDOP), is designed. Numerical results show how the proposed multi-agent Q-learning allows the UAVs to reach low root mean square error (RMSE) in the target localization, even without previous knowledge about the environment.	Proceedings Paper	2020	10.1109/metroagrifor50201.2020.9277630	[]	[]	-1	-1	-1
J	Chen, Xue-Mei; Xu, Shu-Yuan; Wang, Zi-Jia; Zheng, Xue-Long; Han, Xin-Tong; Liu, En-Hao	A Decision-Making Model for Autonomous Vehicles at Intersections Based on Hierarchical Reinforcement Learning	2023	UNMANNED SYSTEMS				By aiming at addressing the left-turning problem of an autonomous vehicle considering the oncoming vehicles at an urban unsignallized intersection, a hierarchical reinforcement learning is proposed and a two-layer model is established to study behaviors of left-turning driving. Compared with the conventional decision-making models with a fixed path, the proposed multi-paths decision-making algorithm with horizontal and vertical strategies can improve the efficiency of autonomous vehicles crossing intersections while ensuring safety.	Article; Early Access		10.1142/S2301385024500122	[]	[]	-1	-1	-1
B	Sadiq, M.F.; Tasneem, N.; Ahmed, M.; Sultan, S.M.; Hasan, S.	Towards Mitigating Probable Road Mishaps through DQN Based Deep Reinforcement Learning	2021	2021 IEEE 3rd Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)		222	5	Road accident is a widespread problem in many urban areas nowadays. Reckless Driving (Over-Speed) is one of the attributes leading to the enormous destruction of pedestrian life. Autonomous vehicle speed control is an important research area in the Intelligent Transportation System (ITS) to mitigate road accidents in densely populated areas. Many existed works utilized Reinforcement Learning (RL) to control the vehicle speed intelligently. However, controlling the over-speed of the vehicle using RL under the speed limit of the different zones (i.e., Town, Rural Main Road, Highways, and so forth) is also another demanding task because of the speed limit variation. Therefore, we propose a Deep-Q-Network (DQN) based Deep Reinforcement Learning (Deep RL) method to overcome the over-speed problem in various speed limit areas. In this paper, the proposed system will reduce the vehicle speed intelligently if the driver passes over the speed limit in a particular zone. The simulation results show that our proposed solution can successfully reduce the vehicle speed under various zone-based speed limits.	Conference Paper	2021	10.1109/ECBIOS51820.2021.9510732	[]	[]	-1	-1	-1
J	Cao Jie; Shao Zixuan; Hou Liang	Research on autonomous vehicle U-turn problem based on hierarchical reinforcement learning	2022	Application Research of Computers		3008	3012,3045	The U-turn task is one of the contents of autonomous driving research,and most of the solutions under the standard roads in cities cannot be implemented on non-standard roads.Aiming at solving this problem,this paper established a vehicle U-turn dynamical model and designed a multi-scale convolutional neural network to extract feature maps as the input of the agent.In addition,for the sparse reward problem in the U-turn task,this paper proposed a hierarchical proximal policy optimization algorithm that combined hierarchical reinforcement learning and proximal policy optimization algorithm.In experiments with simple and complex scena-rios,this algorithm learns policies faster and has a higher success rate of U-turn compared to other algorithms.	Article	2022		[]	[]	-1	-1	-1
J	[Anonymous]	REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES, 53-57. SI	2023	Mechatronic Systems and Control		5 pp.	5 pp.	Reinforcement learning (RL) is one of the most emerging domains of artificial intelligence. It is widely used in almost all sort of applications, including medical field, stock market, forecasting, and engineering field. One of the most effective uses of RL is in the control engineering domain owing to its learning by trial, for example, in building autonomous system like autonomous vehicle and robotics. In this paper, we have focused on the applications of RL in various control engineering problems. Stability of controller (or agent) using RL paradigm is a very crucial task due to exploration-exploitation policy used by any RL. Also, the unavailability of exact model of system or environment may lead to unsafe behaviour of the agent. Therefore, this paper focuses mainly on the stability aspect in RL-based controller. Many concepts are used to study and analyse the stability of a system viz Lyapunov method and Barrier function. This paper surveys the detailed application of these well-established stability certifier methods in various model-free and model-based RL framework.	Journal Paper	2023	10.2316/Journal.201.2023.1.201-0347	[]	[]	-1	-1	-1
C	Clemmons, Joseph; Jin, Yu-Fang	Reinforcement Learning-Based Guidance of Autonomous Vehicles	2023	2023 24TH INTERNATIONAL SYMPOSIUM ON QUALITY ELECTRONIC DESIGN, ISQED	International Symposium on Quality Electronic Design	496	501	Reinforcement learning (RL) has attracted significant research efforts to guide an autonomous vehicle (AV) for a collision-free path due to its advantages in investigating interactions among multiple vehicles and dynamic environments. This study deploys a Deep Q-Network (DQN) based RL algorithm with reward shaping to control an ego AV in an environment with multiple vehicles. Specifically, the state space of the RL algorithm depends on the desired destination, the ego vehicle's location and orientation, and the location of other vehicles in the system. The training time of the proposed RL algorithm is much shorter than most current image-based algorithms. The RL algorithm also provides an extendable framework to include a varying number of vehicles in the environment and can be easily adapted to different maps without changing the setup of the RL algorithm. Three scenarios were simulated in the Cars Learn to Act (CARLA) simulator to examine the effects of the proposed RL algorithm on guiding the ego AV interacting with multiple vehicles on straight and curvy roads. Our results showed that the ego AV could learn to reach its destination within 5000 episodes for all scenarios tested.	Proceedings Paper	2023	10.1109/ISQED57927.2023.10129362	[]	[]	-1	-1	-1
J	Kicki, Piotr; Gawron, Tomasz; Cwian, Krzysztof; Ozay, Mete; Skrzypczynski, Piotr	Learning from experience for rapid generation of local car maneuvers	2021	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE				Being able to rapidly respond to the changing scenes and traffic situations by generating feasible local paths is of pivotal importance for car autonomy. We propose to train a deep neural network (DNN) to plan feasible and nearly-optimal paths for kinematically constrained vehicles in a small constant time. Our DNN model is trained using a novel weakly supervised approach and a gradient-based policy search. On real and simulated scenes and a large set of local planning problems, we demonstrate that our approach outperforms the existing planners with respect to the number of successfully completed tasks. While the path generation time is about 40 ms, the generated paths are smooth and comparable to those obtained from conventional path planners.	Article; Early Access		10.1016/j.engappai.2021.104399	[]	[]	-1	-1	-1
C	Huang, Wenhui; Braghin, Francesco; Wang, Zhuo	Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning	2019	2019 IEEE 31ST INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE (ICTAI 2019)	Proceedings-International Conference on Tools With Artificial Intelligence	1536	1540	With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.	Proceedings Paper	2019	10.1109/ICTAI.2019.00220	[]	[]	-1	-1	-1
J	Xiang, J.; Guo, L.	Comfort Improvement for Autonomous Vehicles Using Reinforcement Learning with In-Situ Human Feedback	2022	SAE Technical Papers		2022	01-0807	In this paper, a reinforcement learning-based method is proposed to adapt autonomous vehicle passengers' expectation of comfort through in-situ human-vehicle interaction. Ride comfort has a significant influence on the user's experience and thus acceptance of autonomous vehicles. There is plenty of research about the motion planning and control of autonomous vehicles. However, limited studies have explicitly considered the comfort of passengers in autonomous vehicles. This paper studies the comfort of humans in autonomous vehicles longitudinal autonomous driving. The paper models and then improves passengers' feelings about autonomous driving behaviors. This proposed approach builds a control and adaptation strategy based on reinforcement learning using human's in-situ feedback on autonomous driving. It also proposes an adaptation of humans to autonomous vehicles to account for improper human driving expectations. The proposed approaches are implemented and tested with human-in-the-loop experiments and the results demonstrate that the proposed approaches can successfully adapt the vehicle behaviors, improve the ride comfort of humans in autonomous vehicles, and also correct improper human driving expectations.	Conference Paper; Journal Paper	2022	10.4271/2022-01-0807	[]	[]	-1	-1	-1
R	Lu, Chengjie; Shi, Yize; Zhang, Huihui; Zhang, Man; Wang, Tiexin; Yue, Tao; Ali, Shaukat	DeepCollision: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions	2022	Zenodo				With the aim to test autonomous driving systems, we propose a novel reinforcement learning (RL)-based approach named DeepCollision to learn operating environment configurations of autonomous vehicles, including formalizing environment configuration learning as an MDP and adopting DQN algorithm as the RL solution;DeepCollisionlearns environment configurations to maximize collisions of an Autonomous Vehicle Under Test (AVUT). This dataset contains: algorithms- The algorithm of DeepCollision, which includes the network architecture and the DQN hyperparameter settings; pilot-study- All the raw data and plots for the pilot study; formal-experiment- A dataset contains all the raw data for analysis and the scenarios with detailed demand values; rest-api- The REST API endpoints for environment configuration and oneexampleto show the usage of the APIs. Copyright: Creative Commons Attribution 4.0 International Open Access	Data set	2022-09-06	https://doi.org/10.5281/ZENODO.5906633	[]	[]	-1	-1	-1
C	Ding, Guohui; Aghli, Sina; Heckman, Christoffer; Chen, Lijun	Game-Theoretic Cooperative Lane Changing Using Data-Driven Models	2018	2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	3640	3647	Self-driving vehicles are being increasingly deployed in the wild. One of the most important next hurdles for autonomous driving is how such vehicles will optimally interact with one another and with their surroundings. In this paper, we consider the lane changing problem that is fundamental to road-bound multi-vehicle systems, and approach it through a combination of deep reinforcement learning (DRL) and game theory. We introduce a proactive-passive lane changing framework and formulate the lane changing problem as a Markov game between the proactive and passive vehicles. Based on different approaches to carry out DRL to solve the Markov game, we propose an asynchronous lane changing scheme as in a single-agent RL setting and a synchronous cooperative lane changing scheme that takes into consideration the adaptive behavior of the other vehicle in a vehicle's decision. Experimental results show that the synchronous scheme can effectively create and find proper merging moment after sufficient training. The framework and solution developed here demonstrate the potential of using reinforcement learning to solve multi-agent autonomous vehicle tasks such as the lane changing as they are formulated as Markov games.	Proceedings Paper	2018		[]	[]	-1	-1	-1
B	Abdalkarim, Mahmoud	Using V2X and Reinforcement Learning to Improve Autonomous Vehicles Algorithms with CARLA	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Manjanna, S.; van Hoof, H.; Dudek, G.	Reinforcement learning with non-uniform state representations for adaptive search [arXiv]	2019	arXiv		11 pp.	11 pp.	Efficient spatial exploration is a key aspect of search and rescue. In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher. This should allow an autonomous vehicle find one or more lost targets as rapidly as possible. We do this by performing non-uniform sampling of the search region. The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning. We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target. Key features of our search algorithm are the ability to employ a very general non-deterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions. One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics. [doi:10.1109/SSRR.2018.8468649].	Journal Paper	15 June 2019		[]	[]	-1	-1	-1
B	Ha, V.T.; Tu, T.N.; Dung, N.T.; Mien, T.L.; Thuy, C.T.	Deep Q-Network (DQN) Approach for Automatic Vehicles Applied in the Intelligent Transportation System (ITS)	2023	2023 International Conference on System Science and Engineering (ICSSE)		527	32	This paper presents the design of an intelligent controller applying reinforcement learning using a deep Q-network (DQN) algorithm for autonomous vehicles. The deep Q-network (DQN) algorithm is an online, model-free reinforcement learning approach. A DQN agent is a value-based reinforcement learning agent that teaches a critic to predict future rewards or returns. Deep Q-network is to replace the action-state Q table with a neural network. This solution applies to building a self-propelled agent capable of correcting static and moving obstacles according to the physical environment. As a result, the autonomous vehicle can move and avoid collisions with obstacles. The correctness of the theory is demonstrated through MATLAB simulation.	Conference Paper	2023	10.1109/ICSSE58758.2023.10227206	[]	[]	-1	-1	-1
B	Zhou, Jinkai	Autonomous Vehicle Fleet Operations and Planning With User Activity Scheduling Constraints	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Nageshrao, S.; Tseng, E.; Filev, D.	Autonomous Highway Driving using Deep Reinforcement Learning [arXiv]	2019	arXiv		8 pp.	8 pp.	"The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. This may lead to a scenario that was not postulated in the design phase. Due to this, formulating a rule based decision maker for selecting maneuvers may not be ideal. Similarly, it may not be effective to design an a-priori cost function and then solve the optimal control problem in real-time. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with simulated traffic. The decision maker for AV is implemented as a deep neural network providing an action choice for a given system state. In a critical application such as driving, an RL agent without explicit notion of safety may not converge or it may need extremely large number of samples before finding a reliable policy. To best address the issue, this paper incorporates reinforcement learning with an additional short horizon safety check (SC). In a critical scenario, the safety check will also provide an alternate safe action to the agent provided if it exists. This leads to two novel contributions. First, it generalizes the states that could lead to undesirable ""near-misses"" or ""collisions "". Second, inclusion of safety check can provide a safe and stable training environment. This significantly enhances learning efficiency without inhibiting meaningful exploration to ensure safe and optimal learned behavior. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density in a highway setting."	Journal Paper	29 March 2019		[]	[]	-1	-1	-1
J	Yang, Jui-An; Kuo, Chung-Hsien	Integrating Vehicle Positioning and Path Tracking Practices for an Autonomous Vehicle Prototype in Campus Environment	2021	ELECTRONICS				This paper presents the implementation of an autonomous electric vehicle (EV) project in the National Taiwan University of Science and Technology (NTUST) campus in Taiwan. The aim of this work was to integrate two important practices of realizing an autonomous vehicle in a campus environment, including vehicle positioning and path tracking. Such a project is helpful to the students to learn and practice key technologies of autonomous vehicles conveniently. Therefore, a laboratory-made EV was equipped with real-time kinematic GPS (RTK-GPS) to provide centimeter position accuracy. Furthermore, the model predictive control (MPC) was proposed to perform the path tracking capability. Nevertheless, the RTK-GPS exhibited some robust positioning concerns in practical application, such as a low update rate, signal obstruction, signal drift, and network instability. To solve this problem, a multisensory fusion approach using an unscented Kalman filter (UKF) was utilized to improve the vehicle positioning performance by further considering an inertial measurement unit (IMU) and wheel odometry. On the other hand, the model predictive control (MPC) is usually used to control autonomous EVs. However, the determination of MPC parameters is a challenging task. Hence, reinforcement learning (RL) was utilized to generalize the pre-trained datum value for the determination of MPC parameters in practice. To evaluate the performance of the RL-based MPC, software simulations using MATLAB and a laboratory-made, full-scale electric vehicle were arranged for experiments and validation. In a 199.27 m campus loop path, the estimated travel distance error was 0.82% in terms of UKF. The MPC parameters generated by RL also achieved a better tracking performance with 0.227 m RMSE in path tracking experiments, and they also achieved a better tracking performance when compared to that of human-tuned MPC parameters.	Article	NOV 2021	10.3390/electronics10212703	[]	[]	-1	-1	-1
B	Tianshu Chu; Kalabic, U.	Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoon	2019	2019 IEEE 58th Conference on Decision and Control (CDC)		4079	84	This paper proposes a model-based deep reinforcement learning (DRL) algorithm for cooperative adaptive cruise control (CACC) of connected vehicles. Differing from most existing CACC works, we consider a platoon consisting of both human-driven and autonomous vehicles. The humandriven vehicles are heterogeneous and connected via vehicleto-vehicle (V2V) communication and the autonomous vehicles are controlled by a cloud-based centralized DRL controller via vehicle-to-cloud (V2C) communication. To overcome the safety and robustness issues of RL, the algorithm informs lowerlevel controllers of desired headway signals instead of directly controlling vehicle accelerations. The lower-level behavior is modeled according to the optimal velocity model (OVM), which determines vehicle acceleration according to a headway input. Numerical experiments show that the model-based DRL algorithm outperforms its model-free version in both safety and stability of CACC. Furthermore, we study the impact of different penetration ratios of autonomous vehicles on the safety, stability, and optimality of the CACC policy.	Conference Paper	2019	10.1109/CDC40024.2019.9030110	[]	[]	-1	-1	-1
B	Hossain, J.; Faridee, A.-Z.; Roy, N.; Basak, A.; Asher, D.E.	CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning	2023	2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)		127	32	Autonomous navigation in off-road environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an under-explored area. In this paper, we propose CoverNav, a novel Deep Reinforcement Learning (DRL) based algorithm, for identifying covert and navigable trajectories with minimal cost in off-road terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low-cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL-based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state-of-the-art (SOTA) methods without compromising accuracy.	Conference Paper	2023	10.1109/ACSOS58161.2023.00030	[]	[]	-1	-1	-1
J	Prathiba, Sahaya Beni; Raja, Gunasekaran; Kumar, Neeraj	Intelligent Cooperative Collision Avoidance at Overtaking and Lane Changing Maneuver in 6G-V2X Communications	2022	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		112	122	The rapid growth in Autonomous Vehicle (AV) technology endeavors increased attention towards road safety in recent days. Particularly, a higher number of road accidents occurs when the AV tries to overtake or change the lane. To cut down the number of accidents and improve traffic reliability, the AV should be capable of making intelligent decisions and communicating those to other AVs. Therefore, in this paper, a Cooperative Collision avoidance scheme for AVs at Overtaking and Lane Changing maneuver (CCAV-OLC) is proposed. The Inverse Reinforcement Learning (IRL) in the CCAV-OLC scheme, processes on the given number of expert demonstrations for automatically acquiring the reward function, and thereby imitating actual human driving strategy and decisions. However, the adaptability of IRL to a high-dimensional AV environment restricts the performance of the CCAV-OLC scheme. To overcome this, the IRL in CCAV-OLC leverages the Gaussian Process (GP) regression model (IRL-GP), which enables data-efficient Bayesian prediction even when the number of demonstrations is very low. After taking intelligent decisions in overtaking and lane changing maneuvers, the AVs cooperatively communicate and exchange the decisions with each other by 6th Generation Vehicle-to-Everything (6G-V2X) communications, which further improves the accuracy and lessens the time taken for making optimal decisions. The experimental results show that the AVs clone the expert's optimal driving strategy and avoid the collisions to a greater extent.	Article	JAN 2022	10.1109/TVT.2021.3127219	[]	[]	-1	-1	-1
B	Shoaraee, H.; Liang Chen; Fan Jiang	Decision-Making of an Autonomous Vehicle when Approached by an Emergency Vehicle using Deep Reinforcement Learning	2021	2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)		185	91	Autonomous Vehicles (AVs) are the future of road transportation which can increase safety, efficiency, and productivity. Decision-making of AVs in a highway environment with different goals like overtaking, staying in a lane, and merging have been the focus of many studies. In this study, we want to address a new edge case in autonomous driving when the AV (ego) needs to make the best lateral and longitudinal decisions when approached by an emergency vehicle (emg). To achieve the desired behavior and learn the sequence decision process, we trained ego with the help of Deep Reinforcement Learning (DRL) algorithms and compared the results with rule-based algorithms. We proposed two neural networks as function approximators that help the ego to learn the optimum actions. The driving environment for this problem was developed by using Simulation Urban Mobility (SUMO) as an open-source traffic simulator. We will show our proposed solution based on the DRL outperforming the rule-based solution and demonstrate that it has a decent performance both in normal driving situations and when an emergency vehicle is approaching.	Conference Paper	2021	10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00041	[]	[]	-1	-1	-1
J	Chen, Jing; Zhao, Cong; Jiang, Shengchuan; Zhang, Xinyuan; Li, Zhongxin; Du, Yuchuan	Safe, Efficient, and Comfortable Autonomous Driving Based on Cooperative Vehicle Infrastructure System	2023	INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH				Traffic crashes, heavy congestion, and discomfort often occur on rough pavements due to human drivers' imperfect decision-making for vehicle control. Autonomous vehicles (AVs) will flood onto urban roads to replace human drivers and improve driving performance in the near future. With the development of the cooperative vehicle infrastructure system (CVIS), multi-source road and traffic information can be collected by onboard or roadside sensors and integrated into a cloud. The information is updated and used for decision-making in real-time. This study proposes an intelligent speed control approach for AVs in CVISs using deep reinforcement learning (DRL) to improve safety, efficiency, and ride comfort. First, the irregular and fluctuating road profiles of rough pavements are represented by maximum comfortable speeds on segments via vertical comfort evaluation. A DRL-based speed control model is then designed to learn safe, efficient, and comfortable car-following behavior based on road and traffic information. Specifically, the model is trained and tested in a stochastic environment using data sampled from 1341 car-following events collected in California and 110 rough pavements detected in Shanghai. The experimental results show that the DRL-based speed control model can improve computational efficiency, driving efficiency, longitudinal comfort, and vertical comfort in cars by 93.47%, 26.99%, 58.33%, and 6.05%, respectively, compared to a model predictive control-based adaptive cruise control. The results indicate that the proposed intelligent speed control approach for AVs is effective on rough pavements and has excellent potential for practical application.	Article	JAN 2023	10.3390/ijerph20010893	[]	[]	-1	-1	-1
J	Kunming Li; Mao Shan; Narula, K.; Worrall, S.; Nebot, E.	Socially Aware Crowd Navigation with Multimodal Pedestrian Trajectory Prediction for Autonomous Vehicles [arXiv]	2020	arXiv		8 pp.	8 pp.	Seamlessly operating an autonomous vehicle in a crowded pedestrian environment is a very challenging task. This is because human movement and interactions are very hard to predict in such environments. Recent work has demonstrated that reinforcement learning-based methods have the ability to learn to drive in crowds. However, these methods can have very poor performance due to inaccurate predictions of the pedestrians' future state as human motion prediction has a large variance. To overcome this problem, we propose a new method, SARL-SGAN-KCE, that combines a deep socially aware attentive value network with a human multimodal trajectory prediction model to help identify the optimal driving policy. We also introduce a novel technique to extend the discrete action space with minimal additional computational requirements. The kinematic constraints of the vehicle are also considered to ensure smooth and safe trajectories. We evaluate our method against the state of art methods for crowd navigation and provide an ablation study to show that our method is safer and closer to human behaviour.	Journal Paper	22 Nov. 2020		[]	[]	-1	-1	-1
C	Sama, Kyle; Morales, Yoichi; Akai, Naoki; Takeuchi, Eijiro; Takeda, Kazuya	Learning How to Drive in Blind Intersections from Human Data	2018	2018 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC)	IEEE International Conference on Systems Man and Cybernetics Conference Proceedings	317	324	In this paper we present a method to learn how to drive in different types of blind intersections using expert driving data. We cluster different intersections based on the velocity of how drivers approach them, and train a linear SVM classifier for each class of intersection. Through clustering we found that there were three different classes of intersections in typical residential areas in Japan. We used inverse reinforcement learning (IRL) to build a driving model for each type of intersection. The models were trained from 308 trajectories traversed by 5 different drivers. The models and policies were implemented and evaluated in a ROS simulator where the agent is provided a global path, and upon it reaching an intersection, it selects the appropriate trained policy. By doing this, the simulated autonomous vehicle can perform proactive safe driving behaviors when approaching blind intersections.	Proceedings Paper	2018	10.1109/SMC.2018.00064	[]	[]	-1	-1	-1
J	Abeywickrama, Dhaminda B.; Griffiths, Nathan; Xu, Zhou; Mouzakitis, Alex	Emergence of norms in interactions with complex rewards	2023	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS				Autonomous agents are becoming increasingly ubiquitous and are playing an increasing role in wide range of safety-critical systems, such as driverless cars, exploration robots and unmanned aerial vehicles. These agents operate in highly dynamic and heterogeneous environments, resulting in complex behaviour and interactions. Therefore, the need arises to model and understand more complex and nuanced agent interactions than have previously been studied. In this paper, we propose a novel agent-based modelling approach to investigating norm emergence, in which such interactions can be investigated. To this end, while there may be an ideal set of optimally compatible actions there are also combinations that have positive rewards and are also compatible. Our approach provides a step towards identifying the conditions under which globally compatible norms are likely to emerge in the context of complex rewards. Our model is illustrated using the motivating example of self-driving cars, and we present the scenario of an autonomous vehicle performing a left-turn at a T-intersection.	Article	JUN 2023	10.1007/s10458-022-09585-3	[]	[]	-1	-1	-1
B	G, A.; Hemalatha, R.; N, S.; Kavitha, K.R.; Tamilselvi, M.; Kumar, A.K.	MNNP: Design and Development of Traffic Sign Identification and Recognition System to Support Smart Vehicles using Modified Neural Network Principles	2023	2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)		1	7	In recent times, the acceleration of automotive and computer vision technologies has sparked significant interest in autonomous vehicles. These vehicles' capacity to operate securely and effectively hinges on their adeptness at accurately identifying traffic signs. Consequently, traffic sign recognition has emerged as a pivotal element within autonomous driving systems. Scientists have been hard at work looking at various approaches to accomplishing this recognition, mostly drawing on machine learning and deep learning techniques. To address this issue, we offer a Modified Neural Network approach that combines the Haar cascade algorithm with a MobileNet model classifier. Our developed model is rigorously trained on the GTSRB dataset and then put to the test on a wide variety of traffic sign types. The culmination of our efforts yields a remarkable testing accuracy rate of 97.25%. This finding highlights the possible effectiveness of our technique in improving autonomously vehicle traffic sign identification, which in turn helps to improve the security and effectiveness of autonomous driving systems.	Conference Paper	2023	10.1109/RMKMATE59243.2023.10369847	[]	[]	-1	-1	-1
J	Mosharafian, Sahand; Afzali, Shirin; Mohammadpour Velni, Javad	Leveraging autonomous vehicles in mixed-autonomy traffic networks with reinforcement learning-controlled intersections	2023	TRANSPORTATION LETTERS-THE INTERNATIONAL JOURNAL OF TRANSPORTATION RESEARCH		1218	1229	Development of new approaches to adaptive traffic signal control has received significant attention; an example is the reinforcement learning (RL), where training and implementation of an RL agent can allow adaptive signal control in real time, considering the agent's past experiences. Furthermore, autonomous vehicle (AV) technology has shown promise to enhancing the traffic mobility at highways and intersections. In this paper, delayed action deep Q-learning is developed for a vehicle network with signalized intersections to control the signal phase. A model predictive control (MPC) scheme is proposed to allow AVs to adapt their speed. Several case studies that consider mixed autonomy are examined aiming at reducing network traffic and fuel consumption in the traffic network with multiple intersections. Simulation studies reveal that even with a few AVs in the network, the waiting time, fuel consumption, and the number of stop-and-go movements are significantly reduced, while the travel time is increased.	Article; Early Access		10.1080/19427867.2022.2146302	[]	[]	-1	-1	-1
B	Zhang, Y.; Zhang, S.; Liang, Z.; Li, H.L.; Wu, H.; Liu, Q.	Dynamical Driving Interactions between Human and Mentalizing-designed Autonomous Vehicle	2022	2022 IEEE International Conference on Development and Learning (ICDL)		178	83	Autonomous vehicle (AV) is progressing rapidly, but there are still many shortcomings when interacting with humans. To address this problem, it is necessary to study the human behaviors in human-AV interactions, and build a predictive model of human decision-making in the interaction. In turn, modelling human behavior in human-AV interaction can help us better understand human perception of AVs and human driving strategies. In this work, we first train multi-level AV agents using reinforcement learning (RL) models to imitate three mentalizing levels (i.e., level-0, level-1, and level-2), and then design a human-AV driving task that subjects interact with each level of AV agents in a two-lane merging scenario. Both human and AV driving behaviors are recorded. We found that conservative subjects obtain more rewards because of the randomness of the RL agents. Our results indicate that (i) human driving strategies are flexible and changeable, which allows to quickly adjust the strategy to maximize the reward when gaming against AV; (ii) human driving strategies are related to mentalizing ability, and subjects with higher mentalizing scores drive more conservatively. Our study shed lights on the relationship between human driving policy and mentalizing in human-AV interactions, and it can inspire the next-generation AV.	Conference Paper	2022	10.1109/ICDL53763.2022.9962198	[]	[]	-1	-1	-1
C	Soni, Himanshu; Gupta, Vishu; Kumar, Rajesh	Motion Planning using Reinforcement Learning for Electric Vehicle Battery Optimization(EVBO)	2019	2019 INTERNATIONAL CONFERENCE ON POWER ELECTRONICS, CONTROL AND AUTOMATION (ICPECA-2019)		11	16	The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning. Motion planning is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour. In this paper, reinforcement learning techniques are explored for the motion planning of an electric vehicle(EV) while optimizing battery consumption. The EV travel time has also been evaluated under different reinforcement learning schemes. A traffic simulation network is developed for a high-trunk zone of Jaipur city using Simulation for Urban Mobility(SUMO) software. Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network. The results show that value iteration and q-learning have shown improved battery consumption. However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.	Proceedings Paper	2019	10.1109/icpeca47973.2019.8975684	[]	[]	-1	-1	-1
C	Salvato, Erica; Ghosh, Arnob; Fenu, Gianfranco; Parisini, Thomas	Control of a Mixed Autonomy Signalised Urban Intersection: An Action-Delayed Reinforcement Learning Approach	2021	2021 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	2042	2047	We consider a mixed autonomy scenario where the traffic intersection controller decides whether the traffic light will be green or red at each lane for multiple traffic-light blocks. The objective of the traffic intersection controller is to minimize the queue length at each lane and maximize the outflow of vehicles over each block. We consider that the traffic intersection controller informs the autonomous vehicle (AV) whether the traffic light will be green or red for the future traffic-light block. Thus, the AV can adapt its dynamics by solving an optimal control problem. We model the decision process of the traffic intersection controller as a deterministic delayed Markov decision process owing to the delayed action by the traffic controller. We propose Reinforcement Learning based model-free algorithm to obtain the optimal policy. We show - by extensive simulations - that our algorithm converges and drastically reduces the energy costs of AVs as the traffic controller communicates with the AVs.	Proceedings Paper	2021	10.1109/ITSC48978.2021.9564983	[]	[]	-1	-1	-1
B	Siddique, U.; Sinha, A.; Cao, Y.	On Deep Reinforcement Learning for Target Capture Autonomous Guidance	2024	AIAA SCITECH 2024 Forum		0957	0957	This paper explores the prospects of motion planning of autonomous vehicles using deep reinforcement learning (DRL). We are particularly interested in a goal-directed setting where the need is to design an optimal guidance strategy for a pursuing autonomous vehicle (the pursuer, which is also the DRL agent) to capture an adversary (the target). To this end, we first formulate the target capture guidance problem as a Markov Decision Process (MDP) wherein the kinematics of relative motion between the vehicles constitute the MDP, and the pursuer's lateral acceleration (chosen as its steering control to account for turn constraints) is the action of DRL agent. We show that a multifaceted reward function motivated by the collision conditions is sufficient and effective in designing the reinforcement learning action that enables the pursuer to capture the target regardless of the latter's motion. We then empirically evaluate the performance of the trained agent in various target capture scenarios.	Conference Paper	2024	10.2514/6.2024-0957	[]	[]	-1	-1	-1
B	Auxilius jude, M.J.; Diniesh, V.C.; K, P.K.; S, R.; S, N.K.; E n, S.	An Improved Retransmission Timeout Forecasting Algorithm for Vehicular Networks	2022	2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT).		5 pp.	5 pp.	The vehicular relay network remains a promising path of future intelligent transportation systems (ITS) that connects the autonomous vehicle with the internet for better traffic control and information sharing. The transmission control protocol (TCP) remains the backbone of data traffic on the internet as it harbors an enormous portion of global internet traffic between end devices. In VANET, TCP experiences drastic performance degradation during early or spurious RTO timeouts. This paper proposes a recursive learning retransmission timeout (RL-RTO), which efficiently reduces spurious timeouts and enhances fast recovery after timeouts. The RL-RTO forecast timer value is based on the three control parameters. The performance of RL-RTO is validated against recent RTO approaches under multi-hop vehicular environments. The proposed RL-RTO considerably regulates estimation errors and improves variance.	Conference Paper	2022	10.1109/ICAECT54875.2022.9807985	[]	[]	-1	-1	-1
J	Kamel, Ahmed; Sayed, Tarek; Fu, Chuanyun	Real-time safety analysis using autonomous vehicle data: a Bayesian hierarchical extreme value model	2023	TRANSPORTMETRICA B-TRANSPORT DYNAMICS		826	846	This study proposes an approach for real-time road network safety analysis using autonomous vehicles (AVs) generated data. The approach utilises a Bayesian hierarchical spatial random parameter extreme value model (BHSRP). The model simultaneously addresses the scarcity and non-stationarity of conflict extremes and unobserved spatial heterogeneity. Two real-time safety metrics are estimated: the risk of crash (RC) and return level (RL). The RC and RL were applied to three months AVs data for evaluating the real-time safety level of an urban corridor in Palo Alto, California. The indicator time to collision (TTC) was used to characterise traffic conflicts. The conflict extreme was defined as the maxima of negated TTC in a 20-min interval (block). The results show that RC can differentiate the block-level risk level, while RL can reflect safety levels among blocks. For the RC, the hot (crash risk prone) segments and intersections are associated with more severe conflict frequency.	Article; Early Access		10.1080/21680566.2022.2135634	[]	[]	-1	-1	-1
B	Wan, A.D.M.; Braspenning, P.J.	Computational design principles for multiple autonomous vehicle organization	1995	28th International Symposium on Automotive Technology and Automation. Proceedings for the Dedicated Conference on Robotics, Motion and Machine Vision in the Automotive Industries		383	94	In distributed artificial intelligence (AI), a bifurcation in design principles can be observed. On the one hand there seems to be a resurgence of the `classical' AI approach of bestowing agents with human-like intelligent capacities and on the other hand there is an anti-representational school emerging. Classical AI has had to endure severe criticisms while anti-representational approaches fall short on providing a programme for developing and implementing higher-order intelligent functions. The approach taken in this paper is a middle-out strategy between these extremes and describes the associated principles of autonomous agent design including rigorous adaptive capacities based on reinforcement learning and possibilities of integration with higher-order intelligent functions. An outline of an architecture is presented that enables multiple autonomous vehicles (mobile agents), if equipped with it, to communicate and to coordinate thereby achieving a synthesis between the logico-linguistic and the reactive approaches.	Conference Paper	1995		[]	[]	-1	-1	-1
J	Ottoni, Andre L. C.; Nepomuceno, Erivelton G.; Oliveira, Marcos S. de; Oliveira, Daniela C. R. de	Reinforcement learning for the traveling salesman problem with refueling	2022	COMPLEX & INTELLIGENT SYSTEMS		2001	2015	The traveling salesman problem (TSP) is one of the best-known combinatorial optimization problems. Many methods derived from TSP have been applied to study autonomous vehicle route planning with fuel constraints. Nevertheless, less attention has been paid to reinforcement learning (RL) as a potential method to solve refueling problems. This paper employs RL to solve the traveling salesman problem With refueling (TSPWR). The technique proposes a model (actions, states, reinforcements) and RL-TSPWR algorithm. Focus is given on the analysis of RL parameters and on the refueling influence in route learning optimization of fuel cost. Two RL algorithms: Q-learning and SARSA are compared. In addition, RL parameter estimation is performed by Response Surface Methodology, Analysis of Variance and Tukey Test. The proposed method achieves the best solution in 15 out of 16 case studies.	Article; Early Access		10.1007/s40747-021-00444-4	[]	[]	-1	-1	-1
J	Razzaq, Waleed; Hongwei, Mo	Neural Circuit Policies Imposing Visual Perceptual Autonomy	2023	NEURAL PROCESSING LETTERS		9101	9116	We presented a hybrid deep learning architecture by combining biological brain-inspired Neural Circuit Policies with convolutional neural architecture. This allows the agent to learn key coherent features from various environments, express generalizability and interpret dynamics. We found that a combination of 12 inter-neurons and 8 command neurons can effectively perform high-stakes decision-making tasks such as autonomous vehicles. Our architecture not only has 305 times fewer trainable parameters than competing approaches but shows superior performance, robustness, and noise resiliency in famous autonomous vehicle simulations i.e. OpenAI-CarRacing and Udacity Simulator.	Article; Early Access		10.1007/s11063-023-11194-4	[]	[]	-1	-1	-1
J	Kim, Hyunkun; Pyeon, Hyeongoo; Park, Jong Sool; Hwang, Jin Young; Lim, Sejoon	Autonomous Vehicle Fuel Economy Optimization with Deep Reinforcement Learning	2020	ELECTRONICS				The ever-increasing number of vehicles on the road puts pressure on car manufacturers to make their car fuel-efficient. With autonomous vehicles, we can find new strategies to optimize fuels. We propose a reinforcement learning algorithm that trains deep neural networks to generate a fuel-efficient velocity profile for autonomous vehicles given road altitude information for the planned trip. Using a highly accurate industry-accepted fuel economy simulation program, we train our deep neural network model. We developed a technique for adapting the heterogeneous simulation program on top of an open-source deep learning framework, and reduced dimension of the problem output with suitable parameterization to train the neural network much faster. The learned model combined with reinforcement learning-based strategy generation effectively generated the velocity profile so that autonomous vehicles can follow to control itself in a fuel efficient way. We evaluate our algorithm's performance using the fuel economy simulation program for various altitude profiles. We also demonstrate that our method can teach neural networks to generate useful strategies to increase fuel economy even on unseen roads. Our method improved fuel economy by 8% compared to a simple grid search approach.	Article	NOV 2020	10.3390/electronics9111911	[]	[]	-1	-1	-1
C	Hester, Todd; Quinlan, Michael; Stone, Peter	RTMBA: A Real-Time Model-Based Reinforcement Learning Architecture for Robot Control	2012	2012 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	85	90	Reinforcement Learning (RL) is a paradigm for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. Existing model-based RL methods learn in relatively few samples, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes in a novel way such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.	Proceedings Paper	2012		[]	[]	-1	-1	-1
J	Zhiqian Qiao; Schneider, J.; Dolan, J.M.	Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning [arXiv]	2020	arXiv		7 pp.	7 pp.	For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but for some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.	Journal Paper	9 Nov. 2020		[]	[]	-1	-1	-1
C	Chen, Wei-Lun; Lee, Kwan-Hung; Hsiung, Pao-Ann	Intersection Crossing for Autonomous Vehicles based on Deep Reinforcement Learning	2019	2019 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN (ICCE-TW)	IEEE International Conference on Consumer Electronics-Taiwan			Future intersection crossings for autonomous vehicles will not be controlled by traffic signals, rather a controller will be used for communication among vehicles that need to cross an intersection. In this work, we propose an innovative management system called Deep Reinforcement Learning-based Autonomous Intersection Management (DRLAIM) system, which is the first system to use deep reinforcement learning. We train the system to learn a good intersection control policy by interacting with traffic environment through reinforcement learning. The brake-safe control model is used to ensure the safety of each autonomous vehicle while crossing. Experiment results show that after training using reinforcement learning, the throughput of intersection control model increased by 83%. In comparison with the Fast First Service (FFS) policy, the average waiting time of DRLAIM reduced by about 1.2% to 11.4%.	Proceedings Paper	2019	10.1109/icce-tw46550.2019.8991738	[]	[]	-1	-1	-1
J	Wang, Kezhi; Wang, Liang; Pan, Cunhua; Ren, Hong	Deep Reinforcement Learning-Based Resource Management for Flexible Mobile Edge Computing Architectures, Applications, and Research Issues	2022	IEEE VEHICULAR TECHNOLOGY MAGAZINE		85	93		Article; Early Access		10.1109/MVT.2022.3156745	[]	[]	-1	-1	-1
C	Ibn-Khedher, Hatem; Laroui, Mohammed; Ben Mabrouk, Mouna; Moungla, Hassine; Afifi, Hossam; Oleari, Alberto Nai; Kamal, Ahmed E.	Edge Computing Assisted Autonomous Driving Using Artificial Intelligence	2021	IWCMC 2021: 2021 17TH INTERNATIONAL WIRELESS COMMUNICATIONS & MOBILE COMPUTING CONFERENCE (IWCMC)	International Wireless Communications and Mobile Computing Conference	254	259	The emergence of new vehicles generation such as connected and autonomous vehicles led to new challenges in the vehicular networking and computing managements to provide efficient services and guarantee the quality of service. The edge computing facility allows the decentralization of processing from the cloud to the edge of the network. In this paper, we design and propose an end-to-end, reliable and low latency communication architecture that allows the allocation of compute-intensive autonomous driving services, in particular autopilot, to shared resources on edge computing servers and improve the level of performance for autonomous vehicles. The reference architecture is used to design an Advanced Autonomous Driving (AAD) communication protocol between autonomous vehicles, edge computing servers, and the centralized cloud. Then, a mathematical programming approach using Integer Linear Programming (ILP) is formulated to model the autopilot chain resources Offloading at the network edge. Further, a deep reinforcement learning (DRL) approach is proposed to deal with dense Internet of Autonomous Vehicle (IoAV) networks. Moreover, several scenarios are considered to quantify the behavior of the optimization approaches. We compare their efficiency in terms of Total Edge Servers Utilization, Total Edge Servers Allocation Time, and Successfully Allocated Edge Autopilots.	Proceedings Paper	2021	10.1109/IWCMC51323.2021.9498627	[]	[]	-1	-1	-1
B	Nian, G.; Xiao, J.; Zhou, X.	Dueling DQN-Rollout for Collision Avoidance Path Planning with Vehicle Speed Location	2023	2023 3rd International Symposium on Computer Technology and Information Science (ISCTIS)		552	7	The rapid progress of artificial intelligence has led to significant advancements in the field of autonomous driving, yet effective collision avoidance path planning remains a challenging task. In response, deep reinforcement learning offers an efficient and modern alternative to traditional navigation strategies. This paper proposes a novel approach that incorporates vehicle speed location into the deep reinforcement learning process, utilizing the Dueling DQN-Rollout framework to consider both the distance of the road and obstacles ahead. The agent interacts with the environment to learn a policy, with a reward function that accounts for deviations from the intended path and collisions with obstacles. The training process focuses on imparting human-like driving skills to the autonomous vehicle. By employing the rollout algorithm, the rough Q-value is optimized to reduce training costs. Experimental results demonstrate that this approach can successfully plan a collision-free path for autonomous driving from origin to destination on a simulation platform.	Conference Paper	2023	10.1109/ISCTIS58954.2023.10213163	[]	[]	-1	-1	-1
B	Yang, Q.; Li, G.; Li, G.; Liu, Y.	Trajectory Tracking Control of Autonomous Vehicles Based on Reinforcement Learning and Curvature Feedforward	2022	2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	6	This paper proposes a trajectory tracking algorithm with adaptive parameter PID (Proportional-Integral-Derivative) controller based on reinforcement learning and curvature feedforward controller. This paper uses the deep reinforcement learning PPO algorithm (Proximal Policy Optimization) to adjust the parameters of the PID controller online, which is used for the lateral control of the autonomous vehicle. In addition, the maximum policy entropy is introduced based on PPO algorithm to enhance the exploration of policy. And a curvature feedforward controller is added, which will limit the speed according to the curvature of the road ahead, to avoid the problem that the vehicle is difficult to control when turning due to excessive speed. The training results show that compared with the traditional PPO algorithm, this method can get more rewards in the later stage of training. The simulation results show that this algorithm can reduce the average track error, reduce the overshoot and oscillation, and improve the tracking accuracy and robustness.	Conference Paper	2022	10.1109/CVCI56766.2022.9964583	[]	[]	-1	-1	-1
C	Xu, Zhi; Liu, Shuncheng; Wu, Ziniu; Chen, Xu; Zeng, Kai; Zheng, Kai; Su, Han	PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning	2021	PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021		2271	2280	The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution. It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks. The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e. harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane. This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles. Specifically, we propose a velocity control framework, called PATROL (sPAtial-Temporal ReinfOrcement Learning). First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g. velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment. Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time. At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB. We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments. Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.	Proceedings Paper	2021	10.1145/3459637.3482283	[]	[]	-1	-1	-1
C	Gonzalez-Miranda, Oscar; Miranda, Luis Antonio Lopez; Ibarra-Zannatha, Juan Manuel	Q-Learning for autonomous vehicle navigation	2023	2023 XXV ROBOTICS MEXICAN CONGRESS, COMROB		138	142	In this work, we proposed and developed a reinforcement Q-learning method to do the lane-keeping and obstacle evasion driving maneuvers. We detail how to design a simple car simulator and how to use it to do the training. For each problem, we define different states, actions, and reward functions to obtain a Q-table. Next, we use it as a driving maneuver controller in a different simulation environment. With this method, our car successfully droves on a road different to where it was training. An important conclusion is the possibility to build, more complex controllers to do passing or behavior selectors.	Proceedings Paper	2023	10.1109/COMROB60035.2023.10349747	[]	[]	-1	-1	-1
J	Drungilas, Darius; Kurmis, Mindaugas; Senulis, Audrius; Lukosius, Zydrunas; Andziulis, Arunas; Januteniene, Jolanta; Bogdevicius, Marijonas; Jankunas, Valdas; Voznak, Miroslav	Deep reinforcement learning based optimization of automated guided vehicle time and energy consumption in a container terminal	2023	ALEXANDRIA ENGINEERING JOURNAL		397	407	The energy efficiency of port container terminal equipment and the reduction of CO2 emissions are among one of the biggest challenges facing every seaport in the world. The article pre-sents the modeling of the container transportation process in a terminal from the quay crane to the stack using battery-powered Automated Guided Vehicle (AGV) to estimate the energy consump-tion parameters. An AGV speed control algorithm based on Deep Reinforcement Learning (DRL) is proposed to optimize the energy consumption of container transportation. The results obtained and compared with real transportation measurements showed that the proposed DRL-based approach dynamically changing the driving speed of the AGV reduces energy consumption by 4.6%. The obtained results of the research provide the prerequisites for further research in order to find optimal strategies for autonomous vehicle movement including context awareness and infor-mation sharing with other vehicles in the terminal.(c) 2022 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Engineering, Alexandria University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/ licenses/by-nc-nd/4.0/).	Article; Early Access		10.1016/j.aej.2022.12.057	[]	[]	-1	-1	-1
J	Feher, Arpad; Aradi, Szilard; Becsi, Tamas	Online Trajectory Planning with Reinforcement Learning for Pedestrian Avoidance	2022	ELECTRONICS				Planning the optimal trajectory of emergency avoidance maneuvers for highly automated vehicles is a complex task with many challenges. The algorithm needs to decrease accident risk by reducing the severity and keeping the car in a controllable state. Optimal trajectory generation considering all aspects of vehicle and environment dynamics is numerically complex, especially if the object to be avoided is moving. This paper presents a hierarchical method for the avoidance of moving objects in an autonomous vehicle, where a reinforcement learning agent is responsible for local planning, while longitudinal and lateral control is performed by the low-level model-predictive controller and Stanley controllers. In the developed architecture, the agent is responsible for the optimization. It is trained in various scenarios to provide the necessary parameters for a polynomial-based path and a velocity profile in a neural network output. The vehicle performs only the first step of the trajectory, which is redesigned repeatedly by the planner based on the new state. In the training phase, the vehicle executes the entire trajectory via low-level controllers to determine the reward value, which realizes a prediction for the future. The agent receives feedback and can further improve its performance. Finally, the proposed framework was tested in a simulation environment and was also compared to human drivers' abilities.	Article	AUG 2022	10.3390/electronics11152346	[]	[]	-1	-1	-1
C	Khaitan, Shivesh; Dolan, John M.	State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections	2022	2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	12219	12224	Traversing intersections is a challenging problem for autonomous vehicles, especially when the intersections do not have traffic control. Recently deep reinforcement learning has received massive attention due to its success in dealing with autonomous driving tasks. In this work, we address the problem of traversing unsignalized intersections using a novel curriculum for deep reinforcement learning. The proposed curriculum leads to: 1) A faster training process for the reinforcement learning agent, and 2) Better performance compared to an agent trained without curriculum. Our main contribution is two-fold: 1) Presenting a unique curriculum for training deep reinforcement learning agents, and 2) demonstrating the performance improvement using the proposed curriculum in the unsignalized intersection traversal task. The framework expects processed observations of the surroundings from the perception system of the autonomous vehicle. We test our method in the CommonRoad motion planning simulator on T-intersections and four-way intersections.	Proceedings Paper	2022	10.1109/IROS47612.2022.9981109	[]	[]	-1	-1	-1
C	Guo, Youtian; Gao, Qi; Pan, Fengan	Trained Model Reuse of Autonomous-Driving in Pygame with Deep Reinforcement Learning	2020	PROCEEDINGS OF THE 39TH CHINESE CONTROL CONFERENCE	Chinese Control Conference	5660	5664	Autonomous-Driving technology has begun to bring great convenience to daily trip, transportation, and surveying harsh environment. Considering that deep reinforcement learning has requirements for the convergence performance of the training results, and the actual training results sometimes cannot converge steadily or fail to reach the training goals, in this paper, the trained model reuse method was proposed, which can use the trained model generates Q(S-t, A(t)) and can he used as a part of Deep Reinforcement Learning model, and this model was built based on the value function that could predict the Q value corresponding to the various actions performed in the environment state. In the Pygame platform, a simplified traffic simulation environment was set, it is observed that the Autonomous-Driving vehicle could run smoothly without collision in a fixed-length test simulation environment, and this trained model reuse method could help autonomous vehicle accelerate the learning process, obtain better simulation results during most of the training process, save simulation time and computing resources.	Proceedings Paper	2020		[]	[]	-1	-1	-1
B	Cui, Z.; Li, M.; Huang, Y.; Wang, Y.; Chen, H.	An interpretation framework for autonomous vehicles decision-making via SHAP and RF	2022	2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	7	Decision-making for autonomous vehicles is critical to achieving safe and efficient autonomous driving. In recent years, deep reinforcement learning (DRL) techniques have emerged as the most promising way to enable intelligent decision-making. However, DRL with 'black box' nature is not widely understood by humans, thus hindering their social acceptance. In this paper, we combine SHapley Additive exPlanation (SHAP) and random forest (RF) techniques to bring transparency to decision-making obtained by DRL. Specifically, we first implement decision-making of autonomous vehicles following in discrete action space based on DRL algorithm with the goal of safety and efficiency. Then we use the SHAP technique to simplify the feature space, which shows that relative distance, longitudinal speed of the ego vehicle, and longitudinal speed of the proceeding vehicle have a critical impact on vehicle following task. Finally, we collect the state-action pairs generated by the DRL model and perform feature filtering, and fit the decision model with an interpretable RF model. The simulation results show that the RF model achieves the behavioral explanation of autonomous vehicle following.	Conference Paper	2022	10.1109/CVCI56766.2022.9964561	[]	[]	-1	-1	-1
J	He, Rui; Lv, Haipeng; Zhang, Sumin; Zhang, Dong; Zhang, Hang	Lane Following Method Based on Improved DDPG Algorithm	2021	SENSORS				In an autonomous vehicle, the lane following algorithm is an important component, which is a basic function of autonomous driving. However, the existing lane following system has a few shortcomings: first, the control method it adopts requires an accurate system model, and different vehicles have different parameters, which needs a lot of parameter calibration work. The second is that it may fail on road sections where the lateral acceleration requirements of vehicles are large, such as large curves. Third, its decision-making system is defined based on rules, which has disadvantages: it is difficult to formulate; human subjective factors cannot guarantee objectivity; coverage is difficult to guarantee. In recent years, the deep deterministic policy gradient (DDPG) algorithm has been widely used in the field of autonomous driving due to its strong nonlinear fitting ability and generalization performance. However, the DDPG algorithm has overestimated state action values and large cumulative errors, low training efficiency and other issues. Therefore, this paper improves the DDPG algorithm based on the double critic networks and priority experience replay mechanism. Then this paper proposes a lane following method based on this algorithm. Experiment shows that the algorithm can achieve excellent following results under various road conditions.	Article	JUL 2021	10.3390/s21144827	[]	[]	-1	-1	-1
C	Jafari, Rouhollah; Ashari, Alireza Esna; Huber, Marcus	CHAMP: Integrated Logic with Reinforcement Learning for Hybrid Decision Making for Autonomous Vehicle Planning	2023	2023 AMERICAN CONTROL CONFERENCE, ACC	Proceedings of the American Control Conference	3310	3315	A Cognitive Hybrid Autonomous Motion Planner (CHAMP) is developed for autonomous driving applications in challenging driving scenarios. The proposed hybrid planner unifies a hierarchical rule-based decision-making architecture with Reinforcement Learning (RL). For challenging intersection scenarios, RL agents are trained to replace a subset of the rules in the logical planner. The hybrid planner is systematically tested and benchmarked to demonstrate its effectiveness in handling challenging road scenario with congested and chaotic traffic conditions.	Proceedings Paper	2023		[]	[]	-1	-1	-1
C	Boada, MJL; Salichs, MA	Visual tracking skill reinforcement learning for a mobile robot	2002	INTELLIGENT AUTONOMOUS VEHICLES 2001	IFAC PROCEEDINGS SERIES	173	178	In this paper, we implement a reinforcement learning algorithm that allows an autonomous mobile robot, with a single CCD camera mounted on a pan-tilt platform, to learn simple skills such as watch and orientation, and to obtain the skill called approach combining the learned skills previously. The results obtained show the advantages of 1) decomposing complex skills in simpler skills due to the learning rate improves, 2) using the reinforcement as a learning mechanism because of the possibility to learn on-line without the robot having a previous knowledge of what it has to do for a given situation. We also present the neural network architecture used for the learning mechanism implementation. Copyright (C) 2001 IFAC.	Proceedings Paper	2002		[]	[]	-1	-1	-1
J	Zihui Zhu; Zhengming Zhang; Wen Yan; Yongming Huang; Luxi Yang	Proactive caching in auto driving scene via deep reinforcement learning	2019	2019 11th International Conference on Wireless Communications and Signal Processing (WCSP)		6 pp.	6 pp.	The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry. The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction. In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network. First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively. Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy. Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.	Conference Paper	2019	10.1109/WCSP.2019.8928131	[]	[]	-1	-1	-1
B	Wen Fu; Yanjie Li; Zhaohui Ye; Qi Liu	Decision Making for Autonomous Driving Via Multimodal Transformer and Deep Reinforcement Learning*	2022	2022 IEEE International Conference on Real-time Computing and Robotics (RCAR)		481	6	On the basis of environmental information processed by the sensing module, the decision module in automatic driving integrates environmental and vehicle information to make the autonomous vehicle produce safe and reasonable driving behavior. Considering the complexity and variability of the driving environment of autonomous vehicles, researchers have begun to apply deep reinforcement learning (DRL) in the study of autonomous driving control strategies in recent years. In this paper, we apply an algorithm framework combining multimodal transformer and DRL to solve the autonomous driving decision problem in complex scenarios. We use ResNet and transformer to extract the features of LiDAR point cloud and image. We use Deep Deterministic Policy Gradient (DDPG) algorithm to complete the subsequent autonomous driving decision-making task. And we use information bottleneck to improve the sampling efficiency of RL. We use CARLA simulator to evaluate our approach. The results show that our approach allows the agent to learn better driving strategies.	Conference Paper	2022	10.1109/RCAR54675.2022.9872180	[]	[]	-1	-1	-1
B	Shengduo Chen; Yaowei Sun; Dachuan Li; Qiang Wang; Qi Hao; Sifakis, J.	Runtime Safety Assurance for Learning-enabled Control of Autonomous Driving Vehicles	2022	2022 International Conference on Robotics and Automation (ICRA)		8978	84	Providing safety guarantees for Autonomous Vehicle (AV) systems with machine-learning based controllers remains a challenging issue. In this work, we propose Simplex-Drive, a framework that can achieve runtime safety assurance for machine-learning enabled controllers of AVs. The proposed Simplex-Drive consists of an unverified Deep Reinforcement Learning (DRL)-based advanced controller (AC) that achieves desirable performance in complex scenarios, a Velocity-Obstacle (VO) based baseline safe controller (BC) with provably safety guarantees, and a verified mode management unit that monitors the operation status and switches the control authority between AC and BC based on safety-related conditions. We provide a formal correctness proof of Simplex-Drive and conduct a lane-changing case study in dense traffic scenarios. The simulation experiment results demonstrate that Simplex-Drive can always ensure the operation safety without sacrificing control performance, even if the DRL policy may lead to deviations from the safe status.	Conference Paper	2022	10.1109/ICRA46639.2022.9812177	[]	[]	-1	-1	-1
J	Folkers, A.; Rick, M.; Buskens, C.	Controlling an Autonomous Vehicle with Deep Reinforcement Learning [arXiv]	2019	arXiv		7 pp.	7 pp.	We present a control approach for autonomous vehicles based on deep reinforcement learning. A neural network agent is trained to map its estimated state to acceleration and steering commands given the objective of reaching a specific target state while considering detected obstacles. Learning is performed using state-of-the-art proximal policy optimization in combination with a simulated environment. Training from scratch takes five to nine hours. The resulting agent is evaluated within simulation and subsequently applied to control a full-size research vehicle. For this, the autonomous exploration of a parking lot is considered, including turning maneuvers and obstacle avoidance. Altogether, this work is among the first examples to successfully apply deep reinforcement learning to a real vehicle. [doi:10.1109/IVS.2019.8814124].	Journal Paper	24 Sept. 2019		[]	[]	-1	-1	-1
B	Soni, H.; Gupta, V.; Kumar, R.	Motion Planning using Reinforcement Learning for Electric Vehicle Battery optimization(EVBO)	2019	2019 International Conference on Power Electronics, Control and Automation (ICPECA). Proceedings		6 pp.	6 pp.	The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning. Motion planning is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour. In this paper, reinforcement learning techniques are explored for the motion planning of an electric vehicle(EV) while optimizing battery consumption. The EV travel time has also been evaluated under different reinforcement learning schemes. A traffic simulation network is developed for a high-traffic zone of Jaipur city using Simulation for Urban Mobility(SUMO) software. Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network. The results show that value iteration and q-learning have shown improved battery consumption. However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.	Conference Paper	2019	10.1109/ICPECA47973.2019.8975684	[]	[]	-1	-1	-1
C	Gu, Ziqing; Yang, Yujie; Duan, Jingliang; Li, Shengbo Eben; Chen, Jianyu; Cao, Wenhan; Zheng, Sifa	Belief state separated reinforcement learning for autonomous vehicle decision making under uncertainty	2021	2021 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	586	592	In autonomous driving, the ego vehicle and its surrounding traffic environments always have uncertainties like parameter and structural errors, behavior randomness of road users, etc. Furthermore, environmental sensors are noisy or even biased. This problem can be formulated as a partially observable Markov decision process. Existing methods lack a good representation of historical information, making it very challenging to find an optimal policy. This paper proposes a belief state separated reinforcement learning (RL) algorithm for decision-making of autonomous driving in uncertain environments. We extend the separation principle from linear Gaussian systems to general nonlinear stochastic environments, where the belief state, defined as the posterior distribution of the true state, is found to be a sufficient statistic of historical information. This belief state is estimated by action-enhanced variational inference from historical information and is proved to satisfy the Markovian property, thus allowing us to obtain the optimal policy using traditional RL algorithms for Markov decision processes. The policy gradient of a task-specific prior model is mixed with that of the interaction data to improve learning performance. The proposed algorithm is evaluated in a multi-lane autonomous driving task, where the surrounding vehicles are subject to behavior uncertainty and observation noise. The simulation results show that compared with existing RL algorithms, the proposed method can achieve a higher average return with better driving performance.	Proceedings Paper	2021	10.1109/ITSC48978.2021.9564576	[]	[]	-1	-1	-1
C	Lin, Yuan; McPhee, John; Azad, Nasser L.	Longitudinal Dynamic versus Kinematic Models for Car-Following Control Using Deep Reinforcement Learning	2019	2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	1504	1510	The majority of current studies on autonomous vehicle control via deep reinforcement learning (DRL) utilize point-mass kinematic models, neglecting vehicle dynamics which includes acceleration delay and acceleration command dynamics. The acceleration delay, which results from sensing and actuation delays, results in delayed execution of the control inputs. The acceleration command dynamics dictates that the actual vehicle acceleration does not rise up to the desired command acceleration instantaneously due to dynamics. In this work, we investigate the feasibility of applying DRL controllers trained using vehicle kinematic models to more realistic driving control with vehicle dynamics. We consider a particular longitudinal car-following control, i.e., Adaptive Cruise Control (ACC), problem solved via DRL using a point-mass kinematic model. When such a controller is applied to car following with vehicle dynamics, we observe significantly degraded car-following performance. Therefore, we redesign the DRL framework to accommodate the acceleration delay and acceleration command dynamics by adding the delayed control inputs and the actual vehicle acceleration to the reinforcement learning environment state, respectively. The training results show that the redesigned DRL controller results in near-optimal control performance of car following with vehicle dynamics considered when compared with dynamic programming solutions.	Proceedings Paper	2019		[]	[]	-1	-1	-1
J	Jiqian Dong; Sikai Chen; Ha, P.Y.J.; Yujie Li; Labi, S.	A DRL-based Multiagent Cooperative Control Framework for CAV Networks: a Graphic Convolution Q Network [arXiv]	2020	arXiv		20 pp.	20 pp.	Connected Autonomous Vehicle (CAV) Network can be defined as a collection of CAVs operating at different locations on a multilane corridor, which provides a platform to facilitate the dissemination of operational information as well as control instructions. Cooperation is crucial in CAV operating systems since it can greatly enhance operation in terms of safety and mobility, and high-level cooperation between CAVs can be expected by jointly plan and control within CAV network. However, due to the highly dynamic and combinatory nature such as dynamic number of agents (CAVs) and exponentially growing joint action space in a multiagent driving task, achieving cooperative control is NP hard and cannot be governed by any simple rule-based methods. In addition, existing literature contains abundant information on autonomous driving's sensing technology and control logic but relatively little guidance on how to fuse the information acquired from collaborative sensing and build decision processor on top of fused information. In this paper, a novel Deep Reinforcement Learning (DRL) based approach combining Graphic Convolution Neural Network (GCN) and Deep Q Network (DQN), namely Graphic Convolution Q network (GCQ) is proposed as the information fusion module and decision processor. The proposed model can aggregate the information acquired from collaborative sensing and output safe and cooperative lane changing decisions for multiple CAVs so that individual intention can be satisfied even under a highly dynamic and partially observed mixed traffic. The proposed algorithm can be deployed on centralized control infrastructures such as road-side units (RSU) or cloud platforms to improve the CAV operation.	Journal Paper	11 Oct. 2020		[]	[]	-1	-1	-1
J	Dai, X; Li, CK; Rad, AB	An approach to tune fuzzy controllers based on reinforcement learning for autonomous vehicle control	2005	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		285	293	In this paper, we suggest a new approach for tuning parameters of fuzzy controllers based on reinforcement learning. The architecture of the proposed approach is comprised of a Q estimator network (QEN) and a Takagi-Sugeno-type fuzzy inference system (TSK-FIS). Unlike other fuzzy Q-learning approaches that select an optimal action based on finite discrete actions, the proposed controller obtains the control output directly from TSK-FIS. With the proposed architecture, the learning algorithms for all the parameters of the QEN and the FIS are developed based on the temporal-difference (TD) methods as well as the gradient-descent algorithm. The performance of the proposed design technique is illustrated by simulation studies of a vehicle longitudinal-control system.	Article	SEP 2005	10.1109/TITS.2005.853698	[]	[]	-1	-1	-1
J	Hwang, Seulbin; Lee, Kibeom; Jeon, Hyeongseok; Kum, Dongsuk	Autonomous Vehicle Cut-In Algorithm for Lane-Merging Scenarios via Policy-Based Reinforcement Learning Nested Within Finite-State Machine	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		17594	17606	Lane-merging scenarios pose highly challenging problems for autonomous vehicles due to conflicts of interest between the human-driven and cutting-in autonomous vehicles. Such conflicts become severe when traffic increases, and cut-in algorithms suffer from a steep trade-off between safety and cut-in performance. In this study, a reinforcement learning (RL)-based cut-in policy network nested within a finite state machine (FSM)--which is a high-level decision maker, is proposed to achieve high cut-in performance without sacrificing safety. This FSM-RL hybrid approach is proposed to obtain 1) a strategic and adjustable algorithm, 2) optimal safety and cut-in performance, and 3) robust and consistent performance. In the high-level decision making algorithm, the FSM provides a framework for four cut-in phases (ready for safe gap selection, gap approach, negotiation, and lane-change execution) and handles the transitions between these phases by calculating the collision risks associated with target vehicles. For the lane-change phase, a policy-based deep-RL approach with a soft actor-critic network is employed to get optimal cut-in performance. The results of simulations show that the proposed FSM-RL cut-in algorithm consistently achieves a high cut-in success rate without sacrificing safety. In particular, as the traffic increases, the cut-in success rate and safety are significantly improved over existing optimized rule-based cut-in algorithms and end-to-end RL algorithm.	Article; Early Access		10.1109/TITS.2022.3153848	[]	[]	-1	-1	-1
C	Wang, X. N.; Xu, X.; Wu, T.; He, H. G.; Zhang, M.	Hybrid reinforcement learning combined with SVMs and its applications	2006	DYNAMICS OF CONTINUOUS DISCRETE AND IMPULSIVE SYSTEMS-SERIES B-APPLICATIONS & ALGORITHMS		3740	3745	A new learning control method with hybrid learning strategies is presented. The learning controller combines reinforcement learning (RL) with support vector machines (SVMs) so that the performance of the learning controller can be optimized not only by a priori knowledge from human experts but also by online reinforcement learning. In the controller, supervised learning based on SVMs is employed to construct the initial policy of reinforcement learning and a policy gradient reinforcement learning algorithm is implemented to make use of the initial policy for online optimization. Thus, the learning control approach has three advantages: (1) Prior knowledge can be easily used only by providing training examples to SVM-based supervised learning. (2) The controller performance can be optimized using online RL to compensate unknown disturbances. (3) The controller structure is determined by SVMs, which is data-driven, not predefined. We tested the performance of the learning controller on several path tracking tasks of a four-wheeled autonomous vehicle. The simulation results demonstrate the effectiveness of the learning controller.	Proceedings Paper	DEC 2006		[]	[]	-1	-1	-1
C	Hejase, Bilal; Yurtsever, Ekim; Han, Teawon; Singh, Baljeet; Filev, Dimitar P.; Tseng, H. Eric; Ozguner, Umit	Dynamic and Interpretable State Representation for Deep Reinforcement Learning in Automated Driving	2022	IFAC PAPERSONLINE		129	134	Understanding the causal relationship between an autonomous vehicle's input state and its output action is important for safety mitigation and explainable automated driving. However, reinforcement learning approaches have the drawback of being black box models. This work proposes an interpretable state representation that can capture state-action causalities for an automated driving agent, while also allowing for the underlying formulation to be general enough to be adapted to different driving scenarios. It also proposes encoding temporally-extended information in the state representation for better driving performance. We test this approach on a reinforcement learning agent in a highway simulation environment and demonstrate that the proposed state representation can capture state-action causalities in an interpretable manner. Experimental results show that the formulation and interpretation can be used to adapt the behavior of the driving agent to achieve desired, even unseen, driving behaviors after training. Copyright (c) 2022 The Authors. This is an open access article under the CC BY-NC-ND license	Proceedings Paper	2022	10.1016/j.ifaco1.2022.10.273	[]	[]	-1	-1	-1
J	Perez-Gil, Oscar; Barea, Rafael; Lopez-Guillen, Elena; Bergasa, Luis M.; Gomez-Huelamo, Carlos; Gutierrez, Rodrigo; Diaz-Diaz, Alejandro	Deep reinforcement learning based control for Autonomous Vehicles in CARLA	2022	MULTIMEDIA TOOLS AND APPLICATIONS		3553	3576	Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them. This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle. More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them. The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route. In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands. For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world. The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance. DDPG perfoms trajectories very similar to classic controller as LQR. In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m. To conclude, some conclusions and future works are commented.	Article; Early Access		10.1007/s11042-021-11437-3	[]	[]	-1	-1	-1
J	Zhang, X.; Wu, L.; Liu, H.; Wang, Y.; Li, H.; Xu, B.	High-Speed Ramp Merging Behavior Decision for Autonomous Vehicles Based on Multiagent Reinforcement Learning	2023	IEEE Internet of Things Journal		22664	72	To improve the decision success rate of a multiagent reinforcement learning algorithm in merging high-speed ramps of autonomous vehicles, the independent proximal policy optimization (IPPO) method is presented. The Markov decision process (MDP) model for autonomous vehicle behavioral decision making is developed. Moreover, the state space, reward function, and action space are all designed. An IPPO method is proposed using independent learning and parameter-sharing strategies based on the proximal policy optimization algorithm. And further, a decision-making model for autonomous driving behavior is built. For simulation experiments, a highway ramp scenario is set. The experiment findings indicate that the IPPO algorithm can significantly increase the decision success rate of autonomous vehicles in the ramp merging assignment. Also, as compared to the MAACKTR and GPPO algorithms, the IPPO algorithm can achieve a better average reward and finish the ramp merging more rapidly.	Journal Paper	2023	10.1109/JIOT.2023.3304890	[]	[]	-1	-1	-1
B	Chakraborty, S.; Kumar, S.; Bhatt, N.; Pasumarthy, R.	End-to-end Autonomous Driving in Heterogeneous Traffic Scenario Using Deep Reinforcement Learning	2023	2023 European Control Conference (ECC)		1	6	In this paper, we propose an end-to-end autonomous driving architecture for safe maneuvering in heterogeneous traffic using a reinforcement learning (RL) algorithm. Using the proposed architecture we develop an RL agent that can make driving decisions directly from the sensor data. We formulate the autonomous driving problem as a Markov Decision Process and propose different architectures using Deep Q -Networks for two types of sensor data - top view images of the autonomous vehicle (AV) and its surrounding vehicles and information on relative position and velocities of the surrounding vehicles w.r.t the AV. We consider a highway scenario and analyze the performance of the RL agent using the proposed architectures using the highway-env simulator. We compare the driving performance of the AV for both sensor types and discuss their efficacy under varying traffic densities.	Conference Paper	2023		[]	[]	-1	-1	-1
B		Proceedings of 2020 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)	2020	Proceedings of 2020 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)					Book; Meeting	2020		[]	[]	-1	-1	-1
J	Zhou, Zejian; Xu, Hao	Decentralized Adaptive Optimal Tracking Control for Massive Autonomous Vehicle Systems With Heterogeneous Dynamics: A Stackelberg Game	2021	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS		5654	5663	"In this article, a decentralized optimal tracking control problem has been studied for a large-scale autonomous vehicle system with heterogeneous system dynamics. Due to the ultralarge number of agents, the notorious ""curse of dimension"" problem as well as the unrealistic assumption of the existence of reliable very large-scale communication links in uncertain environments have challenged the traditional multiagent system (MAS) algorithms for decades. The emerging mean-field game (MFG) theory has recently been widely adopted to generate a decentralized control method that deals with those challenges by encoding the large scale MASs' information into a novel time-varying probability density functions (PDF) which can be obtained locally. However, the traditional MFG methods assume all agents are homogeneous, which is unrealistic in practical industrial applications, e.g., Internet of Things (IoTs), and so on. Therefore, a novel mean-field Stackelberg game (MFSG) is formulated based on the Stackelberg game, where all the agents have been classified as two different categories where one major leader's decision dominates the other minor agents. Moreover, a hierarchical structure that treats all minor agents as a mean-field group is developed to tackle the assumption of homogeneous agents. Then, the actor-actor-critic-critic-mass (A(2)C(2) M) algorithm with five neural networks is designed to learn the optimal policies by solving the MFSG. The Lyapunov theory is utilized to prove the convergence of A(2)C(2) M neural networks and the closed-loop system's stability. Finally, a series of numerical simulations are conducted to demonstrate the effectiveness of the developed method."	Article	DEC 2021	10.1109/TNNLS.2021.3100417	[]	[]	-1	-1	-1
B	Liu, X.; Ren, Y.	Supervised Learning Guided Reinforcement Learning for Humanized Autonomous Driving Following Decision Making	2023	7th International Conference on Computing, Control and Industrial Engineering (CCIE 2023): Advances in Computing, Control and Industrial Engineering VII. Lecture Notes in Electrical Engineering (1047)		457	64	In order to improve the humanization of the autonomous vehicle following function and reduce the reinforcement learning exploration space, the adaptive following control strategy based on human experience supervised deep reinforcement learning is proposed in this paper. First, an end-to-end human driving reference model based on a temporal LSTM network is built to learn the driver's policy function from temporal state information to action. The model is trained by imitation learning using the driver-following data, and the action signals are output online through the input temporal state, and the output action signals are introduced into reinforcement learning through the reward function. The reward function is designed by combining the safety and comfort of the following process. The decision model is trained by a dual delay depth deterministic policy gradient. The simulation results based on CARLA simulator show that the proposed control strategy is closer to the human driver's driving habits and converges faster than the traditional reinforcement learning.	Conference Paper	2023	10.1007/978-981-99-2730-2_45	[]	[]	-1	-1	-1
B	Yuchuan Zhang; Hui Xie; Kang Song	An Optimal Vehicle Speed Planning Algorithm for Regenerative Braking at Traffic Lights Intersections based on Reinforcement Learning	2020	Proceedings of the 2020 4th CAA International Conference on Vehicular Control and Intelligence (CVCI)		193	8	For electric vehicle or hybrid electric vehicles, the regenerative braking is one of the important means to realize energy saving, for which braking ahead of a traffic light intersection is a representative scenario. The uncertainty in driver behavior and future traffic flow, however, make it challenging to achieve optimal dynamic energy recovery through conventional braking operation by drivers. Therefore, in this paper, an energy recovery optimization-oriented vehicle speed planning algorithm ahead of traffic lights intersection is proposed, for autonomous vehicle or driving assistance system. First, the reward function is designed, taking the energy recovery amount, traffic efficiency and driving smoothness into consideration. Then, the information of traffic lights at intersections is obtained in advance through V2I (vehicle to infrastructure) communication. Finally, the q-table and neural network are trained in the framework of reinforcement learning, deriving optimal vehicle speed profile. Simulation results on a high-fidelity model show that the amount of recovered electrical energy using q-learning algorithm is 45.08% higher than that of uniform deceleration. The amount of electrical energy using DQN (Deep Q-network) algorithm is 2.24% higher than q-learning, showing to be a better candidate in terms of comprehensive optimality than q-learning.	Conference Paper	2020	10.1109/CVCI51460.2020.9338590	[]	[]	-1	-1	-1
B	Mengqi Li; Ziyao Geng; Yi Wang	Research on Vehicle Dispatch Problem Based on Kuhn-Munkres and Reinforcement Learning Algorithm	2021	Proceedings of the 2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)		986	92	With the development of artificial intelligence and 5G communication technology, autonomous vehicles are gradually becoming more achievable. Autonomous vehicles are used in urban transportation to provide taxi service, which effectively reduces labor cost and realizes intelligent transportation systems. The vehicle system combined with 5G technology can quickly obtain traffic information, which provides a decision basis for the vehicle dispatching. Thus, it is necessary to develop an efficient way to distribute and allocate these vehicles to maximize the potential income for the system. This paper studies the vehicle dispatching based on the travel data from the 2016 New York City Green Taxi data and propose two dispatching methods. First, we consider the dispatching problem as a maximum weight value matching problem. Then a distance-based dispatching method is proposed with the goal of minimizing the waiting time of passengers by using the Kuhn and Munkres (KM) algorithm. Finally, we formulate the decision of vehicle dispatch with a Markov Decision Process (MDP) and introduce a Reinforcement Learning (RL)-based dispatching method, which combines RL algorithm and KM algorithm to solve the dispatching problem with the goal of maximizing long-term revenue of divers. In the experiment, KM algorithm is compared with the full permutation algorithm to prove the effectiveness of KM algorithm. The performance of the distance-based dispatching method and RL-based dispatching method are presented in a small-scale dispatching and a large-scale dispatching. Experiment results show that the total revenue of vehicles is improved by about 20% by using RL-based dispatching method, compared to dispatching method based on the distance. Thus, RL-based dispatching method is more effective in a dispatching platform. It could be used by future public autonomous vehicle companies to achieve the fulfill the need of maximizing the potential income for the system.	Conference Paper	2021	10.1109/ICPECA51329.2021.9362615	[]	[]	-1	-1	-1
J	Makantasis, Konstantinos; Kontorinaki, Maria; Nikolos, Ioannis	Deep reinforcement-learning-based driving policy for autonomous road vehicles	2020	IET INTELLIGENT TRANSPORT SYSTEMS		13	24	In this work, the problem of path planning for an autonomous vehicle that moves on a freeway is considered. The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics. On the contrary, this work proposes the development of a driving policy based on reinforcement learning. In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required. Driving scenarios where the road is occupied both by autonomous and manual driving vehicles are considered. To the best of the authors' knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments. The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established SUMO microscopic traffic flow simulator. Finally, some initial results regarding the effect of autonomous vehicles' behaviour on the overall traffic flow are presented.	Article	JAN 2020	10.1049/iet-its.2019.0249	[]	[]	-1	-1	-1
J	Hu, Jinchao; Li, Xu; Cen, Yanqing; Xu, Qimin; Zhu, Xuefen; Hu, Weiming	A Roadside Decision-Making Methodology Based on Deep Reinforcement Learning to Simultaneously Improve the Safety and Efficiency of Merging Zone	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		18620	18631	The safety and efficiency of the merging zone is particularly important for traffic networks. Although autonomous vehicle improves the safety and efficiency from vehicle view, traffic controlling in merging zone mostly focus on improving efficiency from roadside view. Lacking of detailed driving recommendation, it ignores the safety of merging zone where commercial vehicle pose a high collision risk in real traffic. This paper proposes a roadside decision-making methodology to simultaneously improve the safety and efficiency of merging zone. We have built two modules, namely assessment and decision-making. Assessment module takes advantage of Bayesian inference to evaluate dynamic collision risk. Decision-making module based on deep reinforcement learning recommends the actions to commercial vehicles by roadside unit. A series of typical simulation tests show that our method increases the TTC of commercial vehicles by an average of 62.7%. In the free flow, the overall travel time of vehicles in the merging zone is reduced by 11.68%. Most notably, when congestion occurred, the average jam length is reduced by 59.68% on the premise of safety. Moreover, the average accuracy of the roadside decision-making method on the evaluation metrics of TTC, travel time, and jam length are 93.73%, 91.65%, and 94.45%, respectively. The experimental results show that the roadside decision-making methodology simultaneously improves safety and efficiency, and it dynamically adapts free and congested traffic flow.	Article; Early Access		10.1109/TITS.2022.3157910	[]	[]	-1	-1	-1
J	Liu, Jiaxin; Wang, Hong; Peng, Liang; Cao, Zhong; Yang, Diange; Li, Jun	PNNUAD: Perception Neural Networks Uncertainty Aware Decision-Making for Autonomous Vehicle	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		24355	24368	Most environment perception methods in autonomous vehicles rely on deep neural networks because of their impressive performance. However, neural networks have black-box characteristics in nature, which may lead to perception uncertainty and untrustworthy autonomous vehicles. Thus, this work proposes a decision-making method to adapt the potential perception uncertainty due to the sensor noises, fuzzy features, and unfamiliar inputs. The whole method is named as Perception Neural Networks Uncertainty Aware Decision-Making (PNNUAD) method. PNNUAD first uses the Monte Carlo dropout method to estimate the perception neural network uncertainty into a distribution around the original output. Then, the perception uncertainty will be considered in a designed reinforcement learning-based planner using a distributed value function. Finally, a backup policy will maintain the vehicle's performance to avoid disastrous perception uncertainty. The evaluation section uses an augmented reality urban driving scenario; namely, the scenario builds in the CARLA simulator while the perception uncertainty comes from the real dataset. This case study focuses on the object class uncertainty of a widely used neural network, i.e., YOLO-V3. The results indicate that the proposed method can maintain AV safety even with poor perception performance. Meanwhile, the AV has not become too conservative by defending the perception uncertainty. This work is necessary for applying the statistics neural networks to safety-critical autonomous vehicles, and the source code will be open-source in this work.	Article; Early Access		10.1109/TITS.2022.3197602	[]	[]	-1	-1	-1
B	Yadavalli, S.R.; Das, L.C.; Won, M.	RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging	2023	2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)		5514	21	A platoon refers to a group of vehicles traveling together in very close proximity using automated driving technology. Owing to its immense capacity to improve fuel efficiency, driving safety, and driver comfort, platooning technology has garnered substantial attention from the autonomous vehicle research community. Although highly advantageous, recent research has uncovered that an excessively small intra-platoon gap can impede traffic flow during highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a challenge due to the massive computational complexity. In this paper, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The framework's state space has been meticulously designed in consultation with the transportation literature to take into account critical traffic parameters that bear direct relevance to merging efficiency. An intra-platoon gap decision making method based on the deep deterministic policy gradient algorithm is created to incorporate the continuous action space to ensure precise and continuous adaptation of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway on-ramp merging scenarios.	Conference Paper	2023	10.1109/IROS55552.2023.10341918	[]	[]	-1	-1	-1
B	Han, S.; Wang, H.; Su, S.; Shi, Y.; Miao, F.	Stable and Efficient Shapley Value-Based Reward Reallocation for Multi-Agent Reinforcement Learning of Autonomous Vehicles	2022	2022 International Conference on Robotics and Automation (ICRA)		8765	71	With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs). However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability. When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process. In this work, we propose to reallocate the system's total reward efficiently to motivate stable cooperation among autonomous vehicles. We formally define and quantify how to reallocate the system's total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system's total reward. We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game. Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group. We then propose a cooperative policy learning algorithm with Shapley value reward reallocation. In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.	Conference Paper	2022	10.1109/ICRA46639.2022.9811626	[]	[]	-1	-1	-1
J	Mackay, Andrew K.; Riazuelo, Luis; Montano, Luis	RL-DOVS: Reinforcement Learning for Autonomous Robot Navigation in Dynamic Environments	2022	SENSORS				Autonomous navigation in dynamic environments where people move unpredictably is an essential task for service robots in real-world populated scenarios. Recent works in reinforcement learning (RL) have been applied to autonomous vehicle driving and to navigation around pedestrians. In this paper, we present a novel planner (reinforcement learning dynamic object velocity space, RL-DOVS) based on an RL technique for dynamic environments. The method explicitly considers the robot kinodynamic constraints for selecting the actions in every control period. The main contribution of our work is to use an environment model where the dynamism is represented in the robocentric velocity space as input to the learning system. The use of this dynamic information speeds the training process with respect to other techniques that learn directly either from raw sensors (vision, lidar) or from basic information about obstacle location and kinematics. We propose two approaches using RL and dynamic obstacle velocity (DOVS), RL-DOVS-A, which automatically learns the actions having the maximum utility, and RL-DOVS-D, in which the actions are selected by a human driver. Simulation results and evaluation are presented using different numbers of active agents and static and moving passive agents with random motion directions and velocities in many different scenarios. The performance of the technique is compared with other state-of-the-art techniques for solving navigation problems in environments such as ours.	Article	MAY 2022	10.3390/s22103847	[]	[]	-1	-1	-1
B	D'Orazio, T.; Cicirelli, G.; Attolico, G.; Distante, C.	A reinforcement learning approach for a goal-reaching behavior	1999	Proceedings of the Twelfth International Florida AI Research Society Conference		79	83	Developing elementary behavior is the starting point for the realization of complex systems. In this paper we will describe a learning algorithm that realizes a simple goal-reaching behavior for an autonomous vehicle when a-priori knowledge of the environment is not provided. The state of the system is based on information received by a visual sensor. A Q-learning algorithm associates the optimal action to each state, developing the optimal state-action rules (optimal policy). A few training trials are sufficient, in simulation, to learn the optimal policy since during the test trials the set of actions is initially limited. The state and action sets are then enlarged, introducing fuzzy variables with their membership functions to the extent of tackling errors in state estimation due to the noise in the vision measurements. Experimental results, both in simulated and real environment, are shown.	Conference Paper	1999		[]	[]	-1	-1	-1
J	Chen, I-Ming; Chan, Ching-Yao	Deep reinforcement learning based path tracking controller for autonomous vehicle	2021	PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING		541	551	Path tracking is an essential task for autonomous vehicles (AV), for which controllers are designed to issue commands so that the AV will follow the planned path properly to ensure operational safety, comfort, and efficiency. While solving the time-varying nonlinear vehicle dynamic problem is still challenging today, deep neural network (NN) methods, with their capability to deal with nonlinear systems, provide an alternative approach to tackle the difficulties. This study explores the potential of using deep reinforcement learning (DRL) for vehicle control and applies it to the path tracking task. In this study, proximal policy optimization (PPO) is selected as the DRL algorithm and is combined with the conventional pure pursuit (PP) method to structure the vehicle controller architecture. The PP method is used to generate a baseline steering control command, and the PPO is used to derive a correction command to mitigate the inaccuracy associated with the baseline from PP. The blend of the two controllers makes the overall operation more robust and adaptive and attains the optimality to improve tracking performance. In this paper, the structure, settings and training process of the PPO are described. Simulation experiments are carried out based on the proposed methodology, and the results show that the path tracking capability in a low-speed driving condition is significantly enhanced.	Article; Early Access		10.1177/0954407020954591	[]	[]	-1	-1	-1
C	Folkers, Andreas; Rick, Matthias; Bueskens, Christof	Controlling an Autonomous Vehicle with Deep Reinforcement Learning	2019	2019 30TH IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV19)	IEEE Intelligent Vehicles Symposium	2025	2031	We present a control approach for autonomous vehicles based on deep reinforcement learning. A neural network agent is trained to map its estimated state to acceleration and steering commands given the objective of reaching a specific target state while considering detected obstacles. Learning is performed using state-of-the-art proximal policy optimization in combination with a simulated environment. Training from scratch takes five to nine hours. The resulting agent is evaluated within simulation and subsequently applied to control a full-size research vehicle. For this, the autonomous exploration of a parking lot is considered, including turning maneuvers and obstacle avoidance. Altogether, this work is among the first examples to successfully apply deep reinforcement learning to a real vehicle.	Proceedings Paper	2019		[]	[]	-1	-1	-1
B	Ben Elallid, B.; El Alaoui, H.; Benamar, N.	Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation	2023	2023 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)		308	13	In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforcement learning applications in autonomous driving and highlights the potential of single-agent, cost-effective methods for addressing more complex driving scenarios and advancing reinforcement learning algorithms in the future.	Conference Paper	2023	10.1109/3ICT60104.2023.10391453	[]	[]	-1	-1	-1
J	Manchella, Kaushik; Umrawal, Abhishek K.; Aggarwal, Vaneet	FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers and Goods Transportation	2021	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		2035	2047	The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries. On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching. The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems. This article considers combining passenger transportation with goods delivery to improve vehicle-based transportation. We propose FlexPool: a distributed model-free deep reinforcement learning algorithm that jointly serves passengers & goods workloads by learning optimal dispatch policies from its interaction with the environment. The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method. These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods. The dispatching algorithm based on deep reinforcement learning is integrated with an efficient matching algorithm for passengers and goods. Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers & goods. FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers & goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.	Article	APR 2021	10.1109/TITS.2020.3048361	[]	[]	-1	-1	-1
C	Bcrceanu, Andrei; Leba, Monica; Risteiu, Marius; Mija, Neln; Lonica, Andreea	Simulation of an Autonomous Car Drive Scenario	2022	2022 7TH INTERNATIONAL CONFERENCE ON MATHEMATICS AND COMPUTERS IN SCIENCES AND INDUSTRY, MCSI		170	174	In everyday life, people are dependent on public or private means of transport. Day by day, this phenomenon leads to a continuous increase in road traffic, a fact that implies the appearance of urban agglomerations, jeopardizing traffic safety, but also a major impact on the environment. Thanks to modern information and communication technologies, these aforementioned problems will be able to be given at least one pertinent advanced solution to lessen the number of transportation related problems we face today. These intelligent transport systems have a wide area of applicability in increasing road safety, minimizing the impact on the environment but also in improving traffic management and maximizing the benefits due to transport. By autonomous vehicle it is understood that the system assists the driver by maintaining a constant speed according to the requirements and conditions of the road segment on which the movement is carried out, by correctly entering the lane without inadvertently leaving it, but also by maintaining a safe distances from surrounding vehicles and obstacles, all this to ensure a safe and incident-free journey. By using a simulation environment such as Carla, we can create an autonomous driving scenario and its training model that we can later use, if it meets all the requirements, on a real vehicle. The simulation allows us to see if the model can be validated or where it still needs improvement without endangering someone's life and without material damage.	Proceedings Paper	2022	10.1109/MCSI55933.2022.00034	[]	[]	-1	-1	-1
J	Karunakaran, D.; Worrall, S.; Nebot, E.	Efficient falsification approach for autonomous vehicle validation using a parameter optimisation technique based on reinforcement learning [arXiv]	2020	arXiv		11 pp.	11 pp.	The widescale deployment of Autonomous Vehicles (AV) appears to be imminent despite many safety challenges that are yet to be resolved. It is well-known that there are no universally agreed Verification and Validation (VV) methodologies guarantee absolute safety, which is crucial for the acceptance of this technology. The uncertainties in the behaviour of the traffic participants and the dynamic world cause stochastic reactions in advanced autonomous systems. The addition of ML algorithms and probabilistic techniques adds significant complexity to the process for real-world testing when compared to traditional methods. Most research in this area focuses on generating challenging concrete scenarios or test cases to evaluate the system performance by looking at the frequency distribution of extracted parameters as collected from the real-world data. These approaches generally employ Monte-Carlo simulation and importance sampling to generate critical cases. This paper presents an efficient falsification method to evaluate the System Under Test. The approach is based on a parameter optimisation problem to search for challenging scenarios. The optimisation process aims at finding the challenging case that has maximum return. The method applies policy-gradient reinforcement learning algorithm to enable the learning. The riskiness of the scenario is measured by the well established RSS safety metric, euclidean distance, and instance of a collision. We demonstrate that by using the proposed method, we can more efficiently search for challenging scenarios which could cause the system to fail in order to satisfy the safety requirements.	Journal Paper	15 Nov. 2020		[]	[]	-1	-1	-1
J	Samak, T.V.; Samak, C.V.; Krovi, V.	Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem [arXiv]	2023	Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem [arXiv]				This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the approaches in stochastic environments, since the agents were mutually independent and exhibited asynchronous motion behavior. The problems were further aggravated by providing the agents with sparse observation spaces and requiring them to sample control commands that implicitly satisfied the imposed kinodynamic as well as safety constraints. The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training as well as deployment phases.	Preprint	2023		[]	[]	-1	-1	-1
J	Farooq, Muhammad Shoaib; Khalid, Haris; Arooj, Ansif; Umer, Tariq; Asghar, Aamer Bilal; Rasheed, Jawad; Shubair, Raed M. M.; Yahyaoui, Amani	A Conceptual Multi-Layer Framework for the Detection of Nighttime Pedestrian in Autonomous Vehicles Using Deep Reinforcement Learning	2023	ENTROPY				The major challenge faced by autonomous vehicles today is driving through busy roads without getting into an accident, especially with a pedestrian. To avoid collision with pedestrians, the vehicle requires the ability to communicate with a pedestrian to understand their actions. The most challenging task in research on computer vision is to detect pedestrian activities, especially at nighttime. The Advanced Driver-Assistance Systems (ADAS) has been developed for driving and parking support for vehicles to visualize sense, send and receive information from the environment but it lacks to detect nighttime pedestrian actions. This article proposes a framework based on Deep Reinforcement Learning (DRL) using Scale Invariant Faster Region-based Convolutional Neural Networks (SIFRCNN) technologies to efficiently detect pedestrian operations through which the vehicle, as agents train themselves from the environment and are forced to maximize the reward. The SIFRCNN has reduced the running time of detecting pedestrian operations from road images by incorporating Region Proposal Network (RPN) computation. Furthermore, we have used Reinforcement Learning (RL) for optimizing the Q-values and training itself to maximize the reward after getting the state from the SIFRCNN. In addition, the latest incarnation of SIFRCNN achieves near-real-time object detection from road images. The proposed SIFRCNN has been tested on KAIST, City Person, and Caltech datasets. The experimental results show an average improvement of 2.3% miss rate of pedestrian detection at nighttime compared to the other CNN-based pedestrian detectors.	Article	JAN 2023	10.3390/e25010135	[]	[]	-1	-1	-1
J	Fu, Yuchuan; Li, Changle; Yu, F. Richard; Luan, Tom H.; Zhang, Yao	A Selective Federated Reinforcement Learning Strategy for Autonomous Driving	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		1655	1668	Currently, the complex traffic environment challenges the fast and accurate response of a connected autonomous vehicle (CAV). More importantly, it is difficult for different CAVs to collaborate and share knowledge. To remedy that, this paper proposes a selective federated reinforcement learning (SFRL) strategy to achieve online knowledge aggregation strategy to improve the accuracy and environmental adaptability of the autonomous driving model. First, we propose a federated reinforcement learning framework that allows participants to use the knowledge of other CAVs to make corresponding actions, thereby realizing online knowledge transfer and aggregation. Second, we use reinforcement learning to train local driving models of CAVs to cope with collision avoidance tasks. Third, considering the efficiency of federated learning (FL) and the additional communication overhead it brings, we propose a CAVs selection strategy before uploading local models. When selecting CAVs, we consider the reputation of CAVs, the quality of local models, and time overhead, so as to select as many high-quality users as possible while considering resources and time constraints. With above strategic processes, our framework can aggregate and reuse the knowledge learned by CAVs traveling in different environments to assist in driving decisions. Extensive simulation results validate that our proposal can improve model accuracy and learning efficiency while reducing communication overhead.	Article; Early Access		10.1109/TITS.2022.3219644	[]	[]	-1	-1	-1
J	Kumar, D.; Muhammad, N.	A Survey on Localization for Autonomous Vehicles	2023	IEEE Access		115865	83	Research on autonomous vehicles has made significant advances in recent years. To operate an autonomous vehicle safely and effectively, precise localization is essential. This study aims to present the state of the art in localization to scientists new to the area. It presents and summarizes works from the field of localization and suggests a classification for the works. Approaches to localization are mainly divided into three categories: conventional localization, machine-learning-based localization, and vehicle-to-everything (V2X) localization. Conventional localization primarily depends on high-definition (HD) maps or certain marks, such as landmarks and road marks. Machine-learning-based localization approaches include using neural networks, end-to-end approaches, as well as reinforcement learning for performing or improving localization. Moreover, V2X localization methods localize vehicles by communicating with other vehicles (V2V) or infrastructures (V2I). This study not only presents a bigger picture of the area of localization in autonomous driving but also presents the potentials and drawbacks of different localization methods. At the end of the review, some research areas open for future research are also highlighted.	Journal Paper	2023	10.1109/ACCESS.2023.3326069	[]	[]	-1	-1	-1
C	Li, Yan; Zhang, Hao; Wang, Zhuping	Data-Driven Lateral Fault-tolerance Control of Autonomous Vehicle System Using Reinforcement Learning	2020	2020 IEEE 16TH INTERNATIONAL CONFERENCE ON CONTROL & AUTOMATION (ICCA)	IEEE International Conference on Control and Automation ICCA	1410	1415	In this paper, a data driven lateral fault-tolerant control (LFTC) method is proposed for four wheels drive vehicles, which considering both vehicle velocity and trajectory tracking performance. The novel design of the lateral control employs with zero-sum game and adaptive dynamics programming technique to solve the Riccati equation without requiring the knowledge of system, only using online data. The LFTC consists of the data driven off-policy and adaptive control. The adaptive parameters adjusted online to compensate the actuator faults automatically. The tracking system is asymptotically stable with the disturbance attenuation level gamma. Finally, simulation is provided to show the effectiveness of the proposed method.	Proceedings Paper	2020		[]	[]	-1	-1	-1
B	Qi Zhu; Zhenhua Huang; Zhenping Sun; Daxue Liu; Bin Dai	Reinforcement learning based throttle and brake control for autonomous vehicle following	2017	2017 Chinese Automation Congress (CAC). Proceedings		6657	62	In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower. A reinforcement learning based throttle and brake control approach is developed for the follower vehicle. Near optimal control law is directly learned by trial and error with the neural dynamic programming algorithm. According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower. Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim.	Conference Paper	2017	10.1109/CAC.2017.8243976	[]	[]	-1	-1	-1
B	Cai, H.	Pursuit-escape Strategy for Autonomous Agents	2023	2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)		1622	7	"The pursuit-escape strategy is the cutting-edge research related to autonomous vehicle. This paper focuses on training 2 agents based on the Q-Learning algorithm model in reinforce learning, in a pursuit-escape game confrontation to solve the game strategy selection problem for a single pursuer and a single escapee in a simulation environment. First of all, the simulation model uses the gym API library provided by the OpenAI research organization to call the pre-defined environment to build the simulation environment. At the same time, the training model of the pursuit-escape game would be built and trained in the cloud computing platform of Baidu PaddlePaddle, and the final training effect that meets the expectation proves that the algorithm is verified. This research also introduces the current interference in to increase the realism of the simulation and optimizes the algorithm related to obstacle judgment. The simulation and experimental results demonstrate that the sensor model can assist in providing ""rewards and punishments"" in the pursuit process of an agent, which can be applied to improve the autonomous motion strategy selection of the agent."	Conference Paper	2023	10.1109/EEBDA56825.2023.10090700	[]	[]	-1	-1	-1
C	Zhu, Zihui; Zhang, Zhengming; Yan, Wen; Huang, Yongming; Yang, Luxi	Proactive Caching in Auto Driving Scene via Deep Reinforcement Learning	2019	2019 11TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS AND SIGNAL PROCESSING (WCSP)	International Conference on Wireless Communications and Signal Processing			The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry. The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction. In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network. First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively. Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy. Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.	Proceedings Paper	2019	10.1109/wcsp.2019.8928131	[]	[]	-1	-1	-1
J	Qian, Lilin; Xu, Xin; Zeng, Yujun; Huang, Junwen	Deep, Consistent Behavioral Decision Making with Planning Features for Autonomous Vehicles	2019	ELECTRONICS				Autonomous driving promises to be the main trend in the future intelligent transportation systems due to its potentiality for energy saving, and traffic and safety improvements. However, traditional autonomous vehicles' behavioral decisions face consistency issues between behavioral decision and trajectory planning and shows a strong dependence on the human experience. In this paper, we present a planning-feature-based deep behavior decision method (PFBD) for autonomous driving in complex, dynamic traffic. We used a deep reinforcement learning (DRL) learning framework with the twin delayed deep deterministic policy gradient algorithm (TD3) to exploit the optimal policy. We took into account the features of topological routes in the decision making of autonomous vehicles, through which consistency between decision making and path planning layers can be guaranteed. Specifically, the features of a route extracted from path planning space are shared as the input states for the behavioral decision. The actor-network learns a near-optimal policy from the feasible and safe candidate emulated routes. Simulation tests on three typical scenarios have been performed to demonstrate the performance of the learning policy, including the comparison with a traditional rule-based expert algorithm and the comparison with the policy considering partial information of a contour. The results show that the proposed approach can achieve better decisions. Real-time test on an HQ3 (HongQi the third ) autonomous vehicle also validated the effectiveness of PFBD.	Article	DEC 2019	10.3390/electronics8121492	[]	[]	-1	-1	-1
J	Masmitja, I.; Martin, M.; Katija, K.; Gomariz, S.; Navarro, J.	A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles [arXiv]	2023	arXiv				Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems. Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position. Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption. To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms. Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies. The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g. the median predicted error at the beginning of the target's localisation is 17% less. These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles. This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios	Journal Paper	17 Jan. 2023		[]	[]	-1	-1	-1
J	Lichtle, N.; Jang, K.; Shah, A.; Vinitsky, E.; Lee, J.W.; Bayen, A.M.	Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data [arXiv]	2024	Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data [arXiv]				Designing traffic-smoothing cruise controllers that can be deployed onto autonomous vehicles is a key step towards improving traffic flow, reducing congestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass the common issue of having to carefully fine-tune a large traffic microsimulator by leveraging real-world trajectory data from the I-24 highway in Tennessee, replayed in a one-lane simulation. Using standard deep reinforcement learning methods, we train energy-reducing wave-smoothing policies. As an input to the agent, we observe the speed and distance of only the vehicle in front, which are local states readily available on most recent vehicles, as well as non-local observations about the downstream state of the traffic. We show that at a low 4% autonomous vehicle penetration rate, we achieve significant fuel savings of over 15% on trajectories exhibiting many stop-and-go waves. Finally, we analyze the smoothing effect of the controllers and demonstrate robustness to adding lane-changing into the simulation as well as the removal of downstream information.	Preprint	2024		[]	[]	-1	-1	-1
J	Everett, Michael; Lutjens, Bjorn; How, Jonathan P.	Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning	2022	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS		4184	4198	Deep neural network-based systems are now state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a deep Q-network (DQN) policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios, a classic control task, and Atari Pong. This article extends our prior work with new performance guarantees, extensions to other reinforcement learning algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.	Article; Early Access		10.1109/TNNLS.2021.3056046	[]	[]	-1	-1	-1
B	Gao, L.; Wu, Y.; Wang, L.; Wang, L.; Zhang, J.; Li, K.	End-to-end autonomous vehicle navigation control method guided by the dynamic window approach	2023	2023 IEEE 6th International Electrical and Energy Conference (CIEEC)		4472	6	Existing end-to-end vehicle navigation control methods based on deep reinforcement learning generally have low exploration efficiency and difficulty converging the model to the ideal state. To address these problems, this paper proposes a hybrid reinforcement learning framework that can fuse the traditional path planning algorithm (dynamic window approach, DWA) with the deep reinforcement learning approach. By taking advantage of DWA's ability to plan a collision-free trajectory with guaranteed vehicle dynamics constraints quickly, giving positive guidance to the DRL module at the early stage of its training, thus improving exploration efficiency while ensuring exploration breadth. To verify the effectiveness of the algorithm, a joint CARLA and ROS simulation environment is built and simulated in a typical scenario. The simulation results show that compared with existing deep reinforcement learning methods, the proposed method in this paper has significantly improved in terms of model convergence speed, stability, and pre-mid-term decision performance, in which the training time of TD3 decision network can be shortened by more than 85%.	Conference Paper	2023	10.1109/CIEEC58067.2023.10167001	[]	[]	-1	-1	-1
C	Yang, Fan; Wang, Ping; Wang, XinHong	Continuous Control in Car Simulator with Deep Reinforcement Learning	2018	PROCEEDINGS OF 2018 THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2018) / 2018 THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND MULTIMEDIA TECHNOLOGY (ICIMT 2018)		566	570	Deep reinforcement learning (DRL), which can be trained without abundant labeled data required in supervised learning, plays an important role in autonomous vehicle researches. According to action space, DRL can be further divided into two classes: discrete domain and continuous domain. In this work, we focus on continuous steering control since it's impossible to switch among different discrete steering values at short intervals in reality. We first define the steering smoothness to quantify the degree of continuity. Then we propose a new penalty in reward shaping. We carry experiments based on Deep Deterministic Policy Gradient (DDPG) and Asynchronous Advantage Actor Critic (A3C), which are the state of the art in continuous domain. Results show that the proposed penalty improves the steering smoothness with both algorithms.	Proceedings Paper	2018	10.1145/3297156.3297217	[]	[]	-1	-1	-1
B	Guo, H.; Li, G.; Liu, J.; Tan, Z.; Yu, D.; Zhang, Y.	Autonomous Vehicle Path Tracking Control based on Adaptive Dynamic Programming	2023	2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	6	This paper presents a path tracking control method for autonomous vehicles by utilizing Adaptive Dynamic Programming (ADP). The primary goal is to enhance the path tracking performance under various longitudinal speeds. The proposed approach incorporates the consideration of nonlinear tire characteristics and establishes a nonlinear vehicle system model based on the fundamental principles of vehicle dynamics. By formulating a performance index function that captures the correlation between the vehicle's position and desired paths, the optimal control strategy is obtained using the Dual Heuristic Programming algorithm (DHP), known for its computational efficiency, within the ADP framework. The simulation results verify the effectiveness of the proposed path tracking control scheme, which can have better control performance under different vehicle speeds.	Conference Paper	2023	10.1109/CVCI59596.2023.10397352	[]	[]	-1	-1	-1
C	Cao, Mingcong; Chen, Jiayi; Wang, Junmin	A Novel Vehicle Tracking Method for Cross-Area Sensor Fusion with Reinforcement Learning Based GMM	2020	2020 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	442	447	Radars, LiDARs and cameras have been widely adopted in autonomous driving applications due to their complementary capabilities of environment perception. However, one problem lies in how to effectively improve the cross-area tracking accuracy with massive data from multiple sensors. This paper proposes a novel tracking solution that is composed of a reinforcement-learning-based Gaussian mixture model (GMM), submodel center realignment, and data-driven trajectory association. Specifically, developed with a Q-learning-based cluster number, an improved GMM-EM algorithm is firstly investigated to cluster the dense short-range radar data points. Subsequently, an innovative kinetic-energy-aware approach is presented to realign the Q-learning GMM cluster centers for position error mitigation. In addition to Q-learning GMM clustering, a weight-scheduled method is presented to associate the data from a long-range radar and cameras for cross-area object extraction and trajectory fusion. Eighteen experiments for training and one experiment for verification were conducted on a fully-instrumented autonomous vehicle. Experimental results demonstrate that a better tracking performance in crossing detection areas can be achieved by the proposed method.	Proceedings Paper	2020	10.23919/acc45564.2020.9147318	[]	[]	-1	-1	-1
B	Jamgochian, A.; Buehrle, E.; Fischer, J.; Kochenderfer, M.J.	SHAIL: Safety-Aware Hierarchical Adversarial Imitation Learning for Autonomous Driving in Urban Environments	2023	2023 IEEE International Conference on Robotics and Automation (ICRA)		1530	6	Designing a safe and human-like decision-making system for an autonomous vehicle is a challenging task. Generative imitation learning is one possible approach for automating policy-building by leveraging both real-world and simulated decisions. Previous work that applies generative imitation learning to autonomous driving policies focuses on learning a low-level controller for simple settings. However, to scale to complex settings, many autonomous driving systems combine fixed, safe, optimization-based low-level controllers with high-level decision-making logic that selects the appropriate task and associated controller. In this paper, we attempt to bridge this gap in complexity by employing Safety-Aware Hierarchical Adversarial Imitation Learning (SHAIL), a method for learning a high-level policy that selects from a set of low-level controller instances in a way that imitates low-level driving data on-policy. We introduce an urban roundabout simulator that controls non-ego vehicles using real data from the Interaction dataset. We then demonstrate empirically that even with simple controller options, our approach can produce better behavior than previous approaches in driver imitation that have difficulty scaling to complex environments. Our implementation is available at https://github.com/sisl/InteractionImitation.	Conference Paper	2023	10.1109/ICRA48891.2023.10161449	[]	[]	-1	-1	-1
C	Kang, Liuwang; Shen, Haiying	A Reinforcement Learning based Decision-making System with Aggressive Driving Behavior Consideration for Autonomous Vehicles	2021	2021 18TH ANNUAL IEEE INTERNATIONAL CONFERENCE ON SENSING, COMMUNICATION, AND NETWORKING (SECON)	Annual IEEE International Conference on Sensing, Communication, and Networking Workshops			With the fast development of autonomous vehicle (AV) technology and possible popularity of AVs in the near future, a mixed-vehicle type driving environment where both AVs and their surrounding human-driving vehicles drive on the same road will exist and last for a long time. An AV measures its driving environments in real time and make control decisions to ensure driving safety. However, surrounding human-driving vehicles may conduct aggressive driving behaviors (e.g., sudden deceleration, sudden acceleration, sudden left or right lane change) in practice, which requires an AV to make correct control decisions to eliminate the effect of aggressive driving behaviors on its driving safety. In this paper, we propose a rein-inrcement learning based decision-making system (ReDS) which considers aggressive driving behaviors of surrounding human-driving vehicles during the decision making process. In ReDS, we firstly build a mixture density network based aggressive driving behavior detection method to detect possible aggressive driving behaviors among surrounding vehicles of an AV. We then build a reward function based on aggressive driving behavior detection results and incorporate the reward function into a reinforcement learning model to make optimal control decisions considering aggressive driving behaviors. We use a real-world traffic dataset from the United States Department of Transportation Federal Highway Administration to evaluate optimal control decision determination performance of ReDS in comparison with the state-of-the-art methods. The comparison results show that ReDS can improve optimal control decision success rate by 43% compared with existing methods, which demonstrates that ReDS has good optimal control decision determination performance.	Proceedings Paper	2021	10.1109/SECON52354.2021.9491587	[]	[]	-1	-1	-1
J	Mo, Shuojie; Pei, Xiaofei; Wu, Chaoxian	Safe Reinforcement Learning for Autonomous Vehicle Using Monte Carlo Tree Search	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		6766	6773	Reinforcement learning has gradually demonstrated its decision-making ability in autonomous driving. Reinforcement learning is learning how to map states to actions by interacting with environment so as to maximize the long-term reward. Within limited interactions, the learner will get a suitable driving policy according to the designed reward function. However there will be a lot of unsafe behaviors during training in traditional reinforcement learning. This paper proposes a RL-based method combined with RI, agent and Monte Carlo tree search algorithm to reduce unsafe behaviors. The proposed safe reinforcement learning framework mainly consists of two modules: risk state estimation module and safe policy search module. Once the future state will he risky calculated by the risk state estimation module using current state information and the action outputted by the RL agent, the MCTS based safe policy search module will activate to guarantee a safer exploration by adding an additional reward for risk actions. We test the approach in several random overtake scenarios, resulting in faster convergence and safer behaviors compared to traditional reinforcement learning.	Article; Early Access		10.1109/TITS.2021.3061627	[]	[]	-1	-1	-1
J	de Morais, Gustavo A. P.; Marcos, Lucas B.; Bueno, Jose Nuno A. D.; de Resende, Nilo F.; Terra, Marco Henrique; Grassi Jr, Valdir	Vision-based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles	2020	CONTROL ENGINEERING PRACTICE				Given the recent advances in computer vision, image processing and control systems, self-driving vehicles has been one of the most promising and challenging research topics nowadays. The design of vision -based robust controllers to keep an autonomous car in the center of the lane, despite uncertainties and disturbances, is still an ongoing challenge. This paper presents a hybrid control architecture that combines Deep Reinforcement Learning (DRL) and Robust Linear Quadratic Regulator (RLQR) for vision-based lateral control of an autonomous vehicle. Evolutionary estimation is used to model the vehicle uncertainties. For performance comparison, a DRL method and three other hybrid controllers are also evaluated. The inputs for each controller are real-time semantically segmented RGB camera images which serve as the basis to calculate continuous steering actions to keep the vehicle on the center of the lane with a constant velocity. Simulation results show that the proposed hybrid RLQR with evolutionary estimation of uncertainties architecture outperforms the other algorithms implemented. It presents lower tracking errors, smoother steering inputs, total collision avoidance and better generalization in new urban environments. Furthermore, it significantly decreases the required training time.	Article	NOV 2020	10.1016/j.conengprac.2020.104630	[]	[]	-1	-1	-1
C	Landgraf, Daniel; Voelz, Andreas; Kontes, Georgios; Mutschler, Christopher; Graichen, Knut	Hierarchical Learning for Model Predictive Collision Avoidance	2022	IFAC PAPERSONLINE		355	360	Recent progress in model predictive control (MPC) has shown great potential to control complex nonlinear systems in real-time. However, if parts of the controlled system cannot be modeled exactly by differential equations, the performance of MPC can decrease significantly. This paper approaches this problem by combining MPC with deep reinforcement learning (DRL) to a hierarchical control system, which is applied to control the motion of an autonomous vehicle. While the DRL algorithm is responsible for the decision-making with regard to obstacles on the street, the model predictive controller deals with the nonlinear dynamics of the vehicle. To this end, the vehicle dynamics are modeled by differential equations and the decision-making problem is modeled as a Markov decision process (MDP). The decisions are considered in the optimization problem of the controller, whose cost function, in turn, is considered in the reward function of the MDP. The performance of the hierarchical vehicle control is evaluated in scenarios with static and moving obstacles. Furthermore, it is examined whether adding information about the predicted trajectory to the state space of the MDP can increase the convergence speed. Copyright (C) 2022 The Authors.	Proceedings Paper; Early Access		10.1016/j.ifacol.2022.09.121	[]	[]	-1	-1	-1
J	Di, Xuan; Shi, Rongye	A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning	2021	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV). It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions. We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AVdominated, and the pure AVs. This paper is primarily focused on the latter three phases. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? We also provide a list of public datasets and simulation software related to AVs. Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.	Article; Early Access		10.1016/j.trc.2021.103008	[]	[]	-1	-1	-1
B	Sun, Q.; Zhang, L.; Yu, H.; Zhang, W.; Mei, Y.; Xiong, H.	Hierarchical Reinforcement Learning for Dynamic Autonomous Vehicle Navigation at Intelligent Intersections	2023	KDD '23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining		4852	61	Recent years have witnessed the rapid development of the Cooperative Vehicle Infrastructure System (CVIS), where road infrastructures such as traffic lights (TL) and autonomous vehicles (AVs) can share information among each other and work collaboratively to provide safer and more comfortable transportation experience to human beings. While many efforts have been made to develop efficient and sustainable CVIS solutions, existing approaches on urban intersections heavily rely on domain knowledge and physical assumptions, preventing them from being practically applied. To this end, this paper proposes NavTL, a learning-based framework to jointly control traffic signal plans and autonomous vehicle rerouting in mixed traffic scenarios where human-driven vehicles and AVs co-exist. The objective is to improve travel efficiency and reduce total travel time by minimizing congestion at the intersections while guiding AVs to avoid the temporally congested roads. Specifically, we design a graph-enhanced multi-agent decentralized bi-directional hierarchical reinforcement learning framework by regarding TLs as manager agents and AVs as worker agents. At lower temporal resolution timesteps, each manager sets a goal for the workers within its controlled region. Simultaneously, managers learn to take the signal actions based on the observation from the environment as well as an intention information extracted from its workers. At higher temporal resolution timesteps, each worker makes rerouting decisions along its way to the destination based on its observation from the environment, an intention-enhanced manager state representation, and a goal from its present manager. Finally, extensive experiments on one synthetic and two real-world network-level datasets demonstrate the effectiveness of our proposed framework in terms of improving travel efficiency.	Conference Paper	2023	10.1145/3580305.3599839	[]	[]	-1	-1	-1
B	E, S.; Krishnan, M.S.; S, K.; Tanguturi, R.C.; Arunagirinathan, S.; Gunasekaran, K.N.; Ramkumar, M.S.	Reinforcement Learning Based Autonomous E-Vehicle Speed Control	2023	2023 International Conference on Inventive Computation Technologies (ICICT)		192	8	Greater efficiency in both energy use and traffic flow are two benefits of autonomous self-driving cars. Due to their superior performance, effectiveness, and lack of carbon emission, electric vehicles (EVs) have recently become popular and used in an autonomous vehicle. As EVs are making a splash in the automotive industry, researchers are taking a greater interest in studying, modelling, and simulating them. Controlling EV speed is not an easy task. Modelling and Conventional Proportional Integral Derivative (PID) controller tuning for EV speed regulation are presented in this paper. To design and simulation of EV speed control in MATLAB, the transfer function of EV is derived and considered. PID controller is found to be easy to implement, practical, and provide superior closed-loop performance. Two PID tuning techniques, Ziegler-Nichols (ZN) and a Reinforcement Learning (RL) method are designed to regulate the EV speed. Characteristics of the time domain have been used to perform a comparative analysis. Additionally, the Integral Square Error (ISE) is analysed to determine the optimal PID tuning strategy for EV speed regulation.	Conference Paper	2023	10.1109/ICICT57646.2023.10134037	[]	[]	-1	-1	-1
B	Zhiqian Qiao; Schneider, J.; Dolan, J.M.	Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning*	2021	2021 IEEE International Conference on Robotics and Automation (ICRA)		2667	73	For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure [1] allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.	Conference Paper	2021	10.1109/ICRA48506.2021.9561095	[]	[]	-1	-1	-1
C	Zhu, Qi; Huang, Zhenhua; Sun, Zhenping; Liu, Daxue; Dai, Bin	Reinforce lent Learning based Throttle and Brake Control for Autonomous Vehicle Following	2017	2017 CHINESE AUTOMATION CONGRESS (CAC)	Chinese Automation Congress	6657	6662	"In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower. A reinforcement learning based throttle and brake control approach is developed for the follower vehicle. Near optimal control law is directly learned by ""trial and error"" with the neural dynamic programming algorithm. According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower. Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim."	Proceedings Paper	2017		[]	[]	-1	-1	-1
J	Chen, Longquan; He, Ying; Wang, Qiang; Pan, Weike; Ming, Zhong	Joint Optimization of Sensing, Decision-Making and Motion-Controlling for Autonomous Vehicles: A Deep Reinforcement Learning Approach	2022	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		4642	4654	The three main modules of autonomous vehicles, i.e., sensing, decision making, and motion controlling, have been studied separately in most existing works on autonomous driving, which overlook the correlations among these modules, leading to a result of unsatisfactory performance. In this paper, we propose a novel scheme that first tactfully processes the sensing data, then jointly learns and optimizes the decision-making and motion-controlling using reinforcement learning (RL). Specifically, the proposed scheme designs a novel state representation mechanism, where the sensing data goes through the attention layer and the convolutional neural network (CNN) layer sequentially. The attention layer focuses on extracting the most important local information and then CNN layer takes a broad view to comprehensively consider the global information for a better representation. Furthermore, the proposed scheme jointly learns decision-making and motion-controlling, therefore, the relevance of these two modules is implicitly considered, which helps to achieve a better autonomous driving policy. Extensive simulation results show that the proposed scheme is better than classic control methods and some RL methods in terms of safety, velocity, etc. We also demonstrate the respective functions of the attention layer and the CNN layer through ablation studies. Finally, we construct a traffic scene with a real autonomous vehicle, and verified the feasibility of the proposed scheme.	Article	MAY 2022	10.1109/TVT.2022.3150793	[]	[]	-1	-1	-1
J	Fu, Yuchuan; Li, Changle; Yu, Fei Richard; Luan, Tom H.; Zhang, Yao	A Decision-Making Strategy for Vehicle Autonomous Braking in Emergency via Deep Reinforcement Learning	2020	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		5876	5888	Autonomous braking through vehicle precise decision-making and control to reduce accidents is a key issue, especially in the early diffusion phase of autonomous vehicle development. This paper proposes a deep reinforcement learning (DRL)-based autonomous braking decision-making strategy in an emergency situation. Three key influencing factors, including efficiency, accuracy and passengers' comfort, are fully considered and satisfied by the proposed strategy. First, the vehicle lane-changing process and the braking process are analyzed in detail, which include the critical factors in the design of the autonomous braking strategy. Second, we propose a DRL process that determines the optimal strategy for autonomous braking. Particularly, a multi-objective reward function is designed, which can compromise the rewards achieved of different brake moments, the degree of the accident, and the comfort of the passenger. Third, a typical actor-critic (AC) algorithm named deep deterministic policy gradient (DDPG) is adopted for solving the autonomous braking problem, which can improve the efficiency of the optimal strategy and be stable in continuous control tasks. Once the strategy is well trained, the vehicle can automatically take optimal braking behavior in an emergency to improve driving safety. Extensive simulations validate the effectiveness and efficiency of our proposal in terms of learning effectiveness, decision-making accuracy and driving safety.	Article	JUN 2020	10.1109/TVT.2020.2986005	[]	[]	-1	-1	-1
J	ê¹€ë¬¸ì¢…; ìµœê¸°ì°½; ì˜¤ë³‘í™”; ì–‘ì§€í›ˆ	Local Path Generation Method for Unmanned Autonomous Vehicles Using Reinforcement Learning	2014	KIPS Transactions on Software and Data Engineering		369	374	Path generation methods are required for safe and efficient driving in unmanned autonomous vehicles. There are two kinds ofpaths: global and local. A global path consists of all the way points including the source and the destination. A local path is thetrajectory that a vehicle needs to follow from a way point to the next in the global path. In this paper, we propose a novel methodfor local path generation through machine learning, with an effective curve function used for initializing the trajectory. First,reinforcement learning is applied to a set of candidate paths to produce the best trajectory with maximal reward. Then the optimalsteering angle with respect to the trajectory is determined by training an artificial neural network. Our method outperformed existingapproaches and successfully found quality paths in various experimental settings, including the cases with obstacles.	research-article	2014	10.3745/KTSDE.2014.3.9.369	[]	[]	-1	-1	-1
J	Guan, Jiayi; Chen, Guang; Huang, Jin; Li, Zhijun; Xiong, Lu; Hou, Jing; Knoll, Alois	A Discrete Soft Actor-Critic Decision-Making Strategy With Sample Filter for Freeway Autonomous Driving	2023	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		2593	2598	Autonomous driving is a promising technology to reduce traffic accidents and improve driving efficiency. Although significant progress has been achieved, existing decision-making systems of autonomous vehicle still cannot meet the safety and driving efficiency requirements in highly dynamic environments. In this work, we design a discrete decision-making strategy based on the discrete soft actor-critic with sample filter algorithm (DSAC-SF) to improve driving efficiency and safety on freeways with dynamics traffic. Specifically, we first propose a sample filter method for discrete soft actor-critic, which improves the sample efficiency and stability of the algorithm via enhancing the utilization of effective samples. Subsequently, we construct the discrete decision-making strategy for autonomous driving based on the DSAC-SF algorithm, and further design the area observation method and the multi-objective reward function to improve the driving safety and efficiency. Finally, we carry out comparison and ablation experiments on the the scalable multi-agent reinforcement learning training school (SMARTS) simulation environment. Experimental results indicate that our strategy obtains a high success rate and a fast vehicle speed in the decision-making tasks on freeways. Moreover, our DSAC-SF algorithm also achieves improved performance in training efficiency and stability compared to the commonly used discrete reinforcement learning algorithm.	Article	FEB 2023	10.1109/TVT.2022.3212996	[]	[]	-1	-1	-1
J	Bhattacharyya, Raunak; Wulfe, Blake; Phillips, Derek J.; Kuefler, Alex; Morton, Jeremy; Senanayake, Ransalu; Kochenderfer, Mykel J.	Modeling Human Driving Behavior Through Generative Adversarial Imitation Learning	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		2874	2887	An open problem in autonomous vehicle safety validation is building reliable models of human driving behavior in simulation. This work presents an approach to learn neural driving policies from real world driving demonstration data. We model human driving as a sequential decision making problem that is characterized by non-linearity and stochasticity, and unknown underlying cost functions. Imitation learning is an approach for generating intelligent behavior when the cost function is unknown or difficult to specify. Building upon work in inverse reinforcement learning (IRL), Generative Adversarial Imitation Learning (GAIL) aims to provide effective imitation even for problems with large or continuous state and action spaces, such as modeling human driving. This article describes the use of GAIL for learning-based driver modeling. Because driver modeling is inherently a multi-agent problem, where the interaction between agents needs to be modeled, this paper describes a parameter-sharing extension of GAIL called PS-GAIL to tackle multi-agent driver modeling. In addition, GAIL is domain agnostic, making it difficult to encode specific knowledge relevant to driving in the learning process. This paper describes Reward Augmented Imitation Learning (RAIL), which modifies the reward signal to provide domain-specific knowledge to the agent. Finally, human demonstrations are dependent upon latent factors that may not be captured by GAIL. This paper describes Burn-InfoGAIL, which allows for disentanglement of latent variability in demonstrations. Imitation learning experiments are performed using NGSIM, a real-world highway driving dataset. Experiments show that these modifications to GAIL can successfully model highway driving behavior, accurately replicating human demonstrations and generating realistic, emergent behavior in the traffic flow arising from the interaction between driving agents.	Article; Early Access		10.1109/TITS.2022.3227738	[]	[]	-1	-1	-1
C	Xing, Jinwei; Zou, Xinyun; Krichmar, Jeffrey L.	Neuromodulated Patience for Robot and Self-Driving Vehicle Navigation	2020	2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)	IEEE International Joint Conference on Neural Networks (IJCNN)			Robots and self-driving vehicles face a number of challenges when navigating through real environments. Successful navigation in dynamic environments requires prioritizing subtasks and monitoring resources. Animals are under similar constraints. It has been shown that the neuromodulator serotonin (5-HT) regulates impulsiveness and patience in animals. In the present paper, we take inspiration from the serotonergic system and apply it to the task of robot navigation. In a set of outdoor experiments, we show how changing the level of patience can affect the amount of time the robot will spend searching for a desired location. To navigate GPS compromised environments, we introduce a deep reinforcement learning paradigm in which the robot learns to follow sidewalks. This may further regulate a tradeoff between a smooth long route and a rough shorter route. Using patience as a parameter may be beneficial for autonomous systems under time pressure.	Proceedings Paper	2020	10.1109/ijcnn48605.2020.9206642	[]	[]	-1	-1	-1
J	Cho, Kyunghoon	A Hierarchical Learning Approach to Autonomous Driving Using Rule Specifications	2022	IEEE ACCESS		74815	74824	"Understanding the movement of surrounding objects and controlling robot platforms (such as autonomous vehicles and social robots) in a safe way are challenging problems. In the autonomous driving problem, autonomous vehicles must take into account the future behaviors of nearby vehicles and make appropriate controls accordingly. This is the biggest factor that makes autonomous driving problems difficult. In this work, this problem is tackled by combining benefits from both sequence prediction and deep reinforcement learning in a hierarchical manner. The driver's behavior is classified according to the driving style defined by the rules selected in the autonomous driving situation. High-level behavior represents driving style, and the vehicle's movement model is trained to condition itself to high-level behavior. Instead of directly finding low-level controls, we focus on finding high-level behaviors to increase efficiency. For example, if an autonomous vehicle needs to change lanes in certain situations, the high-level behavior ""change lanes"" is first selected and the corresponding vehicle movement model is used to find the appropriate low-level controls. Reinforcement learning is used to help select the best high-level behavior, and future behaviors of nearby vehicles are jointly reasoned to lead to better understanding of the current situation. The feasibility of the proposed approach is tested in publicly available datasets. The proposed method shows better efficiency and performance compared to existing learning-based control algorithms."	Article	2022	10.1109/ACCESS.2022.3191434	[]	[]	-1	-1	-1
C	Martinson, Michael; Skrynnik, Alexey; Panov, Aleksandr, I	Navigating Autonomous Vehicle at the Road Intersection Simulator with Reinforcement Learning	2020	ARTIFICIAL INTELLIGENCE	Lecture Notes in Artificial Intelligence	71	84	In this paper, we consider the problem of controlling an agent that simulates the behavior of an self-driving car when passing a road intersection together with other vehicles. We consider the case of using smart city systems, which allow the agent to get full information about what is happening at the intersection in the form of video frames from surveillance cameras. The paper proposes the implementation of a control system based on a trainable behavior generation module. The agent's model is implemented using reinforcement learning (RL) methods. In our work, we analyze various RL methods (PPO, Rainbow, TD3), and variants of the computer vision subsystem of the agent. Also, we present our results of the best implementation of the agent when driving together with other participants in compliance with traffic rules.	Proceedings Paper	2020	10.1007/978-3-030-59535-7_6	[]	[]	-1	-1	-1
J	Sanmin, Kim; Kim, Youngseok; Jeon, Hyeongseok; Kum, Dongsuk; Lee, Kibeom	Autonomous Driving Technology Trend and Future Outlook: Powered by Artificial Intelligence	2022	Transactions of KSAE		819	830	Autonomous driving is not a new concept, and relevant technology has been developed for a long time. However, in recent years, autonomous driving technology has been leaping forward, fueled by the advance of AI-based technologies. In particular, the essential components of autonomous driving, such as perception, prediction, and planning, deliver entirely different performances from those of the pre-AI era. In this study, the trends and development of autonomous driving technology will be analyzed by decomposing it into element technologies ranging from perception, prediction, and planning, focusing on AI-based research. For the perception part, LiDAR and camera-based research and sensor fusion technologies will be examined. For the prediction part, we will look into various prediction paradigms such as interaction-aware and map-based prediction. The planning part will cover maneuver decisions, motion planning, and reinforcement learning-based methods.	research-article	2022		[]	[]	-1	-1	-1
J	Kumar, Abhishek	REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES	2023	MECHATRONIC SYSTEMS AND CONTROL		53	57	Reinforcement learning (RL) is one of the most emerging domains of artificial intelligence. It is widely used in almost all sort of applications, including medical field, stock market, forecasting, and engineering field. One of the most effective uses of RL is in the control engineering domain owing to its learning by trial, for example, in building autonomous system like autonomous vehicle and robotics. In this paper, we have focused on the applications of RL in various control engineering problems. Stability of controller (or agent) using RL paradigm is a very crucial task due to exploration-exploitation policy used by any RL. Also, the unavailability of exact model of system or environment may lead to unsafe behaviour of the agent. Therefore, this paper focuses mainly on the stability aspect in RL-based controller. Many concepts are used to study and analyse the stability of a system viz Lyapunov method and Barrier function. This paper surveys the detailed application of these well-established stability certifier methods in various model-free and model-based RL framework.	Article	2023	10.2316/J.2023.201-0347	[]	[]	-1	-1	-1
J	Irshayyid, Ali; Chen, Jun	Comparative Study of Cooperative Platoon Merging Control Based on Reinforcement Learning	2023	SENSORS				The time that a vehicle merges in a lane reduction can significantly affect passengers' safety, comfort, and energy consumption, which can, in turn, affect the global adoption of autonomous electric vehicles. In this regard, this paper analyzes how connected and automated vehicles should cooperatively drive to reduce energy consumption and improve traffic flow. Specifically, a model-free deep reinforcement learning approach is used to find the optimal driving behavior in the scenario in which two platoons are merging into one. Several metrics are analyzed, including the time of the merge, energy consumption, and jerk, etc. Numerical simulation results show that the proposed framework can reduce the energy consumed by up to 76.7%, and the average jerk can be decreased by up to 50%, all by only changing the cooperative merge behavior. The present findings are essential since reducing the jerk can decrease the longitudinal acceleration oscillations, enhance comfort and drivability, and improve the general acceptance of autonomous vehicle platooning as a new technology.	Article	JAN 2023	10.3390/s23020990	[]	[]	-1	-1	-1
J	Clark, E.; Hunter, A.; Isupova, O.; Donnelly, M.	Exploring the use of AI in marine acoustic sensor management	2022	Proceedings of Meetings on Acoustics		070008 (11 pp.)	070008 (11 pp.)	Underwater passive acoustic source detection and tracking is important for various marine applications, including marine mammal monitoring and naval surveillance. The performance in these applications is dependent on the placement and operation of sensing assets, such as autonomous underwater vehicles. Conventionally, these decisions have been made by human operators aided by acoustic propagation modelling tools, situational and environmental data, and experience. However, this is time-consuming and computationally expensive. We consider a 'toy problem' of a single autonomous vehicle (agent) in search of a stationary source of low frequency within a reinforcement learning (RL) architecture. We initially choose the observation space to be the agent's current position. The agent is allowed to explore the environment with a limited action space, taking equal distance steps in one of $n$ directions. Rewards are received for positive detections of the source. Using OpenAI's PPO algorithm an increase in median episode reward of approximately 20 points in the RL environment developed is seen when the agent is given a history of it's previous moves and signal-to-noise ratio compared to the simple state. The future expansion of the RL framework is discussed in terms of the observation and action spaces, reward function and RL architecture. [The copyright for the referenced work is owned by Acoustical Society of America. Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]	Journal Paper	2022	10.1121/2.0001601	[]	[]	-1	-1	-1
C	De Silva, Varuna; Wang, Xiongzhao; Aladagli, Deniz; Kondoz, Ahmet; Ekmekcioglu, Erhan	An Agent-Based Modelling Framework for Driving Policy Learning in Connected and Autonomous Vehicles	2019	INTELLIGENT SYSTEMS AND APPLICATIONS, INTELLISYS, VOL 2	Advances in Intelligent Systems and Computing	113	125	Due to the complexity of the natural world, a programmer cannot foresee all possible situations, a connected and autonomous vehicle (CAV) will face during its operation, and hence, CAVs will need to learn to make decisions autonomously. Due to the sensing of its surroundings and information exchanged with other vehicles and road infrastructure, a CAV will have access to large amounts of useful data. While different control algorithms have been proposed for CAVs, the benefits brought about by connectedness of autonomous vehicles to other vehicles and to the infrastructure, and its implications on policy learning has not been investigated in literature. This paper investigates a data driven driving policy learning framework through an agent-based modelling approaches. The contributions of the paper are two-fold. A dynamic programming framework is proposed for in-vehicle policy learning with and without connectivity to neighboring vehicles. The simulation results indicate that while a CAV can learn to make autonomous decisions, vehicle-to-vehicle (V2V) communication of information improves this capability. Furthermore, to overcome the limitations of sensing in a CAV, the paper proposes a novel concept for infrastructure-led policy learning and communication with autonomous vehicles. In infrastructure-led policy learning, road-side infrastructure senses and captures successful vehicle maneuvers and learns an optimal policy from those temporal sequences, and when a vehicle approaches the road-side unit, the policy is communicated to the CAV. Deep-imitation learning methodology is proposed to develop such an infrastructure-led policy learning framework.	Proceedings Paper	2019	10.1007/978-3-030-01057-7_10	[]	[]	-1	-1	-1
B	Xu, L.; Sun, S.; Zhang, Y.D.; Petropulu, A.	Joint Antenna Selection and Beamforming in Integrated Automotive Radar Sensing-Communications with Quantized Double Phase Shifters	2023	ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)		1	5	We consider an integrated sensing-communication system operating in a dynamic environment, such as an autonomous vehicle scenario. We propose a novel, low-cost, low power consumption and low-computation approach for designing a beam that can simultaneously reach the radar target of interest and the desired communication destination. The transmitter is a uniform linear array, equipped with quantized double phase shifters, which enables a flexible beam design while using analog only processing. Only a small number of antennas are selected to transmit in each channel use, in order to save system power and reduce antenna coupling. We propose a deep reinforcement learning approach to adaptively adjust the double phase shifters and select the active antennas in order to optimize the transmit beamforming, through a transmission and feedback trail. The actor-critic network strategy together with the Wolpertinger policy is adopted to obtain the optimal solutions efficiently and effectively. Numerical results demonstrate the feasibility of the proposed method.	Conference Paper	2023	10.1109/ICASSP49357.2023.10097184	[]	[]	-1	-1	-1
J	Hart, F.; Waltz, M.; Okhrin, O.	Two-step dynamic obstacle avoidance [arXiv]	2024	Two-step dynamic obstacle avoidance [arXiv]				Dynamic obstacle avoidance (DOA) is a fundamental challenge for any autonomous vehicle, independent of whether it operates in sea, air, or land. This paper proposes a two-step architecture for handling DOA tasks by combining supervised and reinforcement learning (RL). In the first step, we introduce a data-driven approach to estimate the collision risk of an obstacle using a recurrent neural network, which is trained in a supervised fashion and offers robustness to non-linear obstacle movements. In the second step, we include these collision risk estimates into the observation space of an RL agent to increase its situational awareness.~We illustrate the power of our two-step approach by training different RL agents in a challenging environment that requires to navigate amid multiple obstacles. The non-linear movements of obstacles are exemplarily modeled based on stochastic processes and periodic patterns, although our architecture is suitable for any obstacle dynamics. The experiments reveal that integrating our collision risk metrics into the observation space doubles the performance in terms of reward, which is equivalent to halving the number of collisions in the considered environment. Furthermore, we show that the architecture's performance improvement is independent of the applied RL algorithm.	Preprint	2024		[]	[]	-1	-1	-1
J	Rais, Mohamed Saber; Boudour, Rachid; Zouaidia, Khouloud; Bougueroua, Lamine	Decision making for autonomous vehicles in highway scenarios using Harmonic SK Deep SARSA	2023	APPLIED INTELLIGENCE		2488	2505	"The complexity of taking decisions for an autonomous vehicle (AV) to avoid road accident fatalities, provide safety, comfort, and reduce traffic raises the need for improvements in the field of decision making. To solve these challenges, many algorithms and techniques were applied, and the most common ones were reinforcement learning (RL) algorithms combined with deep learning techniques. Therefore, in this paper we proposed a novel extension of the popular ""SARSA"" (State-Action-Reward-State-Action) RL technique called ""Harmonic SK Deep SARSA"" that takes advantage of the stability which SARSA algorithm provides and uses the notion of similar and cumulative states saved in an alternative memory to enhance the stability of the algorithm and achieve remarkable performance that SARSA could not accomplish due to its on policy nature. Through the investigation of our novel extension the adaptability of the algorithm to unexpected situations during learning and to unforeseen changes in the environment was proved while reducing the computational load in the learning process and increasing the convergence rate that plays a key role in upgrading decision making application that require numerous real time consecutive decisions, including autonomous vehicles, industrial robots, gaming, aerial navigation... The novel algorithm was tested in a gym environment simulator called ""Highway-env"" with multiple highway situations (multiple lanes configurations, highway with dynamic number of lanes (from 4-lane to 2-lane, from 4-lane to 6-lane), merge) with numerous dynamic obstacles. For the purpose of comparison, we used a benchmark of cutting edge algorithms known for their prominent performance. The experimental results showed that the proposed algorithm outperformed the comparison algorithms in learning stability and performance that were validated by the following metrics: average loss value per episode, average accuracy per episode, maximum speed value reached per episode, average speed per episode, and the total reward per episode."	Article; Early Access		10.1007/s10489-022-03357-y	[]	[]	-1	-1	-1
J	Hester, Todd; Stone, Peter	TEXPLORE: real-time sample-efficient reinforcement learning for robots	2013	MACHINE LEARNING		385	429	The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations online. RL is a paradigm for learning sequential decision making tasks, usually formulated as a Markov Decision Process (MDP). For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. In addition, the algorithm must learn efficiently in the face of noise, sensor/actuator delays, and continuous state features. In this article, we present texplore, the first algorithm to address all of these challenges together. texplore is a model-based RL method that learns a random forest model of the domain which generalizes dynamics to unseen states. The agent explores states that are promising for the final policy, while ignoring states that do not appear promising. With sample-based planning and a novel parallel architecture, texplore can select actions continually in real-time whenever necessary. We empirically evaluate the importance of each component of texplore in isolation and then demonstrate the complete algorithm learning to control the velocity of an autonomous vehicle in real-time.	Article	MAR 2013	10.1007/s10994-012-5322-7	[]	[]	-1	-1	-1
B	Lakshmanan, M.; Anandha Mala, G.S.	Augmented Random Search based Autonomous Driving System	2023	2023 5th International Conference on Smart Systems and Inventive Technology (ICSSIT)		1421	4	The development of autonomous driving technology led to a rise in the popularity of self-driving automobiles. CARLA is an open-source simulator for autonomous driving research and is used in these autonomous driving systems. From the beginning, CARLA has supported the creation, instruction, and testing of autonomous driving systems. Along with this, CARLA offers free to use open digital assets (such as city plans, structures, and vehicles). To perform addition and deletion of random sounds from the weight and tracking the entire rewards received as a result of this weight modification, the Augmented Random Search (ARS) approach trains a supervised learning on input data. The Augmented Random Generator is then used to measure the weights depending on these incentives in a predetermined number of episodes at a predestined learning rate. The Algorithm for Robotic Search (ARS) will be used to train self-driving cars. Automobiles based on data received from each car's front cameras. As a result of this research, a framework for training self-driving cars has been developed. Carla's policy employing ARS will be created with it as the core algorithm, giving a more realistic picture of how things work. Because of its novelty, ARS is an extremely light tool for analyzing difficult control tasks, and the authors of the study discovered that ARS had at least 15 times the computing efficiency of the fastest competitive learning approaches. CARLA has been used to provide more consistent results from autonomous vehicle training, and this research has proved successful considering how many unique circumstances there are, in opening up the majority of the chances for additional study on the same issue.	Conference Paper	2023	10.1109/ICSSIT55814.2023.10061010	[]	[]	-1	-1	-1
J	Pin Wang; Hanhan Li; Ching-Yao Chan	Quadratic Q-network for Learning Continuous Control for Autonomous Vehicles [arXiv]	2019	arXiv		9 pp.	9 pp.	Reinforcement Learning algorithms have recently been proposed to learn time-sequential control policies in the field of autonomous driving. Direct applications of Reinforcement Learning algorithms with discrete action space will yield unsatisfactory results at the operational level of driving where continuous control actions are actually required. In addition, the design of neural networks often fails to incorporate the domain knowledge of the targeting problem such as the classical control theories in our case. In this paper, we propose a hybrid model by combining Q-learning and classic PID (Proportion Integration Differentiation) controller for handling continuous vehicle control problems under dynamic driving environment. Particularly, instead of using a big neural network as Q-function approximation, we design a Quadratic Q-function over actions with multiple simple neural networks for finding optimal values within a continuous space. We also build an action network based on the domain knowledge of the control mechanism of a PID controller to guide the agent to explore optimal actions more efficiently.We test our proposed approach in simulation under two common but challenging driving situations, the lane change scenario and ramp merge scenario. Results show that the autonomous vehicle agent can successfully learn a smooth and efficient driving behavior in both situations.	Journal Paper	29 Nov. 2019		[]	[]	-1	-1	-1
J	Xiaoyuan Fu; Quan Yuan; Shifan Liu; Baozhu Li; Qi Qi; Jingyu Wang	Communication-efficient decision-making of digital twin assisted Internet of vehicles: A hierarchical multi-agent reinforcement learning approach	2023	China Communications		55	68	The connected autonomous vehicle is considered an effective way to improve transport safety and efficiency. To overcome the limited sensing and computing capabilities of individual vehicles, we design a digital twin assisted decision-making framework for Internet of Vehicles, by leveraging the integration of communication, sensing and computing. In this framework, the digital twin entities residing on edge can effectively communicate and cooperate with each other to plan sub-targets for their respective vehicles, while the vehicles only need to achieve the sub-targets by generating a sequence of atomic actions. Furthermore, we propose a hierarchical multiagent reinforcement learning approach to implement the framework, which can be trained in an end-to-end way. In the proposed approach, the communication interval of digital twin entities could adapt to time-varying environment. Extensive experiments on driving decision-making have been performed in traffic junction scenarios of different difficulties. The experimental results show that the proposed approach can largely improve collaboration efficiency while reducing communication overhead.	Journal Paper	2023	10.23919/JCC.2023.03.005	[]	[]	-1	-1	-1
J	Meneses-Cime, Karina; Guvenc, Bilin Aksun; Guvenc, Levent	Optimization of On-Demand Shared Autonomous Vehicle Deployments Utilizing Reinforcement Learning	2022	SENSORS				Ride-hailed shared autonomous vehicles (SAV) have emerged recently as an economically feasible way of introducing autonomous driving technologies while serving the mobility needs of under-served communities. There has also been corresponding research work on optimization of the operation of these SAVs. However, the current state-of-the-art research in this area treats very simple networks, neglecting the effect of a realistic other traffic representation, and is not useful for planning deployments of SAV service. In contrast, this paper utilizes a recent autonomous shuttle deployment site in Columbus, Ohio, as a basis for mobility studies and the optimization of SAV fleet deployment. Furthermore, this paper creates an SAV dispatcher based on reinforcement learning (RL) to minimize passenger wait time and to maximize the number of passengers served. The created taxi-dispatcher is then simulated in a realistic scenario while avoiding generalization or over-fitting to the area. It is found that an RL-aided taxi dispatcher algorithm can greatly improve the performance of a deployment of SAVs by increasing the overall number of trips completed and passengers served while decreasing the wait time for passengers.	Article	NOV 2022	10.3390/s22218317	[]	[]	-1	-1	-1
C	Chronis, Christos; Sardianos, Christos; Varlamis, Iraklis; Michail, Dimitrios; Tserpes, Konstantinos	A driving profile recommender system for autonomous driving using sensor data and reinforcement learning	2021	25TH PAN-HELLENIC CONFERENCE ON INFORMATICS WITH INTERNATIONAL PARTICIPATION (PCI2021)		33	38	The design of algorithms for autonomous vehicles includes a wide range of machine learning tasks including scene perception by the visual input from cameras and other sensors, monitoring and prediction of the driver and passengers' state, and others. The aim of the present work is to study the task of personalizing the driving experience in an autonomous vehicle, taking into account the particularities and differences of each person in how he/she perceives the vehicle's velocity. For this purpose, we employ the Actor-Critic Reinforcement Learning technique in order to automatically select the best driving mode during driving. The input to the actor-critic model comprises the driver's stress and excitement, which are affected by the route conditions, and the vehicle velocity and angular velocity. The output at each step is the best mode for each driver, which better balances stress, excitement, and route completion time. The whole setup is simulated and tested within the Carla open-source simulator for autonomous driving research.	Proceedings Paper	2021	10.1145/3503823.3503830	[]	[]	-1	-1	-1
C	Zheng, Jinkai; Mu, Phil K.; Man, Ziqian; Luan, Tom H.; Cai, Lin X.; Shan, Hangguan	Device Placement for Autonomous Vehicles using Reinforcement Learning	2021	IEEE CONGRESS ON CYBERMATICS / 2021 IEEE INTERNATIONAL CONFERENCES ON INTERNET OF THINGS (ITHINGS) / IEEE GREEN COMPUTING AND COMMUNICATIONS (GREENCOM) / IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING (CPSCOM) / IEEE SMART DATA (SMARTDATA)		190	196	Autonomous driving is a complex function consisting of multiple parallel AI tasks running at the same time for information sensing, fusion, and decision making. To process the complex computing tasks, an autonomous vehicle is typically equipped with different processing units at the same time, such as CPU, GPU, FPGA, of the different computing capabilities. As the AI tasks have different requirements on the computing resources, a fundamental issue is how to optimally allocate the real-time computation tasks to different processing units (as known as device placement) on board towards maximum utility for autonomous driving. Towards the issue, this paper develops a reinforcement learning algorithm which is based on the proximal policy optimization (PPO) specifically for finding the optimal device placement for running a neural network model. A sequence-to-sequence model is proposed to allocate the operations of a neural network model on appropriate computing units in the vehicle. The execution time and energy consumption of the placement solution is used as a reward signal to further optimize the parameters. We implement our algorithm in different benchmarks, and compare it with different baselines. Experiments have demonstrated that our algorithm can find the optimal device placement position, and its performance is better than previous state-of-the-art RL algorithm as well as traditional methods.	Proceedings Paper	2021	10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics53846.2021.00041	[]	[]	-1	-1	-1
J	Bharilya, V.; Kumar, N.	Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions [arXiv]	2023	arXiv				Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future behavior of nearby traffic participants, similar to the predictive driving abilities of human drivers. Building upon existing literature is crucial to advance the field and develop a comprehensive understanding of trajectory prediction methods in the context of automated driving. To address this need, we have undertaken a comprehensive review that focuses on trajectory prediction methods for AVs, with a particular emphasis on machine learning techniques including deep learning and reinforcement learning-based approaches. We have extensively examined over two hundred studies related to trajectory prediction in the context of AVs. The paper begins with an introduction to the general problem of predicting vehicle trajectories and provides an overview of the key concepts and terminology used throughout. After providing a brief overview of conventional methods, this review conducts a comprehensive evaluation of several deep learning-based techniques. Each method is summarized briefly, accompanied by a detailed analysis of its strengths and weaknesses. The discussion further extends to reinforcement learning-based methods. This article also examines the various datasets and evaluation metrics that are commonly used in trajectory prediction tasks. Encouraging an unbiased and objective discussion, we compare two major learning processes, considering specific functional features. By identifying challenges in the existing literature and outlining potential research directions, this review significantly contributes to the advancement of knowledge in the domain of AV trajectory prediction.	Journal Paper	12 July 2023		[]	[]	-1	-1	-1
B	Forbes, Jeffrey Roderick Norman	Reinforcement learning for autonomous vehicles	2002						Dissertation/Thesis	Jan 01 2002		[]	[]	-1	-1	-1
B	Azadani, M.N.; Boukerche, A.	A Context-Aware Path Forecasting Method for Connected Autonomous Vehicles	2023	ICC 2023 - IEEE International Conference on Communications		247	52	Forecasting the future paths of surrounding vehicles of a Connected Autonomous Vehicle (CAV) can enhance connectivity and efficiency of vehicular networks, and accurate motion forecasting of nearby vulnerable road users can advance road safety and urban mobility. This task needs a high-level situational awareness for the CAV. Early methods rely solely on vehicle kinematics and overlook the uncertainty within agents behavior and the effects of surrounding context on the behavior of nearby agents, resulting in lower performance or infeasible predictions. In the current work, we introduce a novel context-aware forecasting approach for CAVs that benefits from inverse reinforcement learning (IRL) to condition the future motions of nearby agents on scene-based state sequences defined using a Markov Decision Process. More precisely, we map the images of the surrounding context and the behavior history of agents into rewards and learn optimal expert behaviors using IRL. We validate the path forecasting efficiency of our model using two large motion prediction benchmarks with different scenes and achieve state-of-the-art results in terms of FDE and ADE metrics.	Conference Paper	2023	10.1109/ICC45041.2023.10279653	[]	[]	-1	-1	-1
J	Zhang, Ruixian; Han, Yining; Su, Man; Lin, Zefeng; Li, Haowei; Zhang, Lixian	Robust reinforcement learning with UUB guarantee for safe motion control of autonomous robots	2023	SCIENCE CHINA-TECHNOLOGICAL SCIENCES				This paper addresses the issue of safety in reinforcement learning (RL) with disturbances and its application in the safety-constrained motion control of autonomous robots. To tackle this problem, a robust Lyapunov value function (rLVF) is proposed. The rLVF is obtained by introducing a data-based LVF under the worst-case disturbance of the observed state. Using the rLVF, a uniformly ultimate boundedness criterion is established. This criterion is desired to ensure that the cost function, which serves as a safety criterion, ultimately converges to a range via the policy to be designed. Moreover, to mitigate the drastic variation of the rLVF caused by differences in states, a smoothing regularization of the rLVF is introduced. To train policies with safety guarantees under the worst disturbances of the observed states, an off-policy robust RL algorithm is proposed. The proposed algorithm is applied to motion control tasks of an autonomous vehicle and a cartpole, which involve external disturbances and variations of the model parameters, respectively. The experimental results demonstrate the effectiveness of the theoretical findings and the advantages of the proposed algorithm in terms of robustness and safety.	Article; Early Access		10.1007/s11431-023-2435-3	[]	[]	-1	-1	-1
J	Fu, Yuchuan; Li, Changle; Luan, Tom H. H.; Zhang, Yao	IoV and blockchain-enabled driving guidance strategy in complex traffic environment	2023	CHINA COMMUNICATIONS				Diversified traffic participants and complex traffic environment (e.g., roadblocks or road damage exist) challenge the decision-making accuracy of a single connected and autonomous vehicle (CAV) due to its limited sensing and computing capabilities. Using Internet of Vehicles (IoV) to share driving rules between CAVs can break limitations of a single CAV, but at the same time may cause privacy and safety issues. To tackle this problem, this paper proposes to combine IoV and blockchain technologies to form an efficient and accurate autonomous guidance strategy. Specifically, we first use reinforcement learning for driving decision learning, and give the corresponding driving rule extraction method. Then, an architecture combining IoV and blockchain is designed to ensure secure driving rule sharing. Finally, the shared rules will form an effective autonomous driving guidance strategy through driving rules selection and action selection. Extensive simulation proves that the proposed strategy performs well in complex traffic environment, mainly in terms of accuracy, safety, and robustness.	Article; Early Access		10.23919/JCC.ea.2020-0174.202302	[]	[]	-1	-1	-1
J	Lee, R.; Puig-Navarro, J.; Agogino, A.K.; Giannakopoulou, D.; Mengshoel, O.J.; Kochenderfer, M.J.; Allen, B.D.	Adaptive Stress Testing of Trajectory Planning Systems	2019	2019 AIAA Science and Technology Forum and Exposition (SciTech)		16 pp.	16 pp.	Trajectory planners play a critical role in many autonomous vehicle systems, such as unmanned aircraft and driverless cars. However, analyzing the safety of these systems is very challenging due to their complexity. Existing validation approaches are generally focused on the correctness of the plans in isolation, while ignoring that the planner may not be able to generate a valid plan at all due to unanticipated operational conditions. While large scale simulation testing can find some failures, they are extremely unlikely to uncover complex failure modes due to the complexity of the system and the rarity of failures. This paper proposes to use adaptive stress testing to efficiently search for failure scenarios. Adaptive stress testing is a black box testing method that uses reinforcement learning to sample paths from a simulator of the system under test and its operating environment. The algorithm adversarially optimizes the stochastic elements in the system's environment to find the most likely sequence of states that results in a failure event. We provide a detailed discussion of sources of uncertainty and failure modes in trajectory planning systems. We apply our approach to stress test a prototype trajectory planner and show that adaptive stress testing can find a variety of operational failures including collisions with obstacles and various planning failures.	Conference Paper	2019		[]	[]	-1	-1	-1
J	Zhang, Mei; Chen, Kai; Zhu, Jinhui	An efficient planning method based on deep reinforcement learning with hybrid actions for autonomous driving on highway	2023	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS		3483	3499	Due to the complexity and uncertainty of the traffic, planning for autonomous driving (AD) on highway is challenging. Traditional planning algorithms have the problems of low and unstable efficiency, which reduces the real-time performance of the autonomous vehicle (AV). Deep reinforcement learning (DRL) is an emerging and promising method that has achieved amazing performance in many fields. In this paper, we propose a novel planning approach based on soft actor critic (SAC) with hybrid actions. The algorithm takes the structured information of the ego vehicle and the surroundings as input, and generates a termination state on the Frenet space for ego vehicle, then a feasible and continuous spatiotemporal trajectory will be output by a polynomial planner based on the intermediate state. Different from other sampling-based planning methods, only single polynomial planning is required, which improves planning efficiency significantly. Experiments show that DRL agent with hybrid actions is more secure than the agents with only continuous or discrete actions. Compared with other planning methods, the proposed algorithm has the least and most robust time for planning in different scenarios.	Article; Early Access		10.1007/s13042-023-01845-2	[]	[]	-1	-1	-1
C	Friji, Hamdi; Ghazzai, Hakim; Besbes, Hichem; Massoud, Yehia	A DQN-based Autonomous Car-following Framework Using RGB-D Frames	2020	2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT)		45	50	Modeling car-following behavior has recently garnered much attention due to the wide variety of applications it may be utilized in, such as accident analysis, driver assessment, and support systems. Some of the latest approaches investigate scenario-based autonomous driving algorithms. In this paper, we propose an end-to-end car-following framework that, based on high dimensional RGB-D features only, it ensures autonomous driving by following the actions of a leader car while taking into account other environmental factors (e.g. pedestrians, sidewalk crashing, etc.) To this end, a reinforcement learning (RL) algorithm, precisely an improved Deep Q-Network algorithm, is designed to avoid crashes with the leader car and its detection loss while effectively driving on road. The model is trained and tested using the CARLA simulator in different environments. Our preliminary tests show promising results for enhancing the driving capabilities of autonomous vehicles in many situations such as highways, one-way roads, and no-overtaking roads.	Proceedings Paper	2020	10.1109/GCAIOT51063.2020.9345899	[]	[]	-1	-1	-1
C	Kimura, Hikaru; Takahashi, Masaki; Nishiwaki, Kazuhiro; Iezawa, Masahiro	Decision-Making Based on Reinforcement Learning and Model Predictive Control Considering Space Generation for Highway On-Ramp Merging	2022	IFAC PAPERSONLINE		241	246	Reducing traffic accidents pertaining to autonomous vehicles has garnered attention. Merging on a highway is one of the most challenging problems that must be addressed for the realization of autonomous vehicles. It is difficult because an agent must decide where to merge in a complex and everchanging environment. Merging with congested highway traffic involves significant interaction with vehicles in the main lane. If there is no space for the autonomous vehicle to merge, it needs to work on vehicles in the main lane to create space and subsequently decide to merge or not. Reinforcement learning (RL) is a promising method for solving decision -making problems. However, it is difficult to guarantee the safety of the controller obtained using RL. Therefore, we propose a combined method in which decision -making is performed by RL and vehicle control by model predictive control (MPC) to ensure safety. The performance of the proposed system is tested by simulations. The proposed system made appropriate decisions according to the situation, and by controlling the vehicle in consideration of collision avoidance constraints, it showed a high merge success rate even in a crowded situation. Copyright 2022 The Authors. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)	Proceedings Paper	2022	10.1016/j.ifacol.2022.10.519	[]	[]	-1	-1	-1
J	Makantasis, K.; Kontorinaki, M.; Nikolos, I.	A deep reinforcement-learning-based driving policy for autonomous road vehicles [arXiv]	2019	arXiv		17 pp.	17 pp.	In this work we consider the problem of path planning for an autonomous vehicle that moves on a freeway. The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics. On the contrary, we propose the development of a driving policy based on reinforcement learning. In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required. We consider driving scenarios where the road is occupied both by autonomous and manual driving vehicles. To the best of our knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments. The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established SUMO microscopic traffic flow simulator. Finally, we present some initial results regarding the effect of autonomous vehicles' behavior on the overall traffic flow.	Journal Paper	10 July 2019		[]	[]	-1	-1	-1
J	Chen, Ziqin; Anglea, Timothy; Zhang, Yuanzhao; Wang, Yongqiang	Optimal synchronization in pulse-coupled oscillator networks using reinforcement learning	2023	PNAS NEXUS				Spontaneous synchronization is ubiquitous in natural and man-made systems. It underlies emergent behaviors such as neuronal response modulation and is fundamental to the coordination of robot swarms and autonomous vehicle fleets. Due to its simplicity and physical interpretability, pulse-coupled oscillators has emerged as one of the standard models for synchronization. However, existing analytical results for this model assume ideal conditions, including homogeneous oscillator frequencies and negligible coupling delays, as well as strict requirements on the initial phase distribution and the network topology. Using reinforcement learning, we obtain an optimal pulse-interaction mechanism (encoded in phase response function) that optimizes the probability of synchronization even in the presence of nonideal conditions. For small oscillator heterogeneities and propagation delays, we propose a heuristic formula for highly effective phase response functions that can be applied to general networks and unrestricted initial phase distributions. This allows us to bypass the need to relearn the phase response function for every new network.	Article	APR 3 2023	10.1093/pnasnexus/pgad102	[]	[]	-1	-1	-1
J	Huang Hui; Wei Hanbing	Lane Changing Trajectory Planning of Autonomous Vehicle Based on Driving Characteristic Learning	2021	Automobile Technology		19	25	Large deviation between vehicle planning trajectory and driver decision trajectory exists in the process of lane change for autonomous vehicles.To solve this problem,a lane change trajectory planning algorithm is developed based on learning trajectory feature.Based on the sampling and cost optimization combination of trajectory planning,the algorithm collects the driver's lane changing trajectory function characteristics.By means of the maximum entropy inverse reinforcement learning,cost function weight is updated iteratively.According to the achieved cost function,the alternative sampling paths are designated to generate lane changing trajectory of autonomous vehicles which reflect the characteristics of drivers' trajectories.The experimental results show that the lane changing trajectory after learning of drivers' characteristics are incorporated in the lane changing trajectory area of the driver.The trajectory's features are more similar to the real lane changing trajectory's features of the driver,and can reflect driver's subjective feeling.	Article	2021		[]	[]	-1	-1	-1
B	Tsai, J.; Chang, Y.-T.; Chuang, P.-H.; You, Z.	An Autonomous Vehicle-Following Technique for Self-Driving Cars Based on the Semantic Segmentation Technique	2023	2023 IEEE International Symposium on Robotic and Sensors Environments (ROSE)		1	7	Self-driving cars are an increasingly concerned and researched popular field. In this paper, we utilize the CARLA (Car Learning to Act) self-driving car simulator to build a custom simulation environment. The adopted environment consists of a figure-8-shaped driveway with two lanes, where the training car will follow a lead reference car with a varying speed on the inner lane and multiple reference cars will intermittently cut into the inner lane from the outer lane so as to interfere with the training car. This setup aims to more accurately emulate real-world traffic conditions. Specifically, we employ the Convolutional Neural Network (CNN) combined with the Deep Reinforcement Learning (DRL) technology to achieve autonomous control of the self-driving car. A semantic segmentation camera is installed beside the rearview mirror of the training car to capture the observation image of the road ahead while the car is moving, which is then fed into the DRL models for training and decision making, along with the car speed. Additionally, we design an appropriate reward mechanism for our models according to the current situation of the self-driving car to improve driving safety and comfort. We adopt the Deep Deterministic Policy Gradient (DDPG) and time-cycling Recurrent Deterministic Policy Gradient (RDPG) learning algorithms in DRL to train the self-driving car, aiming to achieve the goal of autonomously determining safe and comfortable driving paths while following other vehicles.	Conference Paper	2023	10.1109/ROSE60297.2023.10410810	[]	[]	-1	-1	-1
J	Lelko, A.; Nemeth, B.; Fenyes, D.; Gaspar, P.	Integration of Robust Control with Reinforcement Learning for Safe Autonomous Vehicle Motion	2023	IFAC PapersOnLine		1101	6	This paper presents a control design framework for the integration of robust control and reinforcement learning-based (RL) control agent. The proposed integration method is applied for motion control of autonomous road vehicles, providing safe motion. In the integrated motion control, longitudinal and lateral dynamics are incorporated. The high-performance motion of the vehicle, e.g., high-velocity motion, path following, and reduction of lateral acceleration, through the RL-based control agent is achieved. The training through Proximal Policy Optimization during episodes is performed. Safe motion with guaranteed performances, i.e., keeping limits on lateral error, through the robust control and the supervisor is achieved. The robust control is designed through the H method, and in the supervisor, a constrained quadratic programming task is performed. As a result, lateral and longitudinal control inputs of the vehicle are calculated by the integrated control system. The effectiveness of the proposed control method using simulation scenarios and test scenarios on a small-scaled test vehicle is illustrated. All rights reserved Elsevier.	Journal Paper	2023	10.1016/j.ifacol.2023.10.1711	[]	[]	-1	-1	-1
C	Greenwood, Garrison W.; Elsayed, Saber; Sarker, Ruhul; Abbass, Hussein A.	Online Generation of Trajectories for Autonomous Vehicles using a Multi-Agent System	2014	2014 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC)		1218	1224	Autonomous vehicles are frequently deployed in environments where only certain trajectories are feasible. Classical trajectory generation methods attempt to find a feasible trajectory that satisfies a set of constraints. In some cases the optimal trajectory may be known, but it is hidden from the autonomous vehicle. Under such circumstance the vehicle must discover a feasible trajectory. This paper describes a multi-agent system that uses a combination of reinforcement learning and differential evolution to generate a trajectory that is epsilon-close to a target trajectory that is hidden.	Proceedings Paper	2014		[]	[]	-1	-1	-1
J	Tsai, Jichiang; Chang, Che-Cheng; Li, Tzu	Autonomous Driving Control Based on the Technique of Semantic Segmentation	2023	SENSORS				Advanced Driver Assistance Systems (ADAS) are only applied to relatively simple scenarios, such as highways. If there is an emergency while driving, the driver should take control of the car to deal properly with the situation at any time. Obviously, this incurs the uncertainty of safety. Recently, in the literature, several studies have been proposed for the above-mentioned issue via Artificial Intelligence (AI). The achievement is exactly the aim that we look forward to, i.e., the autonomous vehicle. In this paper, we realize the autonomous driving control via Deep Reinforcement Learning (DRL) based on the CARLA (Car Learning to Act) simulator. Specifically, we use the ordinary Red-Green-Blue (RGB) camera and semantic segmentation camera to observe the view in front of the vehicle while driving. Then, the captured information is utilized as the input for different DRL models so as to evaluate the performance, where the DRL models include DDPG (Deep Deterministic Policy Gradient) and RDPG (Recurrent Deterministic Policy Gradient). Moreover, we also design an appropriate reward mechanism for these DRL models to realize efficient autonomous driving control. According to the results, only the RDPG strategies can finish the driving mission with the scenario that does not appear/include in the training scenario, and with the help of the semantic segmentation camera, the RDPG control strategy can further improve its efficiency.	Article	JAN 2023	10.3390/s23020895	[]	[]	-1	-1	-1
J	Wang, Yue; Sarkar, Esha; Li, Wenqing; Maniatakos, Michail; Jabari, Saif Eddin	Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems	2021	IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY		4772	4787	Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.	Article	2021	10.1109/TIFS.2021.3114024	[]	[]	-1	-1	-1
C	Chalaki, Behdad; Beaver, Logan E.; Remer, Ben; Jang, Kathy; Vinitsky, Eugene; Bayen, Alexandre M.; Malikopoulos, Andreas A.	Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning	2020	2020 IEEE 16TH INTERNATIONAL CONFERENCE ON CONTROL & AUTOMATION (ICCA)	IEEE International Conference on Control and Automation ICCA	35	40	In this article, we demonstrate a zero-shot transfer of an autonomous driving policy from simulation to University of Delaware's scaled smart city with adversarial multi-agent reinforcement learning, in which an adversary attempts to decrease the net reward by perturbing both the inputs and outputs of the autonomous vehicles during training. We train the autonomous vehicles to coordinate with each other while crossing a roundabout in the presence of an adversary in simulation. The adversarial policy successfully reproduces the simulated behavior and incidentally outperforms, in terms of travel time, both a human-driving baseline and adversary-free trained policies. Finally, we demonstrate that the addition of adversarial training considerably improves the performance of the policies after transfer to the real world compared to Gaussian noise injection.	Proceedings Paper	2020		[]	[]	-1	-1	-1
J	Lee, Dongcheul	A Comparative Analysis of Reinforcement Learning Activation Functions for Parking of Autonomous Vehicles	2022	The Journal of The Institute of Internet, Broadcasting and Communication		75	81	Autonomous vehicles, which can dramatically solve the lack of parking spaces, are making great progress through deep reinforcement learning. Activation functions are used for deep reinforcement learning, and various activation functions have been proposed, but their performance deviations were large depending on the application environment. Therefore, finding the optimal activation function depending on the environment is important for effective learning. This paper analyzes 12 functions mainly used in reinforcement learning to compare and evaluate which activation function is most effective when autonomous vehicles use deep reinforcement learning to learn parking. To this end, a performance evaluation environment was established, and the average reward of each activation function was compared with the success rate, episode length, and vehicle speed. As a result, the highest reward was the case of using GELU, and the ELU was the lowest. The reward difference between the two activation functions was 35.2%.	research-article	2022	10.7236/JIIBC.2022.22.6.75	[]	[]	-1	-1	-1
J	Ye, Qiming; Feng, Yuxiang; Macias, Jose Javier Escribano; Stettler, Marc; Angeloudis, Panagiotis	Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions Using Reinforcement Learning	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		2024	2034	The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure. In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed. Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand. Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions. We implement a centralised learning paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations. Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians. Furthermore, the distributive learning algorithm outperforms its centralised counterpart regarding computational cost (49.55%), benchmark rewards (25.35%), best cumulative rewards (24.58%), optimal actions (13.49%) and rate of convergence. This novel road management technique could potentially contribute to the flow-adaptive and active mobility-friendly streets in the AVs era.	Article; Early Access		10.1109/TITS.2022.3220110	[]	[]	-1	-1	-1
J	Yan, Ruidong; Li, Penghui; Gao, Hongbo; Huang, Jin; Wang, Chengbo	Car-following strategy of intelligent connected vehicle using extended disturbance observer adjusted by reinforcement learning	2023	CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY				"Disturbance observer-based control method has achieved good results in the car-following scenario of intelligent and connected vehicle (ICV). However, the gain of conventional extended disturbance observer (EDO)-based control method is usually set manually rather than adjusted adaptively according to real time traffic conditions, thus declining the car-following performance. To solve this problem, a car-following strategy of ICV using EDO adjusted by reinforcement learning is proposed. Different from the conventional method, the gain of proposed strategy can be adjusted by reinforcement learning to improve its estimation accuracy. Since the ""equivalent disturbance"" can be compensated by EDO to a great extent, the disturbance rejection ability of the car-following method will be improved significantly. Both Lyapunov approach and numerical simulations are carried out to verify the effectiveness of the proposed method."	Article; Early Access		10.1049/cit2.12252	[]	[]	-1	-1	-1
B	Mohammed, S.T.; Kastouri, M.; Niederfahrenhorst, A.; Ascheid, G.	Video representation learning for decoupled deep reinforcement learning applied to autonomous driving	2023	2023 IEEE/SICE International Symposium on System Integration (SII)		1	6	This work focuses on using Deep Reinforcement Learning (DRL) to control an autonomous vehicle in the hyper-realistic urban simulation LGSVL. Classical control systems such as MPC maneuver vehicles based on a given trajectory, current velocity, position, distances, and more. Our approach does not pass this information to the DRL agent but only images provided by the camera. Current DRL efforts also exploit similar approaches for autonomous driving, but they are only suitable for small, simple tasks using simple simulations. Our approach consists of two differently trained neural networks (NN), a perceptual NN for representation learning and an actor NN for selecting the correct action. The perception NN will be trained via representation and self-supervised learning to strengthen our DRL agent's understanding of the scene. It can recognize temporal information and the dynamics of a complex environment. This work shows the importance of decoupling the perception and decision (actor) model for autonomous driving. All in all, we could drive autonomously in a hyper-realistic urban simulation using our modular DRL framework. Moreover, our approach also provides a solution for other similar tasks in the field of robotics based on images.	Conference Paper	2023	10.1109/SII55687.2023.10039291	[]	[]	-1	-1	-1
J	Jin Lisheng; Han Guangde; Xie Xianyi; Guo Baicang; Liu Guofeng; Zhu Wentao	Review of Autonomous Driving Decision-Making Research Based on Reinforcement Learning	2023	Automotive Engineering		527	540	Decision-making technology of autonomous vehicle is promoted by the development of reinforcement learning, and intelligent decision-making technology has become a key issue of high concern in the field of autonomous driving. Taking the development of reinforcement learning algorithm as the main line in this paper, the indepth application of this algorithm in the field of single-car autonomous driving decision-making is summarized. Traditional reinforcement learning algorithms, classic algorithms and frontier algorithms are summarized and compared from the aspect of basic principles and theoretical modeling methods. According to the classification of autonomous driving decision-making methods in different scenarios, the impact of environmental state observability on modeling is analyzed, and the application technology routes of typical reinforcement learning algorithms at different levels are emphasized. The research prospects for the autonomous driving decision-making method are proposed in order to provide a useful reference for the research of autonomous driving decision-making.	Article	2023		[]	[]	-1	-1	-1
C	Du, Yali; Ma, Chengdong; Liu, Yuchen; Lin, Runji; Dong, Hao; Wang, Jun; Yang, Yaodong	Scalable Model-based Policy Optimization for Decentralized Networked Systems	2022	2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	9019	9026	Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks. Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly, requiring communications or shifting or resources. This work aims to improve data efficiency of multi-agent control by model-based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO). In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts. To alleviate the bias of model-generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation. To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traffic signal control (ATSC). Empirical results show that our method achieves superior data efficiency and matches the performance of model-free methods using true models.The source code of our algorithm and baselines can be found at https://github.com/PKU-MARL/Model-Based-MARL.	Proceedings Paper	2022	10.1109/IROS47612.2022.9982253	[]	[]	-1	-1	-1
J	Wang, Jianfei; Lv, Tiejun; Huang, Pingmu; Mathiopoulos, P. Takis	Mobility-Aware Partial Computation Offloading in Vehicular Networks: A Deep Reinforcement Learning Based Scheme	2020	CHINA COMMUNICATIONS		31	49	Encouraged by next-generation networks and autonomous vehicle systems, vehicular networks must employ advanced technologies to guarantee personal safety, reduce traffic accidents and ease traffic jams. By leveraging the computing ability at the network edge, multi-access edge computing (MEC) is a promising technique to tackle such challenges. Compared to traditional full offloading, partial offloading offers more flexibility in the perspective of application as well as deployment of such systems. Hence, in this paper, we investigate the application of partial computing offloading in-vehicle networks. In particular, by analyzing the structure of many emerging applications, e.g., AR and online games, we convert the application structure into a sequential multi-component model. Focusing on shortening the application execution delay, we extend the optimization problem from the single-vehicle computing offloading (SVCOP) scenario to the multi-vehicle computing offloading (MVCOP) by taking multiple constraints into account. A deep reinforcement learning (DRL) based algorithm is proposed as a solution to this problem. Various performance evaluation results have shown that the proposed algorithm achieves superior performance as compared to existing offloading mechanisms in deducing application execution delay.	Article	OCT 2020		[]	[]	-1	-1	-1
B	Krodel, N.; Kuhnert, K.-D.	Pattern matching as the nucleus for either autonomous driving or driver assistance systems	2003	IV'2002. IEEE Intelligent Vehicle Symposium. Proceedings (Cat. No.02TH8607)		135	40 vol.1	Concerns autonomous vehicle driving by pattern matching combined with reinforcement learning. In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicle or obstacle but with the strict requirement to self-improve driving behaviour. A camera is used to build quickly an abstract complete description (ACSD) of vehicle's current situation. This combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an approximate nearest neighbour pattern matching algorithm with a O(n log m) characteristic compared to O(nÂ·m) for the conventional nearest neighbour calculation. In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.	Conference Paper	2003	10.1109/IVS.2002.1187941	[]	[]	-1	-1	-1
C	Baheri, Ali; Kolmanovsky, Ilya; Girard, Anouck; Tseng, H. Eric; Filev, Dimitar	Vision-Based Autonomous Driving: A Model Learning Approach	2020	2020 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	2520	2525	We present an integrated approach for perception and control for an autonomous vehicle and demonstrate this approach in a high-fidelity urban driving simulator. Our approach first builds a model for the environment, then trains a policy exploiting the learned model to identify the action to take at each time-step. To build a model for the environment, we leverage several deep learning algorithms. To that end, first we train a variational autoencoder to encode the input image into an abstract latent representation. We then utilize a recurrent neural network to predict the latent representation of the next frame and handle temporal information. Finally, we utilize an evolutionary-based reinforcement learning algorithm to train a controller based on these latent representations to identify the action to take. We evaluate our approach in CARLA, a high-fidelity urban driving simulator, and conduct an extensive generalization study. Our results demonstrate that our approach outperforms several previously reported approaches in terms of the percentage of successfully completed episodes for a lane keeping task.	Proceedings Paper	2020	10.23919/acc45564.2020.9147510	[]	[]	-1	-1	-1
J	Nguyen, Hung Duy; Han, Kyoungseok	Safe Reinforcement Learning-based Driving Policy Design for Autonomous Vehicles on Highways	2023	INTERNATIONAL JOURNAL OF CONTROL AUTOMATION AND SYSTEMS		4098	4110	Safe decision-making strategy of autonomous vehicles (AVs) plays a critical role in avoiding accidents. This study develops a safe reinforcement learning (safe-RL)-based driving policy for AVs on highways. The hierarchical framework is considered for the proposed safe-RL, where an upper layer executes a safe exploration-exploitation by modifying the exploring process of the epsilon-greedy algorithm, and a lower layer utilizes a finite state machine (FSM) approach to establish the safe conditions for state transitions. The proposed safe-RL-based driving policy improves the vehicle's safe driving ability using a Q-table that stores the values corresponding to each action state. Moreover, owing to the trade-off between the epsilon-greedy values and safe distance threshold, the simulation results demonstrate the superior performance of the proposed approach compared to other alternative RL approaches, such as the epsilon-greedy Q-learning (GQL) and decaying epsilon-greedy Q-learning (DGQL), in an uncertain traffic environment. This study's contributions are twofold: it improves the autonomous vehicle's exploration-exploitation and safe driving ability while utilizing the advantages of FSM when surrounding cars are inside safe-driving zones, and it analyzes the impact of safe-RL parameters in exploring the environment safely.	Article	DEC 2023	10.1007/s12555-023-0255-4	[]	[]	-1	-1	-1
J	Zarrouki, B.; Wang, C.; Betz, J.	Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control [arXiv]	2023	Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control [arXiv]				In this paper, we present a Deep Reinforcement Learning (RL)-driven Adaptive Stochastic Nonlinear Model Predictive Control (SNMPC) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance. To this end, we conceive an RL agent to proactively anticipate upcoming control tasks and to dynamically determine the most suitable combination of key SNMPC parameters - foremost the robustification factor $\kappa$ and the Uncertainty Propagation Horizon (UPH) $T_u$. We analyze the trained RL agent's decision-making process and highlight its ability to learn context-dependent optimal parameters. One key finding is that adapting the constraints robustification factor with the learned policy reduces conservatism and improves closed-loop performance while adapting UPH renders previously infeasible SNMPC problems feasible when faced with severe disturbances. We showcase the enhanced robustness and feasibility of our Adaptive SNMPC (aSNMPC) through the real-time motion control task of an autonomous passenger vehicle to follow an optimal race line when confronted with significant time-variant disturbances. Experimental findings demonstrate that our look-ahead RL-driven aSNMPC outperforms its Static SNMPC (sSNMPC) counterpart in minimizing the lateral deviation both with accurate and inaccurate disturbance assumptions and even when driving in previously unexplored environments.	Preprint	2023		[]	[]	-1	-1	-1
J	Ben Naveed, K.; Zhiqian Qiao; Dolan, J.M.	Trajectory Planning for Autonomous Vehicles Using Hierarchical Reinforcement Learning [arXiv]	2020	arXiv		7 pp.	7 pp.	Planning safe trajectories under uncertain and dynamic conditions makes the autonomous driving problem significantly complex. Current sampling-based methods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this problem because of the high computational cost. Supervised learning methods such as Imitation Learning lack generalization and safety guarantees. To address these problems and in order to ensure a robust framework, we propose a Hierarchical Reinforcement Learning (HRL) structure combined with a Proportional-Integral-Derivative (PID) controller for trajectory planning. HRL helps divide the task of autonomous vehicle driving into sub-goals and supports the network to learn policies for both high-level options and low-level trajectory planner choices. The introduction of sub-goals decreases convergence time and enables the policies learned to be reused for other scenarios. In addition, the proposed planner is made robust by guaranteeing smooth trajectories and by handling the noisy perception system of the ego-car. The PID controller is used for tracking the waypoints, which ensures smooth trajectories and reduces jerk. The problem of incomplete observations is handled by using a Long-Short-Term-Memory (LSTM) layer in the network. Results from the high-fidelity CARLA simulator indicate that the proposed method reduces convergence time, generates smoother trajectories, and is able to handle dynamic surroundings and noisy observations.	Journal Paper	9 Nov. 2020		[]	[]	-1	-1	-1
J	Lee, Cheonghwa; An, Dawn	Decision-Making in Fallback Scenarios for Autonomous Vehicles: Deep Reinforcement Learning Approach	2023	APPLIED SCIENCES-BASEL				This paper proposes a decision-making algorithm based on deep reinforcement learning to support fallback techniques in autonomous vehicles. The fallback technique attempts to mitigate or escape risky driving conditions by responding to appropriate avoidance maneuvers essential for achieving a Level 4+ autonomous driving system. However, developing a fallback technique is difficult because of the innumerable fallback situations to address and eligible optimal decision-making among multiple maneuvers. We employed a decision-making algorithm utilizing a scenario-based learning approach to address these issues. First, we crafted a specific fallback scenario encompassing the challenges to be addressed and matched the anticipated optimal maneuvers as determined by heuristic methods. In this scenario, the ego vehicle learns through trial and error to determine the most effective maneuver. We conducted 100 independent training sessions to evaluate the proposed algorithm and compared the results with those of heuristic-derived maneuvers. The results were promising; 38% of the training sessions resulted in the vehicle learning lane-change maneuvers, whereas 9% mastered slow following. Thus, the proposed algorithm successfully learned human-equivalent fallback capabilities from scratch within the provided scenario.	Article	NOV 2023	10.3390/app132212258	[]	[]	-1	-1	-1
B	Hao Zeng; Lintao Zhang; Yuliang Tang; Yanglong Sun; Yuqi Ruan	Intelligent Resource Allocation Based on Reinforcement Learning for NOMA Vehicular Cooperative Communication Networks	2021	2021 16th International Conference on Computer Science & Education (ICCSE)		622	7	There exits difficulty in environment sensing for autonomous vehicles in Internet of vehicles (IoV) networks. The autonomous vehicle can obtain a full sense of the environment from connected vehicles by vehicle-to-vehicle (V2V) communications and make better decisions for driving safety. However, limited spectrum resources cannot adapt to the increasing number of communication links and the traditional frequency multiplexing will lead to serious co-frequency interference and fail to connect more vehicles. Therefore, we develop a resource allocation scheme through jointly optimizing sub-band selection and power control based on V2X Network. Meanwhile, the non-orthogonal multiple access (NOMA) technology is adopted for multi-V2V communication that shares the spectrum allocated to vehicle-to-infrastructure (V2I) links. Considering time-varying channel conditions caused by vehicle movement and power control over a continuous range in a vehicular environment, we investigate a deep deterministic policy gradient algorithm to maximize the sum transmission rate of the V2I links and V2V links while ensuring the quality of service of V2V links. Simulation results demonstrate the proposed solution can effectively optimize the total throughput.	Conference Paper	2021	10.1109/ICCSE51940.2021.9569512	[]	[]	-1	-1	-1
B	Manjanna, S.; van Hoof, H.; Dudek, G.	Reinforcement Learning with Non-uniform State Representations for Adaptive Search	2018	2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)		7 pp.	7 pp.	Efficient spatial exploration is a key aspect of search and rescue. In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher. This should allow an autonomous vehicle find one or more lost targets as rapidly as possible. We do this by performing non-uniform sampling of the search region. The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning. We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target. Key features of our search algorithm are the ability to employ a very general nondeterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions. One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics. We compare the paths generated by our algorithm with other accepted spatial coverage techniques such as distribution independent boustrophedonic coverage and model dependent spiral search. We present a proof showing that rewarding for clearing probability mass instead of locating the target does not bias the objective function. The experiments show that the learned policy outperforms several well-known baselines even in scenarios different from the one it has been trained on.	Conference Paper	2018	10.1109/SSRR.2018.8468649	[]	[]	-1	-1	-1
B	Thakkar, H.K.; Desai, A.; Singh, P.; Samhitha, K.	ReLearner: a reinforcement learning-based self driving car model using gym environment	2022	Advanced Computing: 11th International Conference, IACC 2021, Revised Selected Papers. Communications in Computer and Information Science (1528)		399	409	"In the recent past, Artificial intelligence and its sister technology such as Machine Learning, Deep Learning, and Reinforcement learning have grown rapidly in several applications. The self-driving car is one of the applications, which is the need of the hour. In this paper, we describe the trends in autonomous vehicle technology for the self-driving car. There are many different approaches to mathematically formulate a design for the self-driving car such as deep Q-learning, Q-learning, and machine learning. However, in this paper, we propose a very basic and less compute-intensive simplistic self-driving car model called ""ReLearner"" using the Gym environment. To simulate the self-driving car model, we preferred to create a simple small environment OpenAi gym which is a deterministic environment. The OpenAi gym provides the virtual simulation environment and parameter tuning to train and test the model. We have focused on two methods to test our model. The basic approach is to compare the performance of the car when tested using Q-Learning and another using a random action agent, i.e., No reinforcement learning. We have derived a theoretical model and analyzed how to use Q-learning to train cars to drive. We have carried out a simulation and on evaluating the performance and found that Q-learning is a more optimal approach to solve the issue of a self-driving car."	Conference Paper	2022	10.1007/978-3-030-95502-1_30	[]	[]	-1	-1	-1
C	Ferdowsi, Aidin; Challita, Ursula; Saad, Walid; Mandayam, Narayan B.	Robust Deep Reinforcement Learning for Security and Safety in Autonomous Vehicle Systems	2018	2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	307	312	The dependence of autonomous vehicles (AVs) on sensors and communication links exposes them to cyber-physical (CP) attacks by adversaries that seek to take control of the AVs by manipulating their data. In this paper, the state estimation process for monitoring AV dynamics, in presence of CP attacks, is analyzed and a novel adversarial deep reinforcement learning (RL) algorithm is proposed to maximize the robustness of AV dynamics control to CP attacks. The attacker's action and the AV's reaction to CP attacks are studied in a game-theoretic framework. In the formulated game, the attacker seeks to inject faulty data to AV sensor readings so as to manipulate the inter-vehicle optimal safe spacing and potentially increase the risk of AV accidents or reduce the vehicle flow on the roads. Meanwhile, the AV, acting as a defender, seeks to minimize the deviations of spacing so as to ensure robustness to the attacker's actions. Since the AV has no information about the attacker's action and due to the infinite possibilities for data value manipulations, each player uses long short term memory (LSTM) blocks to learn the expected spacing deviation resulting from its own action and feeds this deviation to a reinforcement learning (RL) algorithm. Then, the attacker's RL algorithm chooses the action which maximizes the spacing deviation, while the AV's RL algorithm seeks to find the optimal action that minimizes such deviation. Simulation results show that the proposed adversarial deep RL algorithm can improve the robustness of the AV dynamics control as it minimizes the intra-AV spacing deviation.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Du, Danfeng; Shen, Mingyu; Guo, Xiurong; Sun, Chaowei	Hierarchical Driving Strategy for Connected and Autonomous Vehicles Making a Protected Left Turn at Signalized Intersections	2023	JOURNAL OF TRANSPORTATION ENGINEERING PART A-SYSTEMS				Left-turn execution in autonomous driving at urban intersections is often complex and characterized by unpredicted events, such as vehicles speeding and running a red light. Despite these hazards, autonomous vehicles must drive through intersections safely and efficiently. To solve this problem, a new hierarchical driving strategy (HDS) is proposed for connected and autonomous vehicles making a protected left turn at signalized intersections, which combines the rule-based method and deep reinforcement learning (DRL). The high level of the HDS is the rule-based decision-making module, and the low level is the driving skill, which is dependent on the status. Specifically, the vehicle status when turning left is divided into safe and alert statuses. Further, DRL is used to train the driving skill of the vehicle for each status. The HDS design effectively combines the advantages of end-to-end driving. Moreover, the algorithm was experimentally evaluated in multiple protected left-turn scenarios. Compared with the pure rule-based method, the HDS achieved a 34% reduction in failure rate in the test scenario, and the driving behavior of the autonomous vehicle employing HDS was more intelligent. Moreover, the HDS is highly robust to complex scenarios.	Article	MAR 1 2023	10.1061/JTEPBS.TEENG-7243	[]	[]	-1	-1	-1
J	Xu, Xin; Zuo, Lei; Li, Xin; Qian, Lilin; Ren, Junkai; Sun, Zhenping	A Reinforcement Learning Approach to Autonomous Decision Making of Intelligent Vehicles on Highways	2020	IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS		3884	3897	Autonomous decision making is a critical and difficult task for intelligent vehicles in dynamic transportation environments. In this paper, a reinforcement learning approach with value function approximation and feature learning is proposed for autonomous decision making of intelligent vehicles on highways. In the proposed approach, the sequential decision making problem for lane changing and overtaking is modeled as a Markov decision process with multiple goals, including safety, speediness, smoothness, etc. In order to learn optimized policies for autonomous decision-making, a multiobjective approximate policy iteration (MO-API) algorithm is presented. The features for value function approximation are learned in a data-driven way, where sparse kernel-based features or manifold-based features can be constructed based on data samples. Compared with previous RL algorithms such as multiobjective Q-learning, the MO-API approach uses data-driven feature representation for value and policy approximation so that better learning efficiency can be achieved. A highway simulation environment using a 14 degree-of-freedom vehicle dynamics model was established to generate training data and test the performance of different decision-making methods for intelligent vehicles on highways. The results illustrate the advantages of the proposed MO-API method under different traffic conditions. Furthermore, we also tested the learned decision policy on a real autonomous vehicle to implement overtaking decision and control under normal traffic on highways. The experimental results also demonstrate the effectiveness of the proposed method.	Article	OCT 2020	10.1109/TSMC.2018.2870983	[]	[]	-1	-1	-1
R	Lu, Chengjie; Shi, Yize; Zhang, Huihui; Zhang, Man; Wang, Tiexin; Yue, Tao; Ali, Shaukat	DeepCollision: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions	2022	Zenodo				With the aim to test autonomous driving systems, we propose a novel reinforcement learning (RL)-based approach named DeepCollision to learn operating environment configurations of autonomous vehicles, including formalizing environment configuration learning as an MDP and adopting DQN algorithm as the RL solution;DeepCollisionlearns environment configurations to maximize collisions of an Autonomous Vehicle Under Test (AVUT). This dataset contains: algorithms- The algorithm of DeepCollision, which includes the network architecture and the DQN hyperparameter settings; pilot-study- All the raw data and plots for the pilot study; formal-experiment- A dataset contains all the raw data for analysis and the scenarios with detailed demand values; rest-api- The REST API endpoints for environment configuration and oneexampleto show the usage of the APIs. Copyright: Creative Commons Attribution 4.0 International Open Access	Data set	2022-09-06	https://doi.org/10.5281/ZENODO.5906634	[]	[]	-1	-1	-1
J	Jang, K.; Beaver, L.; Chalaki, B.; Remer, B.; Vinitsky, E.; Malikopoulos, A.; Bayen, A.	Simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles [arXiv]	2018	arXiv		9 pp.	9 pp.	Using deep reinforcement learning, we train control policies for autonomous vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library for deep reinforcement learning in micro-simulators, we train two policies, one policy with noise injected into the state and action space and one without any injected noise. In simulation, the autonomous vehicle learns an emergent metering behavior for both policies in which it slows to allow for smoother merging. We then directly transfer this policy without any tuning to the University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for connected and automated vehicles. We characterize the performance of both policies on the scaled city. We show that the noise-free policy winds up crashing and only occasionally metering. However, the noise-injected policy consistently performs the metering behavior and remains collision-free, suggesting that the noise helps with the zero-shot policy transfer. Additionally, the transferred, noise-injected policy leads to a 5% reduction of average travel time and a reduction of 22% in maximum travel time in the UDSSC. Videos of the controllers can be found at https://sites.google.com/view/iccps-policy-transfer.	Journal Paper	14 Dec. 2018		[]	[]	-1	-1	-1
B	Zhou, C.; Liao, M.; Jiao, L.; Tao, F.	Lane Change Decision Control of Autonomous Vehicle Based on A3C Algorithm	2024	Cognitive Systems and Information Processing: 8th International Conference, ICCSIP 2023, Revised Selected Papers. Communications in Computer and Information Science (1918)		217	29	Coordinating the lateral and longitudinal control of vehicles during lane changing, while considering the vehicle's operational state and the surrounding environment, poses a highly challenging task. In recent years, deep reinforcement learning (DRL) technology has experienced rapid development and has found widespread applications in the traditional automatic control industry. With the improvement of driving safety requirements, DRL technology provides a new research direction and an effective way for the development of vehicle autonomous lane change. This paper investigates automated decision control for lane changing in autonomous vehicles using the Asynchronous Advantage Actor-Critic (A3C) algorithm, while also proposing reasonable multi-objective performance evaluation metrics. Nevertheless, the traditional A3C algorithm frequently encounters convergence oscillation or degradation issues, hindering agents from attaining the highest reward. To address the mentioned issues, an improved parameter updating method based on a weighted average of advantage value is proposed. The simulation results on the highway simulation platform demonstrate that the enhanced A3C algorithm offers increased stability in comparison to the traditional A3C algorithm. Moreover, in comparison to the Deep Q-Network (DQN) algorithm and the Deep Deterministic Policy Gradient (DDPG) algorithm, the enhanced A3C algorithm showcases faster convergence speed and a higher success rate, hence confirming the superiority of the proposed improvement.	Conference Paper	2024	10.1007/978-981-99-8018-5_16	[]	[]	-1	-1	-1
C	Chandramohan, Aashik; Poel, Mannes; Meijerink, Bernd; Heijenk, Geert	Machine Learning for Cooperative Driving in a Multi-Lane Highway Environment	2019	2019 WIRELESS DAYS (WD)	IFIP Wireless Days			Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them. In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment. A driving algorithm is designed using deep Q learning, a type of reinforcement learning. In order to train and test driving algorithms, we deploy a simulated traffic system, using SUMO (Simulation of Urban Mobility). The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles. Furthermore, the impact of limited communication range and random packet loss is investigated. Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high. We propose directions for additional research to improve the performance of the algorithm.	Proceedings Paper	2019	10.1109/wd.2019.8734192	[]	[]	-1	-1	-1
C	Chu, Tianshu; Kalabic, Uros	Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoons	2019	2019 IEEE 58TH CONFERENCE ON DECISION AND CONTROL (CDC)	IEEE Conference on Decision and Control	4079	4084	This paper proposes a model-based deep reinforcement learning (DRL) algorithm for cooperative adaptive cruise control (CACC) of connected vehicles. Differing from most existing CACC works, we consider a platoon consisting of both human-driven and autonomous vehicles. The human-driven vehicles are heterogeneous and connected via vehicle-to-vehicle (V2V) communication and the autonomous vehicles are controlled by a cloud-based centralized DRL controller via vehicle-to-cloud (V2C) communication. To overcome the safety and robustness issues of RL, the algorithm informs lower-level controllers of desired headway signals instead of directly controlling vehicle accelerations. The lower-level behavior is modeled according to the optimal velocity model (OVM), which determines vehicle acceleration according to a headway input. Numerical experiments show that the model-based DRL algorithm outperforms its model-free version in both safety and stability of CACC. Furthermore, we study the impact of different penetration ratios of autonomous vehicles on the safety, stability, and optimality of the CACC policy.	Proceedings Paper	2019		[]	[]	-1	-1	-1
B	Farkas, P.; Szoke, L.; Aradi, S.	Defining metrics for scenario-based evaluation of autonomous vehicle models	2022	2022 IEEE 1st International Conference on Cognitive Mobility (CogMob)		000155	60	The paper deals with the evaluation of autonomous vehicles along with the quantification of their behavior and maneuvers. The article outlines the positive aspects of autonomy and lists several arguments in their favor, e.g. convenience and efficiency considerations. Furthermore, it also addresses the associated difficulties including the feasibility of road testing and the establishment of appropriate simulations. The current work aims to define methods providing objective indicators to compare algorithms solving the complex tasks of road transport. Rule-based, supervised and reinforcement learning control models, test environments, accelerated test methods and assessment indicators of the corresponding literature are reviewed and evaluated. After investigating the different metrics, we formulate an evaluation framework that can be applied in the development and assessment process of new artificial intelligence controlled models. As an outcome of this work, we aim to aid a missing sector in the field of autonomous driving function development by collecting and defining metrics that intend to help qualitatively evaluate and compare algorithms. The key aspect during the definition of the suggested method was to ensure its extensive applicability by selecting only metrics that can be obtained from the already installed sensors of the vehicles. Additionally, we also assess multiple agents to observe how their behavior compares and whether the proposed metrics reflect the expected behavior.	Conference Paper	2022	10.1109/CogMob55547.2022.10117768	[]	[]	-1	-1	-1
B	Bogosyan, S.; Gokasan, M.; Vamvoudakis, K.G.	Zero-Sum Game (ZSG) based Integral Reinforcement Learning for Trajectory Tracking Control of Autonomous Smart Car	2022	2022 IEEE 31st International Symposium on Industrial Electronics (ISIE)		1	4	The ultimate aim of our research study is the development, practical implementation, and benchmarking of continuous-time, online reinforcement learning (RL) schemes for the trajectory tracking control (TTC) of fully autonomous vehicles (AVs) in real-world scenarios. The adaptive optimality and model-free nature offered by RL has a stronger promise against its model-based counterparts, such as MPC, against uncertainties related to the vehicle, road, tire-terrain and environmental dynamics. The existing studies on RL based AV control are mostly theoretical, often dealing with high-level TTC, and perform evaluations in simulations considering simplified or linear models with no disturbance and slip effects. The literature also demonstrates the lack of practical implementations in overall RL based autonomous vehicle control. Our ultimate goal is to fill these theoretical and practical gaps by designing and practically evaluating novel RL strategies that will improve the performance of TTC against uncertainties at all levels. This paper presents the simulation results of our preliminary studies in the online, longitudinal tracking control of a realistic AV (with uncertain nonlinear dynamics, as well as disturbance, and slip effects), which we treat as a Zero-Sum Game (ZSG) problem using an Integral Reinforcement Learning (IRL) approach with synchronous actor and critic updates (SyncIRL). The results are promising and motivate the practical implementation of the approach for combined longitudinal and lateral control of AV.	Conference Paper	2022	10.1109/ISIE51582.2022.9948217	[]	[]	-1	-1	-1
J	Crumpacker, James B.; Robbins, Matthew J.; Jenkins, Phillip R.	An approximate dynamic programming approach for solving an air combat maneuvering problem	2022	EXPERT SYSTEMS WITH APPLICATIONS				Within visual range air combat involves execution of highly complex and dynamic activities, requiring rapid, sequential decision-making to achieve success. Fighter pilots spend years perfecting tactics and maneuvers for these types of combat engagements, yet the ongoing emergence of unmanned, autonomous vehicle technologies elicits a natural question - can an autonomous unmanned combat aerial vehicle (AUCAV) be imbued with the necessary artificial intelligence to perform challenging air combat maneuvering tasks independently? We formulate and solve the air combat maneuvering problem (ACMP) to examine this important question, developing a Markov decision process (MDP) model to control a defending AUCAV seeking to destroy an attacking adversarial vehicle. The MDP model includes a 5-degree-of-freedom, point-mass aircraft state transition model to accurately represent both kinematics and energy while maneuvering. An approximate dynamic programming (ADP) approach is proposed wherein we develop and test an approximate policy iteration algorithm that implements value function approximation via neural network regression to attain high-quality maneuver policies for the AUCAV. A representative intercept scenario is specified for testing purposes wherein the AUCAV must engage and destroy an adversary aircraft attempting to penetrate the defended airspace. Several designed experiments are conducted to determine how aircraft velocity and adversary maneuvering tactics impact the efficacy of the proposed ADP solution approach and to enable efficient algorithm parameter tuning. ADP-generated policies are compared to two benchmark maneuver policies constructed from two reward shaping functions found in the ACMP literature, attaining improved mean probabilities of kill for 24 of 36 air combat situations considered	Article; Early Access		10.1016/j.eswa.2022.117448	[]	[]	-1	-1	-1
C	Liu, Yongyang; Zhou, Anye; Wang, Yu; Peeta, Srinivas	Proactive Longitudinal Control to Manage Disruptive Lane Changes of Human-Driven Vehicles in Mixed-Flow Traffic	2021	IFAC PAPERSONLINE		321	326	Connected and autonomous vehicles (CAVs) can be leveraged to enable cooperative platooning control to alleviate traffic oscillations. However, in the near future, CAVs and human-driven vehicles (HDV5) will coexist on roads, creating a mixed-flow traffic environment. In mixed-flow traffic, CAV platoons would inevitably encounter lane changes by HDVs in adjacent lanes. These lane changes can generate disturbances and oscillations upstream, jeopardizing the performance of platooning control. Hence, it is necessary to explore the interactions between CAVs and HDVs in the lane -change process, to analyze how CAVs can be used to manage disruptive lane changes of HDVs in mixed-flow traffic. This study proposes deep reinforcement learning-based proactive longitudinal control for CAVs to counteract disruptive HDV lane -change behaviors that can induce disturbances, such that the smoothness of traffic flow can be preserved in the platooning control process. Results from numerical experiments suggest that CAVs controlled by the proposed control strategy can effectively reduce the occurrence of disruptive lane change maneuvers of HDVs to improve string stability performance in mixed-flow traffic. Further, the reliability of the proposed control strategy for different HDV driver types is illustrated. Copyright (c) 2021 The Authors.	Proceedings Paper; Early Access		10.1016/j.ifacol.2021.06.037	[]	[]	-1	-1	-1
J	Gao, Weinan; Gao, Jingqin; Ozbay, Kaan; Jiang, Zhong-Ping	Reinforcement-Learning-Based Cooperative Adaptive Cruise Control of Buses in the Lincoln Tunnel Corridor with Time-Varying Topology	2019	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		3796	3805	The exclusive bus lane (XBL) is one of the most popular bus transit systems in the U.S. The Lincoln Tunnel utilizes an XBL through the tunnel in the AM peak period. This paper proposes a novel data-driven cooperative adaptive cruise control (CACC) algorithm that aims to minimize a cost function for connected and autonomous buses along the XBL. Different from existing model-based CACC algorithms, the proposed approach employs the idea of reinforcement learning, which does not rely on accurate knowledge of bus dynamics. Considering a time-varying topology, where each autonomous vehicle can only receive information from preceding vehicles that are within its communication range, a distributed controller is learned real-time by online headway, velocity, and acceleration data collected from the system trajectories. The convergence of the proposed algorithm and the stability of the closed-loop system are rigorously analyzed. The effectiveness of the proposed approach is demonstrated using a well-calibrated Paramics microscopic traffic simulation model of the XBL corridor. The simulation results show that the travel time in the autonomous version of the XBL are close to the present day travel time even when the bus volume is increased by 30%.	Article	OCT 2019	10.1109/TITS.2019.2895285	[]	[]	-1	-1	-1
C	Brunnbauer, Axel; Berducci, Luigi; Brandstatter, Andreas; Lechner, Mathias; Hasani, Ramin; Rus, Daniela; Grosu, Radu	Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing	2022	2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022)		7513	7520	World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms. While learning world models for high-dimensional observations (e.g., pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored. In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model. We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.	Proceedings Paper	2022	10.1109/ICRA46639.2022.9811650	[]	[]	-1	-1	-1
J	Berbar, Anas; Gastli, Adel; Meskin, Nader; Al-Hitmi, Mohammed A.; Ghommam, Jawhar; Mesbah, Mostefa; Mnif, Faical	Reinforcement Learning-Based Control of Signalized Intersections Having Platoons	2022	IEEE ACCESS		17683	17696	Smart transportation cities are based on intelligent systems and data sharing, whereas human drivers generally have limited capabilities and imperfect traffic observations. The perception of Connected and Autonomous Vehicle (CAV) utilizes data sharing through Vehicle-To-Vehicle (V2V) and Vehicle-To-Infrastructure (V2I) communications to improve driving behaviors and reduce traffic delays and fuel consumption. This paper proposes a Double Agent (DA) intelligent traffic signal module based on the Reinforcement Learning (RL) method, where the first agent, the Velocity Agent (VA) aims to minimize the fuel consumption by controlling the speed of platoons and single CAVs crossing a signalized intersection, while the second agent, the Signal Agent (SA) proceeds to efficiently reduce traffic delays through signal sequencing and phasing. Several simulation studies have been conducted for a signalized intersection with different traffic flows and the performance of the single-agent with only VA, DA with both VA and SA, and Intelligent Driver Model (IDM) are compared. It is shown that the proposed DA solution improves the average delay by 47.3% and the fuel efficiency by 13.6% compared to the Intelligent Driver Model (IDM).	Article	2022	10.1109/ACCESS.2022.3149161	[]	[]	-1	-1	-1
B	Liu, T.; Lei, L.; Liu, Z.; Zheng, K.	Jointly Learning V2X Communication and Platoon Control with Deep Reinforcement Learning	2023	2023 IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)		1	6	In autonomous vehicle platooning, Vehicle-to-Everything (V2X) communications are leveraged in cooperative adaptive cruise control (CACC) to improve control performance. Since exchanging information at all times incurs significant communication overhead in vehicular networks, it is important to determine when V2X communication is necessary. To solve this problem, we propose a Deep Reinforcement Learning (DRL)-based algorithm named Attention-DDPG, which learns platoon control with Deep Deterministic Policy Gradient (DDPG), and learns when to communicate with an attention network. Specifically, each preceding vehicle is equipped with a deep neural network (DNN), which takes as input its local state and platoon control action and determines whether to transmit its acceleration or not to the following vehicle at each time step. The attention network of a preceding vehicle is trained using the feedback from the following vehicle on the value of V2X information in the form of an advantage function. In order to evaluate Attention-DDPG, simulations are performed using real driving data, and performance is compared with those of two baselines that communicate and do not communicate at all times, respectively. The results demonstrate that Attention-DDPG strikes a competitive tradeoff between control performance and communication overhead while ensuring platoon string stability.	Conference Paper	2023	10.1109/PIMRC56721.2023.10293991	[]	[]	-1	-1	-1
J	Zhu, Tong; Li, Xiaohu; Fan, Wei; Wang, Changshuai; Liu, Haoxue; Zhao, Runqing	Trajectory Optimization of CAVs in Freeway Work Zone considering Car-Following Behaviors Using Online Multiagent Reinforcement Learning	2021	JOURNAL OF ADVANCED TRANSPORTATION				Work zone areas are frequent congested sections considered as the freeway bottleneck. Connected and autonomous vehicle (CAV) trajectory optimization can improve the operating efficiency in bottleneck areas by harmonizing vehicles' manipulations. This study presents a joint trajectory optimization of cooperative lane changing, merging, and car-following actions for CAV control at a local merging point together with upstream points. The multiagent reinforcement learning (MARL) method is applied in this system, with one agent providing a merging advisory service at the merging point and controlling the inner-lane vehicles' headway for smooth outer-lane vehicle merging, while other agents provide lane-changing advisory services at advance lane-changing points to control how vehicles make lane changes in advance and perform corresponding headway adjustment, similar to and jointly with the merging advisory service. Uniting all agents, the coordination graph (CG) method is applied to seek the global optimum, overcoming the exponential growth problem in MARL. Using MATLAB and the VISSIM COM interface, an online simulation platform is established. The simulation results show that MARL is effective for online computation with in-timing response. More importantly, comparisons of the results obtained in various scenarios demonstrate that the proposed system obtained smoother vehicle trajectories in all controlled sections, rather than only in the merging area, indicating that it can achieve better traffic conditions in freeway work zone areas.	Article	NOV 3 2021	10.1155/2021/9805560	[]	[]	-1	-1	-1
J	Benvenuti, A.; Hawkins, C.; Fallin, B.; Chen, B.; Bialy, B.; Dennis, M.; Hale, M.	Differentially Private Reward Functions for Multi-Agent Markov Decision Processes [arXiv]	2023	Differentially Private Reward Functions for Multi-Agent Markov Decision Processes [arXiv]				Reward functions encode desired behavior in multi-agent Markov decision processes, but onlookers may learn reward functions by observing agents, which can reveal sensitive information. Therefore, in this paper we introduce and compare two methods for privatizing reward functions in policy synthesis for multi-agent Markov decision processes. Reward functions are privatized using differential privacy, a statistical framework for protecting sensitive data. Both methods we develop rely on the Gaussian mechanism, which is a method of randomization we use to perturb (i) each agent's individual reward function or (ii) the joint reward function shared by all agents. We prove that both of these methods are differentially private and compare the abilities of each to provide accurate reward values for policy synthesis. We then develop an algorithm for the numerical computation of the performance loss due to privacy on a case-by-case basis. We also exactly compute the computational complexity of this algorithm in terms of system parameters and show that it is inherently tractable. Numerical simulations are performed on a gridworld example and in waypoint guidance of an autonomous vehicle, and both examples show that privacy induces only negligible performance losses in practice.	Preprint	2023		[]	[]	-1	-1	-1
J	Gao, Weinan; Jiang, Zhong-Ping; Ozbay, Kaan	Data-Driven Adaptive Optimal Control of Connected Vehicles	2017	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		1122	1133	In this paper, a data-driven non-model-based approach is proposed for the adaptive optimal control of a class of connected vehicles that is composed of n human-driven vehicles only transmitting motional data and an autonomous vehicle in the tail receiving the broadcasted data from preceding vehicles by wireless vehicle-to-vehicle (V2V) communication devices. Considering the cases of range-limited V2V communication and input saturation, several optimal control problems are formulated to minimize the errors of distance and velocity and to optimize the fuel usage. By employing an adaptive dynamic programming technique, the optimal controllers are obtained without relying on the knowledge of system dynamics. The effectiveness of the proposed approaches is demonstrated via the online learning control of the connected vehicles in Paramics' traffic microsimulation.	Article	MAY 2017	10.1109/TITS.2016.2597279	[]	[]	-1	-1	-1
J	Yadavalli, S.R.; Das, L.C.; Won, M.	RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging [arXiv]	2022	arXiv				A platoon refers to a group of vehicles traveling together in very close proximity. It has received significant attention from the autonomous vehicle research community due to its strong potential to significantly enhance fuel efficiency, driving safety, and driver comfort. Despite these advantages, recent research has revealed a detrimental effect of the extremely small intra-platoon gap on traffic flow for highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a significant challenge due to the massive computational complexity. To this end, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The state space of the framework is carefully designed in consultation with the transportation literature to incorporate critical traffic parameters relevant to merging efficiency. A deep deterministic policy gradient algorithm is adopted to account for the continuous action space to ensure precise and continuous adjustment of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway merging scenarios.	Journal Paper	07 Dec. 2022		[]	[]	-1	-1	-1
J	Liu, Yuqi; Gao, Yinfeng; Zhang, Qichao; Ding, Dawei; Zhao, Dongbin	Multi-task safe reinforcement learning for navigating intersections in dense traffic	2023	JOURNAL OF THE FRANKLIN INSTITUTE-ENGINEERING AND APPLIED MATHEMATICS		13737	13760	Multi-task intersection navigation, which includes unprotected turning left, turning right, and going straight in heavy traffic, remains a difficult task for autonomous vehicles. For the human driver, negotiation skills with other interactive vehicles are the key to guaranteeing safety and efficiency. However, it is hard to balance the safety and efficiency of the autonomous vehicle for multi-task intersection navigation. In this paper, we formulate a multi-task safe reinforcement learning framework with social attention to improve the safety and efficiency when interacting with other traffic participants. Specifically, the social attention module is used to focus on the states of negotiation vehicles. In addition, a safety layer is added to the multi-task reinforcement learning framework to guarantee safe negotiation. We deploy experiments in the simulators SUMO, which has abundant traffic flows, and CARLA, which has high-fidelity vehicle models. Both show that the proposed algorithm improves safety while maintaining stable traffic efficiency for the multi-task intersection navigation problem. More details and demonstrations are available at https:// github.com/ liuyuqi123/ SAT.(c) 2022 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.	Article; Early Access		10.1016/j.jfranklin.2022.06.052	[]	[]	-1	-1	-1
B	Weingertner, P.; Ho, M.; Timofeev, A.; Aubert, S.; Pita-Gil, G.	Monte Carlo Tree Search With Reinforcement Learning for Motion Planning	2020	2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)		7 pp.	7 pp.	Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.	Conference Paper	2020	10.1109/ITSC45102.2020.9294697	[]	[]	-1	-1	-1
B	Hunor toth, S.; Janos viharos, Z.; Bardos, A.	Autonomous Vehicle Drift With a Soft Actor-critic Reinforcement Learning Agent	2022	2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI).		000015	000020	Self-driving vehicles have become a more and more important field in recent years. Supported by the techniques of Artificial Intelligence (AI), the current tendency of positive results in applications is making it a promising area to focus further research. Additionally, Reinforcement Learning (RL) is already proved to be an efficient approach for complex problems, e.g. robots, industrial systems and also games (like chess, Go), etc.Drifting is a driving technique at handling limits where the driver intentionally oversteers, with loss of traction, while driving the vehicle through the entirety of a corner. It is a very challenging control task and often results in an accident when it occurs on public roads, consequently, the efficient control of this motion is especially important in the safety of autonomous vehicles.The paper reports novel research results whose main goal is to develop a self-driving agent for drift motion control based on vehicle simulation by Matlab/Simulink. Longitudinal and lateral velocity together with the yaw rate formed the state representation of the vehicle. The agent action space consists of two continuous actuator values: pedal ratio and roadwheel angle. The goal of the agent is twofold: first, it has to jump into a drifting state, second, it has to keep the vehicle in drift.The simulation results show that the proposed Soft Actor-Critic (SAC) RL agent is capable of learning to approach a pre-determined drift equilibrium from cornering and staying in this drift situation as well. For the training, the solution excluded using any kind of prior data, it only works with information gained from the simulation model, which is a remarkable difference from the actual state-of-the-art RL-based solutions.	Conference Paper	2022	10.1109/SAMI54271.2022.9780803	[]	[]	-1	-1	-1
B	[Anonymous]	2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)	2010	2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)				The following topics are dealt with: stroke patient rehabilitation; augmented reality; autonomous vehicle control; human-robot interaction; mobile surveillance; epileptic seizure onset detection; tutorial dialogues; wireless sensor network routing; fuzzy quadtree; mobile robots; landscape visualization; fuzzy control; induction motor drives; handwriting classification; reinforcement learning; power system operations; nonlinear state estimation; dynamic programming; fingerprint image segmentation; sensorless control; and speech recognition.	Conference Proceedings	2010		[]	[]	-1	-1	-1
J	Josef, Shirel; Degani, Amir	Deep Reinforcement Learning for Safe Local Planning of a Ground Vehicle in Unknown Rough Terrain	2020	IEEE ROBOTICS AND AUTOMATION LETTERS		6748	6755	Safe unmanned ground vehicle navigation in unknown rough terrain is crucial for various tasks such as exploration, search and rescue and agriculture. Offline global planning is often not possible when operating in harsh, unknown environments, and therefore, online local planning must be used. Most online rough terrain local planners require heavy computational resources, used for optimal trajectory searching and estimating vehicle orientation in positions within the range of the sensors. In this work, we present a deep reinforcement learning approach for local planning in unknown rough terrain with zero-range to local-range sensing, achieving superior results compared to potential fields or local motion planning search spaces methods. Our approach includes reward shaping which provides a dense reward signal. We incorporate self-attention modules into our deep reinforcement learning architecture in order to increase the explainability of the learnt policy. The attention modules provide insight regarding the relative importance of sensed inputs during training and planning. We extend and validate our approach in a dynamic simulation, demonstrating successful safe local planning in environments with a continuous terrain and a variety of discrete obstacles. By adding the geometric transformation between two successive timesteps and the corresponding action as inputs, our architecture is able to navigate on surfaces with different levels of friction. Reinforcement learning, autonomous vehicle navigation, motion and path planning.	Article	OCT 2020	10.1109/LRA.2020.3011912	[]	[]	-1	-1	-1
J	Schwarting, Wilko; Pierson, Alyssa; Alonso-Mora, Javier; Karaman, Sertac; Rus, Daniela	Social behavior for autonomous vehicles	2019	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA		24972	24978	Deployment of autonomous vehicles on public roads promises increased efficiency and safety. It requires understanding the intent of human drivers and adapting to their driving styles. Autonomous vehicles must also behave in safe and predictable ways without requiring explicit communication. We integrate tools from social psychology into autonomous-vehicle decision making to quantify and predict the social behavior of other drivers and to behave in a socially compliant way. A key component is Social Value Orientation (SVO), which quantifies the degree of an agent's selfishness or altruism, allowing us to better predict how the agent will interact and cooperate with others. We model interactions between agents as a best-response game wherein each agent negotiates to maximize their own utility. We solve the dynamic game by finding the Nash equilibrium, yielding an online method of predicting multiagent interactions given their SVOs. This approach allows autonomous vehicles to observe human drivers, estimate their SVOs, and generate an autonomous control policy in real time. We demonstrate the capabilities and performance of our algorithm in challenging traffic scenarios: merging lanes and unprotected left turns. We validate our results in simulation and on human driving data from the NGSIM dataset. Our results illustrate how the algorithm's behavior adapts to social preferences of other drivers. By incorporating SVO, we improve autonomous performance and reduce errors in human trajectory predictions by 25%.	Article	DEC 10 2019	10.1073/pnas.1820676116	[]	[]	-1	-1	-1
C	Li, Huayi; Li, Nan; Kolmanovsky, Ilya; Girard, Anouck	Energy-Efficient Autonomous Vehicle Control Using Reinforcement Learning and Interactive Traffic Simulations	2020	2020 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	3029	3034	Connected and autonomous vehicles are expected to improve mobility and transportation, as well as to provide energy efficiency benefits. The integration of safety and energy efficiency aspects is challenging as there are certain trade-offs between them, and also because the assessment of these attributes requires different time horizons. This paper illustrates the development of a controller for highway driving that, through reinforcement learning, can simultaneously address requirements of safety, comfort, performance and energy efficiency for battery electric vehicles. The training process of the decision policy exploits traffic simulations that are capable of representing the interactive behavior of vehicles in traffic based on game theory. Results indicate the potential for improved energy efficiency by adding powertrain-related states in the decision policy and by suitably defining the reward function.	Proceedings Paper	2020	10.23919/acc45564.2020.9147717	[]	[]	-1	-1	-1
J	Yao, Yu; Zhao, Junhui; Li, Zeqing; Cheng, Xu; Wu, Lenan	Jamming and Eavesdropping Defense Scheme Based on Deep Reinforcement Learning in Autonomous Vehicle Networks	2023	IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY		1211	1224	As a legacy from conventional wireless services, illegal eavesdropping is regarded as one of the critical security challenges in Connected and Autonomous Vehicles (CAVs) network. Our work considers the use of Distributed Kalman Filtering (DKF) and Deep Reinforcement Learning (DRL) techniques to improve anti-eavesdropping communication capacity and mitigate jamming interference. Aiming to improve the security performance against smart eavesdropper and jammer, we first develop a DKF algorithm that is capable of tracking the attacker more accurately by sharing state estimates among adjacent nodes. Then, a design problem for controlling transmission power and selecting communication channel is established while ensuring communication quality requirements of the authorized vehicular user. Since the eavesdropping and jamming model is uncertain and dynamic, a hierarchical Deep Q-Network (DQN)-based architecture is developed to design the anti-eavesdropping power control and possibly channel selection policy. Specifically, the optimal power control scheme without prior information of the eavesdropping behavior can be quickly achieved first. Based on the system secrecy rate assessment, the channel selection process is then performed when necessary. Simulation results confirm that our jamming and eavesdropping defense technique enhances the secrecy rate as well as achievable communication rate compared with currently available techniques.	Article	2023	10.1109/TIFS.2023.3236788	[]	[]	-1	-1	-1
B	Khalid, M.; Aslam, N.; Liang Wang	A Reinforcement Learning based Path Guidance Scheme for Long-range Autonomous Valet Parking in Smart Cities	2020	2020 IEEE Eighth International Conference on Communications and Networking (ComNet)		7 pp.	7 pp.	Finding a parking slot in the city centre has always been a great challenge. In many cases, drivers spend a lot of time roaming around looking for an empty and suitable parking slot. The emerging machine learning technologies in intelligent transport system has made it more flexible for Electric Autonomous Vehicle (EAV) to find a parking slot and get parked. The Long-range Autonomous Valet Parking (LAVP) allows an EAV to drop user at a suitable drop-off spot and select an economical parking slot. With the evolution of battery operated vehicles, the primary concern is efficient use of battery resources. This can be done either by maximizing battery capacity or by smartly using battery with existing capacity. During the parking process, most of the energy is consumed by finding an optimal path to parking slot. The work proposed in this paper guides EAV from a random starting point to nearest drop-off spot and CP. A Reinforcement Learning based Autonomous Valet Parking technique (RL-LAVP) has been designed to guide EAV to drop-off spot, CP and minimize the total distance covered during this process. The RL-LAVP results show a significant improvement towards minimizing covered distance and consumed energy when compared with RaNdom (RN) parking and LAVP parking techniques.	Conference Paper	2020	10.1109/ComNet47917.2020.9306103	[]	[]	-1	-1	-1
C	Vasquez, Dizan; Yu, Yufeng; Kumar, Suryansh; Laugier, Christian	An open framework for human-like autonomous driving using Inverse Reinforcement Learning	2014	2014 IEEE VEHICLE POWER AND PROPULSION CONFERENCE (VPPC)	IEEE Vehicle Power and Propulsion Conference			Research on autonomous car driving and advanced driving assistance systems has come to occupy a very significant place in robotics research. On the other hand, there are significant entry barriers (eg cost, legislation, logistics) that make it very difficult for small research groups and individual researchers to have access to a real autonomous vehicle for their experiments. This paper proposes to leverage an existing driving simulator (Torcs) by developing a ROS communication bridge for it. We use is as the basis for an experimental framework for the development and evaluation of Human-like autonomous driving based on Inverse Reinforce Learning (IRL). Based on an extensible and open architecture, this framework provides efficient GPU-based implementations of state-of the art IRL algorithms, as well as two challenging test environments and a set of evaluation metrics as a first step toward a benchmark.	Proceedings Paper	2014		[]	[]	-1	-1	-1
J	Wang, Shupei; Wang, Ziyang; Jiang, Rui; Zhu, Feng; Yan, Ruidong; Shang, Ying	A multi-agent reinforcement learning-based longitudinal and lateral control of CAVs to improve traffic efficiency in a mandatory lane change scenario	2024	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Bottleneck areas are prone to severe traffic congestion due to the sudden drop in capacity. To improve traffic efficiency in the bottleneck area, this paper proposes a multi-agent deep reinforcement learning framework integrating collision avoidance strategies to improve traffic efficiency in a mandatory lane change scenario. The proposed method considers distance-keeping and lane-changing coordination in a connected autonomous vehicle (CAV) environment, by controlling vehicles' longitudinal and lateral movement to effectively reduce traffic congestion in a mandatory lane change scenario. This framework was trained and tested in a simulation environment that is the same as the natural driving environment. Compared with real-world data and the benchmark model (a Dueling Double Deep Q-Network-based model), the proposed model shows better performance in terms of average speed, travel time, throughput, and safety in the bottleneck area. The results show that the proposed model can effectively reduce traffic congestion and improve traffic efficiency in a mandatory lane change scenario.	Article; Early Access		10.1016/j.trc.2023.104445	[]	[]	-1	-1	-1
C	Weingertner, Philippe; Ho, Minnie; Timofeev, Andrey; Aubert, Sebastien; Pita-Gil, Guillermo	Monte Carlo Tree Search With Reinforcement Learning for Motion Planning	2020	2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC			Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.	Proceedings Paper	2020	10.1109/itsc45102.2020.9294697	[]	[]	-1	-1	-1
J	Kuutti, Sampo; Bowden, Richard; Jin, Yaochu; Barber, Phil; Fallah, Saber	A Survey of Deep Learning Applications to Autonomous Vehicle Control	2021	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		712	733	Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.	Article	FEB 2021	10.1109/TITS.2019.2962338	[]	[]	-1	-1	-1
J	Wang, Rong; Yang, Peng; Gong, Yeming; Chen, Cheng	Operational policies and performance analysis for overhead robotic compact warehousing systems with bin reshuffling	2023	INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH				This paper studies a novel robotic warehousing system called the overhead robotic compact storage and retrieval system, which can free up the floor space occupation at a low cost. Bins, as basic storage containers, are stacked on top of each other to form a bin stack. Along overhead tracks, bin-picking robots transport bins between storage/retrieval positions and workstations with the aid of track-changing robots. Little research has been done to study operational policies and performance analysis for this new robotic compact warehousing system. We propose a nested queuing network model that considers two transportation resources and performs reinforcement learning using real data to improve the reshuffling efficiency. We find that reinforcement learning based reshuffling policy greatly reduces the reshuffling distance and saves computation time compared to existing policies. We find that the storage policy of stacks affects the optimal width/length ratio regardless of the system height. Interestingly, we obtain the number of robots that can stabilise the system to avoid an explosion of the order queue; two more robots than that number will produce relatively low throughput times. Compared to an AutoStore system, using our system reduces cost by 30% with a slight increase in throughput time.	Article; Early Access		10.1080/00207543.2023.2289643	[]	[]	-1	-1	-1
B	Ekechi, Chijioke Cyriacus	Intelligent Control of a Swarm of Unmanned Aerial Vehicles in Turbulent Environments Using Clustering-PPO Algorithm	2023						Dissertation/Thesis	Jan 01 2023		[]	[]	-1	-1	-1
C	Koren, Mark; Kochenderfer, Mykel J.	Adaptive Stress Testing without Domain Heuristics using Go-Explore	2020	2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC			Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more likely actions, in order to to find more likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.	Proceedings Paper	2020	10.1109/itsc45102.2020.9294729	[]	[]	-1	-1	-1
C	Li, Xiaohu; Cao, Zehong; Bai, Quan	A Novel Mountain Driving Unity Simulated Environment for Autonomous Vehicles	2021	THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE	AAAI Conference on Artificial Intelligence	16075	16077	The simulated driving environment provides a low cost and time-saving platform to test the performance of the autonomous vehicle by linkage with existing machine learning approaches. However, most of existing simulated driving environments focus on building flat roads in urban areas. Still, they neglected to endeavour the tough steep, curvy hill roads, such as mountain paths around suburban areas. In this study, by deploying in Unity engine, we developed the first complex mountain driving simulated environment with characterizing continuous curves and up/downhill. Then, two state-of-art reinforcement learning (RL) algorithms are used to train a vehicle agent and test the performance of autonomous vehicles in our developed simulated environment. Also, we set 5 different levels of vehicle's speeds and observe the cumulative rewards during the vehicle agent training. Our demonstration presents the developed environment supports for complex mountain scenario configurations and RL-based autonomous vehicles, and our findings show that the vehicle agent could achieve high cumulative rewards during the training stage, suggesting that our work is a potential new simulation environment for autonomous vehicles research. The demonstration video can be viewed via the link. https://youtu.be/0wSqGeCn-NU.	Proceedings Paper	2021		[]	[]	-1	-1	-1
J	Mahmoud, Sara; Billing, Erik; Svensson, Henrik; Thill, Serge	How to train a self-driving vehicle: On the added value (or lack thereof) of curriculum learning and replay buffers	2023	FRONTIERS IN ARTIFICIAL INTELLIGENCE				Learning from only real-world collected data can be unrealistic and time consuming in many scenario. One alternative is to use synthetic data as learning environments to learn rare situations and replay buffers to speed up the learning. In this work, we examine the hypothesis of how the creation of the environment affects the training of reinforcement learning agent through auto-generated environment mechanisms. We take the autonomous vehicle as an application. We compare the effect of two approaches to generate training data for artificial cognitive agents. We consider the added value of curriculum learning-just as in human learning-as a way to structure novel training data that the agent has not seen before as well as that of using a replay buffer to train further on data the agent has seen before. In other words, the focus of this paper is on characteristics of the training data rather than on learning algorithms. We therefore use two tasks that are commonly trained early on in autonomous vehicle research: lane keeping and pedestrian avoidance. Our main results show that curriculum learning indeed offers an additional benefit over a vanilla reinforcement learning approach (using Deep-Q Learning), but the replay buffer actually has a detrimental effect in most (but not all) combinations of data generation approaches we considered here. The benefit of curriculum learning does depend on the existence of a well-defined difficulty metric with which various training scenarios can be ordered. In the lane-keeping task, we can define it as a function of the curvature of the road, in which the steeper and more occurring curves on the road, the more difficult it gets. Defining such a difficulty metric in other scenarios is not always trivial. In general, the results of this paper emphasize both the importance of considering data characterization, such as curriculum learning, and the importance of defining an appropriate metric for the task.	Article	JAN 25 2023	10.3389/frai.2023.1098982	[]	[]	-1	-1	-1
J	Desjardins, Charles; Chaib-draa, Brahim	Cooperative Adaptive Cruise Control: A Reinforcement Learning Approach	2011	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		1248	1260	Recently, improvements in sensing, communicating, and computing technologies have led to the development of driver-assistance systems (DASs). Such systems aim at helping drivers by either providing a warning to reduce crashes or doing some of the control tasks to relieve a driver from repetitive and boring tasks. Thus, for example, adaptive cruise control (ACC) aims at relieving a driver from manually adjusting his/her speed to maintain a constant speed or a safe distance from the vehicle in front of him/her. Currently, ACC can be improved through vehicle-to-vehicle communication, where the current speed and acceleration of a vehicle can be transmitted to the following vehicles by intervehicle communication. This way, vehicle-to-vehicle communication with ACC can be combined in one single system called cooperative adaptive cruise control (CACC). This paper investigates CACC by proposing a novel approach for the design of autonomous vehicle controllers based on modern machine-learning techniques. More specifically, this paper shows how a reinforcement-learning approach can be used to develop controllers for the secure longitudinal following of a front vehicle. This approach uses function approximation techniques along with gradient-descent learning algorithms as a means of directly modifying a control policy to optimize its performance. The experimental results, through simulation, show that this design approach can result in efficient behavior for CACC.	Article	DEC 2011	10.1109/TITS.2011.2157145	[]	[]	-1	-1	-1
J	Salvi, A.; Coleman, J.; Buzhardt, J.; Krovi, V.; Tallapragada, P.	Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning*	2022	IFAC PapersOnLine		276	81	Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort. The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle. The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled. The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios. Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance. This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action. In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity. All rights reserved Elsevier.	Journal Paper	2022	10.1016/j.ifacol.2022.11.197	[]	[]	-1	-1	-1
C	Zhou, Weitao; Jiang, Kun; Cao, Zhong; Deng, Nanshan; Yang, Diange	Integrating Deep Reinforcement Learning with Optimal Trajectory Planner for Automated Driving	2020	2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC			Trajectory planning in the intersection is a challenging problem due to the strong uncertain intentions of surrounding agents. Conventional methods may fail in some corner cases when the ad-hoc parameters or predictions do not match the real traffic. This paper proposes a trajectory planning method, adaptive to the uncertain interactions, called Value-Estimation-Guild (VEG) trajectory planner. The method builds on the Frenet frame trajectory planner, in the meantime, uses the deep reinforcement learning to deal with the high uncertainty. The deep reinforcement learning learns from past failures and adjusts the sample direction of the optimal planner under the Frenet frame. In this way, the generated trajectory can be partially optimal and adapt to the stochastic as well. This method drives the automated vehicle through intersections and completes the unprotected left turn mission. During the testing, traffic density, surrounding vehicles' types, and intentions are all generated randomly. The statistics results show that the proposed trajectory planner works well under high uncertainty. It helps the automatic vehicles to finish the unprotected left turn with a success rate of 94.4 %, compared with the baseline method of 90%.	Proceedings Paper	2020	10.1109/itsc45102.2020.9294275	[]	[]	-1	-1	-1
J	Ngai, Chi Kit	Reinforcement-learning-based autonomous vehicle navigation in a dynamically changing environment	2008						Dissertation/Thesis	Jan 01 2008		[]	[]	-1	-1	-1
J	Ibn-Khedher, Hatem; Laroui, Mohammed; Moungla, Hassine; Afifi, Hossam; Abd-Elrahman, Emad	Next-Generation Edge Computing Assisted Autonomous Driving Based Artificial Intelligence Algorithms	2022	IEEE ACCESS		53987	54001	Edge Computing and Network Function Virtualization (NFV) concepts can improve network processing and multi-resources allocation when intelligent optimization algorithms are deployed. Multiservice offloading and allocation approaches pose interesting challenges in the current and next-generation vehicle networks. The state-of-the-art optimization approaches still formulate exact algorithms, and tune approximation methods to get sufficient solutions. These approaches are data-centric that aim to use heterogeneous data inputs to find the near optimal solutions. In the context of connected and autonomous vehicles (CAVs), these techniques show an exponential computational time and deal only with small and medium scale networks. Therefore, we are motivated by using recent Deep Reinforcement Learning (DRL) techniques to learn the behavior of exact optimization algorithms while enhancing the Quality of Service (QoS) of network operators and satisfying the requirements of the next-generation Autonomous Vehicles (AVs). DRL algorithms can improve AVs service offloading and optimize edge resources. An Optimal Virtual Edge Autopilot Placement (OVEAP) algorithm is proposed using Integer Linear Programming (ILP). Moreover, an autopilot placement protocol is presented to support the algorithm. Optimal allocation and Virtual Network Function (VNF) placement and chaining of the autopilot, based on several new constraints such as computing and networking loads, network edge infrastructure, and placement cost, are designed. Further, a DRL approach is formulated to deal with dense Internet of Autonomous Vehicle (IoAV) networks. Extensive simulations and evaluations are carried out. Results show that the proposed allocation strategies outperform the state-of-the-art solutions and give better performance in terms of Total Edge Servers Utilization, Total Edge Servers Allocation Time, and Successfully Allocated autopilots.	Article	2022	10.1109/ACCESS.2022.3174548	[]	[]	-1	-1	-1
B	Huang, Mengzhe	Learning-Based Optimal Control of Connected and Autonomous Vehicles	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Manchella, K.; Umrawal, A.K.; Aggarwal, V.	FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers & Goods Transportation [arXiv]	2020	arXiv		13 pp.	13 pp.	The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries. On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching. The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems. This paper considers combining passenger transportation with goods delivery to improve vehicle-based transportation. Even though the problem has been studied with a defined dynamics model of the transportation system environment, this paper considers a model-free approach that has been demonstrated to be adaptable to new or erratic environment dynamics. We propose FlexPool, a distributed model-free deep reinforcement learning algorithm that jointly serves passengers & goods workloads by learning optimal dispatch policies from its interaction with the environment. The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method. These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods. Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers & goods. FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers & goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.	Journal Paper	27 July 2020		[]	[]	-1	-1	-1
J	Wang, Chengyu; Wang, Luhan; Lu, Zhaoming; Chu, Xinghe; Shi, Zhengrui; Deng, Jiayin; Su, Tianyang; Shou, Guochu; Wen, Xiangming	SRL-TR<SUP>2</SUP>: A <i>S</i>afe <i>Re</i>inforcement <i>L</i>earning Based <i>TR</i>ajectory <i>TR</i>acker Framework	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		5765	5780	This paper aims to solve the trajectory tracking con-trol problem for an autonomous vehicle based on reinforcement learning methods. Existing reinforcement learning approaches have found limited successful applications on safety-critical tasks in the real world mainly due to two challenges: 1) sim-to-real transfer; 2) closed-loop stability and safety concern. In this paper, we propose an actor-critic-style framework SRL-TR2, in which the RL-based TRajectory TRackers are trained under the safety constraints and then deployed to a full-size vehicle as the lateral controller. To improve the generalization ability, we adopt a light-weight adapter State and Action Space Alignment (SASA) to establish mapping relations between the simulation and reality. To address the safety concern, we leverage an expert strategy to take over the control when the safety constraints are not satisfied. Hence, we conduct safe explorations during the training process and improve the stability of the policy. The experiments show that our agents can achieve one-shot transfer across simulation scenarios and unseen realistic scenarios, finishing the field tests with average running time less than 10 ms/step and average lateral error less than 0.1 m under the speed ranging from 12 km/h to 18 km/h. A video of the field tests is available at https://youtu.be/pjWcN_fV24g.	Article; Early Access		10.1109/TITS.2023.3250720	[]	[]	-1	-1	-1
B	Li, J.; Abusharkh, M.; Xu, Y.	DeepRacer Model Training for autonomous vehicles on AWS EC2	2022	2022 International Telecommunications Conference (ITC-Egypt)		5 pp.	5 pp.	Autonomous vehicle (AV) is the future of public transportation to reduce the road congestion and accidents. However, fully self-driving car is still a challenge for carmaker, and they must ensure the maximum driving security to avoid the ethical issue. To develop a mature model to AV, reinforcement learning becomes a solution which can explore many possibilities and choose the best possible action choice facing different road conditions. AWS DeepRacer is a comprehensive platform for researchers to start reinforcement learning for AV. With using DeepRacer Console, all the parameters including action spaces, reward functions and hyper parameters can be edited online and trained remotely. However, the price for training a converged model on DeepRacer Console is quite expensive for beginners. This paper present a DeepRacer simulation process built on EC2 instance which demand no hardware and cost significantly less than regular DeepRacer Console. Based on the default models from AWS, the authors experimentally adjusted hyper parameters and added reward functions to achieve higher speed and smoother driving actions. Even though the computing resources still limited the agent performance and stopped the model from convergence, there are 2-3m/s speed increases when adding low speed penalty and progress penalty on reward function.	Conference Paper	2022	10.1109/ITC-Egypt55520.2022.9855675	[]	[]	-1	-1	-1
J	Park, Jehyun; Choi, Jongeun; Nah, Sungjae; Kim, Dohee	Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations	2023	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE				Reinforcement learning has shown remarkable success in various applications, and in some cases, even out-performs human performance. However, despite the potential of reinforcement learning, numerous challenges still exist. In this paper, we introduce a novel approach that exploits the synergies between hierarchical reinforcement learning and distributional reinforcement learning to address complex sparse-reward tasks, where noisy state observations or non-stationary exogenous perturbations are present. Our proposed method has a hierarchical policy structure, where random rewards are modeled as random variables that follow a value distribution. This approach enables the handling of complex tasks and increases robustness to uncertainties arising from measurement noise or exogenous perturbations, such as wind. To achieve this, we extend the distributional soft Bellman operator and temporal difference error to include the hierarchical structure, and we use quantile regression to approximate the reward distribution. We evaluate our method using a bipedal robot in the OpenAI Gym environment and an electric autonomous vehicle in the SUMO traffic simulator. The results demonstrate the effectiveness of our approach in solving complex tasks with the aforementioned uncertainties when compared to state-of-the-art methods. Our approach demonstrates promising results in handling uncertainties caused by noise and perturbations for challenging sparse-reward tasks, and could potentially pave the way for the development of more robust and effective reinforcement learning algorithms in real physical systems.	Article	AUG 2023	10.1016/j.engappai.2023.106465	[]	[]	-1	-1	-1
J	D'alfonso, L.; Giannini, F.; Franze, G.; Fedele, G.; Pupo, F.; Fortino, G.	Autonomous vehicle platoons in urban road networks: a joint distributed reinforcement learning and model predictive control approach	2024	IEEE/CAA Journal of Automatica Sinica		141	56	In this paper, platoons of autonomous vehicles operating in urban road networks are considered. From a methodological point of view, the problem of interest consists of formally characterizing vehicle state trajectory tubes by means of routing decisions complying with traffic congestion criteria. To this end, a novel distributed control architecture is conceived by taking advantage of two methodologies: deep reinforcement learning and model predictive control. On one hand, the routing decisions are obtained by using a distributed reinforcement learning algorithm that exploits available traffic data at each road junction. On the other hand, a bank of model predictive controllers is in charge of computing the more adequate control action for each involved vehicle. Such tasks are here combined into a single framework: the deep reinforcement learning output (action) is translated into a set-point to be tracked by the model predictive controller; conversely, the current vehicle position, resulting from the application of the control move, is exploited by the deep reinforcement learning unit for improving its reliability. The main novelty of the proposed solution lies in its hybrid nature: on one hand it fully exploits deep reinforcement learning capabilities for decision-making purposes; on the other hand, time-varying hard constraints are always satisfied during the dynamical platoon evolution imposed by the computed routing decisions. To efficiently evaluate the performance of the proposed control architecture, a co-design procedure, involving the SUMO and MATLAB platforms, is implemented so that complex operating environments can be used, and the information coming from road maps (links, junctions, obstacles, semaphores, etc.) and vehicle state trajectories can be shared and exchanged. Finally by considering as operating scenario a real entire city block and a platoon of eleven vehicles described by double-integrator models, several simulations have been performed with the aim to put in light the main features of the proposed approach. Moreover, it is important to underline that in different operating scenarios the proposed reinforcement learning scheme is capable of significantly reducing traffic congestion phenomena when compared with well-reputed competitors.	Journal Paper	2024	10.1109/JAS.2023.123705	[]	[]	-1	-1	-1
J	Lee, Dongsu; Kwon, Minhae	Stability Analysis in Mixed-Autonomous Traffic With Deep Reinforcement Learning	2023	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		2848	2862	The emergence of autonomous driving vehicles on roads has increased the importance of research on autonomous driving in mixed-autonomous traffic. In mixed-autonomous traffic scenarios, it is necessary to comprehend the instability of autonomous vehicles and traffic flow corresponding to the uncertainty level in human-driven behaviors. However, studies of stability analysis in deep reinforcement learning are limited. This study focuses on the impact of deep reinforcement learning based autonomous vehicles in mixed-autonomous traffic from the stability perspective. We define the policy instability and traffic flow instabilities using the entropy of the velocity distributions to quantitatively measure the instability of an autonomous vehicle. Subsequently, we provide mathematical analyses to explain logarithmic growth patterns of instability. Moreover, we propose a novel deep reinforcement learning approach that jointly determines discrete and continuous actions under partial observation. To verify the proposed solution, we perform extensive simulations of various traffic scenarios (e.g., increasing traffic volumes, increasing the number of autonomous vehicles on the road, and setting the multiple uncertainty levels for human-driven behaviors) with ablation studies on reward function. Moreover, we analyze instabilities when human-driven vehicles are modeled using the human-like noisy controller and a policy that imitates actual human-driving data based on imitation learning. The simulation results support the theoretical analysis and confirm that the proposed method is stabler compared to a conventional control-theoretic approach.	Article	MAR 2023	10.1109/TVT.2022.3215505	[]	[]	-1	-1	-1
J	Tian Kang; Yu Di; Li Qing; Zhang Hongchang; Wu Yingnian; Fan Lingling	Lane keeping decision-making method of autonomous vehicles based on improved TD3	2022	Journal of Beijing Jiaotong University		84	94	This paper proposes a new end-to-end decision-making scheme for lane keeping based on the improved TD3 algorithm. First, a multi-data fusion TD3 algorithm framework is constructed to perceive the kinematics data information and visual image information of autonomous vehicle to enhance the stability of the algorithm. Second, combined with the concept of attention mechanism, image features are refined so that the algorithm pays attention to the key road information, which enhances the interpretability of the algorithm. Then, a guidance-based reward function is designed with comprehensive consideration of driving safety, comfort and efficiency to guide the intelligent agent to learn a more human-like driving strategy. Finally, a classification and high-value prioritized experience replay method is applied to improve the sample utilization and accelerate the algorithm convergence. With the aid of TORCS simulation platform, multiple sets of comparative experiments are designed to verify the effectiveness and feasibility of the proposed method. Furthermore, through simulation tests in multiple scenarios, it is verified that the overall performance of the proposed improved TD3 algorithm is better than that of the TD3 algorithm.	Article	2022		[]	[]	-1	-1	-1
J	Holen, Martin; Knausgard, Kristian Muri; Goodwin, Morten	Development of a Simulator for Prototyping Reinforcement Learning-Based Autonomous Cars	2022	INFORMATICS-BASEL				Autonomous driving is a research field that has received attention in recent years, with increasing applications of reinforcement learning (RL) algorithms. It is impractical to train an autonomous vehicle thoroughly in the physical space, i.e., the so-called 'real world'; therefore, simulators are used in almost all training of autonomous driving algorithms. There are numerous autonomous driving simulators, very few of which are specifically targeted at RL. RL-based cars are challenging due to the variety of reward functions available. There is a lack of simulators addressing many central RL research tasks within autonomous driving, such as scene understanding, localization and mapping, planning and driving policies, and control, which have diverse requirements and goals. It is, therefore, challenging to prototype new RL projects with different simulators, especially when there is a need to examine several reward functions at once. This paper introduces a modified simulator based on the Udacity simulator, made for autonomous cars using RL. It creates reward functions, along with sensors to create a baseline implementation for RL-based vehicles. The modified simulator also resets the vehicle when it gets stuck or is in a non-terminating loop, making it more reliable. Overall, the paper seeks to make the prototyping of new systems simple, with the testing of different RL-based systems.	Article	JUN 2022	10.3390/informatics9020033	[]	[]	-1	-1	-1
J	Zhang, Ethan; Zhang, Ruixuan; Masoud, Neda	Predictive trajectory planning for autonomous vehicles at intersections using reinforcement learning	2023	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				In this work we put forward a predictive trajectory planning framework to help autonomous vehicles plan future trajectories. We develop a partially observable Markov decision process (POMDP) to model this sequential decision making problem, and a deep reinforcement learning solution methodology to learn high-quality policies. The POMDP model utilizes driving scenar-ios, condensed into graphs, as inputs. More specifically, an input graph contains information on the history trajectory of the subject vehicle, predicted trajectories of other agents in the scene (e.g., other vehicles, pedestrians, and cyclists), as well as predicted risk levels posed by surrounding vehicles to devise safe, comfortable, and energy-efficient trajectories for the subject vehicle to follow. In order to obtain sufficient driving scenarios to use as training data, we propose a simulation framework to generate socially acceptable driving scenarios using a real world autonomous vehicle dataset. The simulation framework utilizes Bayesian Gaussian mixture models to learn trajectory patterns of different agent types, and Gibbs sampling to ensure that the distribution of simulated scenarios matches that of the real-world dataset collected by an autonomous fleet. We evaluate the proposed work in two complex urban driving environments: a non-signalized T-junction and a non-signalized lane merge intersection. Both environments provide vastly more complex driving scenarios compared to a highway driving environment, which has been mostly the focus of previous studies. The framework demonstrates promising performance for planning horizons as long as five seconds. We compare safety, comfort, and energy efficiency of the planned trajectories against human-driven trajectories in both experimental driving environments, and demonstrate that it outperforms human-driven trajectories in a statistically significant fashion in all aspects.	Article; Early Access		10.1016/j.trc.2023.104063	[]	[]	-1	-1	-1
J	Li, Zhuo; You, Keyou; Sun, Jian; Wang, Gang	Informative Trajectory Planning Using Reinforcement Learning for Minimum-Time Exploration of Spatiotemporal Fields	2023	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS				This article studies the informative trajectory planning problem of an autonomous vehicle for field exploration. In contrast to existing works concerned with maximizing the amount of information about spatial fields, this work considers efficient exploration of spatiotemporal fields with unknown distributions and seeks minimum-time trajectories of the vehicle while respecting a cumulative information constraint. In this work, upon adopting the observability constant as an information measure for expressing the cumulative information constraint, the existence of a minimum-time trajectory is proven under mild conditions. Given the spatiotemporal nature, the problem is modeled as a Markov decision process (MDP), for which a reinforcement learning (RL) algorithm is proposed to learn a continuous planning policy. To accelerate the policy learning, we design a new reward function by leveraging field approximations, which is demonstrated to yield dense rewards. Simulations show that the learned policy can steer the vehicle to achieve an efficient exploration, and it outperforms the commonly-used coverage planning method in terms of exploration time for sufficient cumulative information.	Article; Early Access		10.1109/TNNLS.2023.3300926	[]	[]	-1	-1	-1
C	Tian, Zhaofeng; Shi, Weisong	Design and Implement an Enhanced Simulator for Autonomous Delivery Robot	2022	2022 FIFTH INTERNATIONAL CONFERENCE ON CONNECTED AND AUTONOMOUS DRIVING (METROCAD 2022)		21	29	As autonomous driving technology is getting more and more mature today, autonomous delivery companies like Starship, Marble, and Nuro has been making progress in the tests of their autonomous delivery robots. While simulations and simulators are very important for the final product landing of the autonomous delivery robots since the autonomous delivery robots need to navigate on the sidewalk, campus, and other urban scenarios, where the simulations can avoid real damage to pedestrians and properties in the real world caused by any algorithm failures and programming errors and thus accelerate the whole developing procedure and cut down the cost. In this case, this study proposes an open-source simulator based on our autonomous delivery robot ZebraT to accelerate the research on autonomous delivery. The simulator developing procedure is illustrated step by step. What is more, the applications on the simulator that we are working on are also introduced, which includes autonomous navigation in the simulated urban environment, cooperation between an autonomous vehicle and an autonomous delivery robot, and reinforcement learning practice on the task training in the simulator. We have published the proposed simulator in Github.	Proceedings Paper	2022	10.1109/MetroCAD56305.2022.00009	[]	[]	-1	-1	-1
J	Syavasya, C. V. S. R.; Muddana, A. Lakshmi	Optimization of autonomous vehicle speed control mechanisms using hybrid DDPG-SHAP-DRL-stochastic algorithm	2022	ADVANCES IN ENGINEERING SOFTWARE				Autonomous Vehicles (AV) are the future milestones of the automobile industry, which functions without the intervention of human being. Numerous researches have been stimulated by leading automobile sectors of the world, to address the anticipated challenges in implementing the autonomous vehicles in a practical scenario. The speed control mechanism is the predominant challenge which acts in the basis of Machine Learning mechanism is the major thrust area associated with autonomous vehicles. Reinforcement Learning (RL) is the effective algorithm to solve the challenges associated with the autonomous driving of vehicles and its decision on complex scenarios. A simulative environment is advantageous for training and validation of an RL algorithm because it reduces risk and saves resources. This research work introduces a novel hybrid algorithm composed of Deep Deterministic Policy Gradient (DDPG) -SHapley Additive exPlanations (SHAP) - Deep Reinforcement Learning (DRL)-stochastic algorithm. The primary objective of this research work is to introduce an RL envi-ronment for optimizing longitudinal control.	Article; Early Access		10.1016/j.advengsoft.2022.103245	[]	[]	-1	-1	-1
J	Ahadi, Ramin; Ketter, Wolfgang; Collins, John; Daina, Nicolo	Cooperative Learning for Smart Charging of Shared Autonomous Vehicle Fleets	2023	TRANSPORTATION SCIENCE		613	630	We study the operational problem of shared autonomous electric vehicles that cooperate in providing on-demand mobility services while maximizing fleet profit and service quality. Therefore, we model the fleet operator and vehicles as interactive agents enriched with advanced decision-making aids. Our focus is on learning smart charging policies (when and where to charge vehicles) in anticipation of uncertain future demands to accommodate long charging times, restricted charging infrastructure, and time-varying electricity prices. We propose a distributed approach and formulate the problem as a semiMarkov decision process to capture its stochastic and dynamic nature. We use cooperative multiagent reinforcement learning with reshaped reward functions. The effectiveness and scalability of the proposed model are upgraded through deep learning. A mean-field approximation deals with environment instabilities, and hierarchical learning distinguishes high-level and low-level decisions. We evaluate our model using various numerical examples based on real data from ShareNow in Berlin, Germany. We show that the policies learned using our decentralized and dynamic approach outperform central static charging strategies. Finally, we conduct a sensitivity analysis for different fleet characteristics to demonstrate the proposed model's robustness and provide managerial insights into the impacts of strategic decisions on fleet performance and derived charging policies.	Article; Early Access		10.1287/trsc.2022.1187	[]	[]	-1	-1	-1
J	Ji, Yuan; Zhang, Junzhi; Lv, Chen; He, Chengkun; Chen, Hao; Han, Jinheng; Hou, Xiaohui	Optimal Path Tracking Control Based on Online Modeling for Autonomous Vehicle With Completely Unknown Parameters	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		15207	15218	Reliable path tracking control (PTC) method is essential for autonomous driving. However, existing PTC methods count on prior vehicle parameters to achieve good performance. This paper presents an optimal PTC method without requiring any prior vehicle parameters based on online modeling with strict parameter convergence ability. First, we build a virtual optimal control problem using adaptive dynamic programming (ADP) scheme to guide the data collection and solve two characteristic matrices containing parameter information. Then, the model construction method is derived using the solved matrices and the optimal PTC method is constructed using the constructed model. Finally, a fault-tolerant control scheme is further designed using the constructed model and the online modeling ability of the proposed method. The effectiveness of the proposed method is validated through co-simulation between Matlab/Simulink and high-fidelity vehicle dynamic simulation software CarSim (R) under both fault-free and fault-tolerant situations.	Article	DEC 2023	10.1109/TITS.2023.3306040	[]	[]	-1	-1	-1
J	Shi, Yanjun; Liu, Yuanzhuo; Qi, Yuhan; Han, Qiaomei	A Control Method with Reinforcement Learning for Urban Un-Signalized Intersection in Hybrid Traffic Environment	2022	SENSORS				To control autonomous vehicles (AVs) in urban unsignalized intersections is a challenging problem, especially in a hybrid traffic environment where self-driving vehicles coexist with human driving vehicles. In this study, a coordinated control method with proximal policy optimization (PPO) in Vehicle-Road-Cloud Integration System (VRCIS) is proposed, where this control problem is formulated as a reinforcement learning (RL) problem. In this system, vehicles and everything (V2X) was used to keep communication between vehicles, and vehicle wireless technology can detect vehicles that use vehicles and infrastructure (V2I) wireless communication, thereby achieving a cost-efficient method. Then, the connected and autonomous vehicle (CAV) defined in the VRCIS learned a policy to adapt to human driving vehicles (HDVs) across the intersection safely by reinforcement learning (RL). We have developed a valid, scalable RL framework, which can communicate topologies that may be dynamic traffic. Then, state, action and reward of RL are designed according to urban unsignalized intersection problem. Finally, how to deploy within the RL framework was described, and several experiments with this framework were undertaken to verify the effectiveness of the proposed method.	Article	FEB 2022	10.3390/s22030779	[]	[]	-1	-1	-1
C	Lu, Liping; Ning, Qinjian; Qiu, Yujie; Chu, Duanfeng	Vehicle Trajectory Prediction Model Based on Attention Mechanism and Inverse Reinforcement Learning	2022	2022 IEEE 34TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI	Proceedings-International Conference on Tools With Artificial Intelligence	1160	1166	Predicting the future trajectory of a vehicle in a dynamic scene is not a simple problem because the future trajectory of a vehicle is not only influenced by its historical trajectory but also by other vehicles. To solve this problem, we propose a vehicle trajectory prediction model based on attention mechanism and inverse reinforcement learning. The model uses the LSTM encoder-decoder framework as an infrastructure to efficiently extract the temporal features of vehicle trajectories. A social attention module is proposed to model the degree of inter-vehicle influence based on the distance between vehicles. The module generates feature vectors and serves as the weight values of the attention mechanism, enabling the prediction network to focus more on the surrounding vehicles with a greater degree of influence. An inverse reinforcement learning framework is introduced to regularize the encoder network using a reward function. The reward function effectively evaluates the gap between the predicted and true positions of the encoder output and enables the predicted positions to be closer to the true positions by training the network parameters. Based on the experimental results of public datasets SDD and NGSIM, our model can predict the future trajectory of vehicles more accurately than other models.	Proceedings Paper	2022	10.1109/ICTAI56018.2022.00177	[]	[]	-1	-1	-1
J	Sun, Yipu; Chen, Xin; Wang, Wei; Fu, Hao; Wu, Min	Model-Free Output Consensus Control for Partially Observable Heterogeneous Multivehicle Systems	2020	IEEE INTERNET OF THINGS JOURNAL		7135	7147	Internet of Vehicles (IoV) is a typical application of Internet-of-Things (IoT) technology in the field of intelligent transportation systems. In the actual IoV, such as the autonomous vehicle fleet, there exists the problem of heterogeneous multivehicle coordination based on IoT communication. How to ensure the synchronization of multiple vehicles is a hot issue. In particular, when the system can only obtain a partial state of the vehicle, and does not know the dynamic model, including the vehicle itself and the companion model. To overcome these deficiencies, this article deals with the model-free output consensus control problem for a class of partially observable heterogeneous multivehicle systems (MVSs). Using measurable input/output data without any system knowledge, this article develops a Q-function-based adaptive dynamic programming (ADP). First, an adaptive distributed observer is designed to estimate the output of the leader. The augmented state representation is built using historical measurable input/output data instead of the unmeasurable inner system state. Then, a Q-function-based ADP method using measurable input/output data was introduced. The method is used to solve this distributed tracking control problem without the requirement for the MVSs dynamics. The convergence analysis of the proposed method is also given. To facilitate the implementation of the proposed method, an actor-critic framework is adopted to approximate the optimal Q-functions and the optimal control policies. It shows that the approximated control policies achieve the distributed optimal tracking control. Finally, the simulation results verify the effectiveness of the developed method for solving multivehicle formation control problems.	Article	AUG 2020	10.1109/JIOT.2020.2981654	[]	[]	-1	-1	-1
J	Teng Liu; Bo Wang; Dongpu Cao; Xiaolin Tang; Yalian Yang	Integrated Longitudinal Speed Decision-Making and Energy Efficiency Control for Connected Electrified Vehicles [arXiv]	2020	arXiv		11 pp.	11 pp.	To improve the driving mobility and energy efficiency of connected autonomous electrified vehicles, this paper presents an integrated longitudinal speed decision-making and energy efficiency control strategy. The proposed approach is a hierarchical control architecture, which is assumed to consist of higher-level and lower-level controls. As the core of this study, model predictive control and reinforcement learning are combined to improve the powertrain mobility and fuel economy for a group of automated vehicles. The higher-level exploits the signal phase and timing and state information of connected autonomous vehicles via vehicle to infrastructure and vehicle to vehicle communication to reduce stopping at red lights. The higher-level outputs the optimal vehicle velocity using model predictive control technique and receives the power split control from the lower-level con-troller. These two levels communicate with each other via a controller area network in the real vehicle. The lower-level utilizes a model-free reinforcement learning method to improve the fuel economy for each connected autonomous vehicle. Numerical tests illustrate that vehicle mobility can be noticeably improved (traveling time reduced by 30%) by reducing red-light idling. The effectiveness and performance of the proposed method are validated via comparison analysis among different energy efficiency controls (fuel economy promoted by 13%).	Journal Paper	24 July 2020		[]	[]	-1	-1	-1
J	Ma, Haitong; Liu, Changliu; Li, Shengbo Eben; Zheng, Sifa; Sun, Wenchao; Chen, Jianyu	Learn Zero-Constraint-Violation Safe Policy in Model-Free Constrained Reinforcement Learning.	2024	IEEE transactions on neural networks and learning systems				We focus on learning the zero-constraint-violation safe policy in model-free reinforcement learning (RL). Existing model-free RL studies mostly use the posterior penalty to penalize dangerous actions, which means they must experience the danger to learn from the danger. Therefore, they cannot learn a zero-violation safe policy even after convergence. To handle this problem, we leverage the safety-oriented energy functions to learn zero-constraint-violation safe policies and propose the safe set actor-critic (SSAC) algorithm. The energy function is designed to increase rapidly for potentially dangerous actions, locating the safe set on the action space. Therefore, we can identify the dangerous actions prior to taking them and achieve zero-constraint violation. Our major contributions are twofold. First, we use the data-driven methods to learn the energy function, which releases the requirement of known dynamics. Second, we formulate a constrained RL problem to solve the zero-violation policies. We prove that our Lagrangian-based constrained RL solutions converge to the constrained optimal zero-violation policies theoretically. The proposed algorithm is evaluated on the complex simulation environments and a hardware-in-loop (HIL) experiment with a real autonomous vehicle controller. Experimental results suggest that the converged policies in all environments achieve zero-constraint violation and comparable performance with model-based baseline.	Journal Article	2024-Jan-17	10.1109/TNNLS.2023.3348422	[]	[]	-1	-1	-1
C	Liessner, Roman; Dohmen, Jan; Wiering, Marco	Explainable Reinforcement Learning for Longitudinal Control	2021	ICAART: PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE - VOL 2		874	881	Deep Reinforcement Learning (DRL) has the potential to surpass the existing state of the art in various practical applications. However, as long as learned strategies and performed decisions are difficult to interpret, DRL will not find its way into safety-relevant fields of application. SHAP values are an approach to overcome this problem. It is expected that the addition of these values to DRL provides an improved understanding of the learned action-selection policy. In this paper, the application of a SHAP method for DRL is demonstrated by means of the OpenAI Gym LongiControl Environment. In this problem, the agent drives an autonomous vehicle under consideration of speed limits in a single lane route. The controls learned with a DDPG algorithm are interpreted by a novel approach combining learned actions and SHAP values. The proposed RL-SHAP representation makes it possible to observe in every time step which features have a positive or negative effect on the selected action and which influences are negligible. The results show that RL-SHAP values are a suitable approach to interpret the decisions of the agent.	Proceedings Paper	2021	10.5220/0010256208740881	[]	[]	-1	-1	-1
J	; ; ; 	Autonomous driving in the uncertain traffica deep reinforcement learning approach	2018	The Journal of China Universities of Posts and Telecommunications		21	30,96	Driving in the complex traffic safely and efficiently is a difficult task for autonomous vehicle because of the stochastic characteristics of engaged human drivers. Deep reinforcement learning (DRL),which combines the abstract representation capability of deep learning (DL) and the optimal decision making and control capability of reinforcement learning (RL),is a good approach to address this problem. Traffic environment is built up by combining intelligent driver model (IDM) and lane-change model as behavioral model for vehicles. To increase the stochastic of the established traffic environment,tricks such as defining a speed distribution with cutoff for traffic cars and using various politeness factors to represent distinguished lane-change style,are taken. For training an artificial agent to achieve successful strategies that lead to the greatest long-term rewards and sophisticated maneuver,deep deterministic policy gradient (DDPG) algorithm is deployed for learning. Reward function is designed to get a trade-off between the vehicle speed,stability and driving safety. Results show that the proposed approach can achieve good autonomous maneuvering in a scenario of complex traffic behavior through interaction with the environment.	Article	2018		[]	[]	-1	-1	-1
J	Tung, Tze-Yang; Kobus, Szymon; Roig, Joan Pujol; Gunduz, Deniz	Effective Communications: A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning Over Noisy Channels	2021	IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS		2590	2603	"We propose a novel formulation of the ""effectiveness problem"" in communications, put forth by Shannon and Weaver in their seminal work ""The Mathematical Theory of Communication"", by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework. Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment, can also communicate with each other over a noisy communication channel. The noisy communication channel is considered explicitly as part of the dynamics of the environment, and the message each agent sends is part of the action that the agent can take. As a result, the agents learn not only to collaborate with each other but also to communicate ""effectively"" over a noisy channel. This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the ""learning to communicate"" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free. We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP. This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems."	Article	AUG 2021	10.1109/JSAC.2021.3087248	[]	[]	-1	-1	-1
J	Barbu, Clara; Mocanu, Stefan Alexandru	ON THE DEVELOPMENT OF AUTONOMOUS AGENTS USING DEEP REINFORCEMENT LEARNING	2021	UNIVERSITY POLITEHNICA OF BUCHAREST SCIENTIFIC BULLETIN SERIES C-ELECTRICAL ENGINEERING AND COMPUTER SCIENCE		97	116	This paper presents a study on the general concept of autonomous agents, with an accent on the development of such agents using deep reinforcement learning. This is combined with the domain of autonomous vehicles, as illustrated by a practical application: having a vehicle agent learn how to navigate and park by itself on a designated spot, in a virtual parking lot environment created in Unity. The reinforcement learning method Deep Q-Learning is implemented, with the addition of a few improvements such as Double Deep Q-Learning and Experience Replay.	Article	2021		[]	[]	-1	-1	-1
B	Jardine, P. Travis	A reinforcement learning approach to predictive control design: Autonomous vehicle applications	2018						Dissertation/Thesis	Jan 01 2018		[]	[]	-1	-1	-1
J	Deng, Huifan; Zhao, Youqun; Wang, Qiuwei; Nguyen, Anh-Tu	Deep Reinforcement Learning Based Decision-Making Strategy of Autonomous Vehicle in Highway Uncertain Driving Environments	2023	AUTOMOTIVE INNOVATION		438	452	Uncertain environment on multi-lane highway, e.g., the stochastic lane-change maneuver of surrounding vehicles, is a big challenge for achieving safe automated highway driving. To improve the driving safety, a heuristic reinforcement learning decision-making framework with integrated risk assessment is proposed. First, the framework includes a long short-term memory model to predict the trajectory of surrounding vehicles and a future integrated risk assessment model to estimate the possible driving risk. Second, a heuristic decaying state entropy deep reinforcement learning algorithm is introduced to address the exploration and exploitation dilemma of reinforcement learning. Finally, the framework also includes a rule-based vehicle decision model for interaction decision problems with surrounding vehicles. The proposed framework is validated in both low-density and high-density traffic scenarios. The results show that the traffic efficiency and vehicle safety are both improved compared to the common dueling double deep Q-Network method and rule-based method.	Article; Early Access		10.1007/s42154-023-00231-6	[]	[]	-1	-1	-1
J	Chen, Baiming; Chen, Xiang; Wu, Qiong; Li, Liang	Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		10333	10342	Autonomous vehicles must be comprehensively evaluated before deployed in cities and highways. However, most existing evaluation approaches for autonomous vehicles are static and lack adaptability, so they are usually inefficient in generating challenging scenarios for tested vehicles. In this paper, we propose an adaptive evaluation framework to efficiently evaluate autonomous vehicles in adversarial environments generated by deep reinforcement learning. Considering the multimodal nature of dangerous scenarios, we use ensemble models to represent different local optimums for diversity. We then utilize a nonparametric Bayesian method to cluster the adversarial policies. The proposed method is validated in a typical lane-change scenario that involves frequent interactions between the ego vehicle and the surrounding vehicles. Results show that the adversarial scenarios generated by our method significantly degrade the performance of the tested vehicles. We also illustrate different patterns of generated adversarial environments, which can be used to infer the weaknesses of the tested vehicles.	Article; Early Access		10.1109/TITS.2021.3091477	[]	[]	-1	-1	-1
J	Teruhiko, U.; Noriaki, S.	Distributed scheduling for autonomous vehicles by reinforcement learning	1997	Transactions of the Institute of Electrical Engineers of Japan, Part C		1513	20	We propose an autonomous vehicle scheduling schema in large physical distribution terminals publicly used as the next generation wide area physical distribution bases. This schema uses learning automata for vehicle scheduling based on the Contract Net Protocol, in order to obtain useful emergent behaviors of agents in the system based on the local decision-making of each agent. The state of the automaton is updated at each instant on the basis of new information that includes the arrival estimation time of vehicles. Each agent estimates the arrival time of vehicles by using a Bayesian learning process. Using traffic simulation, we evaluate the schema in various simulated environments. The result shows the advantage of the schema over when each agent provides the same criteria from the top down, and each agent voluntarily generates criteria via interactions with the environment, playing an individual role in the system.	Journal Paper	Oct. 1997		[]	[]	-1	-1	-1
C	Yen, Yi-Tung; Chou, Jyun-Jhe; Shih, Chi-Sheng; Chen, Chih-Wei; Tsung, Pei-Kuei	Proactive Car-Following Using Deep-Reinforcement Learning	2020	2020 IEEE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC			Car-following is a fundamental operation for vehicle control for both ADAS on modern vehicles and vehilce control on autonomous vehicles. Most existing car following mechanisms react to the observations of nearby vehicles in real-time. Unfortunately, lack of capability of taking into account multiple constraints and objectives, these mechanisms lead to poor efficiency, discomfort, and unsafe operations. In this paper, we design and implement a proactive car-following model to take into account safety regulation, efficiency, and comfort using deep reinforcement learning. The evaluation results show that the proactive model not only reduces the number of inefficient and unsafe headway but also eliminates the traffic jerk, compared to human drivers. The model outperformed 79% human drivers in public data set and the road efficiency is only 2% less than the optimal bound. Compared to ACC model, the DDPG model allows 4.1% more vehicles to finish the simulation than ACC model does, and increases the average speed for 28.4%.	Proceedings Paper	2020	10.1109/itsc45102.2020.9294194	[]	[]	-1	-1	-1
J	Zhu, Zixuan; Teng, Chenglong; Cai, Yingfeng; Chen, Long; Lian, Yubo; Wang, Hai	Vehicle Safety Planning Control Method Based on Variable Gauss Safety Field	2022	WORLD ELECTRIC VEHICLE JOURNAL				The existing intelligent vehicle trajectory-planning methods have limitations in terms of efficiency and safety. To overcome these limitations, this paper proposes an automatic driving trajectory-planning method based on a variable Gaussian safety field. Firstly, the time series bird's-eye view is used as the input state quantity of the network, which improves the effectiveness of the trajectory planning policy network in extracting the features of the surrounding traffic environment. Then, the policy gradient algorithm is used to generate the planned trajectory of the autonomous vehicle, which improves the planning efficiency. The variable Gaussian safety field is used as the reward function of the trajectory planning part and the evaluation index of the control part, which improves the safety of the reinforcement learning vehicle tracking algorithm. The proposed algorithm is verified using the simulator. The obtained results show that the proposed algorithm has excellent trajectory planning ability in the highway scene and can achieve high safety and high precision tracking control.	Article	NOV 2022	10.3390/wevj13110203	[]	[]	-1	-1	-1
J	Prathiba, Sahaya Beni; Raja, Gunasekaran; Anbalagan, Sudha; Gurumoorthy, Sugeerthi; Kumar, Neeraj; Guizani, Mohsen	Cybertwin-Driven Federated Learning Based Personalized Service Provision for 6G-V2X	2022	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		4632	4641	The rapid growth of Autonomous Vehicle (AV) technology and the integration of edge computing grasp new challenges along with the ever-increasing mobile internet traffic and services. Tackling such challenges through customized edge computing services is the critical research in 6G Vehicle-to-Everything (6G-V2X) communication. V2X contributes detailed information about the current navigation of vehicles, automatic payments for toll roads, parking fees and other services. With the countless, unique, and personalized service requirements of AVs over computation-intensive applications, exploring the edge resources for the excellent Quality of Service (QoS) provision is the greatest concern. This paper proposes a Federated Learning and edge Cache-assisted Cybertwin (FLCC) framework for personalized service provision in 6G-V2X. Integration of cybertwin in 6G enables the connectivity of the physical system to the digital realm, allowing for adequate instantaneous wireless access. The FLCC jointly considers the edge cooperation and optimizations through the proposed Federated Multi-agent Deep Reinforcement Learning based (FM-DRL) algorithm. The FM-DRL algorithm balances the FLCC's learning accuracy. It minimizes the time and cost by taking the factors such as cybertwin association, training data batch size, and bandwidth. Finally, caching is performed using the Federated Reinforcement Learning-based Edge Caching (FREC) algorithm to obtain the desired datasets required that train the model for providing personalized 6G-V2X services for the AVs. Numerical studies and simulation results reveal that the proposed system outperforms the baseline learning approaches by 17.6%.	Article	MAY 2022	10.1109/TVT.2021.3133291	[]	[]	-1	-1	-1
J	He, Xiangkun; Lv, Chen	Towards Safe Autonomous Driving: Decision Making with Observation-Robust Reinforcement Learning	2023	AUTOMOTIVE INNOVATION		509	520	Most real-world situations involve unavoidable measurement noises or perception errors which result in unsafe decision making or even casualty in autonomous driving. To address these issues and further improve safety, automated driving is required to be capable of handling perception uncertainties. Here, this paper presents an observation-robust reinforcement learning against observational uncertainties to realize safe decision making for autonomous vehicles. Specifically, an adversarial agent is trained online to generate optimal adversarial attacks on observations, which attempts to amplify the average variation distance on perturbed policies. In addition, an observation-robust actor-critic approach is developed to enable the agent to learn the optimal policies and ensure that the changes of the policies perturbed by optimal adversarial attacks remain within a certain bound. Lastly, the safe decision making scheme is evaluated on a lane change task under complex highway traffic scenarios. The results show that the developed approach can ensure autonomous driving performance, as well as the policy robustness against adversarial attacks on observations.	Article; Early Access		10.1007/s42154-023-00256-x	[]	[]	-1	-1	-1
J	Zhai, Yuanzhao; Ding, Bo; Liu, Xuan; Jia, Hongda; Zhao, Yong; Luo, Jie	Decentralized Multi-Robot Collision Avoidance in Complex Scenarios With Selective Communication	2021	IEEE ROBOTICS AND AUTOMATION LETTERS		8379	8386	Deep reinforcement learning has been demonstrated to be an effective solution to the multi-robot collision avoidance problem. However, with existing methods, robots typically generate actions only based on local observations, sometimes augmented with global communication. Their performance deteriorates in limited bandwidth environments and complex scenarios with various obstacles and high robot density. We propose SelComm, a selective communication framework to generate cooperative and collision-free actions for robots in multi-robot navigation tasks. Specifically, we develop a decentralized message selector, enabling each robot to calculate relations with other robots using both agent-level information and sensor-level information, and select the most valuable messages to meet the bandwidth limitation. Then we introduce the attentional communication channel for efficient communication. Our experimental evaluations based on various scenarios demonstrate that SelComm learns more cooperative behaviors and outperforms state-of-the-art methods in limited bandwidth environments and complex scenarios.	Article	OCT 2021	10.1109/LRA.2021.3102636	[]	[]	-1	-1	-1
J	Wang, Zhe; Huang, Helai; Tang, Jinjun; Meng, Xianwei; Hu, Lipeng	Velocity control in car-following behavior with autonomous vehicles using reinforcement learning	2022	ACCIDENT ANALYSIS AND PREVENTION				Car-following behavior is a common driving behavior. It is necessary to consider the following vehicle in the car following model of autonomous vehicle (AV) under the background of the vehicle-to-vehicle transportation system. In this study, a safe velocity control method for AV based on reinforcement learning with considering the following vehicle is proposed. First, the mixed driving environment of AVs and human-driven vehicles is constructed, and the trajectories of the leading and following vehicles are extracted from the naturalistic High D driving dataset. Next, the soft actor-critic (SAC) algorithm is used as the velocity control algorithm, in which the agent is AV, the action is acceleration, and the state is the relative distance and relative speed between the AV and the leading and following vehicles. Then, a reward function based on state and corresponding action is designed to guide AV to choose acceleration without collision between the leading and following vehicles. Furthermore, AVs are gradually able to learn to avoid collisions between the leading and following vehicles after training the model. The test result of the trained model shows that the SAC agent can achieve complete collision avoidance, resulting in zero collision. Finally, the driving performance of the SAC agent and that of human driving are compared and analyzed for safety and efficiency. The results of this study are expected to improve the safety of the car-following process..	Article; Early Access		10.1016/j.aap.2022.106729	[]	[]	-1	-1	-1
C	Garzon, Mario; Spalanzani, Anne	Game theoretic decision making based on real sensor data for autonomous vehicles' maneuvers in high traffic	2020	2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	5378	5384	This paper presents an approach for implementing game theoretic decision making in combination with realistic sensory data input so as to allow an autonomous vehicle to perform maneuvers, such as lane change or merge in high traffic scenarios. The main novelty of this work, is the use of realistic sensory data input to obtain the observations as input of an iterative multi-player game in a realistic simulator. The game model allows to anticipate reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication. Moreover, direct information from the simulator, such as position or speed of the vehicles is also avoided.The solution of the game is based on cognitive hierarchy reasoning and it uses Monte Carlo reinforcement learning in order to obtain a near-optimal policy towards a specific goal. Moreover, the game proposed is capable of solving different situations using a single policy. The system has been successfully tested and compared with previous techniques using a realistic hybrid simulator, where the ego-vehicle and its sensors are simulated on a 3D simulator and the additional vehicles' behavior is obtained from a traffic simulator.	Proceedings Paper	2020		[]	[]	-1	-1	-1
C	Wang, Zhitao; Zhuang, Yuzheng; Gu, Qiang; Chen, Dong; Zhang, Hongbo; Liu, Wulong	Reinforcement Learning based Negotiation-aware Motion Planning of Autonomous Vehicles	2021	2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	4532	4537	For autonomous vehicles integrating onto road-ways with human traffic participants, it requires understanding and adapting to the participants' intention by responding in predictable ways. This paper proposes a reinforcement learning based negotiation-aware motion planning framework, which adopts RL to adjust the driving style of the planner by dynamically modifying the prediction horizon length of the motion planner in real time adaptively. The framework models the interaction between the autonomous vehicle and other traffic participants as a Markov Decision Process. A temporal sequence of occupancy grid maps are taken as inputs for RL module to embed an implicit intention reasoning. Curriculum learning is employed to enhance the training efficiency and the robustness of the algorithm. We applied our method to narrow lane navigation in both simulation and real world to demonstrate that the proposed method outperforms the common alternative due to its advantage in alleviating the social dilemma problem with proper negotiation skills.	Proceedings Paper	2021	10.1109/IROS51168.2021.9635935	[]	[]	-1	-1	-1
C	Zheng, Rui; Liu, Chunming; Guo, Qi	A DECISION-MAKING METHOD FOR AUTONOMOUS VEHICLES BASED ON SIMULATION AND REINFORCEMENT LEARNING	2013	PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS (ICMLC), VOLS 1-4	International Conference on Machine Learning and Cybernetics	362	369	There are still some problems need to be solved though there are a lot of achievements in the field of automatic driving. One of those problems is the difficulty of designing a decision-making system for complex traffic conditions. In recent years, reinforcement learning (RL) shows the potential in solving sequential decision optimization problems, which can be modeled as Markov decision processes (MDPs). In this paper, we establish a 14-DOF dynamic model of an autonomous vehicle and use RL to build a decision-making system for autonomous driving based on simulation. The decision-making process of the vehicle is modeled as an MDP, and the performance of the MDP is improved using an approximate RL. At last, we show the efficiency of the proposed method by simulation in a highway environment.	Proceedings Paper	2013		[]	[]	-1	-1	-1
J	Papini, Gastone Pietro Rosati; Plebe, Alice; Da Lio, Mauro; Dona, Riccardo	A Reinforcement Learning Approach for Enacting Cautious Behaviours in Autonomous Driving System: Safe Speed Choice in the Interaction With Distracted Pedestrians	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		8805	8822	Driving requires the ability to handle unpredictable situations. Since it is not always possible to predict an impending danger, a good driver should preventively assess whether a situation has risks and adopt a safe behavior. Considering, in particular, the possibility of a pedestrian suddenly crossing the road, a prudent driver should limit the traveling speed. We present a work exploiting reinforcement learning to learn a function that specifies the safe speed limit for a given artificial driver agent. The safe speed function acts as a behavioral directive for the agent, thus extending its cognitive abilities. We consider scenarios where the vehicle interacts with a distracted pedestrian that might cross the road in hard-to-predict ways and propose a neural network mapping the pedestrian's context onto the appropriate traveling speed so that the autonomous vehicle can successfully perform emergency braking maneuvers. We discuss the advantages of developing a specialized neural network extension on top of an already functioning autonomous driving system, removing the burden of learning to drive from scratch while focusing on learning safe behavior at a highlevel. We demonstrate how the safe speed function can be learned in simulation and then transferred into a real vehicle. We include a statistical analysis of the network's improvements compared to the original autonomous driving system. The code implementing the presented network is available at https://githuh.com/tonegas/safe-speed-neural-network with MIT license and at https://zenodo.org/communities/dreams4cars.	Article; Early Access		10.1109/TITS.2021.3086397	[]	[]	-1	-1	-1
C	Zhang, Yi; Sun, Ping; Yin, Yuhan; Lin, Lin; Wang, Xuesong	Human-like Autonomous Vehicle Speed Control by Deep Reinforcement Learning with Double Q-Learning	2018	2018 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	1251	1256	Autonomous driving has become a popular research project. How to control vehicle speed is a core problem in autonomous driving. Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to control the vehicle speed. However, the popular Q-learning algorithm is unstable in some games in the Atari 2600 domain. In this paper, a reinforcement learning approach called Double Q-learning is used to control a vehicle's speed based on the environment constructed by naturalistic driving data. Depending on the concept of the direct perception approach, we propose a new method called integrated perception approach to construct the environment. The input of the model is made up of high dimensional data including road information processed from the video data and the low dimensional data processed from the sensors. During experiment, compared with deep Q-learning algorithm, double deep Q-learning has improvements both in terms of value accuracy and policy quality. Our model's score is 271.73% times that of deep Q-learning.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Huang, Chuan; Hu, Ping; Lian, Jing	Online optimum velocity calculation under V2X for smart new energy vehicles	2021	TRANSACTIONS OF THE INSTITUTE OF MEASUREMENT AND CONTROL		2368	2377	In this paper, a vector data net solver is proposed, which can reduce bivariate discrete-time dynamic programming (DP) computation time by 98.0% without losing accuracy. Therefore, for the first time, bivariate discrete-time DP can operate under model predictive control rolling optimization to calculate future optimum vehicle velocity in real time considering future road altitude and instant traffic information. Simulation results indicate that with the solution presented in this paper, front vehicles and the proper windows to pass through front intersections can be constantly considered. Meanwhile, the calculated optimum vehicle velocity almost remains the same as the global optimum solutions. Simulation results are validated by real-car tests, and the test new energy vehicle (NEV) electricity consumption is reduced by up to 48.6%. A comparison experiment is performed between the solution presented in this paper and commonly used adaptive dynamic programming (ADP), and the results indicate that the former has better performance and stability. This paper describes a novel solution for online optimum velocity calculation under vehicle to everything (V2X) environment and can be used by all smart NEVs with autonomous driving or active cruise control functions for lower electricity consumption and better riding comfort.	Article	JUN 2021	10.1177/0142331221997280	[]	[]	-1	-1	-1
J	Hartmann, Gabriel; Shiller, Zvi; Azaria, Amos	Model-Based Reinforcement Learning for Time-Optimal Velocity Control	2020	IEEE ROBOTICS AND AUTOMATION LETTERS		6185	6192	Autonomous navigation has recently gained great interest in the field of reinforcement learning. However, little attention was given to the time-optimal velocity control problem, i.e. controlling a vehicle such that it travels at the maximal speed without becoming dynamically unstable (roll-over or sliding). Time optimal velocity control can be solved numerically using existing methods that are based on optimal control and vehicle dynamics. In this letter, we develop a model-based deep reinforcement learning to generate the time-optimal velocity control. Moreover, we introduce a method that uses a numerical solution that predicts whether the vehicle may become unstable and intervenes if needed. We show that our combined model outperforms several baselines as it achieves higher velocities (with only one minute of training) and does not encounter any failures during the training process.	Article	OCT 2020	10.1109/LRA.2020.3012128	[]	[]	-1	-1	-1
J	Alagumuthukrishnan, S.; Deepajothi, S.; Vani, R.; Velliangiri, S.	Reliable and Efficient Lane Changing Behaviour for Connected Autonomous Vehicle through Deep Reinforcement Learning	2023	Procedia Computer Science		1112	21	The establishment of future intelligent transport systems is dependable on the reliable and seamless function of Connected and Autonomous Vehicles (CAV). Reinforcement learning (RL), which allows autonomous vehicles (AVs) to learn an ideal driving strategy through constant contact with the environment, plays a significant part in the decision-making process of autonomous driving (AD). The networking of CAV is advantageous since it allows for the transmission of traffic-related data to vehicles via Vehicle-to-External (V2X) communication. Recognition and anticipation of driving behaviour are critical for avoiding collisions because they can provide useful information to other drivers and vehicles. The fundamental challenge in developing CAV is the construction of an autonomous controller that can effectively perform close real-time control selections, such as a fast acceleration while merging onto a highway and rapid speed adjustments in stop-and-go traffic congestion. CAV driving behaviours can be considerably improved by utilizing shared information, resulting in more accountable, intelligent, and efficient driving. In the present work, a deep reinforcement learning approach is proposed that integrates the information gathered through connectivity capabilities and sensing from neighbour automobiles in the vicinity of CAV. The fused information is used for providing safe and cooperative lane-changing behaviour. The deployment of an algorithm in CAV is expected to improve the transportation safety of CAV driving behaviours. All rights reserved Elsevier.	Journal Paper	2023	10.1016/j.procs.2023.01.090	[]	[]	-1	-1	-1
J	Li, Duowei; Zhu, Feng; Chen, Tianyi; Wong, Yiik Diew; Zhu, Chunli; Wu, Jianping	COOR-PLT: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning	2023	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Platooning and coordination are two implementation strategies that are frequently proposed for traffic control of connected and autonomous vehicles (CAVs) at signal-free intersections instead of using conventional traffic signals. However, few studies have attempted to integrate both strategies to better facilitate the CAV control at signal-free intersections. To this end, this study proposes a hierarchical control model, named COOR-PLT, to coordinate adaptive CAV platoons at a signal-free intersection based on deep reinforcement learning (DRL). COOR-PLT has a two-layer framework. The first layer uses a centralized control strategy to form adaptive platoons. The optimal size of each platoon is determined by considering multiple objectives (i.e., efficiency, fairness and energy saving). The second layer employs a decentralized control strategy to coordinate multiple platoons passing through the intersection. Each platoon is labeled with coordinated status or independent status, upon which its passing priority is determined. As an efficient DRL algorithm, Deep Q-network (DQN) is adopted to determine platoon sizes and passing priorities respectively in the two layers. The model is validated and examined on the simulator Simulation of Urban Mobility (SUMO). The simulation results demonstrate that the model is able to: (1) achieve satisfactory convergence performances; (2) adaptively determine platoon size in response to varying traffic conditions; and (3) completely avoid deadlocks at the intersection. By comparison with other control methods, the model manifests its superiority of adopting adaptive platooning and DRL-based coordination strategies. Also, the model outperforms several state-of-the-art methods on reducing travel time and fuel consumption in different traffic conditions.	Article; Early Access		10.1016/j.trc.2022.103933	[]	[]	-1	-1	-1
J	Jung, Chanyoung; Shim, David Hyunchul	Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning	2021	IEEE ROBOTICS AND AUTOMATION LETTERS		1662	1669	Autonomous driving in an urban environment with surrounding agents remains challenging. One of the key challenges is to accurately predict the traversability map that probabilistically represents future trajectories considering multiple contexts: inertial, environmental, and social. To address this, various approaches have been proposed; however, they mainly focus on considering the individual context. In addition, most studies utilize expensive prior information (such as HD maps) of the driving environment, which is not a scalable approach. In this study, we extend a deep inverse reinforcement learning-based approach that can predict the traversability map while incorporating multiple contexts for autonomous driving in a dynamic environment. Instead of using expensive prior information of the driving scene, we propose a novel deep neural network to extract contextual cues from sensing data and effectively incorporate them in the output, i.e., the reward map. Based on the reward map, our method predicts the ego-centric traversability map that represents the probability distribution of the plausible and socially acceptable future trajectories. The proposed method is qualitatively and quantitatively evaluated in real-world traffic scenarios with various baselines. The experimental results show that our method improves the prediction accuracy compared to other baseline methods and can predict future trajectories similar to those followed by a human driver.	Article	APR 2021	10.1109/LRA.2021.3059628	[]	[]	-1	-1	-1
J	Young Joun Ha, P.; Sikai Chen; Jiqian Dong; Runjia Du; Yujie Li; Labi, S.	Leveraging the Capabilities of Connected and Autonomous Vehicles and Multi-Agent Reinforcement Learning to Mitigate Highway Bottleneck Congestion [arXiv]	2020	arXiv		22 pp.	22 pp.	Active Traffic Management strategies are often adopted in real-time to address such sudden flow breakdowns. When queuing is imminent, Speed Harmonization (SH), which adjusts speeds in upstream traffic to mitigate traffic showckwaves downstream, can be applied. However, because SH depends on driver awareness and compliance, it may not always be effective in mitigating congestion. The use of multiagent reinforcement learning for collaborative learning, is a promising solution to this challenge. By incorporating this technique in the control algorithms of connected and autonomous vehicle (CAV), it may be possible to train the CAVs to make joint decisions that can mitigate highway bottleneck congestion without human driver compliance to altered speed limits. In this regard, we present an RL-based multi-agent CAV control model to operate in mixed traffic (both CAVs and human-driven vehicles (HDVs)). The results suggest that even at CAV percent share of corridor traffic as low as 10%, CAVs can significantly mitigate bottlenecks in highway traffic. Another objective was to assess the efficacy of the RL-based controller vis-\`a-vis that of the rule-based controller. In addressing this objective, we duly recognize that one of the main challenges of RL-based CAV controllers is the variety and complexity of inputs that exist in the real world, such as the information provided to the CAV by other connected entities and sensed information. These translate as dynamic length inputs which are difficult to process and learn from. For this reason, we propose the use of Graphical Convolution Networks (GCN), a specific RL technique, to preserve information network topology and corresponding dynamic length inputs. We then use this, combined with Deep Deterministic Policy Gradient (DDPG), to carry out multi-agent training for congestion mitigation using the CAV controllers.	Journal Paper	11 Oct. 2020		[]	[]	-1	-1	-1
B	Wei-Lun Chen; Kwan-Hung Lee; Pao-Ann Hsiung	Intersection crossing for autonomous vehicles based on deep reinforcement learning	2019	2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)		2 pp.	2 pp.	Future intersection crossings for autonomous vehicles will not be controlled by traffic signals, rather a controller will be used for communication among vehicles that need to cross an intersection. In this work, we propose an innovative management system called Deep Reinforcement Learning-based Autonomous Intersection Management (DRLAIM) system, which is the first system to use deep reinforcement learning. We train the system to learn a good intersection control policy by interacting with traffic environment through reinforcement learning. The brake-safe control model is used to ensure the safety of each autonomous vehicle while crossing. Experiment results show that after training using reinforcement learning, the throughput of intersection control model increased by 83%. In comparison with the Fast First Service (FFS) policy, the average waiting time of DRLAIM reduced by about 1.2% to 11.4%.	Conference Paper	2019	10.1109/ICCE-TW46550.2019.8991738	[]	[]	-1	-1	-1
J	Peng, Bile; Keskin, Musa Furkan; Kulcsar, Balazs; Wymeersch, Henk	Connected autonomous vehicles for improving mixed traffic efficiency in unsignalized intersections with deep reinforcement learning	2021	COMMUNICATIONS IN TRANSPORTATION RESEARCH				Human driven vehicles (HDVs) with selfish objectives cause low traffic efficiency in an un-signalized intersection. On the other hand, autonomous vehicles can overcome this inefficiency through perfect coordination. In this paper, we propose an intermediate solution, where we use vehicular communication and a small number of autonomous vehicles to improve the transportation system efficiency in such intersections. In our solution, two connected autonomous vehicles (CAVs) lead multiple HDVs in a double-lane intersection in order to avoid congestion in front of the intersection. The CAVs are able to communicate and coordinate their behavior, which is controlled by a deep reinforcement learning (DRL) agent. We design an altruistic reward function which enables CAVs to adjust their velocities flexibly in order to avoid queuing in front of the intersection. The proximal policy optimization (PPO) algorithm is applied to train the policy and the generalized advantage estimation (GAE) is used to estimate state values. Training results show that two CAVs are able to achieve significantly better traffic efficiency compared to similar scenarios without and with one altruistic autonomous vehicle.	Article	DEC 2021	10.1016/j.commtr.2021.100017	[]	[]	-1	-1	-1
J	Cho, Youngwan; Lee, Jongseok; Lee, Kwangyup	CNN based Reinforcement Learning for Driving Behavior of Simulated Self-Driving Car	2020			1740	1749	This paper proposes a self-learning method for autonomous vehicle driving behavior using reinforcement learning without considering the dynamic model of the vehicle. In order to make decision needed for determine the optimal driving behavior (steering, throttle, brake) to achieve a given driving purpose in each state by using state information of the vehicle, such as vehicle movement speed, direction, degree of deviation from the center of the track, and distance to the edge of the track, we propose a method of applying the reinforcement learning by the DDPG structure and further using the driving image to improve driving performance. In this paper, we propose structures of an action decision network(Actor) and an action value evaluation network(Critic) to implement the DDPG learning model. We also propose a prediction model for predict the next state driving image based on the current driving image to improve driving performance in the corner path and a corner classifier for classifying the driving track type. The method proposed in this paper was implemented in a TORCS simulator environment, and the performance of the target driving behavior was evaluated through applying the learning model to driving agent.	research-article	2020		[]	[]	-1	-1	-1
B	Li, Qianwen	Trajectory Optimization for Connected and Autonomous Vehicle Platooning and Split Operations: Modeling and Experiments	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
C	KrÃ¶del, N; Kuhnert, KD	Pattern matching as the nucleus for either autonomous driving or driver assistance systems	2002	IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS		135	140	Researchers in the area of autonomous vehicle driving are mainly focused on two different directions: neural networks (e.g. [ 1], [2], [3], [4]) or explicit modelling (e.g. [5],[6],[7]). This research starts a new direction called Driving by Pattern Matching combined with a Reinforcement Learning algorithm.In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicles respectively obstacles on the road but with the strict requirement to self-improve driving behaviour based on delayed, self-created rewards or punishments.A single connected camera is being used as the basis to quickly (real-time-requirement) build an abstract complete situation description (ACSD) of the situation the vehicle is currently in. Such conversion process is a reduction process and in addition to the requirement of the reduction to the relevant value-containing information, the calculation of such ACSD's has to be very quick. Therefore, this part combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an Approximate Nearest Neighbour Pattern Matching Algorithm with a O(n log m) characteristic compared to O(n a m) for the conventional nearest neighbour calculation.In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.	Proceedings Paper	2002		[]	[]	-1	-1	-1
C	Ko, Eisaku; Chen, Kwang-Cheng	Wireless Communications Meets Artificial Intelligence: An Illustration by Autonomous Vehicles on Manhattan Streets	2018	2018 IEEE GLOBAL COMMUNICATIONS CONFERENCE (GLOBECOM)	IEEE Global Communications Conference			Interactions of multiple smart agents serve a fundamental aspect of Internet of Things (IoT), or known as social IoT. Such smart agents are equipped with sophisticated machine learning for mobile operation, such as autonomous vehicles and robots. Although wireless networking is intuitively important in such scenarios, there lacks investigations to provide a holistic and in-depth understanding on wireless networked multi-agent systems. In this paper, we disruptively use reinforcement learning to model each agent of artificial intelligence, and explore the interplay between wireless communications and multi-agent systems. Autonomous vehicles navigating over Manhattan streets serve the illustrating system. The first new finding is the need to modify reinforcement learning of policy exchange due to getting information from other agents through wireless communication. The advantage of applying wireless communication is clearly observed. We also demonstrate the impacts of communication errors to result in penalty in system performance. Multi-agent systems equipped with direct vehicleto-vehicle communication and vehicle-to-infrastructure communication are compared to initially conclude favorable using infrastructure of small cells. Finally, we explore multiple access communication over multi-agent systems by employing realtime ALOHA. Different from traditional thinking on reliable delivery of packets using re-transmit after collisions, real-time ALOHA discards re-transmission mechanism to ensure in-time contributions from wireless communication on the learning algorithm of a multi-agent system with satisfactory performance.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Ye, Qiming; Feng, Yuxiang; Candela, Eduardo; Escribano Macias, Jose; Stettler, Marc; Angeloudis, Panagiotis	Spatial-Temporal Flows-Adaptive Street Layout Control Using Reinforcement Learning	2022	SUSTAINABILITY				Complete streets scheme makes seminal contributions to securing the basic public right-of-way (ROW), improving road safety, and maintaining high traffic efficiency for all modes of commute. However, such a popular street design paradigm also faces endogenous pressures like the appeal to a more balanced ROW for non-vehicular users. In addition, the deployment of Autonomous Vehicle (AV) mobility is likely to challenge the conventional use of the street space as well as this scheme. Previous studies have invented automated control techniques for specific road management issues, such as traffic light control and lane management. Whereas models and algorithms that dynamically calibrate the ROW of road space corresponding to travel demands and place-making requirements still represent a research gap. This study proposes a novel optimal control method that decides the ROW of road space assigned to driveways and sidewalks in real-time. To solve this optimal control task, a reinforcement learning method is introduced that employs a microscopic traffic simulator, namely SUMO, as its environment. The model was trained for 150 episodes using a four-legged intersection and joint AVs-pedestrian travel demands of a day. Results evidenced the effectiveness of the model in both symmetric and asymmetric road settings. After being trained by 150 episodes, our proposed model significantly increased its comprehensive reward of both pedestrians and vehicular traffic efficiency and sidewalk ratio by 10.39%. Decisions on the balanced ROW are optimised as 90.16% of the edges decrease the driveways supply and raise sidewalk shares by approximately 9%. Moreover, during 18.22% of the tested time slots, a lane-width equivalent space is shifted from driveways to sidewalks, minimising the travel costs for both an AV fleet and pedestrians. Our study primarily contributes to the modelling architecture and algorithms concerning centralised and real-time ROW management. Prospective applications out of this method are likely to facilitate AV mobility-oriented road management and pedestrian-friendly street space design in the near future.	Article	JAN 2022	10.3390/su14010107	[]	[]	-1	-1	-1
C	Leal Cota, Jamir; Tavares Rodriguez, Jose A.; Garcia Alonso, Brandon; Vazquez Hurtado, Carlos	Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle	2022	PROCEEDINGS OF THE 2022 IEEE GLOBAL ENGINEERING EDUCATION CONFERENCE (EDUCON 2022)	IEEE Global Engineering Education Conference	1355	1364	Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.	Proceedings Paper	2022	10.1109/EDUCON52537.2022.9766659	[]	[]	-1	-1	-1
J	Karalakou, Athanasia; Troullinos, Dimitrios; Chalkiadakis, Georgios; Papageorgiou, Markos	Deep Reinforcement Learning Reward Function Design for Autonomous Driving in Lane-Free Traffic	2023	SYSTEMS				Lane-free traffic is a novel research domain, in which vehicles no longer adhere to the notion of lanes, and consider the whole lateral space within the road boundaries. This constitutes an entirely different problem domain for autonomous driving compared to lane-based traffic, as there is no leader vehicle or lane-changing operation. Therefore, the observations of the vehicles need to properly accommodate the lane-free environment without carrying over bias from lane-based approaches. The recent successes of deep reinforcement learning (DRL) for lane-based approaches, along with emerging work for lane-free traffic environments, render DRL for lane-free traffic an interesting endeavor to investigate. In this paper, we provide an extensive look at the DRL formulation, focusing on the reward function of a lane-free autonomous driving agent. Our main interest is designing an effective reward function, as the reward model is crucial in determining the overall efficiency of the resulting policy. Specifically, we construct different components of reward functions tied to the environment at various levels of information. Then, we combine and collate the aforementioned components, and focus on attaining a reward function that results in a policy that manages to both reduce the collisions among vehicles and address their requirement of maintaining a desired speed. Additionally, we employ two popular DRL algorithms-namely, deep Q-networks (enhanced with some commonly used extensions), and deep deterministic policy gradient (DDPG), which results in better policies. Our experiments provide a thorough investigative study on the effectiveness of different combinations among the various reward components we propose, and confirm that our DRL-employing autonomous vehicle is able to gradually learn effective policies in environments with varying levels of difficulty, especially when all of the proposed rewards components are properly combined.	Article	MAR 2023	10.3390/systems11030134	[]	[]	-1	-1	-1
J	Kim, Chan; Cho, Jae-Kyung; Yoon, Hyung-Suk; Seo, Seung-Woo; Kim, Seong-Woo	UNICON: Uncertainty-Conditioned Policy for Robust Behavior in Unfamiliar Scenarios	2022	IEEE ROBOTICS AND AUTOMATION LETTERS		9099	9106	"Deep reinforcement learning has been used to solve complex tasks in various fields, particularly in robotics control. However, agents trained using deep reinforcement learning have a problem of taking overconfident actions, even when the input state is far from the learned state distribution. This restricts deep reinforcement learning from being applied to real-world environments as overconfident actions in unlearned situations can result in catastrophic events; such as the collision of an autonomous vehicle. To address this, the agents should know ""what they do not know"" and choose an action by considering not only the state but also its uncertainty. In this study, we propose a novel uncertainty-conditioned policy (UNICON) inspired by the human behavior of changing policies according to uncertainty, e.g., slowing a car on a narrow road that has never been visited before. Our experimental results demonstrate that the proposed method is robust to unfamiliar scenarios that are not seen during training."	Article	OCT 2022	10.1109/LRA.2022.3189447	[]	[]	-1	-1	-1
C	Wu, Yang; Zhang, Zhiyong; Yuan, Jianhua; Ma, Qing; Gao, Lifen	Sequential game solution for lane-merging conflict between autonomous vehicles <i>A multi-agent reinforcement learning approach</i>	2016	2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		1482	1488	"Lane-merging conflict between Autonomous Vehicles (AV) calls for coordinated solution to allocate right-of-way. Related studies resort to centralized decision-making optimization models such as ""right-of-way reservation/auction"", which are suitable only for the scenarios with a centralized intersection agent (acting as arbiter or auctioneer); and involved fiat currency spent on bidding may trigger controversial issues concerning law or taxation. This paper: (i) establishes a prototype of 2-player complete-information 3-stage sequential game architecture within distributed decision-making paradigm, to formalize the interactions between 2 AVs trapped in the lane-merging conflict, so as to suit for scenarios with or without a centralized decision-maker, i.e. intersection or road segment; (ii) designs the dynamic rewards of AV's game-playing (velocity-adjusting) actions which result from the space-time status of AV; (iii) based on the proposed rewards, uses Multi-agent Reinforcement Learning to obtain the optimal (in Nash equilibrium sense) strategies of action-sequence for both AVs after 3-stage game-theoretic negotiations, promisingly avoiding the potential right-of-way deadlock in a lane-merging conflict."	Proceedings Paper	2016		[]	[]	-1	-1	-1
J	Hu, Hui; Wang, Yuge; Tong, Wenjie; Zhao, Jiao; Gu, Yulei	Path Planning for Autonomous Vehicles in Unknown Dynamic Environment Based on Deep Reinforcement Learning	2023	APPLIED SCIENCES-BASEL				Autonomous vehicles can reduce labor power during cargo transportation, and then improve transportation efficiency, for example, the automated guided vehicle (AGV) in the warehouse can improve the operation efficiency. To overcome the limitations of traditional path planning algorithms in unknown environments, such as reliance on high-precision maps, lack of generalization ability, and obstacle avoidance capability, this study focuses on investigating the Deep Q-Network and its derivative algorithm to enhance network and algorithm structures. A new algorithm named APF-D3QNPER is proposed, which combines the action output method of artificial potential field (APF) with the Dueling Double Deep Q Network algorithm, and experience sample rewards are considered in the experience playback portion of the traditional Deep Reinforcement Learning (DRL) algorithm, which enhances the convergence ability of the traditional DRL algorithm. A long short-term memory (LSTM) network is added to the state feature extraction network part to improve its adaptability in unknown environments and enhance its spatiotemporal sensitivity to the environment. The APF-D3QNPER algorithm is compared with mainstream deep reinforcement learning algorithms and traditional path planning algorithms using a robot operating system and the Gazebo simulation platform by conducting experiments. The results demonstrate that the APF-D3QNPER algorithm exhibits excellent generalization abilities in the simulation environment, and the convergence speed, the loss value, the path planning time, and the path planning length of the APF-D3QNPER algorithm are all less than for other algorithms in diverse scenarios.	Article	SEP 2023	10.3390/app131810056	[]	[]	-1	-1	-1
B	Koren, M.; Kochenderfer, M.J.	Adaptive Stress Testing without Domain Heuristics using Go-Explore	2020	2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)		6 pp.	6 pp.	Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domains-pecific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more likely actions, in order to to find more likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.	Conference Paper	2020	10.1109/ITSC45102.2020.9294729	[]	[]	-1	-1	-1
C	Salvi, Ameya; Coleman, John; Buzhardt, Jake; Krovi, Venkat; Tallapragada, Phanindra	Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning	2022	IFAC PAPERSONLINE		276	281	Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort. The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle. The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled. The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios. Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance. This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action. In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity. Copyright (c) 2022 The Authors. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)	Proceedings Paper	2022	10.1016/jifacol.2022.11.197	[]	[]	-1	-1	-1
J	ElSamadisy, Omar; Shi, Tianyu; Smirnov, Ilia; Abdulhai, Baher	Safe, Efficient, and Comfortable Reinforcement-Learning-Based Car-Following for AVs with an Analytic Safety Guarantee and Dynamic Target Speed	2024	TRANSPORTATION RESEARCH RECORD		643	661	Over the last decade, there has been rising interest in automated driving systems and adaptive cruise control (ACC). Controllers based on reinforcement learning (RL) are particularly promising for autonomous driving, being able to optimize a combination of criteria such as efficiency, stability, and comfort. However, RL-based controllers typically offer no safety guarantees. In this paper, we propose SECRM (the Safe, Efficient, and Comfortable RL-based car-following Model) for autonomous car-following that balances traffic efficiency maximization and jerk minimization, subject to a hard analytic safety constraint on acceleration. The acceleration constraint is derived from the criterion that the follower vehicle must have sufficient headway to be able to avoid a crash if the leader vehicle brakes suddenly. We critique safety criteria based on the time-to-collision (TTC) threshold (commonly used for RL controllers), and confirm in simulator experiments that a representative previous TTC-threshold-based RL autonomous-vehicle controller may crash (in both training and testing). In contrast, we verify that our controller SECRM is safe, in training scenarios with a wide range of leader behaviors, and in both regular-driving and emergency-braking test scenarios. We find that SECRM compares favorably in efficiency, comfort, and speed-following to both classical (non-learned) car-following controllers (intelligent driver model, Shladover, Gipps) and a representative RL-based car-following controller.	Article; Early Access		10.1177/03611981231171899	[]	[]	-1	-1	-1
J	Liu, Qi; Li, Xueyuan; Tang, Yujie; Gao, Xin; Yang, Fan; Li, Zirui	Graph Reinforcement Learning-Based Decision-Making Technology for Connected and Autonomous Vehicles: Framework, Review, and Future Trends	2023	SENSORS				The proper functioning of connected and autonomous vehicles (CAVs) is crucial for thesafety and efficiency of future intelligent transport systems. Meanwhile, transitioning to fully autonomousdriving requires a long period of mixed autonomy traffic, including both CAVs andhuman-driven vehicles. Thus, collaborative decision-making technology for CAVs is essential togenerate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomytraffic. In recent years, deep reinforcement learning (DRL) methods have become an efficient way insolving decision-making problems. However, with the development of computing technology, graphreinforcement learning (GRL) methods have gradually demonstrated the large potential to furtherimprove the decision-making performance of CAVs, especially in the area of accurately representingthe mutual effects of vehicles and modeling dynamic traffic environments. To facilitate the developmentof GRL-based methods for autonomous driving, this paper proposes a review of GRL-basedmethods for the decision-making technologies of CAVs. Firstly, a generic GRL framework is proposedin the beginning to gain an overall understanding of the decision-making technology. Then, theGRL-based decision-making technologies are reviewed from the perspective of the constructionmethods of mixed autonomy traffic, methods for graph representation of the driving environment,and related works about graph neural networks (GNN) and DRL in the field of decision-makingfor autonomous driving. Moreover, validation methods are summarized to provide an efficientway to verify the performance of decision-making methods. Finally, challenges and future researchdirections of GRL-based decision-making methods are summarized.	Review	OCT 2023	10.3390/s23198229	[]	[]	-1	-1	-1
J	Qiang, Yuchuan; Wang, Xiaolan; Liu, Xintian; Wang, Yansong; Zhang, Weiwei	Edge-enhanced Graph Attention Network for driving decision-making of autonomous vehicles via Deep Reinforcement Learning	2024	PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING				Despite the rapid advancement in the field of autonomous driving vehicles, developing a safe and sensible decision-making system remains a challenging problem. The driving decision-making module is one of the most essential sections of the entire autonomous driving system, and the decision generated from it can significantly impinge the lives and property of passengers. Complicated interactions among traffic participants have the most profound impact on the decision-making process, yet the interactions are often simplified or overlooked due to their complexity and implicit nature. To address this issue, this work proposes an Edge-Enhanced Graph Attention Reinforcement Learning (EGARL) framework that aims to make rational driving decisions by comprehensively modeling the interactions among agents. EGARL comprises three core components: a graphical representation of the traffic scenario that covers both topological and interactive information; an Edge-enhanced Graph Attention Network (E-GAT) that utilizes the graphical representation to extract interactive features by comprehensively considering nodes and edges of the graph; and a deep reinforcement learning method that generates driving decisions based on the current state and features extracted from E-GAT. Experimental results demonstrate the satisfying performance of EGARL. Our proposed framework can contribute to the development of intelligent transportation systems, enhancing the safety and efficiency of driving.	Article; Early Access		10.1177/09544070231217762	[]	[]	-1	-1	-1
J	Chen Yue; Jiao Pengpeng; Bai Ruyu; Li Rujian	Modeling Car Following Behavior of Autonomous Driving Vehicles Based on Deep Reinforcement Learning	2023	Journal of Transport Information and Safety		67	75,102	In order to enhance the performance of car following behavior of autonomous vehicles and mitigate the negative effects of traffic oscillations, a deep reinforcement learning-based car following model for automated driving is investigated. The existing reward function is improved by incorporating energy consumption, and the related terms for representing energy consumption are established based on the VT-Micro model. In addition, the method of using the time gap between vehicles to establish the reward function related to driving efficiency is improved by adding virtual speed to the time gap, in order to avoid computation overflow and unrealistic short following distance in the traffic oscillation scenario. To overcome the limitations of training on closed-loop simulated roads and simulated vehicle trajectories, human driver behavior extracted from the NGSIM trajectory data during traffic oscillation are used to develop the training environment. By applying the twin delayed deep deterministic policy gradient algorithm (TD3), a multi-objective car following model is then developed. A system for evaluating model performance is established to compare the performance of the TD3 model with traditional models in car following and traffic oscillations scenarios. Study results of car following scenarios show that the TD3 model and the traditional adaptive cruise control (ACC) model perform similarly in terms of comfort and driving efficiency, but both outperform the human drivers. In terms of safety, the TD3 model reduces safety hazards by 53.65% compared to the traditional ACC model, and 36.24% compared to the human drivers. Regarding energy consumption, the TD3 model reduces the energy consumption of the conventional ACC model and human drivers by 6.73% and 15.65%, respectively. Study results show that the TD3 model can reduce the negative impacts of traffic oscillations. In the scenario with a 100% TD3 model penetration rate, driving discomfort decreases by 55.95%, driving efficiency increases by 8.82%, crash risks reduce by 73.21%, and fuel consumption drops by 5.97%, compared to a 100% human-driven environment.	Article	2023		[]	[]	-1	-1	-1
J	Ning Qiang; Liu Yuansheng; Xie Longyang	Application of SAC-Based Autonomous Vehicle Control Method	2023	Computer Engineering and Applications		306	14	In order to improve the problem of slow network convergence and unstable training process caused by equal probability sampling of SAC (soft actor critic) algorithm samples and random initialization of the network, an improved algorithm PE-SAC (priority playback soft actor) is proposed that combines priority playback and expert data. The algorithm classifies the sample pool according to the sample value, uses expert data to pre-train the network, reduces the invalid exploration space of unmanned vehicles, reduces the number of trials and errors, and effectively improves the learning efficiency of the algorithm. At the same time, a reward function for multiple obstacles is designed to enhance the applicability of the algorithm. Simulation experiments are carried out on the CARLA platform, and the results show that the proposed method can better control the safe driving of unmanned vehicles in the environment, and the reward value and convergence speed obtained under the same training times are better than TD3 (twin delayed deep deterministic policy gradient algorithm) and SAC algorithm. Finally, combined with the radar point cloud map and the PID (proportional integral derivative) control method, the difference between the simulation environment and the real scene is reduced, and the training model is transplanted to the low-speed unmanned vehicle in the park to verify the generality of the algorithm.	Journal Paper	2023	10.3778/j.issn.1002-8331.2112-0084	[]	[]	-1	-1	-1
J	Liang, Jing; Weerakoon, Kasun; Guan, Tianrui; Karapetyan, Nare; Manocha, Dinesh	AdaptiveON: Adaptive Outdoor Local Navigation Method for Stable and Reliable Actions	2023	IEEE ROBOTICS AND AUTOMATION LETTERS		648	655	We present a novel outdoor navigation algorithm to generate stable and efficient actions to navigate a robot to reach a goal. We use a multi-stage training pipeline and show that our approach produces policies that result in stable and reliable robot navigation on complex terrains. Based on the Proximal Policy Optimization (PPO) algorithm, we developed a novel method to achieve multiple capabilities for outdoor local navigation tasks, namely alleviating the robot's drifting, keeping the robot stable on bumpy terrains, avoiding climbing on hills with steep elevation changes, and avoiding collisions. Our training process mitigates the reality (sim-to-real) gap by introducing generalized environmental and robotic parameters and training with rich features captured from light detection and ranging (Lidar) sensor in a high-fidelity Unity simulator. We evaluate our method in both simulation and real-world environments using Clearpath Husky and Jackal robots. Further, we compare our method against the state-of-the-art approaches and observe that, in the real world, our method improves stability by at least 30.7% on uneven terrains, reduces drifting by 8.08%, and decreases the elevation changes by 14.75%.	Article	FEB 2023	10.1109/LRA.2022.3229907	[]	[]	-1	-1	-1
J	Han, Songyang; Zhou, Shanglin; Wang, Jiangwei; Pepin, Lynn; Ding, Caiwen; Fu, Jie; Miao, Fei	A Multi-Agent Reinforcement Learning Approach for Safe and Efficient Behavior Planning of Connected Autonomous Vehicles	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				The recent advancements in wireless technology enable connected autonomous vehicles (CAVs) to gather information about their environment by vehicle-to-vehicle (V2V) communication. In this work, we design an information-sharing based multi-agent reinforcement learning (MARL) framework for CAVs, to take advantage of the extra information when making decisions to improve traffic efficiency and safety. The safe actor-critic algorithm we propose has two new techniques: the truncated Q-function and safe action mapping. The truncated Q-function utilizes the shared information from neighboring CAVs such that the joint state and action spaces of the Q-function do not grow in our algorithm for a large-scale CAV system. We prove the bound of the approximation error between the truncated -Q and global Q-functions. The safe action mapping provides a provable safety guarantee for both the training and execution based on control barrier functions. Using the CARLA simulator for experiments, we show that our approach improves the CAV system's efficiency in terms of average velocity and comfort under different CAV ratios and different traffic densities. We also show that our approach avoids the execution of unsafe actions and always maintains a safe distance from other vehicles. We construct an obstacle-at-corner scenario to show that the shared vision can help CAVs to observe obstacles earlier and take action to avoid traffic jams. The experiment video is on https://songyanghan.github.io/cavmarl/.	Article; Early Access		10.1109/TITS.2023.3336670	[]	[]	-1	-1	-1
C	Manjanna, Sandeep; van Hoof, Herke; Dudek, Gregory	Reinforcement Learning with Non-uniform State Representations for Adaptive Search	2018	2018 IEEE INTERNATIONAL SYMPOSIUM ON SAFETY, SECURITY, AND RESCUE ROBOTICS (SSRR)	IEEE International Symposium on Safety Security and Rescue Robotics			Efficient spatial exploration is a key aspect of search and rescue. In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher. This should allow an autonomous vehicle find one or more lost targets as rapidly as possible. We do this by performing non-uniform sampling of the search region. The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning. We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target. Key features of our search algorithm are the ability to employ a very general nondeterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions. One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics. We compare the paths generated by our algorithm with other accepted spatial coverage techniques such as distribution independent boustrophedonic coverage and model dependent spiral search.We present a proof showing that rewarding for clearing probability mass instead of locating the target does not bias the objective function. The experiments show that the learned policy outperforms several well-known baselines even in scenarios different from the one it has been trained on.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Ye, Q.; Feng, Y.; Macias, J.J.E.; Stettler, M.; Angeloudis, P.	Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions using Reinforcement Learning [arXiv]	2023	arXiv				The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure. In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed. Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand. Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions. We implement a centralised paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations. Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians. Furthermore, the distributive learning algorithm outperforms its centralised counterpart regarding computational cost (49.55\%), benchmark rewards (25.35\%), best cumulative rewards (24.58\%), optimal actions (13.49\%) and rate of convergence. This novel road management technique could potentially contribute to the flow-adaptive and active mobility-friendly streets in the AVs era.	Journal Paper	21 March 2023		[]	[]	-1	-1	-1
J	Feng, Shuo; Haykin, Simon	Cognitive Risk Control for Anti-Jamming V2V Communications in Autonomous Vehicle Networks	2019	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		9920	9934	The future of intelligent transportation system (ITS) is expected to be composed of connected and autonomous vehicles (CAVs), the development of which will have great impact on peoples everyday life. Unfortunately, this progress will be accompanied by all kinds of potential threats and attacks rising in CAV network. As a legacy from traditional wireless networks, jamming attack is still one of the major and serious threats to vehicle-to-vehicle (V2V) communications. In this paper, we investigate the anti-jamming V2V communication in CAV networks through power control in conjunction with channel selection. Bringing into play a brain-inspired research tool called cognitive dynamic system (CDS), the general structure of cognitive risk control (CRC) is well-tailored to analyze and address the jamming problem. Specifically, power control is carried out first using reinforcement learning, the result of which is then examined by a module called task-switch control. Based on the risk assessment, a multi-armed bandit (MAB) problem is formulated to perform the channel-selection process when necessary. Through continuous perception-action cycles (PACs), the feature of predictive adaptation is realized for the legitimate vehicle in its behavioral interactions with the jammer. Simulation results have shown that the proposed method has desirable performance in terms of several evaluation metrics.	Article	OCT 2019	10.1109/TVT.2019.2935999	[]	[]	-1	-1	-1
J	Al Younes, Younes; Barczyk, Martin	Adaptive Nonlinear Model Predictive Horizon Using Deep Reinforcement Learning for Optimal Trajectory Planning	2022	DRONES				This paper presents an adaptive trajectory planning approach for nonlinear dynamical systems based on deep reinforcement learning (DRL). This methodology is applied to the authors' recently published optimization-based trajectory planning approach named nonlinear model predictive horizon (NMPH). The resulting design, which we call 'adaptive NMPH', generates optimal trajectories for an autonomous vehicle based on the system's states and its environment. This is done by tuning the NMPH's parameters online using two different actor-critic DRL-based algorithms, deep deterministic policy gradient (DDPG) and soft actor-critic (SAC). Both adaptive NMPH variants are trained and evaluated on an aerial drone inside a high-fidelity simulation environment. The results demonstrate the learning curves, sample complexity, and stability of the DRL-based adaptation scheme and show the superior performance of adaptive NMPH relative to our earlier designs.	Article	NOV 2022	10.3390/drones6110323	[]	[]	-1	-1	-1
J	Chen, Yuying; Liu, Congcong; Shi, Bertram E.; Liu, Ming	Robot Navigation in Crowds by Graph Convolutional Networks With Attention Learned From Human Gaze	2020	IEEE ROBOTICS AND AUTOMATION LETTERS		2754	2761	Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies. However, their performance deteriorates when the crowd size grows. We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation. We propose a novel network utilizing a graph representation to learn the policy. We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd as they perform a navigation task based on a top down view of the environment. We incorporate the learned attention into a graph-based reinforcement learning architecture. The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability. Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods, increasing task completion rate by 18.4% and decreasing navigation time by 16.4%.	Article	APR 2020	10.1109/LRA.2020.2972868	[]	[]	-1	-1	-1
J	Zhang Jian; Li Qingyang; Li Dan; Jiang Xia; Lei Yanhong; Ji Yaping	Merging guidance of exclusive lanes for connected and autonomous vehicles based on deep reinforcement learning	2023	Journal of Jilin University. Engineering and Technology Edition		2508	2518	Exclusive lanes for connected and autonomous vehicles(CAVs)will emerge in order to ensure the safety and efficiency requirements in the process of traffic flow mixed with human-driving vehicles and CAVs. When the inner lane of the expressway is set as the exclusive lane for CAVs, it has important theoretical significance and practical value to study the strategy of guiding CAVs to merge from the ordinary lane to the exclusive lane. Firstly, the entrance area of exclusive lane was designed and vehicle control rules were proposed. Secondly, with the goal of making more CAVs change lanes to the exclusive lane, the strategy of selecting lane-changing signal actions was proposed based on deep reinforcement learning. Finally, the numerical simulation was carried out with Python language compilation. The results show that the proposed algorithm can converge very quickly under 9 scenarios constructed by different factors, such as the CAV penetration rates and the proportion of CAVs arriving at the exclusive lane; it can effectively guide CAVs to merge into exclusive lanes and ensure traffic efficiency; congestion in the second lane can be significantly reduced compared to the unsignalized control when the penetration rate changes from 20% to 40%; the proportion of CAVs changing to the exclusive lane is significantly higher under the two exclusive lane entrances scenario than that under the one entrance scenario. It shows that the proposed strategy has good applicability and can provide reference for engineering construction.	Article	2023		[]	[]	-1	-1	-1
J	Koren, M.; Kochenderfer, M.J.	Adaptive stress testing without domain heuristics using go-explore [arXiv]	2020	arXiv		7 pp.	7 pp.	Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more-likely actions, because we want to find more-likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most-likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.	Journal Paper	8 April 2020		[]	[]	-1	-1	-1
J	Yoon, Hyung-Jin; Jafarnejadsani, Hamidreza; Voulgaris, Petros	Learning When to Use Adaptive Adversarial Image Perturbations Against Autonomous Vehicles	2023	IEEE ROBOTICS AND AUTOMATION LETTERS		4179	4186	Deep neural network (DNN) models are widely used in autonomous vehicles for object detection using camera images. However, these models are vulnerable to adversarial image perturbations. Existing methods for generating these perturbations use the image frame as the decision variable, resulting in a computationally expensive optimization process that starts over for each new image. Few approaches have been developed for attacking online image streams while considering the physical dynamics of autonomous vehicles, their mission, and the environment. To address these challenges, we propose a multi-level stochastic optimization framework that monitors the attacker's capability to generate adversarial perturbations. Our framework introduces a binary decision attack/not attack based on the attacker's capability level to enhance its effectiveness. We evaluate our proposed framework using simulations for vision-guided autonomous vehicles and actual tests with a small indoor drone in an office environment. Our results demonstrate that our method is capable of generating real-time image attacks while monitoring the attacker's proficiency given state estimates.	Article	JUL 2023	10.1109/LRA.2023.3280813	[]	[]	-1	-1	-1
B	Vantsevich, V.; Gorsich, D.; Lozynskyy, A.; Demkiv, L.; Borovets, T.; Klos, S.	Agile Tyre Mobility: Observation and Control in Severe Terrain Environments	2020	Advanced Technologies for Security Applications. Proceedings of the NATO Science for Peace and Security Cluster Workshop on Advanced Technologies'. NATO Science for Peace and Security Series B: Physics and Biophysics (NAPSB)		247	58	This research study develops fundamentals for a new ground vehicle technology to radically improve and protect off-road vehicle mobility by providing agile (fast, exact and pre-emptive) responses and advanced mobility controls in severe terrain conditions. The current framework of terrain vehicle mobility that estimates a vehicle capability to go through or not to go through the given terrain conditions cannot provide an analytical basis for novel system design solutions. Indeed, modern traction control and other mobility related electronic control systems possess control response time within the range of 100-120 milliseconds and greater. With this response time, the actual control occurs after the vehicle has reached a critical motion situation, e.g., a wheel(s) is/are spinning and the vehicle is already losing its mobility. In this study, the developed methods allowed for estimating tyre mobility and controlling tyre motion before the tyre starts spinning. As shown in the conducted analysis, the response time, which occurs within the longitudinal tyre relaxation time constant of 40-60 ms, is sufficient for a tyre to avoid spinning and to maintain its required mobility. Most common traditional approaches to observation of data supplied by virtual sensors were simulated and improved by means of machine learning algorithms. Computational simulations of an one-wheel-locomotion module driven by an electric driveline system demonstrated a sufficient performance of the proposed observation method to estimate mobility margins of the module in real time.A hybrid intelligent control algorithm was designed, in which reinforcement learning was used to fine-tune the parameters of a fuzzy logic controller. A new wheel mobility index was utilized as a cost function to guarantee a designed behavior of the locomotion module. A fuzzy corrector was additionally designed to take into account both the dynamic state of the system and the dynamics of the tyre-terrain interaction. The fuzzy corrector supports upper level controls of autonomous vehicle dynamics by decreasing tyre slippage on severe terrains.Computer simulations testified both stability of the controller (due to utilization of fuzzy logic polynomial control) and its desired performance (due to application of reinforcement learning). The fine-tuned controller requires minimal online computations.This paper provides an extended summary of the above-listed research studies. Further details can be found in publications referenced in the paper.	Conference Paper	2020	10.1007/978-94-024-2021-0_22	[]	[]	-1	-1	-1
J	Zhao, Ning; Wu, Hao; Yu, F. Richard; Wang, Lifu; Zhang, Weiting; Leung, Victor C. M.	Deep-Reinforcement-Learning-Based Latency Minimization in Edge Intelligence Over Vehicular Networks	2022	IEEE INTERNET OF THINGS JOURNAL		1300	1312	A novel paradigm that combines federated learning with blockchain to empower edge intelligence over vehicular networks (FBVN) can enable latency-sensitive deep neural network-based applications to be executed in a distributed pattern. However, the complex environments in FBVN make the system latency much harder to minimize by traditional methods. In this article, we model the training and transmission latency of each autonomous vehicle (AV) and consensus latency of the blockchain in-edge side in FBVN. Considering the dynamic and time-varying wireless channel conditions, unpredictable packet error rate, and unstable data sets quality, we adopt duel deep Q-learning (DDQL) as the solving approach. We propose a federated DDQL algorithm, in which the learning agent is deployed on each AV side, and the sensing states on each AV do not need to be shared so that it increases scalability and flexibility for practical implementation. Simulation results show that the proposed algorithm has better performance in reducing system latency compared with the other schemes.	Article	JAN 15 2022	10.1109/JIOT.2021.3078480	[]	[]	-1	-1	-1
J	Zhang, Ruiqi; Hou, Jing; Chen, Guang; Li, Zhijun; Chen, Jianxiao; Knoll, Alois	Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing	2022	IEEE ROBOTICS AND AUTOMATION LETTERS		11625	11632	Motion planning for autonomous racing is a challenging task due to the safety requirement while driving aggressively. Most previous solutions utilize the prior information or depend on complex dynamics modeling. Classical model-free reinforcement learning methods are based on random sampling, which severely increases the training consumption and undermines the exploration efficiency. In this letter, we propose an efficient residual policy learning method for high-speed autonomous racing named ResRace, which leverages only the real-time raw observation of LiDAR and IMU for low-latency obstacle avoiding and navigation. We first design a controller based on the modified artificial potential field (MAPF) to generate a policy for navigation. Besides, we utilize the deep reinforcement learning (DRL) algorithm to generate a residual policy as a supplement to obtain the optimal policy. Concurrently, the MAPF policy effectively guides the exploration and increases the update efficiency. This complementary property contributes to the fast convergence and few required resources of our method. We also provide extensive experiments to illustrate our method outperforms the leading algorithms and reaches the comparable level of professional human players on the five F1Tenth tracks.	Article	OCT 2022	10.1109/LRA.2022.3192770	[]	[]	-1	-1	-1
B	Franklin, D.M.; Martin, D.	eSense 2.0: modeling multi-agent biomimetic predation with multi-layered reinforcement learning	2020	Advances in Information and Communication. Proceedings of the 2019 Future of Information and Communication Conference (FICC). Lecture Notes in Networks and Systems (LNNS 70)		466	78	Learning in multi-agent systems, especially with adversarial behavior being exhibited, is difficult and challenging. The learning within these complicated environments is often muddied by the multitudinous conflicting or poorly correlated data coming from the multiple agents and their diverse goals. This should not be compared against well-known flocking-type behaviors where each agent has the same policy; rather, in our scenario each agent may have their own policy, sets of behaviors, or overall group strategy. Most learning algorithms will observe the actions of the agents and inform their algorithm which seeks to form the models. When these actions are consistent a reasonable model can be formed; however, eSense was designed to work even when observing complicated and highly-interactive must-agent behavior. eSense provides a powerful yet simplistic reinforcement learning algorithm that employs model-based behavior across multiple learning layers. These independent layers split the learning objectives across multiple layers, avoiding the learning-confusion common in many multi-agent systems. We examine a multi-agent predator-prey biomimetic sensing environment that simulates such coordinated and adversarial behaviors across multiple goals. This work could also be applied to theater wide autonomous vehicle coordination, such as that of the hierarchical command and control of autonomous drones and ground vehicles.	Conference Paper	2020	10.1007/978-3-030-12385-7_35	[]	[]	-1	-1	-1
J	He, Xiangkun; Chen, Hao; Lv, Chen	Robust Multiagent Reinforcement Learning toward Coordinated Decision-Making of Automated Vehicles	2023	SAE INTERNATIONAL JOURNAL OF VEHICLE DYNAMICS STABILITY AND NVH		475	488	Automated driving is essential for developing and deploying intelligent transportation systems. However, unavoidable sensor noises or perception errors may cause an automated vehicle to adopt suboptimal driving policies or even lead to catastrophic failures. Additionally, the automated driving longitudinal and lateral decision-making behaviors (e.g., driving speed and lane changing decisions) are coupled, that is, when one of them is perturbed by unknown external disturbances, it causes changes or even performance degradation in the other. The presence of both challenges significantly curtails the potential of automated driving. Here, to coordinate the longitudinal and lateral driving decisions of an automated vehicle while ensuring policy robustness against observational uncertain-ties, we propose a novel robust coordinated decision-making technique via robust multiagent reinforcement learning. Specifically, the automated driving longitudinal and lateral decisions under observational perturbations are modeled as a constrained robust multiagent Markov decision process. Meanwhile, a nonlinear constraint setting with Kullback-Leibler divergence is developed to keep variation of the driving policy perturbed by stochastic perturbations within bounds. Additionally, robust multiagent policy optimization approach is proposed to approximate the optimal robust coordinated driving policy. Finally, we evaluate the proposed robust coordinated decision-making method in three highway scenarios with different traffic densities. Quantitatively, in the absence noises, the proposed method achieves an approximate average enhancement of 25.58% in traffic efficiency and 91.31% in safety compared to all baselines across the three scenarios. In the presence of noises, our technique improves traffic efficiency and safety by an approximate average of 30.81% and 81.02% compared to all baselines in the three scenarios, respectively. The results demonstrate that the proposed approach is capable of improving automated driving performance and ensuring policy robustness against observational uncertainties.	Article	2023	10.4271/10-07-04-0031	[]	[]	-1	-1	-1
C	Gonzalez, David Sierra; Dibangoye, Jilles Steeve; Laugier, Christian	High-Speed Highway Scene Prediction Based on Driver Models Learned From Demonstrations	2016	2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)		149	155	One of the key factors to ensure the safe operation of autonomous and semi-autonomous vehicles in dynamic environments is the ability to accurately predict the motion of the dynamic obstacles in the scene. In this work, we show how to use a realistic driver model learned from demonstrations via Inverse Reinforcement Learning to predict the long-term evolution of highway traffic scenes. We model each traffic participant as a Markov Decision Process in which the cost function is a linear combination of static and dynamic features. In particular, the static features capture the preferences of the driver while the dynamic features, which change over time depending on the actions of the other traffic participants, capture the driver's risk-aversive behavior. Using such a model for prediction enables us to explicitly consider the interactions between traffic participants while keeping the computational complexity quadratic in the number of vehicles in the scene. Preliminary experiments in simulated and real scenarios show the capability of our approach to produce reliable, human-like scene predictions.	Proceedings Paper	2016		[]	[]	-1	-1	-1
J	Everett, M.; Lutjens, B.; How, J.P.	Certified Adversarial Robustness for Deep Reinforcement Learning [arXiv]	2020	arXiv		14 pp.	14 pp.	Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certified defense for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends our previous paper with new performance guarantees, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.	Journal Paper	11 April 2020		[]	[]	-1	-1	-1
J	Manjanna, S.; Jiahao, T.Z.; Hsieh, M.A.	Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal Fluid Processes [arXiv]	2023	arXiv				Persistent monitoring of a spatiotemporal fluid process requires data sampling and predictive modeling of the process being monitored. In this paper we present PASST algorithm: Predictive-model based Adaptive Sampling of a Spatio-Temporal process. PASST is an adaptive robotic sampling algorithm that leverages predictive models to efficiently and persistently monitor a fluid process in a given region of interest. Our algorithm makes use of the predictions from a learned prediction model to plan a path for an autonomous vehicle to adaptively and efficiently survey the region of interest. In turn, the sampled data is used to obtain better predictions by giving an updated initial state to the predictive model. For predictive model, we use Knowledged-based Neural Ordinary Differential Equations to train models of fluid processes. These models are orders of magnitude smaller in size and run much faster than fluid data obtained from direct numerical simulations of the partial differential equations that describe the fluid processes or other comparable computational fluids models. For path planning, we use reinforcement learning based planning algorithms that use the field predictions as reward functions. We evaluate our adaptive sampling path planning algorithm on both numerically simulated fluid data and real-world nowcast ocean flow data to show that we can sample the spatiotemporal field in the given region of interest for long time horizons. We also evaluate PASST algorithm's generalization ability to sample from fluid processes that are not in the training repertoire of the learned models.	Journal Paper	03 April 2023		[]	[]	-1	-1	-1
C	Yang, Tao; Nan, Zhixiong; Zhang, He; Chen, Shitao; Zheng, Nanning	Traffic Agent Trajectory Prediction Using Social Convolution and Attention Mechanism	2020	2020 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	278	283	The trajectory prediction is significant for the decision-making of autonomous driving vehicles. In this paper, we propose a model to predict the trajectories of target agents around an autonomous vehicle. The main idea of our method is considering the history trajectories of the target agent and the influence of surrounding agents on the target agent. To this end, we encode the target agent history trajectories as an attention mask and construct a social map to encode the interactive relationship between the target agent and its surrounding agents. Given a trajectory sequence, the LSTM networks are firstly utilized to extract the features for all agents, based on which the attention mask and social map are formed. Then, the attention mask and social map are fused to get the fusion feature map, which is processed by the social convolution to obtain a fusion feature representation. Finally, this fusion feature is taken as the input of a variable-length LSTM to predict the trajectory of the target agent. We note that the variable-length LSTM enables our model to handle the case that the number of agents in the sensing scope is highly dynamic in traffic scenes. To verify the effectiveness of our method, we widely compare with several methods on a public dataset, achieving a 20% error decrease. In addition, the model satisfies the real-time requirement with the 32 fps.	Proceedings Paper	2020		[]	[]	-1	-1	-1
J	Guo, Wenfeng; Cao, Haotian; Zhao, Song; Wang, Jianqiang; Yi, Binlin; Song, Xiaolin	Optimal design of a driver assistance controller based on surrounding vehicle?s social behavior game model	2023	APPLIED MATHEMATICAL MODELLING		646	670	Driver assistance control in the cut-in scenarios is challenging, since the controller needs to ensure driving safety and avoid unnecessary intervention, while considering the inter-action with surrounding vehicles. This paper proposes an optimal driver assistance con-troller considering the social behaviors of the surrounding vehicles to assist the drivers in the cut-in scenarios. To model the social behavior of the surrounding vehicle, we first for-mulate the interaction between the semi-autonomous vehicle and the surrounding vehicle as a misinformation game, which is achieved by assuming the surrounding vehicle is in-teracting with a hypothetical vehicle under the framework of non-cooperative game. Then, adaptive dynamic programming theory is utilized to find the Nash equilibrium, which is represented by deep neural networks and solved iteratively. Based on the established so-cial behavior model and the nonlinear driver-vehicle dynamic model, an affine input non-linear system model is obtained for the design of driver assistance controller, and the op-timal assistance control strategy is also derived under the structure of adaptive dynamic programming. Several numerical simulations and driver-in-the-loop simulator experiments are conducted for validation. Results show that the proposed strategy can assist the driver in the cut-in scenario while addressing different social interactions with the surround-ing vehicles. Importantly, by taking into account the surrounding vehicle's social behavior through our established social behavior model, our proposed strategy significantly outper-forms the no-interaction strategy both in terms of driving safety and intervention degree, validating its effectiveness and superiority.(c) 2022 Published by Elsevier Inc.	Article; Early Access		10.1016/j.apm.2022.10.010	[]	[]	-1	-1	-1
J	Chen, Long; Hu, Xuemin; Tian, Wei; Wang, Hong; Cao, Dongpu; Wang, Fei-Yue	Parallel Planning: A New Motion Planning Framework for Autonomous Driving	2019	IEEE-CAA JOURNAL OF AUTOMATICA SINICA		236	246	"Motion planning is one of the most significant technologies for autonomous driving. To make motion planning models able to learn from the environment and to deal with emergency situations, a new motion planning framework called as ""parallel planning"" is proposed in this paper. In order to generate sufficient and various training samples, artificial traffic scenes are firstly constructed based on the knowledge from the reality. A deep planning model which combines a convolutional neural network (CNN) with the Long Short-Term Memory module (LSTM) is developed to make planning decisions in an end-toend mode. This model can learn from both real and artificial traffic scenes and imitate the driving style of human drivers. Moreover, a parallel deep reinforcement learning approach is also presented to improve the robustness of planning model and reduce the error rate. To handle emergency situations, a hybrid generative model including a variational auto-encoder (VAE) and a generative adversarial network (GAN) is utilized to learn from virtual emergencies generated in artificial traffic scenes. While an autonomous vehicle is moving, the hybrid generative model generates multiple video clips in parallel, which correspond to different potential emergency scenarios. Simultaneously, the deep planning model makes planning decisions for both virtual and current real scenes. The final planning decision is determined by analysis of real observations. Leveraging the parallel planning approach, the planner is able to make rational decisions without heavy calculation burden when an emergency occurs."	Article	JAN 2019	10.1109/JAS.2018.7511186	[]	[]	-1	-1	-1
J	Ren, Yangang; Zhan, Guojian; Tang, Liye; Li, Shengbo Eben; Jiang, Jianhua; Li, Keqiang; Duan, Jingliang	Improve generalization of driving policy at signalized intersections with adversarial learning	2023	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Intersections are quite challenging among various driving scenes wherein the interaction of signal lights and distinct traffic actors poses great difficulty to learn a wise and robust driving policy. Current research rarely considers the diversity of intersections and stochastic behaviors of traffic participants. For practical applications, the randomness usually leads to some devastating events, which should be the focus of autonomous driving. This paper introduces an adversarial learning paradigm to boost the intelligence and robustness of driving policy for signalized intersections with dense traffic flow. Firstly, we design a static path planner which is capable of generating trackable candidate paths for multiple intersections with diversified topology. Next, a constrained optimal control problem (COCP) is built based on these candidate paths wherein the bounded uncertainty of dynamic models is considered to capture the randomness of driving environment. We propose adversarial policy gradient (APG) to solve the COCP wherein the adversarial policy is introduced to provide disturbances by seeking the most severe uncertainty while the driving policy learns to handle this situation by competition. Finally, a comprehensive system is established to conduct training and testing wherein the perception module is introduced and the human experience is incorporated to solve the yellow light dilemma. Simulation results indicate that the trained policy can handle the signal lights flexibly meanwhile realizing smooth and efficient passing with a humanoid paradigm. Besides, APG enables a large-margin improvement of the resistance to the abnormal behaviors and thus ensures a high safety level for the autonomous vehicle.	Article; Early Access		10.1016/j.trc.2023.104161	[]	[]	-1	-1	-1
J	Graves, D.; Nguyen, N.M.; Hassanzadeh, K.; Jun Jin	Learning predictive representations in autonomous driving to improve deep reinforcement learning [arXiv]	2020	arXiv		31 pp.	31 pp.	Reinforcement learning using a novel predictive representation is applied to autonomous driving to accomplish the task of driving between lane markings where substantial benefits in performance and generalization are observed on unseen test roads in both simulation and on a real Jackal robot. The novel predictive representation is learned by general value functions (GVFs) to provide out-of-policy, or counter-factual, predictions of future lane centeredness and road angle that form a compact representation of the state of the agent improving learning in both online and offline reinforcement learning to learn to drive an autonomous vehicle with methods that generalizes well to roads not in the training data. Experiments in both simulation and the real-world demonstrate that predictive representations in reinforcement learning improve learning efficiency, smoothness of control and generalization to roads that the agent was never shown during training, including damaged lane markings. It was found that learning a predictive representation that consists of several predictions over different time scales, or discount factors, improves the performance and smoothness of the control substantially. The Jackal robot was trained in a two step process where the predictive representation is learned first followed by a batch reinforcement learning algorithm (BCQ) from data collected through both automated and human-guided exploration in the environment. We conclude that out-of-policy predictive representations with GVFs offer reinforcement learning many benefits in real-world problems.	Journal Paper	26 June 2020		[]	[]	-1	-1	-1
J	Wu, Zhenyu; Qiu, Kai; Gao, Hongbo	Driving policies of V2X autonomous vehicles based on reinforcement learning methods	2020	IET INTELLIGENT TRANSPORT SYSTEMS		331	337	Autonomous driving has been achieving great progress since last several years. However, the autonomous vehicles always ignore the important traffic information on the road because of the uncertainties of driving environment and the limitations of onboard sensors. This might cause serious safety problem in autonomous driving. This study argues that the connected vehicles could share much more environmental information with each other. Therefore, a decision-making method based on reinforcement learning is proposed for V2X autonomous vehicles. First, the V2X autonomous driving architecture with three subsystems is designed. By V2V communication, an autonomous vehicle could obtain much more environmental information. Second, a reinforcement learning based model is applied to learn from the V2V observation data. A simulation environment is setup based on OpenAI reinforcement learning framework. The experimental results demonstrate the effectiveness of the V2X in autonomous driving.	Article	MAY 2020	10.1049/iet-its.2019.0457	[]	[]	-1	-1	-1
C	Fischer, Johannes; Eyberg, Christoph; Werling, Moritz; Lauer, Martin	Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints	2021	2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	791	798	Planning for robotic systems is frequently formulated as an optimization problem. Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL). Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective. The consideration of safety constraints is of paramount importance for human-robot collaboration. For this reason, our work addresses maximum entropy IRL in constrained environments. Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs). (2) We transfer maximum entropy IRL to CMDPs based on CSRL. (3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching. In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.	Proceedings Paper	2021	10.1109/IROS51168.2021.9636672	[]	[]	-1	-1	-1
J	Vijayakumar, V. A.; Shanthini, J.; Karthik, S.; Srihari, K.	Route Planning for Autonomous Transmission of Large Sport Utility Vehicle	2023	COMPUTER SYSTEMS SCIENCE AND ENGINEERING		659	669	The autonomous driving aims at ensuring the vehicle to effectively sense the environment and use proper strategies to navigate the vehicle without the interventions of humans. Hence, there exist a prediction of the background scenes and that leads to discontinuity between the predicted and planned outputs. An optimal prediction engine is required that suitably reads the background objects and make optimal decisions. In this paper, the author(s) develop an autonomous model for vehicle driving using ensemble model for large Sport Utility Vehicles (SUVs) that uses three different modules involving (a) recognition mod-el, (b) planning model and (c) prediction model. The study develops a direct realization method for an autonomous vehicle driving. The direct realization method is designed as a behavioral model that incorporates three different modules to ensure optimal autonomous driving. The behavioral model includes recognition, planning and prediction modules that regulates the input trajectory processing of input video datasets. A deep learning algorithm is used in the proposed approach that helps in the classification of known or unknown objects along the line of sight. This model is compared with conventional deep learning classifiers in terms of recall rate and root mean square error (RMSE) to estimate its efficacy. Simulation results on different traffic environment shows that the Ensemble Convolutional Network Reinforcement Learning (E-CNN-RL) offers increased accuracy of 95.45%, reduced RMSE and increased recall rate than existing Ensemble Convolutional Neural Networks (CNN) and Ensemble Stacked CNN.	Article	2023	10.32604/csse.2023.028400	[]	[]	-1	-1	-1
J		HAZARD DETECTION AI SYSTEM FOR AUTONOMOUS VEHICLE	2020					Perform literature review.Develop a Perception AI system based on deep learning using publicly available data (e.g. KAIST Multi-Spectral Day/Night Data Set (Choi et al., 2018)) to: (a) Predict the intent for different road users e.g. pedestrians, cyclists etc. (b) Detect static road features e.g. road works, road signs etc. and (c) Detect weather condition e.g. fog, rain etc.Investigate the deep learning framework e.g. TensorFlow or PyTorch.Build, train, test and validate the neural network.Determine the performance of the AI in an unconstrained environment.Investigate reinforcement learning for continuous learning of the developed AI system.Develop a Hazard Detection AI system for AV using deep learning based on the output from the Perception AI system.Build, train, test and validate the neural network.Determine the performance of the Hazard Detection AI system system in an unconstrained environment.Investigate reinforcement learning for continuous learning of the developed Hazard Detection AI system.	Awarded Grant	Sep 30 2020		[]	[]	-1	-1	-1
J	JUNG,, Sangchul; Lee, Seungjae; Kim, Jooyoung	The Real-Time Signal Control System Using Reinforcement Learning Considering Priority Signaling for Emergency Vehicle	2021	Korean Society of Transportation		329	344	Recently, with the development of autonomous vehicles and V2X (Vehicle-to-Everyting) technology, researches are being conducted to reduce vehicle delay time and unnecessary stop-and-go phenomenon at the urban signalized intersection. In particular, current urban networks, in which the fixed-time traffic signal control system is dominant, can not reflect the real-time traffic situation in the signaling system. Even if the autonomous vehicle is commercialized, it could not exhibit its performance. At this time, if the artificial intelligence traffic signal control system based on accurate real-time vehicle information obtained through V2I technology is developed, more efficient traffic signal control will be possible and it will affect not only traffic flow but also the performance of the V2X technology-equipped vehicles. Especially, when there is unnecessary waiting time for the emergency vehicle in the urban intersection, it may affect the travel time to reach the destination. In case of driving while leaving the lane for the purpose of shortening the travel time, it can threaten the safety of a patient and surrounding vehicles. At this point, if the traffic signal control system that detects the entrance of the emergency vehicle to an intersection through the V2I technology and gives the priority signal to the emergency vehicle is developed, the safer emergency vehicle operation could be guaranteed. In this paper, the change of traffic flow according to the change of traffic signal control system is analyzed based on micro traffic information at single signalized intersection with V2I technology commercialization. A reinforcement learning model that expresses the optimum signal in real-time by learning the traffic information at the single signalized intersection was constructed based on the deep learning through the tensor flow. The performance of the developed traffic signal control system was verified through Vissim. The developed reinforcement learning model expresses a specific signal phase through real-time traffic information and expresses appropriate signal display in the changed network situation. The traffic signal control system developed in this paper is expected to contribute to achieve system optimization in a complex road network and to contribute to the analysis of traffic flow. In addition, it will contribute to building a smart city when autonomous vehicles are operated in the future V2X environment.	research-article	2021		[]	[]	-1	-1	-1
J	Brunnbauer, A.; Berducci, L.; Brandstatter, A.; Lechner, M.; Hasani, R.; Rus, D.; Grosu, R.	Model-based versus Model-free Deep Reinforcement Learning for Autonomous Racing Cars [arXiv]	2021	arXiv		12 pp.	12 pp.	Despite the rich theoretical foundation of model-based deep reinforcement learning (RL) agents, their effectiveness in real-world robotics-applications is less studied and understood. In this paper, we, therefore, investigate how such agents generalize to real-world autonomous-vehicle control-tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional LiDAR sensors, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination, substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the observation-model choice. Finally, we provide extensive empirical evidence for the effectiveness of model-based agents provided with long enough memory horizons in sim2real tasks.	Journal Paper	8 March 2021		[]	[]	-1	-1	-1
J	Chen, Sikai; Dong, Jiqian; Ha, Paul (Young Joun); Li, Yujie; Labi, Samuel	Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles	2021	COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING		838	857	A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment. The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements. Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations. Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control. The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques. Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources. Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor. In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor. We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp. For purposes of comparison, the proposed model is evaluated vis-a-vis traditional rule-based and long short-term memory-based fusion models. The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility. That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams. The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations. The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities.	Article	JUL 2021	10.1111/mice.12702	[]	[]	-1	-1	-1
J	Liu, Boyi; Wang, Lujia; Liu, Ming	Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems	2019	IEEE ROBOTICS AND AUTOMATION LETTERS		4555	4562	This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments. To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL). In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud. Then, effective transfer learning methods in LFRL are introduced. LFRL is consistent with human cognitive science and fits well in cloud robotic systems. Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation. The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge. In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.	Article	OCT 2019	10.1109/LRA.2019.2931179	[]	[]	-1	-1	-1
J	Wu, Junta; Li, Huiyun	Deep Ensemble Reinforcement Learning with Multiple Deep Deterministic Policy Gradient Algorithm	2020	MATHEMATICAL PROBLEMS IN ENGINEERING				Deep deterministic policy gradient algorithm operating over continuous space of actions has attracted great attention for reinforcement learning. However, the exploration strategy through dynamic programming within the Bayesian belief state space is rather inefficient even for simple systems. Another problem is the sequential and iterative training data with autonomous vehicles subject to the law of causality, which is against the i.i.d. (independent identically distributed) data assumption of the training samples. This usually results in failure of the standard bootstrap when learning an optimal policy. In this paper, we propose a framework of m-out-of-n bootstrapped and aggregated multiple deep deterministic policy gradient to accelerate the training process and increase the performance. Experiment results on the 2D robot arm game show that the reward gained by the aggregated policy is 10%-50% better than those gained by subpolicies. Experiment results on the open racing car simulator (TORCS) demonstrate that the new algorithm can learn successful control policies with less training time by 56.7%. Analysis on convergence is also given from the perspective of probability and statistics. These results verify that the proposed method outperforms the existing algorithms in both efficiency and performance.	Article	JAN 22 2020	10.1155/2020/4275623	[]	[]	-1	-1	-1
B	Manchella, Kaushik	Flexpool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers & Goods Transportation	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Bezerra, Carlos Daniel de Sousa; Vieira, Flavio Henrique Teles; Soares, Anderson da Silva	Deep-Q-Network hybridization with extended Kalman filter for accelerate learning in autonomous navigation with auxiliary security module	2024	TRANSACTIONS ON EMERGING TELECOMMUNICATIONS TECHNOLOGIES				This article proposes an algorithm for autonomous navigation of mobile robots that mixes reinforcement learning with extended Kalman filter (EKF) as a localization technique, namely EKF-DQN, aiming to accelerate the maximization of the learning curve and improve the reward values obtained in the learning process. More specifically, Deep-Q-Networks (DQN) are used to control the trajectory of an autonomous robot in an environment with many obstacles. To improve navigation capability in this environment, we also propose a fusion of visual and nonvisual sensors. Due to the ability of EKF to predict states, this algorithm is used as a learning accelerator for the DQN network, predicting future states and inserting this information into the memory replay. Aiming to increase the safety of the navigation process, a visual safety system is also proposed to avoid collisions between the mobile robot and people circulating in the environment. The efficiency of the proposed control system is verified through computational simulations using the CoppeliaSIM simulator with code insertion in Python. The simulation results show that the EKF-DQN algorithm accelerates the maximization of rewards obtained and provides a higher success rate in fulfilling the mission assigned to the robot when compared to other value-based and policy-based algorithms. A demo video of the navigation system can be seen at: .This article proposes an algorithm for autonomous navigation of mobile robots that merges reinforcement learning with extended Kalman filter (EKF) as a localization technique, namely, EKF-DQN, aiming to accelerate learning and improve the reward values obtained in the process of apprenticeship. More specifically, deep neural networks (DQN-Deep-Q-Networks) are used to control the trajectory of an autonomous vehicle in an indoor environment. Due to the ability of EKF to predict states, this algorithm is proposed to be used as a learning accelerator of the DQN network, predicting states ahead and inserting this information in the memory replay. Aiming at the enhancing safety of the navigation process, it is also proposed a visual safety system that avoids collisions of the mobile vehicle with people moving in the environment. image	Article	FEB 2024	10.1002/ett.4946	[]	[]	-1	-1	-1
J	Pokhrel, Shiva Raj; Choi, Jinho	Understand-Before-Talk (UBT): A Semantic Communication Approach to 6G Networks	2023	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		3544	3556	"In Shannon theory, semantic aspects of communication were identified but considered irrelevant to the technical communication problems. Semantic communication (SC) techniques have recently attracted renewed research interests in 6(th) generation (6G) wireless, because they have the capability to support an efficient interpretation of the significance and meaning intended by a sender (or accomplishment of the goal) when dealing with multi-modal data such as videos, images, audio, text messages, and so on, which would be the case for various applications such as intelligent transportation systems where each autonomous vehicle needs to deal with real-time videos and data from a number of sensors including radars. To this end, most of the emerging SC works focus on specific data types and employ sophisticated machine learning models including deep learning and neural networks. However, they could be impractical for multi-modal data possibly within a real-time constraint, relative to the purpose of the communication. A notable difficulty of existing SC frameworks lies in handling the discrete constraints imposed on the pursued semantic coding and its interaction with the independent knowledge-base, which makes reliable semantic extraction extremely challenging. Therefore, we develop a new hashing-based semantic extraction approach to SC framework, where our learning objective is to generate one time signatures (hash codes) using supervised learning for low latency, secure and efficient management of the SC dynamics. We first evaluate the proposed semantic extraction framework over large image data sets, extend it with domain adaptive hashing and then demonstrate the effectiveness of ""semantics signature"" in bulk transmission and multi-modal data."	Article	MAR 2023	10.1109/TVT.2022.3219363	[]	[]	-1	-1	-1
J	Valiente, Rodolfo; Razzaghpour, Mahdi; Toghi, Behrad; Shah, Ghayoor; Fallah, Yaser P.	Prediction-Aware and Reinforcement Learning-Based Altruistic Cooperative Driving	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS				Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents' behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV's decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.	Article; Early Access		10.1109/TITS.2023.3323440	[]	[]	-1	-1	-1
J	Afifi, Haitham; Ramaswamy, Arunselvan; Karl, Holger	Reinforcement learning for autonomous vehicle movements in wireless multimedia applications	2023	PERVASIVE AND MOBILE COMPUTING				We develop a Deep Reinforcement Learning (DeepRL)-based, multi-agent algorithm to efficiently control autonomous vehicles that are typically used within the context of Wireless Sensor Networks (WSNs), in order to boost application performance. As an application example, we consider wireless acoustic sensor networks where a group of speakers move inside a room. In a traditional setup, microphones cannot move autonomously and are, e.g., located at fixed positions. We claim that autonomously moving microphones improve the application performance. To control these movements, we compare simple greedy heuristics against a DeepRL solution and show that the latter achieves best application performance. As the range of audio applications is broad and each has its own (subjective) per-formance metric, we replace those application metrics by two immediately observable ones: First, quality of information (QoI), which is used to measure the quality of sensed data (e.g., audio signal strength). Second, quality of service (QoS), which is used to measure the network's performance when forwarding data (e.g., delay). In this context, we propose two multi-agent solutions (where one agent controls one microphone) and show that they perform similarly to a single-agent solution (where one agent controls all microphones and has a global knowledge). Moreover, we show via simulations and theoretical analysis how other parameters such as the number of microphones and their speed impacts performance.(c) 2023 Elsevier B.V. All rights reserved.	Article	MAY 2023	10.1016/j.pmcj.2023.101799	[]	[]	-1	-1	-1
B	Ogbebor, JoshuaOnyeka	Distributed Control and Learning of Connected and Autonomous Vehicles Approaching and Departing Signalized Intersections	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Wu, Yukun; Wu, Xuncheng; Qiu, Siyuan; Xiang, Wenbin	A Method for High-Value Driving Demonstration Data Generation Based on One-Dimensional Deep Convolutional Generative Adversarial Networks	2022	ELECTRONICS				As a promising sequential decision-making algorithm, deep reinforcement learning (RL) has been applied in many fields. However, the related methods often demand a large amount of time before they can achieve acceptable performance. While learning from demonstration has greatly improved reinforcement learning efficiency, it poses some challenges. In the past, it has required collecting demonstration data from controllers (either human or controller). However, demonstration data are not always available in some sparse reward tasks. Most importantly, there exist unknown differences between agents and human experts in observing the environment. This means that not all of the human expert's demonstration data conform to a Markov decision process (MDP). In this paper, a method of reinforcement learning from generated data (RLfGD) is presented, and consists of a generative model and a learning model. The generative model introduces a method to generate the demonstration data with a one-dimensional deep convolutional generative adversarial network. The learning model applies the demonstration data to the reinforcement learning process to greatly improve the effectiveness of training. Two complex traffic scenarios were tested to evaluate the proposed algorithm. The experimental results demonstrate that RLfGD is capable of obtaining higher scores more quickly than DDQN in both of two complex traffic scenarios. The performance of reinforcement learning algorithms can be greatly improved with this approach to sparse reward problems.	Article	NOV 2022	10.3390/electronics11213553	[]	[]	-1	-1	-1
J	Agarwal, T.; Arora, H.; Schneider, J.	Affordance-based Reinforcement Learning for Urban Driving [arXiv]	2021	arXiv		15 pp.	15 pp.	Traditional autonomous vehicle pipelines that follow a modular approach have been very successful in the past both in academia and industry, which has led to autonomy deployed on road. Though this approach provides ease of interpretation, its generalizability to unseen environments is limited and hand-engineering of numerous parameters is required, especially in the prediction and planning systems. Recently, deep reinforcement learning has been shown to learn complex strategic games and perform challenging robotic tasks, which provides an appealing framework for learning to drive. In this work, we propose a deep reinforcement learning framework to learn optimal control policy using waypoints and low-dimensional visual representations, also known as affordances. We demonstrate that our agents when trained from scratch learn the tasks of lane-following, driving around inter-sections as well as stopping in front of other actors or traffic lights even in the dense traffic setting. We note that our method achieves comparable or better performance than the baseline methods on the original and NoCrash benchmarks on the CARLA simulator.	Journal Paper	15 Jan. 2021		[]	[]	-1	-1	-1
J		CRII: CPS: RUI: Cognizant Learning for Autonomous Cyber-Physical Systems	2022					This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).  &lt;br/&gt;&lt;br/&gt;The objective of this Computer and Information Science and Engineering (CISE) Research Initiation Initiative (CRII) proposal is to develop a cognizant learning framework for cyber-physical systems (CPS) that incorporates risk-sensitive and irrational decision making. The necessity for such a framework is exemplified by two observations. First, CPS such as self-driving cars will share an environment with other CPS and human users. Human drivers demonstrate a heightened sensitivity to changes in speed and can easily adapt to changes in the environment and road conditions, which makes it essential for a CPS to have an ability to recognize non-rational behaviors. Second, large amounts of data generated during their operation and limited access to models of their environments can make a CPS reliant on machine learning algorithms for decision making to meet performance requirements such as reachability and safety. Our research will be grounded on improving behaviors of autonomous vehicles in realistic traffic situations. Outcomes from this effort will contribute to the development of a research paradigm unifying control, learning, and behavioral economics. Students at a Primarily Undergraduate Institution will benefit by being directly involved in all aspects of the research process. Research tasks will involve a team of undergraduate students in a vertically integrated manner where more experienced students will mentor newer team members. &lt;br/&gt;&lt;br/&gt;The proposed effort comprises two thrusts. Thrust 1 will construct utilities to encode CPS performance objectives consistent with practical models of risk-sensitive and irrational decision making. Strategies will be learned by formulating and solving a reinforcement learning problem to maximize this utility. Methods to enable learned strategies to adequately consider delays between evaluation and execution of actions arising from the physical components of the CPS will be developed. Thrust 2 will design algorithms to learn decentralized cognizant strategies when multiple CPS operate in the same environment. To improve reliability in uncertain environments, or when feedback is sparse, techniques to identify contributions of each CPS to a shared utility will be identified. Solution methodologies will be evaluated empirically through extensive experiments and theoretically by determining probabilistic performance guarantees. The PI will develop a research agenda and new undergraduate curriculum in CPS and machine learning at Western Washington University (WWU). Research and educational goals of the project will be integrated through the CARLA simulator for autonomous vehicle research and the F1/10 Autonomous Vehicle platform. The multidisciplinary scope of the project will be emphasized in outreach efforts through Student Outreach Services and STEM Clubs at WWU to encourage and broaden participation from traditionally underrepresented student groups.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	Awarded Grant	Feb 15 2022		[]	[]	-1	-1	-1
B	Farid, Alec Jacob	Provably Safe Learning-Based Robot Control Via Anomaly Detection and Generalization Theory	2023						Dissertation/Thesis	Jan 01 2023		[]	[]	-1	-1	-1
C	Lecerf, Ugo U. L.; Yemdji-Tchassi, Christelle C. Y.; Aubert, Sebastien S. A.; Michiardi, Pietro P. M.	Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios	2022	PROCEEDINGS OF 2022 7TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING TECHNOLOGIES, ICMLT 2022		209	215	When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment. Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety. Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.In this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment. We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy. We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent. Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.We apply this method to an autonomous driving scenario and show that we are able to learn useful policies that would have otherwise been missed out on during training, and would have been unavailable to the agent when executing the control algorithm.	Proceedings Paper	2022	10.1145/3529399.3529432	[]	[]	-1	-1	-1
J	Lee, Keuntaek; Isele, David; Theodorou, Evangelos A.; Bae, Sangjae	Spatiotemporal Costmap Inference for MPC Via Deep Inverse Reinforcement Learning	2022	IEEE ROBOTICS AND AUTOMATION LETTERS		3194	3201	It can he difficult to autonomously produce driver behavior so that it appears natural to other traffic participants. Through Inverse Reinforcement Learning (IRL), we can automate this process by learning the underlying reward function from human demonstrations. We propose a new IRL algorithm that learns a goal-conditioned spatio-temporal reward function. The resulting costmap is used by Model Predictive Controllers (MPCs) to perform a task without any hand-designing or hand-tuning of the cost function. We evaluate our proposed Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL framework together with MPC in the CARLA simulator for autonomous driving, lane keeping, and lane changing tasks in a challenging dense traffic highway scenario. Our proposed methods show higher success rates compared to other baseline methods including behavior cloning, state-of-the-art RL policies, and MPC with a learning-based behavior prediction model.	Article	APR 2022	10.1109/LRA.2022.3146635	[]	[]	-1	-1	-1
B	Hoel, Carl-Johan E.	Decision-Making in Autonomous Driving Using Reinforcement Learning	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J	Liu, Jiahang; Huang, Zhenhua; Xu, Xin; Zhang, Xinglong; Sun, Shiliang; Li, Dazi	Multi-Kernel Online Reinforcement Learning for Path Tracking Control of Intelligent Vehicles	2021	IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS		6962	6975	Path tracking control of intelligent vehicles has to deal with the difficulties of model uncertainties and nonlinearities. As a class of adaptive optimal control methods, reinforcement learning (RL) has received increasing attention in solving difficult control problems. However, feature representation and online learning ability are two major problems to be solved for learning control of uncertain dynamic systems. In this article, we propose a multi-kernel online RL approach for path tracking control of intelligent vehicles. In the proposed approach, a multiple kernel feature learning framework is designed for online learning control based on dual heuristic programming (DHP) and the new online learning control algorithm is called multi-kernel DHP (MKDHP). In MKDHP, instead of the expert knowledge for selecting and fine-tuning of a suitable kernel function, only a set of basic kernel functions is required to be predefined and the multi-kernel features can be learned for value function approximation in the critic. The simulation studies on path tracking control for intelligent vehicles have been conducted under S-curve and urban road conditions. The results demonstrated that compared with other typical path tracking controllers for intelligent vehicles, such as the linear quadratic regulator (LQR), the pure pursuit controller and the ribbon-based controller, the proposed multi-kernel learning controller can achieve better performance in terms of tracking precision and smoothness.	Article	NOV 2021	10.1109/TSMC.2020.2966631	[]	[]	-1	-1	-1
J	Ma Yining; Jiang Wei; Wu Jingyu; Chen Junyi; Li Nan; Xu Zhigang; Xiong Lu	Self-evolution Scenarios for Simulation Tests of Autonomous Vehicles Based on Different Models of Driving Styles	2023	China Journal of Highway and Transport		216	228	To verify the safety of the decision-making results of autonomous vehicles (AVs), a method for generating driving models with autonomous decision-making and interaction capabilities was proposed, and the driving models were as background vehicles (BVs) and used to build a self-evolution simulation scenario to test the continuous decision-making capability of AVs. First, based on reinforcement learning and a combination of inheritance and evolution ideas, different driving styles with autonomous decision-making and interaction capabilities were designed in this study. Second, in the model-building stage, three styles of driving models, namely, conservative, general, and aggressive, were generated and trained. The simulation training parameters for the general-style driving model were derived from the parameter distribution of a naturalistic driving dataset named highD to ensure fidelity. Finally, based on this, an aggressive-style driving model with significant aggressive features was designed and trained to enhance the complexity and testing effect of the self-evolution scenario. The results show that the distributions of parameters such as the car-following speed, distance headway, and lane-change moment time-to-collision obtained by using the highD dataset are in agreement with real data. An average similarity of 88% is observed between the general-style driving model generated and the corresponding real data, which is an improvement of 20.3% on the results obtained from the rule-based intelligent driver model (IDM). The proposed self-evolution scenario is seven times more testable than the baseline scenario composed of IDMs for the different driving models generated, as confirmed by the number of collisions in which the system under test is primarily responsible. Thus, self-evolution scenarios composed of the driving models designed and generated in this study can effectively support simulation tests for aiding the decision-making system in AVs.	Article	2023		[]	[]	-1	-1	-1
J	Lambert, Reeve; Li, Jianwen; Wu, Li-Fan; Mahmoudian, Nina	Robust ASV Navigation Through Ground to Water Cross-Domain Deep Reinforcement Learning	2021	FRONTIERS IN ROBOTICS AND AI				This paper presents a framework to alleviate the Deep Reinforcement Learning (DRL) training data sparsity problem that is present in challenging domains by creating a DRL agent training and vehicle integration methodology. The methodology leverages accessible domains to train an agent to solve navigational problems such as obstacle avoidance and allows the agent to generalize to challenging and inaccessible domains such as those present in marine environments with minimal further training. This is done by integrating a DRL agent at a high level of vehicle control and leveraging existing path planning and proven low-level control methodologies that are utilized in multiple domains. An autonomy package with a tertiary multilevel controller is developed to enable the DRL agent to interface at the prescribed high control level and thus be separated from vehicle dynamics and environmental constraints. An example Deep Q Network (DQN) employing this methodology for obstacle avoidance is trained in a simulated ground environment, and then its ability to generalize across domains is experimentally validated. Experimental validation utilized a simulated water surface environment and real-world deployment of ground and water robotic platforms. This methodology, when used, shows that it is possible to leverage accessible and data rich domains, such as ground, to effectively develop marine DRL agents for use on Autonomous Surface Vehicle (ASV) navigation. This will allow rapid and iterative agent development without the risk of ASV loss, the cost and logistic overhead of marine deployment, and allow landlocked institutions to develop agents for marine applications.	Article	SEP 20 2021	10.3389/frobt.2021.739023	[]	[]	-1	-1	-1
J	Lombard, Alexandre; Noubli, Ahmed; Abbas-Turki, Abdeljalil; Gaud, Nicolas; Galland, Stephane	Deep Reinforcement Learning Approach for V2X Managed Intersections of Connected Vehicles	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		7178	7189	Intersections are major bottlenecks for road traffic, as well as the origin of many accidents. Efficient management of traffic at intersections is required to ensure both safety and efficiency. Yet, the traditional solutions (static signs, traffic lights) are limited in their efficiency as they consider the flow of vehicles and not the vehicles at the microscopic level. By using inter-vehicular communication of connected vehicles, recent works have shown the possibility to have a great increase in the number of evacuated vehicles thanks to the possibility to give an individual right-of-way directly to each vehicle. In this context of intersections of cooperative vehicles, the scheduling of this right-of-way in order to maximize the throughput of the intersection is still a challenging task, with regard to the hybrid and dynamic aspects of the problem. In this paper, we propose an approach based on Deep Reinforcement Learning (DRL) to efficiently distribute the right-of-way to each vehicle. A Markov Decision Process model of intersections of cooperative vehicles, enabling the application of DRL, is proposed. The performance of the DRL-based scheduling is then compared with classic traffic lights, and with two state-of-the-art cooperative scheduling policies, showing the benefits of the approach (increase of the flow, reduction of CO2 emissions).	Article; Early Access		10.1109/TITS.2023.3253867	[]	[]	-1	-1	-1
J	Feng, Wenhao; Ding, Liang; Zhou, Ruyi; Xu, Chongfu; Yang, Huaiguang; Gao, Haibo; Liu, Guangjun; Deng, Zongquan	Learning-Based End-to-End Navigation for Planetary Rovers Considering Non-Geometric Hazards	2023	IEEE ROBOTICS AND AUTOMATION LETTERS		4084	4091	Autonomous navigation plays an increasingly crucial role in rover-based planetary missions. End-to-end navigation approaches developed upon deep reinforcement learning have enabled great adaptability in complex environments. However, most existing works focus on geometric obstacle avoidance thus have limited capability to cope with ubiquitous non-geometric hazards, such as sinkage and slippage. Autonomous navigation in unstructured harsh environments remains a great challenge requiring further in-depth study. In this letter, a DRL-based navigation method is proposed to autonomously guide a planetary rover towards goals via hazard-free paths with low wheel slip ratios. We introduce an end-to-end network architecture, in which the visual perception and the wheel-terrain interaction are fused to learn the representation of terrain mechanical properties implicitly and further facilitate policy learning for non-geometric hazard avoidance. Our approach outperforms baseline methods in simulation evaluation with superior avoidance capabilities against geometric obstacles and non-geometric hazards. Experiments conducted at a Mars emulation site suggest the successful deployment of our approach on a planetary rover prototype and the capacity of dealing with locomotion risks in real-world navigation tasks.	Article	JUL 2023	10.1109/LRA.2023.3281261	[]	[]	-1	-1	-1
J	Tze-Yang Tung; Pujol, J.R.; Kobus, S.; Gunduz, D.	A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning over Noisy Channels [arXiv]	2021	arXiv		30 pp.	30 pp.	"We propose a novel formulation of the ""effectiveness problem"" in communications, put forth by Shannon and Weaver in their seminal work [2], by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework. Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment can also communicate with each other over a noisy communication channel. The noisy communication channel is considered explicitly as part of the dynamics of the environment and the message each agent sends is part of the action that the agent can take. As a result, the agents learn not only to collaborate with each other but also to communicate ""effectively"" over a noisy channel. This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the ""learning to communicate"" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free. We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP. This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems."	Journal Paper	2 Jan. 2021		[]	[]	-1	-1	-1
C	Koren, Mark; Nassar, Ahmed; Kochenderfer, Mykel J.	Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm	2021	2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)	IEEE International Conference on Intelligent Robots and Systems	5944	5949	Validating the safety of autonomous systems generally requires the use of high-fidelity simulators that adequately capture the variability of real-world scenarios. However, it is generally not feasible to exhaustively search the space of simulation scenarios for failures. Adaptive stress testing (AST) is a method that uses reinforcement learning to find the most likely failure of a system. AST with a deep reinforcement learning solver has been shown to be effective in finding failures across a range of different systems. This approach generally involves running many simulations, which can be very expensive when using a high-fidelity simulator. To improve efficiency, we present a method that first finds failures in a low-fidelity simulator. It then uses the backward algorithm, which trains a deep neural network policy using a single expert demonstration, to adapt the low-fidelity failures to high-fidelity. We have created a series of autonomous vehicle validation case studies that represent some of the ways low-fidelity and high-fidelity simulators can differ, such as time discretization. We demonstrate in a variety of case studies that this new AST approach is able to find failures with significantly fewer high-fidelity simulation steps than are needed when just running AST directly in high-fidelity. As a proof of concept, we also demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art high-fidelity simulator for finding failures in autonomous vehicles.	Proceedings Paper	2021	10.1109/IROS51168.2021.9636072	[]	[]	-1	-1	-1
C	Ozkan, Mehmet Fatih; Ma, Yao	Personalized Adaptive Cruise Control and Impacts on Mixed Traffic	2021	2021 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	412	417	This paper presents a personalized adaptive cruise control (PACC) design that can learn human driver behavior and adaptively control the semi-autonomous vehicle (SAV) in the car-following scenario, and investigates its impacts on mixed traffic. In mixed traffic where the SAV and human-driven vehicles share the road, the SAV's driver can choose a PACC tuning that better fits the driver's preferred driving strategies. The individual driver's preferences are learned through the inverse reinforcement learning (IRL) approach by recovering a unique cost function from the driver's demonstrated driving data that best explains the observed driving style. The proposed PACC design plans the motion of the SAV by minimizing the learned unique cost function considering the short preview information of the preceding human-driven vehicle. The results reveal that the learned driver model can identify and replicate the personalized driving behaviors accurately and consistently when following the preceding vehicle in a variety of traffic conditions. Furthermore, we investigated the impacts of the PACC with different drivers on mixed traffic by considering time headway, gap distance, and fuel economy assessments. A statistical investigation shows that the impacts of the PACC on mixed traffic vary among tested drivers due to their intrinsic driving preferences.	Proceedings Paper	2021		[]	[]	-1	-1	-1
J	Hu, Chaofang; Zhao, Lingxue; Qu, Ge	Event-Triggered Model Predictive Adaptive Dynamic Programming for Road Intersection Path Planning of Unmanned Ground Vehicle	2021	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		11228	11243	Autonomous driving of unmanned ground vehicle (UGV) at road intersection is a challenging task due to the complicated traffic conditions. In this paper, an event-triggered model predictive adaptive dynamic programming (MPADP) algorithm is proposed for path planning of UGV at road intersection. Following the critic-actor scheme of adaptive dynamic programming (ADP), cost function approximation and control policy generation are combined to formulate MPADP. The infinite horizon cost function of ADP is stacked over predictive horizon of model predictive control (MPC), and then the infinite horizon cost function is converted to the finite horizon-stacked cost function in MPADP. By minimizing the approximation error within predictive horizon, the approximation accuracy is enhanced. Considering the limitation of energy consumption, the event-triggered mechanism is designed based on the mismatch of cost function approximation. Three triggering conditions are designed, and the corresponding boundedness of approximation error is proved. Simulation results illustrate the effectiveness, efficiency and feasibility in application of the event-triggered MPADP method for path planning at road intersection.	Article	NOV 2021	10.1109/TVT.2021.3111692	[]	[]	-1	-1	-1
J	Yang, Haodong; Yao, Chenpeng; Liu, Chengju; Chen, Qijun	RMRL: Robot Navigation in Crowd Environments With Risk Map-Based Deep Reinforcement Learning	2023	IEEE ROBOTICS AND AUTOMATION LETTERS		7930	7937	"Achieving safe and effective navigation in crowds is a crucial yet challenging problem. Recent work has mainly encoded the pedestrian-robot state pairs, which cannot fully capture the interactions among humans. Besides, existing work attempts to achieve ""hard"" collision avoidance, which may leave no feasible path to the robot in human-rich scenarios. We suppose that this can be addressed by introducing the local risk map and thus incorporate the risk map into the deep reinforcement learning architecture. The proposed map structure contains the crowd interaction states and geometric information. Meanwhile, a ""soft"" risk mapping of pedestrians is proposed to promote the robot to generate more humanlike motion patterns, and the riskaware dynamic window is designed to enhance the robot's obstacle avoidance ability. Experiments show that our method outperforms the baseline in terms of navigation performance and social attributes. Furthermore, we successfully validate the proposed policy through real-world environments."	Article	DEC 2023	10.1109/LRA.2023.3322093	[]	[]	-1	-1	-1
J	Huang, Mengzhe; Gao, Weinan; Wang, Yebin; Jiang, Zhong-Ping	Data-Driven Shared Steering Control of Semi-Autonomous Vehicles	2019	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS		350	361	This paper presents a cooperative/shared framework of the driver and his/her semi-autonomous vehicle in order to achieve desired steering performance. In particular, a copilot controller and the driver together operate and control the vehicle. Exploiting the classical small-gain theory, our proposed shared steering controller is developed independent of the unmeasurable internal states of the human driver, and only relies on his/her steering torque. Furthermore, by adopting data-driven adaptive dynamic programming and an iterative learning scheme, the shared steering controller is studied from the measurable data of the driver and the vehicle. Meanwhile, the accurate knowledge of the driver and the vehicle dynamics is unnecessary, which settles the problem of their potential parametric variations/uncertainties in practice. The effectiveness of the proposed method is validated by rigorous analysis and demonstrated by numerical simulations.	Article	AUG 2019	10.1109/THMS.2019.2900409	[]	[]	-1	-1	-1
J	Dong, Jiqian; Chen, Sikai; Li, Yujie; Du, Runjia; Steinfeld, Aaron; Labi, Samuel	Space-weighted information fusion using deep reinforcement learning: The context of tactical control of lane-changing autonomous vehicles and connectivity range assessment	2021	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				The connectivity aspect of connected autonomous vehicles (CAV) is beneficial because it facilitates dissemination of traffic-related information to vehicles through Vehicle-to-External (V2X) communication. Onboard sensing equipment including LiDAR and camera can reasonably characterize the traffic environment in the immediate locality of the CAV. However, their performance is limited by their sensor range (SR). On the other hand, longer-range information is helpful for characterizing imminent conditions downstream. By contemporaneously coalescing the shortand long-range information, the CAV can construct comprehensively its surrounding environment and thereby facilitate informed, safe, and effective movement planning in the shortterm (local decisions including lane change) and long-term (route choice). Current literature provides useful information on CAV control approaches that use only local information sensed from the proximate traffic environment but relatively little guidance on how to fuse this information with that obtained from downstream sources and from different time stamps, and how to use the fused information to enhance CAV movements. In this paper, we describe a Deep Reinforcement Learning based approach that integrates the data collected through sensing and connectivity capabilities from other vehicles located in the proximity of the CAV and from those located further downstream, and we use the fused data to guide lane changing, a specific context of CAV operations. In addition, recognizing the importance of the connectivity range (CR) to the performance of not only the algorithm but also of the vehicle in the actual driving environment, the study carried out a case study. The case study demonstrates the application of the proposed algorithm and duly identifies the appropriate CR for each level of prevailing traffic density. It is expected that implementation of the algorithm in CAVs can enhance the safety and mobility associated with CAV driving operations. From a general perspective, its implementation can provide guidance to connectivity equipment manufacturers and CAV operators, regarding the default CR settings for CAVs or the recommended CR setting in a given traffic environment.	Article; Early Access		10.1016/j.trc.2021.103192	[]	[]	-1	-1	-1
J		Adversarial scenario curriculums for navigation and scene understanding	2020					Brief description of the context of the research including potential impact:Autonomous vehicles are increasingly reliant on deep-learned solutions for many tasks within scene understanding and navigation. While broadly improving effectiveness, these solutions often cannot guarantee robustness, whichcontrasts with the safety-critical nature of these applications. Many rare challenging scenarios are likely to be absent from training sets but could lead to unexpected behaviours and disastrous outcomes when encountered in practice.The ability to generate synthetic examples of such rare and adversarial scenarios for a given system during learning will help assess its robustness and improve it.Aims and Objectives:This work will create a framework to automatically discover challenging scenarios that have not been observed in an autonomous vehicle system's training set. The framework will be able to understand how to synthesise an autonomous vehicle scenario along several &quot;axes&quot; of difficulty - which we propose to include the appearance of the surroundings and the configuration of a scene.Moreover, we propose a curriculum-based method to incorporate these synthesised scenarios into the training of the studied systems to improve their performance and robustness before deployment as compared to standard neural network training techniques.Novelty of the research methodology:While many research areas relevant to our project are well explored, they often consider specific components of a challenging scenario (e.g. adversarial attacks for image classification).However, the framework we envision will contain many interacting aspects, such as modelling distributions, synthesising data, efficiently searching for adversarial scenarios, or augmenting training.To develop it, we will combine ideas from generative modelling, active learning, simulation, reinforcement learning, adversarial attacks, and curriculum learning. Our task will include devising how existing methods can interact and combining ideas from different fields into a new framework. Alignment to EPSRC's strategies and research areas:This project relates to the EPSRC's research areas: robotics, artificial intelligence technologies,engineering design, software engineering, and verification and correctness.Companies or collaborators involved:We will conduct the project in collaboration with industry-partner Oxbotica	Awarded Grant	Sep 30 2020		[]	[]	-1	-1	-1
J	Cheng, Yanqiu; Hu, Xianbiao; Chen, Kuanmin; Yu, Xinlian; Luo, Yulong	Online longitudinal trajectory planning for connected and autonomous vehicles in mixed traffic flow with deep reinforcement learning approach	2023	JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS		396	410	This manuscript presents an Adam optimization-based Deep Reinforcement Learning model for Mixed Traffic Flow control (ADRL-MTF), so as to guide Connected and Autonomous vehicle's (CAV) longitudinal trajectory on a typical urban roadway with signal-controlled intersections. Two improvements are made when compared with prior literatures. First, the common assumptions to simplify the problem solving, such as dividing a vehicle trajectory into several segments with constant acceleration/deceleration, are avoided, to improve the modeling realism. Second, built on the efficient Adam Optimization and Deep Q-Learning, the proposed mod& avoids the enumeration of states and actions, and is computational efficient and suitable for real time applications. The mixed traffic flow dynamic is firstly formulated as a finite Markov decision process (MDP) model. Due to the discretization of time, space and speed, this MDP model becomes high-dimensional in state, and is very challenging to solve. We then propose a temporal difference-based deep reinforcement learning approach, with E-greedy for exploration-exploitation balance. Two neural networks are developed to replace the traditional Q function and generate the targets in the Q-learning update. These two neural networks are trained by the Adam optimization algorithm, which extends stochastic gradient descent and considers the second moments of the gradients, and is thus highly computational efficient and has lower memory requirements. The proposed model is shown to reduce fuel consumption by 7.8%, which outperforms a prior benchmark model based on Monte Carlo Tree Search. The model's runtime efficiency and stability are tested, and the sensitivity analysis is also performed.	Article; Early Access		10.1080/15472450.2022.2046472	[]	[]	-1	-1	-1
C	Masmitja, Ivan; Martin, Mario; Katija, Kakani; Gomariz, Spartacus; Navarro, Joan	A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles	2022	2022 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING (CASE)	IEEE International Conference on Automation Science and Engineering	675	682	Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems. Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position. Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption. To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms. Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies. The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g. the median predicted error at the beginning of the target's localisation is 17% less. These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles. This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios.	Proceedings Paper	2022	10.1109/CASE49997.2022.9926499	[]	[]	-1	-1	-1
C	Jiang, Shenghao; Chen, Jiying; Shen, Macheng	An Interactive Lane Change Decision Making Model With Deep Reinforcement Learning	2019	2019 IEEE 7TH INTERNATIONAL CONFERENCE ON CONTROL, MECHATRONICS AND AUTOMATION (ICCMA 2019)		370	376	By considering lane change maneuver as primarily a Partial Observed Markov Decision Process (POMDP) and motion planning problem, this paper presents an interactive model with a Recurrent Neural Network (RNN) approach to determine the adversarial or cooperative intention probability of following vehicle in target lane. To make proper and efficient lane change decision, Deep Q-value network (DQN) is applied to solve POMDP with expected global maximum reward. Then quintic polynomials-based motion planning algorithm is used to obtain both optimal lateral and longitudinal trajectory for autonomous vehicle to pursuit. Experimental results demonstrate the capability of the proposed model to execute lane change maneuver with comfortable and safety reference trajectory at an appropriate time instance and traffic gap in various highway traffic scenarios.	Proceedings Paper	2019	10.1109/iccma46720.2019.8988750	[]	[]	-1	-1	-1
J	Zhang, Han; Liang, Hongbin; Wang, Lei; Yao, Yiting; Lin, Bin; Zhao, Dongmei	Joint resource allocation and security redundancy for autonomous driving based on deep reinforcement learning algorithm	2024	IET INTELLIGENT TRANSPORT SYSTEMS				Autonomous vehicles navigating urban roads require technology that combines low latency with high computing power. The limited resources of the vehicle itself compel it to offload task requirements to edge server (ES) for processing assistance. However, as the number of vehicles continues to increase, how edge servers reasonably allocate limited resources to autonomous vehicles becomes critical to the success of urban intelligent transportation services. This paper establishes an urban road scenario with multiple autonomous vehicles and an edge computing server and considers two main driving behaviour transition resource requests, namely car-following behaviour requests and lane-changing behaviour requests. Simultaneously, acknowledging that vehicles may encounter unforeseen traffic hazards when switching driving behaviours, a safety redundancy setting strategy is employed to allocate additional resources to the vehicle to ensure safety and model the vehicle resource allocation problem in the autonomous driving system. Double-deep Q-network (DDQN) is then used to solve this model and maximize the total system utility by comprehensively considering resource costs, system revenue, and autonomous vehicle safety. Finally, results from the simulation experiment indicate that the proposed dynamic resource allocation scheme, based on deep reinforcement learning for autonomous vehicles under edge computing, not only greatly improves the system's benefits and reduces processing delays compared to traditional greedy algorithms and value iteration, but also effectively ensures security.A dynamic resource allocation scheme is proposed for vehicular edge computing networks in autonomous driving systems using deep reinforcement learning. This approach, leveraging double-deep Q-network, aims to maximize system revenue while balancing resource costs, system income, and security considerations. Simulation results demonstrate significant improvements in system rewards and reduced processing delay, guaranteeing system security compared to traditional greedy algorithms and value iteration methods. image	Article; Early Access		10.1049/itr2.12489	[]	[]	-1	-1	-1
J	Al-Turki, Mohammed; Ratrout, Nedal T.; Rahman, Syed Masiur; Assi, Khaled J.	Signalized Intersection Control in Mixed Autonomous and Regular Vehicles Traffic Environment-A Critical Review Focusing on Future Control	2022	IEEE ACCESS		16942	16951	The recent advancement in industrial technology has offered new opportunities to overcome different problems of stochastic driving behavior of humans through effective implementation of autonomous vehicles (AVs). Optimum utilization of driving behavior and advanced capabilities of the AVs has enabled researchers to propose autonomous cooperative-based methods for signalized intersection control under an AV traffic environment. In the future, AVs will share road networks with regular vehicles (RVs), representing a dynamic mixed traffic environment of two groups of vehicles with different characteristics. Without compromising the safety and level of service, traffic operation and control of such a complex environment is a challenging task. The current study includes a comprehensive review focused on the signalized intersection control methods under a mixed traffic environment. The different proposed methods in the literature are based on certain assumptions, requirements, and constraints mainly associated with traffic composition, connectivity, road infrastructures, intersection, and functional network design. Therefore, these methods should be evaluated with appropriate consideration of the underlying assumptions and limitations. This study concludes that the application of adaptive traffic signal control can effectively optimize traffic signal plans for variations of AV traffic environments. However, artificial intelligence approaches primarily focusing on reinforcement learning should be considered to better utilization of the improved AV characteristics.	Review	2022	10.1109/ACCESS.2022.3148706	[]	[]	-1	-1	-1
J	Cao, Zhong; Yang, Diange; Xu, Shaobing; Peng, Huei; Li, Boqi; Feng, Shuo; Zhao, Ding	Highway Exiting Planner for Automated Vehicles Using Reinforcement Learning	2021	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		990	1000	Exiting from highways in crowded dynamic traffic is an important path planning task for autonomous vehicles (AVs). This task can be challenging because of the uncertain motion of surrounding vehicles and limited sensing/observing window. Conventional path planning methods usually compute a mandatory lane change (MLC) command, but the lane change behavior (e.g., vehicle speed and gap acceptance) should also adapt to traffic conditions and the urgency for exiting. In this paper, we propose a reinforcement learning-enhanced highway-exit planner. The learning-based strategy learns from past failures and adjusts the vehicle motion when the AV fails to exit. The reinforcement learning is based on the Monte Carlo tree search (MCTS) approach. The proposed learning-enhanced highway-exit planner is tested 6000 times in stochastic simulations. The results indicate that the proposed planner achieves a higher probability of successful highway exiting than a benchmark MLC planner.	Article	FEB 2021	10.1109/TITS.2019.2961739	[]	[]	-1	-1	-1
B	Albilani, M.; Bouzeghoub, A.	Guided Hierarchical Reinforcement Learning for Safe Urban Driving	2023	2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)		746	53	Designing a safe decision-making system for end-to-end urban driving is still challenging. Numerous contributions based on Deep Reinforcement Learning (DRL) were developed. However, they all suffer from the cold start issue and require extensive convergence training. Recent solutions for urban driving have emerged based on both Hierarchical Reinforcement Learning (HRL) and imitation learning to overcome these limitations. Nevertheless, they do not guarantee a safe exploration for an autonomous vehicle. In the literature, rule-based systems played a pivotal role in ensuring the safety of self-driving cars, but they require manual rule encoding. This paper introduces GHRL, a decision-making framework for vision-based urban driving that benefits from HRL, and a rule-based system for safe urban driving. The HRL algorithm learns the high-level policies, whereas the low-level policies are guided by the expert demonstration rules modeled with the Answer Set Programming (ASP) formalism. When a critical situation occurs, the system will shift to rely on ASP rules. The state of each policy includes visual features extracted by a convolutional neural network from a monocular camera, information on localization, and waypoints. GHRL is evaluated on the Carla NoCrash benchmark. The results show that by incorporating logical rules, GHRL achieved better performance over state-of-the-art HRL algorithms.	Conference Paper	2023	10.1109/ICTAI59109.2023.00115	[]	[]	-1	-1	-1
J	Li, Guofa; Yang, Yifan; Li, Shen; Qu, Xingda; Lyu, Nengchao; Li, Shengbo Eben	Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness	2022	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Driving safety is the most important element that needs to be considered for autonomous vehicles (AVs). To ensure driving safety, we proposed a lane change decision-making framework based on deep reinforcement learning to find a risk-aware driving decision strategy with the minimum expected risk for autonomous driving. Firstly, a probabilistic-model based risk assessment method was proposed to assess the driving risk using position uncertainty and distance-based safety metrics. Then, a risk aware decision making algorithm was proposed to find a strategy with the minimum expected risk using deep reinforcement learning. Finally, our proposed methods were evaluated in CARLA in two scenarios (one with static obstacles and one with dynamically moving vehicles). The results show that our proposed methods can generate robust safe driving strategies and achieve better driving performances than previous methods.	Article	JAN 2022	10.1016/j.trc.2021.103452	[]	[]	-1	-1	-1
J	Arslan, Bartu; Ekren, Banu Yetkin	Transaction selection policy in tier-to-tier SBSRS by using Deep <i>Q</i>-Learning	2023	INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH		7353	7366	This paper studies a Deep Q-Learning (DQL) method for transaction sequencing problems in an automated warehousing system, Shuttle-based Storage and Retrieval System (SBSRS), in which shuttles can move between tiers flexibly. Here, the system is referred to as tier-to-tier SBSRS (t-SBSRS), developed as an alternative design to tier-captive SBSRS (c-SBSRS). By the flexible travel of shuttles between tiers in t-SBSRS, the number of shuttles in the system may be reduced compared to its simulant c-SBSRS design. The flexible travel of shuttles makes the operation decisions more complex in that system, motivating us to explore whether integration of a machine learning approach would help to improve the system performance. We apply the DQL method for the transaction selection of shuttles in the system to attain process time advantage. The outcomes of the DQN are confronted with the well-applied heuristic approaches: first-come-first-serve (FIFO) and shortest process time (SPT) rules under different racking and numbers of shuttles scenarios. The results show that DQL outperforms the FIFO and SPT rules promising for the future of smart industry applications. Especially, compared to the well-applied SPT rule in industries, DQL improves the average cycle time per transaction by roughly 43% on average.	Article; Early Access		10.1080/00207543.2022.2148767	[]	[]	-1	-1	-1
J	Gu, Ziqing; Gao, Lingping; Ma, Haitong; Li, Shengbo Eben; Zheng, Sifa; Jing, Wei; Chen, Junbo	Safe-State Enhancement Method for Autonomous Driving via Direct Hierarchical Reinforcement Learning	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		9966	9983	Reinforcement learning (RL) has shown excellent performance in the sequential decision-making problem, where safety in the form of state constraints is of great significance in the design and application of RL. Simple constrained end-to-end RL methods might lead to significant failure in a complex system like autonomous vehicles. In contrast, some hierarchical RL (HRL) methods generate driving goals directly, which could be closely combined with motion planning. With safety requirements, some safe-enhanced RL methods add post-processing modules to avoid unsafe goals or achieve expectation-based safety, which accepts the existence of unsafe states and allows some violations of safe constraints. However, ensuring state safety is vital for autonomous vehicles. Therefore, this paper proposes a state-based safety enhancement method for autonomous driving via direct hierarchical reinforcement learning. Finally, we design a constrained reinforcement learner based on the State-based Constrained Markov Decision Process (SCMDP), where a learnable safety module could adjust the constraint strength adaptively. We integrate a dynamic module in the policy training and generate future goals considering safety, temporal-spatial continuity, and dynamic feasibility, which could eliminate dependence on the prior model. Simulations in the typical highway scenes with uncertainties show that the proposed method has better training performance, higher driving safety in interactive scenes, more decision intelligence in traffic congestions, and better economic driving ability on roads with changing slopes.	Article; Early Access		10.1109/TITS.2023.3271642	[]	[]	-1	-1	-1
J	Antonio, Guillen-Perez; Maria-Dolores, Cano	Multi-Agent Deep Reinforcement Learning to Manage Connected Autonomous Vehicles at Tomorrow's Intersections	2022	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		7033	7043	In recent years, the growing development of Connected Autonomous Vehicles (CAV), Intelligent Transport Systems (ITS), and 5G communication networks have led to the advent of Autonomous Intersection Management (AIM) systems. AIMs present a new paradigm for CAV control in future cities, taking control of CAVs in scenarios where cooperation is necessary and allowing safe and efficient traffic flows, eliminating traffic signals. So far, the development of AIM algorithms has been based on basic control algorithms, without the ability to adapt or keep learning new situations. To solve this, in this paper we present a new advanced AIM approach based on end-to-end Multi-Agent Deep Reinforcement Learning (MADRL) and trained using Curriculum through Self-Play, called advanced Reinforced AIM (adv.RAIM). adv.RAIM enables the control of CAVs at intersections in a collaborative way, autonomously learning complex real-life traffic dynamics. In addition, adv.RAIM provides a new way to build smarter AIMs capable of proactively controlling CAVs in other highly complex scenarios. Results show remarkable improvements when compared to traffic light control techniques (reducing travel time by 59% or reducing time lost due to congestion by 95%), as well as outperforming other recently proposed AIMs (reducing waiting time by 56%), highlighting the advantages of using MADRL.	Article	JUL 2022	10.1109/TVT.2022.3169907	[]	[]	-1	-1	-1
J	Hieu Trong Nguyen; Phuong Minh Chu; Park, Jisun; Sung, Yunsick; Cho, Kyungeun	Intelligent motivation framework based on Q-network for multiple agents in Internet of Things simulations	2019	INTERNATIONAL JOURNAL OF DISTRIBUTED SENSOR NETWORKS				Internet of Things simulations play significant roles in the diverse kinds of activities in our daily lives and have been extensively researched. Creating and controlling virtual agents in three-dimensional Internet of Things simulations is a key technology for achieving realism in three-dimensional simulations. Given that traditional virtual agent-based approaches have limitations for realism, it is necessary to improve the realism of three-dimensional Internet of Things simulations. This article proposes a Q-Network-based motivation framework that applies a Q-Network to select motivations from desires and hierarchical task network planning to execute actions based on goals of the selected motivations. The desires are to be identified and calculated based on states. Selected motivations will be chosen to determine the goals that agents must achieve. In the experiments, the proposed framework achieved an average accuracy of up to 85.5% when the Q-Network-based motivation model was trained. To verify the Q-Network-based motivation framework, a traditional Q-learning is also applied in the three-dimensional virtual environment. Comparing the results of the two frameworks, the Q-Network-based motivation framework shows better results than those of traditional Q-learning, as the accuracy of the Q-Network-based motivation is higher by 15.58%. The proposed framework can be applied to the diverse kinds of Internet of Things systems such as a training autonomous vehicle. Moreover, the proposed framework can generate big data on animal behaviors for other training systems.	Article	JUL 2019	10.1177/1550147719866385	[]	[]	-1	-1	-1
B	Selvi, S.S.; Dhananjaya, K.M.V.; Lohith, N.V.; Bhardwaj, M.P.V.; Shetty, K.S.	Deep reinforcement learning algorithms for multi-agent systems - a solution for modeling epidemics	2021	2021 IEEE Mysore Sub Section International Conference (MysuruCon)		322	7	Multi-agent reinforcement learning (MARL) consists of large number of artificial intelligence-based agents interacting with each other in the same environment, often collaborating towards a common end goal. In single-agent reinforcement learning system the change in the environment is only due to the actions of a particular agent. In contrast, a multi-agent environment is subject to the actions of all the agents involved. Multiagent systems can be deployed in various applications like stock trading to maximize profits in stock market, control and coordination of a swarm of robots, modeling of epidemics, autonomous vehicle and traffic control, smart grids and self-healing networks. It is not possible to solve these complex tasks with a pre-programmed single agent. Instead, the many agents should be trained to automatically search for a solution through reinforcement learning (RL) based algorithms. In general, arriving at a decision in a multi-agent system is almost close to impossible due to exponential increase of problem size with an increase in the number of agents. In this paper, multi-agent systems using Deep Reinforcement Learning (DRL) is explored with a possible application in modeling of epidemics. Different stochastic environments are considered, and various multi-agent policies are implemented using DRL. The performance of various MARL algorithms was evaluated against single agent RL algorithms under different environments. MARL agents were able to learn much faster compared to single RL agents with a more stable training phase. Mean Field Q-Learning was able to scale and perform much better even in the situation of hundreds of agents in the environment and is a sure candidate to model and predict the epidemics, in the existing frightening dangerous situation of corona pandemic.	Conference Paper	2021	10.1109/MysuruCon52639.2021.9641663	[]	[]	-1	-1	-1
J	Wang, Huanjie; Yuan, Shihua; Guo, Mengyu; Li, Xueyuan; Lan, Wei	A deep reinforcement learning-based approach for autonomous driving in highway on-ramp merge	2021	PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING		2726	2739	In this paper, we focus on the problem of highway merge via parallel-type on-ramp for autonomous vehicles (AVs) in a decentralized non-cooperative way. This problem is challenging because of the highly dynamic and complex road environments. A deep reinforcement learning-based approach is proposed. The kernel of this approach is a Deep Q-Network (DQN) that takes dynamic traffic state as input and outputs actions including longitudinal acceleration (or deceleration) and lane merge. The total reward for this on-ramp merge problem consists of three parts, which are the merge success reward, the merge safety reward, and the merge efficiency reward. For model training and testing, we construct a highway on-ramp merging simulation experiments with realistic driving parameters. The experimental results show that the proposed approach can make reasonable merging decisions based on the observation of the traffic environment. We also compare our approach with a state-of-the-art approach and the superior performance of our approach is demonstrated by making challenging merging decisions in complex highway parallel-type on-ramp merging scenarios.	Article	SEP 2021	10.1177/0954407021999480	[]	[]	-1	-1	-1
J	Park, Shinkyu; Cap, Michal; Alonso-Mora, Javier; Ratti, Carlo; Rus, Daniela	Social Trajectory Planning for Urban Autonomous Surface Vessels	2021	IEEE TRANSACTIONS ON ROBOTICS		452	465	In this article, we propose a trajectory planning algorithm that enables autonomous surface vessels to perform socially compliant navigation in a city's canal. The key idea behind the proposed algorithm is to adopt an optimal control formulation in which the deviation of movements of the autonomous vessel from nominal movements of human-operated vessels is penalized. Consequently, given a pair of origin and destination points, it finds vessel trajectories that resemble those of human-operated vessels. To formulate this, we adopt kernel density estimation (KDE) to build a nominal movement model of human-operated vessels from a prerecorded trajectory dataset, and use a Kullback-Leibler control cost to measure the deviation of the autonomous vessel's movements from the model. We establish an analogy between our trajectory planning approach and the maximum entropy inverse reinforcement learning (MaxEntIRL) approach to explain how our approach can learn the navigation behavior of human-operated vessels. On the other hand, we distinguish our approach from the MaxEntIRL approach in that it does not require well-defined bases, often referred to as features, to construct its cost function as required in many of inverse reinforcement learning approaches in the trajectory planning context. Through experiments using a dataset of vessel trajectories collected from the automatic identification system, we demonstrate that the trajectories generated by our approach resemble those of human-operated vessels and that using them for canal navigation is beneficial in reducing head-on encounters between vessels and improving navigation safety.	Article	APR 2021	10.1109/TRO.2020.3031250	[]	[]	-1	-1	-1
J	Hieu, Nguyen Quang; Hoang, Dinh Thai; Luong, Nguyen Cong; Niyato, Dusit	iRDRC: An Intelligent Real-Time Dual-Functional Radar-Communication System for Automotive Vehicles	2020	IEEE WIRELESS COMMUNICATIONS LETTERS		2140	2143	This letter introduces an intelligent Real-time Dual-functional Radar-Communication (iRDRC) system for autonomous vehicles (AVs). This system enables an AV to perform both radar and data communications functions to maximize bandwidth utilization as well as significantly enhance safety. In particular, the data communications function allows the AV to transmit data, e.g., of current traffic, to edge computing systems and the radar function is used to enhance the reliability and reduce the collision risks of the AV, e.g., under bad weather conditions. The problem of the iRDRC is to decide when to use the communication mode or the radar mode to maximize the data throughput while minimizing the miss detection probability of unexpected events given the uncertainty of surrounding environment. To solve the problem, we develop a deep reinforcement learning algorithm that allows the AV to quickly obtain the optimal policy without requiring any prior information about the environment. Simulation results show that the proposed scheme outperforms baseline schemes in terms of data throughput, miss detection probability, and convergence rate.	Article	DEC 2020	10.1109/LWC.2020.3014972	[]	[]	-1	-1	-1
B	Song, Li	Impacts of Connected and Autonomous Vehicles on Deep Reinforcement Learning Controlled Intersection Systems	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Chen, Dongliang; Huang, Hongyong; Zheng, Yuchao; Gawkowski, Piotr; Lv, Haibin; Lv, Zhihan	The Scanner of Heterogeneous Traffic Flow in Smart Cities by an Updating Model of Connected and Automated Vehicles	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		25361	25370	The problems of traditional traffic flow detection and calculation methods include limited traffic scenes, high system costs, and lower efficiency over detecting and calculating. Therefore, in this paper, we presented the updating Connected and Automated Vehicles (CAVs) model as the scanner of heterogeneous traffic flow, which uses various sensors to detect the characteristics of traffic flow in several traffic scenes on the roads. The model contains the hardware platform, software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project, where the driving of vehicles is mainly controlled by Reinforcement Learning (RL). Finally, the effectiveness of the proposed model and the corresponding swarm intelligence strategy is evaluated through simulation experiments. The results showed that the traffic flow scanning, tracking, and data recording performed continuously by CAVs are effective. The increase in the penetration rate of CAVs in the overall traffic flow has a significant effect on vehicle detection and identification. In addition, the vehicle occlusion rate is independent of the CAV lane position in all cases. The complete street scanner is a new technology that realizes the perception of the human settlement environment with the help of the Internet of Vehicles based on 5G communications and sensors. Although there are some shortcomings in the experiment, it still provides an experimental reference for the development of smart vehicles.	Article; Early Access		10.1109/TITS.2022.3165155	[]	[]	-1	-1	-1
J	Jang, Jihye; Tulkinbekov, Khikmatullo; Kim, Deok-Hwan	Task Offloading of Deep Learning Services for Autonomous Driving in Mobile Edge Computing	2023	ELECTRONICS				As the utilization of complex and heavy applications increases in autonomous driving, research on using mobile edge computing and task offloading for autonomous driving is being actively conducted. Recently, researchers have been studying task offloading algorithms using artificial intelligence, such as reinforcement learning or partial offloading. However, these methods require a lot of training data and critical deadlines and are weakly adaptive to complex and dynamically changing environments. To overcome this weakness, in this paper, we propose a novel task offloading algorithm based on Lyapunov optimization to maintain the system stability and minimize task processing delay. First, a real-time monitoring system is built to utilize distributed computing resources in an autonomous driving environment efficiently. Second, the computational complexity and memory access rate are analyzed to reflect the characteristics of the deep learning applications to the task offloading algorithm. Third, Lyapunov and Lagrange optimization solves the trade-off issues between system stability and user requirements. The experimental results show that the system queue backlog remains stable, and the tasks are completed within an average of 0.4231 s, 0.7095 s, and 0.9017 s for object detection, driver profiling, and image recognition, respectively. Therefore, we ensure that the proposed task offloading algorithm enables the deep learning application to be processed within the deadline and keeps the system stable.	Article	AUG 2023	10.3390/electronics12153223	[]	[]	-1	-1	-1
J	Turan, B.; Pedarsani, R.; Alizadeh, M.	Dynamic Pricing and Management for Electric Autonomous Mobility on Demand Systems Using Reinforcement Learning [arXiv]	2019	arXiv		14 pp.	14 pp.	The proliferation of ride sharing systems is a major drive in the advancement of autonomous and electric vehicle technologies. This paper considers the joint routing, battery charging, and pricing problem faced by a profit-maximizing transportation service provider that operates a fleet of autonomous electric vehicles. We define the dynamic system model that captures the time dependent and stochastic features of an electric autonomous-mobility-on-demand system. To accommodate for the time-varying nature of trip demands, renewable energy availability, and electricity prices and to further optimally manage the autonomous fleet, a dynamic policy is required. In order to develop a dynamic control policy, we first formulate the dynamic progression of the system as a Markov decision process. We argue that it is intractable to exactly solve for the optimal policy using exact dynamic programming methods and therefore apply deep reinforcement learning to develop a near-optimal control policy. Furthermore, we establish the static planning problem by considering time-invariant system parameters. We define the capacity region and determine the optimal static policy to serve as a baseline for comparison with our dynamic policy. While the static policy provides important insights on optimal pricing and fleet management, we show that in a real dynamic setting, it is inefficient to utilize a static policy. The two case studies we conducted in Manhattan and San Francisco demonstrate the efficacy of our dynamic policy in terms of network stability and profits, while keeping the queue lengths up to 200 times less than the static policy.	Journal Paper	15 Sept. 2019		[]	[]	-1	-1	-1
J	Li, Guofa; Lin, Siyan; Li, Shen; Qu, Xingda	Learning Automated Driving in Complex Intersection Scenarios Based on Camera Sensors: A Deep Reinforcement Learning Approach	2022	IEEE SENSORS JOURNAL		4687	4696	Making proper decisions at intersections that are one of the most dangerous and sophisticated driving scenarios is full of challenges, especially for autonomous vehicles (AVs). The existing decision-making approaches for AVs at intersections are limited as they only consider driving safety in simple intersection scenarios while sacrificing travel efficiency and driving comfort. To solve this issue, a decision-making structure motivated by deep reinforcement learning was proposed for autonomous driving at complex intersection scenarios based on long short-term memory (LSTM). The mapping relationship between traffic images collected from camera sensors and AVs' actions was established by constructing convolutional-recurrent neural networks in a decision-making framework. Traffic images collected from camera sensors at two different timesteps were used to understand the relative motion information between AVs and other vehicles. To model the interaction between the AV and other vehicles, Markov decision process was used. The deep Q-network (DQN) algorithm was applied to generate the optimal driving policy that could comprehensively consider driving safety, travel efficiency and driving comfort. Three crash-prone complex intersection scenarios were reconstructed in CARLA (car learning to act) to evaluate the performance of our proposed method. The results indicate that our method can make AV drive through intersections safely and efficiently with desirable driving comfort in all the examined scenarios.	Article	MAR 1 2022	10.1109/JSEN.2022.3146307	[]	[]	-1	-1	-1
J	Sivashangaran, S.; Eskandarian, A.	XTENTH-CAR: A Proportionally Scaled Experimental Vehicle Platform for Connected Autonomy and All-Terrain Research [arXiv]	2022	arXiv				Connected Autonomous Vehicles (CAVs) are key components of the Intelligent Transportation System (ITS), and all-terrain Autonomous Ground Vehicles (AGVs) are indispensable tools for a wide range of applications such as disaster response, automated mining, agriculture, military operations, search and rescue missions, and planetary exploration. Experimental validation is a requisite for CAV and AGV research, but requires a large, safe experimental environment when using full-size vehicles which is time-consuming and expensive. To address these challenges, we developed XTENTH-CAR (eXperimental one-TENTH scaled vehicle platform for Connected autonomy and All-terrain Research), an open-source, cost-effective proportionally one-tenth scaled experimental vehicle platform governed by the same physics as a full-size on-road vehicle. XTENTH-CAR is equipped with the best-in-class NVIDIA Jetson AGX Orin System on Module (SOM), stereo camera, 2D LiDAR and open-source Electronic Speed Controller (ESC) with drivers written in the new Robot Operating System (ROS 2) to facilitate experimental CAV and AGV perception, motion planning and control research, that incorporate state-of-the-art computationally expensive algorithms such as Deep Reinforcement Learning (DRL). XTENTH-CAR is designed for compact experimental environments, and aims to increase the accessibility of experimental CAV and AGV research with low upfront costs, and complete Autonomous Vehicle (AV) hardware and software architectures similar to the full-sized X-CAR experimental vehicle platform, enabling efficient cross-platform development between small-scale and full-scale vehicles.	Journal Paper	03 Dec. 2022		[]	[]	-1	-1	-1
J	Zakaria, N.J.; Shapiai, M.I.; Wahid, N.	A study of multiple reward function performances for vehicle collision avoidance systems applying the DQN algorithm in reinforcement learning	2021	IOP Conference Series: Materials Science and Engineering		012033 (13 pp.)	012033 (13 pp.)	Reinforcement Learning (RL) is an area of Machine Learning (ML) that intends to improve the acts of agents learning from environmental interconnection. The significant concern in RL is to achieve the promising potential of the training process in the model. However, network convergence speed is often sluggish in RL and converges quickly to local optimal solutions. Reward function has been used to deal with these problems as a useful tool to speed up the agent's learning speed. Even though RL convergence properties have been comprehensively explored, there are no specific rules for choosing the reward function. Therefore, searching for efficient potential reward function is still an exciting field of study. This paper discusses the reward function, execute some analysis, and provides the learning agent with the extracted information to increase the speed of learning for collision avoidance task. We provide an experimental study for selecting one reward function in a simulated collision-avoidance environment of an autonomous vehicle by applying the DQN algorithm. It has been conducted on online environments, which is using the CARLA simulator. This experimental study consists of three cases with a various exploration of reward values. Case 1 consists of the range of the penalty value larger than the reward function by 200 times. Case 2 is similar, but with the small range number of the penalty is applied, case 3, which is the reward function and penalty value, is in the same range value. The result shows that case 3 performances outperform case 1 and case 2 with 94% average accuracy; meanwhile, case 1 obtains 70%, and case 2 achieves 85% accuracy. It is may due to the monumental size of the collision penalty in comparison to all else. Hence, the findings obtained show the efficacy of the exploration of the reward function.	Conference Proceedings	Aug. 2021	10.1088/1757-899X/1176/1/012033	[]	[]	-1	-1	-1
J	Xie, Jiaohong; Liu, Yang; Chen, Nan	Two-Sided Deep Reinforcement Learning for Dynamic Mobility-on-Demand Management with Mixed Autonomy	2023	TRANSPORTATION SCIENCE		1019	1046	Autonomous vehicles (AVs) are expected to operate on mobility-on-demand (MoD) platforms because AV technology enables flexible self-relocation and systemoptimal coordination. Unlike the existing studies, which focus on MoD with pure AV fleet or conventional vehicles (CVs) fleet, we aim to optimize the real-time fleet management of an MoD system with a mixed autonomy of CVs and AVs. We consider a realistic case that heterogeneous boundedly rational drivers may determine and learn their relocation strategies to improve their own compensation. In contrast, AVs are fully compliant with the platform's operational decisions. To achieve a high level of service provided by a mixed fleet, we propose that the platform prioritizes human drivers in the matching decisions when on-demand requests arrive and dynamically determines the AV relocation tasks and the optimal commission fee to influence drivers' behavior. However, it is challenging to make efficient real-time fleet management decisions when spatiotemporal uncertainty in demand and complex interactions among human drivers and operators are anticipated and considered in the operator's decision making. To tackle the challenges, we develop a two-sided multiagent deep reinforcement learning (DRL) approach in which the operator acts as a supervisor agent on one side and makes centralized decisions on the mixed fleet, and each CV driver acts as an individual agent on the other side and learns to make decentralized decisions noncooperatively. We establish a two-sided multiagent advantage actor-critic algorithm to simultaneously train different agents on the two sides. For the first time, a scalable algorithm is developed here for mixed fleet management. Furthermore, we formulate a two-head policy network to enable the supervisor agent to efficiently make multitask decisions based on one policy network, which greatly reduces the computational time. The two-sided multiagent DRL approach is demonstrated using a case study in New York City using real taxi trip data. Results show that our algorithm can make high-quality decisions quickly and outperform benchmark policies. The efficiency of the two-head policy network is demonstrated by comparing it with the case using two separate policy networks. Our fleet management strategy makes both the platform and the drivers better off, especially in scenarios with high demand volume.	Article; Early Access		10.1287/trsc.2022.1188	[]	[]	-1	-1	-1
J	Hu, Yifan; Fu, Junjie; Wen, Guanghui	Safe Reinforcement Learning for Model-Reference Trajectory Tracking of Uncertain Autonomous Vehicles With Model-Based Acceleration	2023	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		2332	2344	Applying reinforcement learning (RL) algorithms to control systems design remains a challenging task due to the potential unsafe exploration and the low sample efficiency. In this paper, we propose a novel safe model-based RL algorithm to solve the collision-free model-reference trajectory tracking problem of uncertain autonomous vehicles (AVs). Firstly, a new type of robust control barrier function (CBF) condition for collision-avoidance is derived for the uncertain AVs by incorporating the estimation of the system uncertainty with Gaussian process (GP) regression. Then, a robust CBF-based RL control structure is proposed, where the nominal control input is composed of the RL policy and a model-based reference control policy. The actual control input obtained from the quadratic programming problem can satisfy the constraints of collision-avoidance, input saturation and velocity boundedness simultaneously with a relatively high probability. Finally, within this control structure, a Dyna-style safe model-based RL algorithm is proposed, where the safe exploration is achieved through executing the robust CBF-based actions and the sample efficiency is improved by leveraging the GP models. The superior learning performance of the proposed RL control structure is demonstrated through simulation experiments.	Article	MAR 2023	10.1109/TIV.2022.3233592	[]	[]	-1	-1	-1
R	Masmitja, Ivan; Martin, Mario; O'Reilly, Tom; Kieft, Brian; Palomeras, Narcis; Navarro, Joan; Katija, Kakani	Dynamic robotic tracking of underwater targets using reinforcement learning	2023	Dryad				To realize the potential of autonomous underwater robots that scale up our observational capacity in the ocean, new approaches and techniques are needed. Fleets of autonomous robots could be used to study complex marine systems and animals with either new imaging configurations or by tracking tagged animals to study their behavior. These activities can then inform and create new policies for community conservation. The role of animal connectivity via active movement of animals represents a major knowledge gap related to the distribution of deep ocean populations. Tracking underwater targets represents a major challenge for observing biological processes in situ, and methods to robustly respond to a changing environment during monitoring missions are needed. Analytical techniques for optimal sensor placement and path planning to locate underwater targets are not straightforward in such cases. The aim of this study is to investigate the use of deep reinforcement learning as a tool for range-only underwater target tracking optimization, whose promising capabilities have been demonstrated in terrestrial scenarios. To evaluate its usefulness, a reinforcement learning method was implemented as a path planning system for an autonomous surface vehicle while tracking an underwater mobile target. A complete description of an open-source model, performance metrics in simulated environments, and evaluated algorithms based on more than 15 hours of at-sea field experiments are presented. These efforts demonstrate that deep reinforcement learning is a powerful approach that enhances the abilities of autonomous robots in the ocean and encourages the deployment of algorithms like these for monitoring marine biological systems in the future. Deep Reinforcement Learning methods for Underwater Target Tracking This is a set of tools developed to train an agent (and multiple agents) to find the optimal path to localize and track a target (and multiple targets). The deep Reinforcement Learning (RL) algorithms implemented are: Deep Deterministic Policy Gradient (DDPG) Twin-Delayed DDPG (TD3) Soft Actor-Critic (SAC) The environment to train the agents is based on theOpenAI Particle. The main objective is to find the optimal path that an autonomous vehicle (e.g. autonomous underwater vehicles (AUV) or autonomous surface vehicles (ASV)) should follow in order to localize and track an underwater target usingrange-only and single-beacon algorithms. The target estimation algorithms implemented are based on: Least Squares (LS) Particle Filter (PF) More information at this Github repository:https://github.com/imasmitja/RLforUTracking Copyright: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication	Data set	2024-01-16	https://doi.org/10.5061/DRYAD.CZ8W9GJ7Z	[]	[]	-1	-1	-1
J	Ngai, Daniel Chi Kit; Yung, Nelson Hon Ching	A Multiple-Goal Reinforcement Learning Method for Complex Vehicle Overtaking Maneuvers	2011	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		509	522	In this paper, we present a learning method to solve the vehicle overtaking problem, which demands a multitude of abilities from the agent to tackle multiple criteria. To handle this problem, we propose to adopt a multiple-goal reinforcement learning (MGRL) framework as the basis of our solution. By considering seven different goals, either Q-learning (QL) or double-action QL is employed to determine action decisions based on whether the other vehicles interact with the agent for that particular goal. Furthermore, a fusion function is proposed according to the importance of each goal before arriving to an overall but consistent action decision. This offers a powerful approach for dealing with demanding situations such as overtaking, particularly when a number of other vehicles are within the proximity of the agent and are traveling at different and varying speeds. A large number of overtaking cases have been simulated to demonstrate its effectiveness. From the results, it can be concluded that the proposed method is capable of the following: 1) making correct action decisions for overtaking; 2) avoiding collisions with other vehicles; 3) reaching the target at reasonable time; 4) keeping almost steady speed; and 5) maintaining almost steady heading angle. In addition, it should also be noted that the proposed method performs lane keeping well when not overtaking and lane changing effectively when overtaking is in progress.	Article	JUN 2011	10.1109/TITS.2011.2106158	[]	[]	-1	-1	-1
J	Ahmic, Kenan; Ultsch, Johannes; Brembeck, Jonathan; Winter, Christoph	Reinforcement Learning-Based Path following Control with Dynamics Randomization for Parametric Uncertainties in Autonomous Driving	2023	APPLIED SCIENCES-BASEL				Reinforcement learning-based controllers for safety-critical applications, such as autonomous driving, are typically trained in simulation, where a vehicle model is provided during the learning process. However, an inaccurate parameterization of the vehicle model used for training heavily influences the performance of the reinforcement learning agent during execution. This inaccuracy is either caused by changes due to environmental influences or by falsely estimated vehicle parameters. In this work, we present our approach of combining dynamics randomization with reinforcement learning to overcome this issue for a path-following control task of an autonomous and over-actuated robotic vehicle. We train three independent agents, where each agent experiences randomization for a different vehicle dynamics parameter, i.e., the mass, the yaw inertia, and the road-tire friction. We randomize the parameters uniformly within predefined ranges to enable the agents to learn an equally robust control behavior for all possible parameter values. Finally, in a simulation study, we compare the performance of the agents trained with dynamics randomization to the performance of an agent trained with the nominal parameter values. Simulation results demonstrate that the former agents obtain a higher level of robustness against model uncertainties and varying environmental conditions than the latter agent trained with nominal vehicle parameter values.	Article	MAR 2023	10.3390/app13063456	[]	[]	-1	-1	-1
J	Bautista-Montesano, Rolando; Bustamante-Bello, Rogelio; Ramirez-Mendoza, Ricardo A.	Explainable navigation system using fuzzy reinforcement learning	2020	INTERNATIONAL JOURNAL OF INTERACTIVE DESIGN AND MANUFACTURING - IJIDEM		1411	1428	Explainable outcomes in autonomous navigation have become crucial for drivers, other vehicles, as well as for pedestrians. Creating trustworthy strategies is mandatory for the integration of self-driving cars into quotidian environments. This paper presents the successful implementation of an explainable Fuzzy Deep Reinforcement Learning approach for autonomous vehicles based on the AWS DeepRacer (TM) platform. A model of the environment is created by transforming crisp values into linguistic variables. A fuzzy inference system is used to define the reward of the vehicle depending on its current state. Guidelines to define the actions and to improve performance of the reinforcement learning agent are given based on the characteristics of the existing hardware. The performance of the models is tested on tracks with distinctive properties using agents with different policies and action spaces, and shows explainable and successful navigation of the agent on diverse scenarios.[GRAPHICS].	Article; Early Access		10.1007/s12008-020-00717-1	[]	[]	-1	-1	-1
B	Ma, Xiaobai	Deep Reinforcement Learning Methods for Autonomous Driving Safety and Interactivity	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J	Le, M.; Huynh-The, T.; Do-Duy, T.; Vu, T.-H.; Hwang, W.-J.; Pham, Q.-V.	Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey [arXiv]	2023	Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey [arXiv]				The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.	Preprint	2023		[]	[]	-1	-1	-1
J	Fayyazi, Mojgan; Abdoos, Monireh; Phan, Duong; Golafrouz, Mohsen; Jalili, Mahdi; Jazar, Reza N.; Langari, Reza; Khayyam, Hamid	Real-time self-adaptive Q-learning controller for energy management of conventional autonomous vehicles	2023	EXPERT SYSTEMS WITH APPLICATIONS				Reducing emissions and energy consumption of autonomous vehicles is critical in the modern era. This paper presents an intelligent energy management system based on Reinforcement Learning (RL) for conventional autonomous vehicles. Furthermore, in order to improve the efficiency, a new exploration strategy is proposed to replace the traditional decayed epsilon-greedy strategy in the Q-learning algorithm associated with RL. Unlike tradi-tional Q-learning algorithms, the proposed self-adaptive Q-learning (SAQ-learning) can be applied in real-time. The learning capability of the controllers can help the vehicle deal with unknown situations in real-time. Nu-merical simulations show that compared to other controllers, Q-learning and SAQ-learning controllers can generate the desired engine torque based on the vehicle road power demand and control the air/fuel ratio by changing the throttle angle efficiently in real-time. Also, the proposed real-time SAQ-learning is shown to improve the operational time by 23% compared to standard Q-learning. Our simulations reveal the effectiveness of the proposed control system compared to other methods, namely dynamic programming and fuzzy logic methods.	Article; Early Access		10.1016/j.eswa.2023.119770	[]	[]	-1	-1	-1
B	Gilbert, Thomas J.	Modes of Deliberation in Machine Ethics	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
B	Wang, Yu	Trajectory Based Traffic Analysis and Control Utilizing Connected Autonomous Vehicles	2019						Dissertation/Thesis	Jan 01 2019		[]	[]	-1	-1	-1
J	Rafiq, Ghazala; Rafiq, Muhammad; Choi, Gyu Sang	Video description: A comprehensive survey of deep learning approaches	2023	ARTIFICIAL INTELLIGENCE REVIEW		13293	13372	Video description refers to understanding visual content and transforming that acquired understanding into automatic textual narration. It bridges the key AI fields of computer vision and natural language processing in conjunction with real-time and practical applications. Deep learning-based approaches employed for video description have demonstrated enhanced results compared to conventional approaches. The current literature lacks a thorough interpretation of the recently developed and employed sequence to sequence techniques for video description. This paper fills that gap by focusing mainly on deep learning-enabled approaches to automatic caption generation. Sequence to sequence models follow an Encoder-Decoder architecture employing a specific composition of CNN, RNN, or the variants LSTM or GRU as an encoder and decoder block. This standard-architecture can be fused with an attention mechanism to focus on a specific distinctiveness, achieving high quality results. Reinforcement learning employed within the Encoder-Decoder structure can progressively deliver state-of-the-art captions by following exploration and exploitation strategies. The transformer mechanism is a modern and efficient transductive architecture for robust output. Free from recurrence, and solely based on self-attention, it allows parallelization along with training on a massive amount of data. It can fully utilize the available GPUs for most NLP tasks. Recently, with the emergence of several versions of transformers, long term dependency handling is not an issue anymore for researchers engaged in video processing for summarization and description, or for autonomous-vehicle, surveillance, and instructional purposes. They can get auspicious directions from this research.	Article; Early Access		10.1007/s10462-023-10414-6	[]	[]	-1	-1	-1
C	Musau, Patrick; Hamilton, Nathaniel; Lopez, Diego Manzanas; Robinette, Preston; Johnson, Taylor T.	On Using Real-Time Reachability for the Safety Assurance of Machine Learning Controllers	2022	2022 IEEE INTERNATIONAL CONFERENCE ON ASSURED AUTONOMY (ICAA 2022)		1	10	"Over the last decade, advances in machine learning and sensing technology have paved the way for the belief that safe, accessible, and convenient autonomous vehicles may be realized in the near future. Despite the prolific competencies of machine learning models for learning the nuances of sensing, actuation, and control, they are notoriously difficult to assure. The challenge here is that some models, such as neural networks, are ""black box"" in nature, making verification and validation difficult, and sometimes infeasible. Moreover, these models are often tasked with operating in uncertain and dynamic environments where design time assurance may only be partially transferable. Thus, it is critical to monitor these components at runtime. One approach for providing runtime assurance of systems with unverified components is the simplex architecture, where an unverified component is wrapped with a safety controller and a switching logic designed to prevent dangerous behavior. In this paper, we propose the use of a real-time reachability algorithm for the implementation of such an architecture for the safety assurance of a 1/10 scale open source autonomous vehicle platform known as F1/10. The reachability algorithm (a) provides provable guarantees of safety, and (b) is used to detect potentially unsafe scenarios. In our approach, the need to analyze the underlying controller is abstracted away, instead focusing on the effects of the controller's decisions on the system's future states. We demonstrate the efficacy of our architecture through experiments conducted both in simulation and on an embedded hardware platform."	Proceedings Paper	2022	10.1109/ICAA52185.2022.00010	[]	[]	-1	-1	-1
B	Luo, Mulong	Hardware-Level Vulnerabilities and Support for Secure and Safe Cyber-Physical Systems	2023						Dissertation/Thesis	Jan 01 2023		[]	[]	-1	-1	-1
J	He, Xiangkun; Lv, Chen	Toward personalized decision making for autonomous vehicles: A constrained multi-objective reinforcement learning technique	2023	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Reinforcement learning promises to provide a state-of-the-art solution to the decision making problem of autonomous driving. Nonetheless, numerous real-world decision making problems involve balancing multiple conflicting or competing objectives. In addition, passengers may typically prefer to explore diversified driving modes through their specific preferences (i.e., relative importance of different objectives). Taking into account these demands, traditional reinforcement learning algorithms with applications in personalized self-driving vehicles remain challenging. Consequently, here we present a novel constrained multi-objective reinforcement learning technique for personalized decision making in autonomous driving, with the goal of learning a single model for Pareto optimal policies across the space of all possible user preferences. Specifically, a nonlinear constraint incorporating a user-specified preference and a vectorized action-value function is introduced to ensure both diversity in learned decision behaviors and efficient alignment between the user-specified preference and the corresponding optimal policy. Additionally, a constrained multi-objective actor-critic approach is advanced to approximate the Pareto optimal policies for any user-specified preferences while adhering to the nonlinear constraint. Finally, the proposed personalized decision making scheme for autonomous driving is assessed in a highway on-ramp merging scenario with dynamic traffic flows. The results demonstrate the effectiveness of our method by comparing it with classical and state-of-the-art baselines.	Article; Early Access		10.1016/j.trc.2023.104352	[]	[]	-1	-1	-1
J	Liu, Xiongqing; Jin, Yan	Reinforcement learning-based collision avoidance: impact of reward function and knowledge transfer	2020	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING		207	222	Collision avoidance for robots and vehicles in unpredictable environments is a challenging task. Various control strategies have been developed for the agent (i.e., robots or vehicles) to sense the environment, assess the situation, and select the optimal actions to avoid collision and accomplish its mission. In our research on autonomous ships, we take a machine learning approach to collision avoidance. The lack of available ship steering data of human ship masters has made it necessary to acquire collision avoidance knowledge through reinforcement learning (RL). Given that the learned neural network tends to be a black box, it is desirable that a method is available which can be used to design an agent's behavior so that the desired knowledge can be captured. Furthermore, RL with complex tasks can be either time consuming or unfeasible. A multi-stage learning method is needed in which agents can learn from simple tasks and then transfer their learned knowledge to closely related but more complex tasks. In this paper, we explore the ways of designing agent behaviors through tuning reward functions and devise a transfer RL method for multi-stage knowledge acquisition. The computer simulation-based agent training results have shown that it is important to understand the roles of each component in a reward function and the various design parameters in transfer RL. The settings of these parameters are all dependent on the complexity of the tasks and the similarities between them.	Article	MAY 2020	10.1017/S0890060420000141	[]	[]	-1	-1	-1
J		RI: Small: Efficient Reinforcement Learning for Generic Large-Scale Tasks	2009					Recent advances in autonomous agents research are pushing our society closer to the brink of the widespread adoption of autonomous agents in everyday life. Applications that incorporate agents already exist or are quickly emerging, such as domestic robots, autonomous vehicles, and financial management agents. Reinforcement learning (RL) of sequential decision making is an important paradigm for enabling the widespread deployment of autonomous agents. However, a few notable successes notwithstanding, state-of-the-art reinforcement learning algorithms are not yet fully capable of addressing generic large-scale applications. <br/><br/>This project is advancing in four directions to scale-up application of RL systems. Specifically, the project is (1) developing algorithms to automatically structure the input, output, and policy representations for learning; (2) introducing parallelizable reinforcement learning algorithms so as to exploit modern parallel architectures; (3) unifying abstraction and hierarchical reasoning with model-based learning for the purpose of enabling intelligent exploration of large-scale environments; and (4) enabling reinforcement learning algorithms to benefit from low-bandwidth interactions with human users. Finally, we intend to unify the four research thrusts above into a single algorithm and conduct empirical evaluation on real-world/large-scale applications, to include biped robot balancing and walking, robot soccer in simulation and with real robots, and a full-size autonomous vehicle capable of planning paths in an urban environment.<br/><br/>In addition to research advances and implications for improving national infrastructure, the project will contribute to undergraduate and graduate curriculum development.	Awarded Grant	Sep 01 2009		[]	[]	-1	-1	-1
J	Zhu, Wei; Hayashibe, Mitsuhiro	Autonomous Navigation System in Pedestrian Scenarios Using a Dreamer-Based Motion Planner	2023	IEEE ROBOTICS AND AUTOMATION LETTERS		3835	3842	Navigation among pedestrians is a crucial capability of service robots; however, it is a challenge to manage time-varying environments stably. Recent deep reinforcement learning (DRL)-based approaches to crowd navigation have yielded numerous promising applications. However, they rely heavily on initial imitation learning and colossal positive datasets. Moreover, the difficulties in accurately localizing robots, detecting and tracking humans, representing and generalizing reciprocal human relationships restrict their deployment in real-world problems. We propose a Dreamer-based motion planner for collision-free navigation in diverse pedestrian scenarios. Our RL framework can completely learn from zero experience via a model-based DRL. The robot and humans are first projected onto a map, which is subsequently decoded into low-dimensional latent state. A predictive dynamic model in the latent space is jointly created to efficiently optimize the navigation policy. Additionally, we leverage the techniques of system identification, domain randomization, clustering and LiDAR SLAM for practical deployment. Simulation ablations and real implementations demonstrate that our motion planner outperforms state-of-the-art methods, and that the navigation system can be physically implemented in the real world.	Article	JUN 2023	10.1109/LRA.2023.3273514	[]	[]	-1	-1	-1
J	Kovari, Balint; Hegedus, Ferenc; Becsi, Tamas	Design of a Reinforcement Learning-Based Lane Keeping Planning Agent for Automated Vehicles	2020	APPLIED SCIENCES-BASEL				Featured ApplicationThe presented method can be used as a real-time trajectory following algorithm for autonomous vehicles using prediction based on lookahead information.Reinforcement learning-based approaches are widely studied in the literature for solving different control tasks for Connected and Autonomous Vehicles, from which this paper deals with the problem of lateral control of a dynamic nonlinear vehicle model, performing the task of lane-keeping. In this area, the appropriate formulation of the goals and environment information is crucial, for which the research outlines the importance of lookahead information, enabling to accomplish maneuvers with complex trajectories. Another critical part is the real-time manner of the problem. On the one hand, optimization or search based methods, such as the presented Monte Carlo Tree Search method, can solve the problem with the trade-off of high numerical complexity. On the other hand, single Reinforcement Learning agents struggle to learn these tasks with high performance, though they have the advantage that after the training process, they can operate in a real-time manner. Two planning agent structures are proposed in the paper to resolve this duality, where the machine learning agents aid the tree search algorithm. As a result, the combined solution provides high performance and low computational needs.	Article	OCT 2020	10.3390/app10207171	[]	[]	-1	-1	-1
J	Sovrano, Francesco; Raymond, Alex; Prorok, Amanda	Explanation-Aware Experience Replay in Rule-Dense Environments	2022	IEEE ROBOTICS AND AUTOMATION LETTERS		898	905	Human environments are often regulated by explicit and complex rulesets. Integrating Reinforcement Learning (RL) agents into such environments motivates the development of learning mechanisms that perform well in rule-dense and exception-ridden environments such as autonomous driving on regulated roads. In this letter, we propose a method for organising experience by means of partitioning the experience buffer into clusters labelled on a per-explanation basis. We present discrete and continuous navigation environments compatible with modular rulesets and 9 learning tasks. For environments with explainable rulesets, we convert rule-based explanations into case-based explanations by allocating state-transitions into clusters labelled with explanations. This allows us to sample experiences in a curricular and task-oriented manner, focusing on the rarity, importance, and meaning of events. We label this concept Explanation-Awareness (XA). We perform XA experience replay (XAER) with intra and inter-cluster prioritisation, and introduce XA-compatible versions of DQN, TD3, and SAC. Performance is consistently superior with XA versions of those algorithms, compared to traditional Prioritised Experience Replay baselines, indicating that explanation engineering can he used in lieu of reward engineering for environments with explainable features.	Article	APR 2022	10.1109/LRA.2021.3135927	[]	[]	-1	-1	-1
J		US-German Collaboration: Strategy Change in Cognitive Biological and Technical Systems	2013					The ability for strategy change, i.e., the change in action selection and action planning while an overarching goal is maintained is a fundamental, but still barely understood capability of cognitive systems. Sudden transitions are well documented in neurophysiological and cognitive experimental data, but application of the underlying theory of the spatio-temporal neurodynamics is yet to be done.  Current physiological and theoretical frameworks of learning focus on incremental learning (as exemplified the reinforcement learning).  This project aims at improved understanding of the nature and functional role of abrupt, large-scale state transitions in complex neuronal systems as the basis of cognitive strategy change. We exploit our experimental and theoretical understanding of a particular rodent learning model to simulate the neuronal mechanisms of instantaneous strategy change.  The investigators will develop an algorithmic formulation of the neurocomputational principles, and apply it in the engineering example of autonomous vehicle control.  This interdisciplinary project is based on the complementary and synergistic expertise of the team members in optimization theory and both theoretical and experimental neuroscience.<br/><br/>This project arises from our deep understanding of the rapid biological and cognitive processes displayed by strategy changes in coping with changing environments.  This research on decision making in human and animal brains provides a platform for developing robust decision support systems that operate in dynamically changing scenarios in the style of brains.  Detailed analysis of the mechanisms underlying rapid strategy change in brains will allow both this research team and other groups to equip various man-made systems with the fundamental property of insightful cognition.  This work addresses important societal needs by creating the foundations of cognitive engineering systems supporting emergency response to natural disasters and cyber security threats by adversaries, as well as optimized control of autonomous vehicles under complex operating conditions.<br/><br/>This award is being co-funded by NSF's Office of International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF).	Awarded Grant	Oct 01 2013		[]	[]	-1	-1	-1
B	Vinitsky, Eugene	From Sim-to-Real: Learning and Deploying Autonomous Vehicle Controllers That Improve Transportation Metrics	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Saha, Olimpiya	Fast, Real-Time Robot Navigation in Initially Unknown Environments via Cross-Domain Transfer Learning of Options	2018						Dissertation/Thesis	Jan 01 2018		[]	[]	-1	-1	-1
C	Garzon, Mario; Spalanzani, Anne	Game theoretic decision making for autonomous vehicles' merge manoeuvre in high traffic scenarios	2019	2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	3448	3453	This paper presents a game theoretic decision making process for autonomous vehicles. Its goal is to provide a solution for a very challenging task: the merge manoeuvre in high traffic scenarios. Unlike previous approaches, the proposed solution does not rely on vehicle-to-vehicle communication or any specific coordination, moreover, it is capable of anticipating both the actions of other players and their reactions to the autonomous vehicle's movements. \The game used is an iterative, multi-player level-k model, which uses cognitive hierarchy reasoning for decision making and has been proved to correctly model human decisions in uncertain situations. This model uses reinforcement learning to obtain a near-optimal policy, and since it is an iterative model, it is possible to define a goal state so that the policy tries to reach it.To test the decision making process, a kinematic simulation was implemented. The resulting policy was compared with a rule-based approach. The experiments show that the decision making system is capable of correctly performing the merge manoeuvre, by taking actions that require reactions of the other players to be successfully completed.	Proceedings Paper	2019		[]	[]	-1	-1	-1
B	Bhadani, Rahul Kumar	Design and Synthesis of Controllers for Societal-Scale Cyber-Physical Systems	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Lee, Dongsu; Kwon, Minhae	ADAS-RL: Safety learning approach for stable autonomous driving	2022	ICT EXPRESS		479	483	Stability is the most significant component of an autonomous driving system, affecting both the lives of drivers and pedestrians and traffic flow. Reinforcement learning (RL) is a representative technology used in autonomous driving, but it has challenges because it is based on trial and error. In this letter, we propose an efficient learning approach for stable autonomous driving. The proposed deep reinforcement learning based approach can address the partially observable scenario in mixed traffic which includes both autonomous vehicles and human-driven vehicles. Simulation results show that the proposed model outperforms the control-theoretic and vanilla RL approaches. Furthermore, we confirm the effect of the sync-penalty, which teaches the agent about unsafe decisions without experiencing the accidents. (C) 2022 The Author(s). Published by Elsevier B.V. on behalf of The Korean Institute of Communications and Information Sciences.	Article; Early Access		10.1016/j.icte.2022.05.004	[]	[]	-1	-1	-1
C	Chung, Seung-Hwan; Kong, Seung-Hyun; Cho, Sangjae; Nahrendra, I. Made Aswin	Segmented Encoding for Sim2Real of RL-based End-to-End Autonomous Driving	2022	2022 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	1290	1296	Among the challenges in the recent research of end-to-end (E2E) driving, interpretability and distribution shift in the simulation-to-real (Sim2Real) have drawn considerable attention. Because of low interpretability, we cannot clearly explain the causal relationship between the input image and the control actions by the network. Moreover, the distribution shift problem in Sim2Real degrades the driving performance of the policy in the realworld deployment. In this paper, we propose a segmentation-based classwise disentangled latent encoding algorithm to cope with the two challenges. In the proposed algorithm, multi-class segmentation transfers RGB images in both simulation and real environments to the same domain, while preserving the necessary information of objects of primary classes, such as pedestrian, road, and cars, for driving decisions. Besides, in the class-wise disentangled latent encoding, segmented images are encoded to a latent vector, which improves the interpretability significantly, since the state input has a structured format. The interpretability improvement is testified by the t-stochastic neighbor embedding, image reconstruction and the causal relationship between the real images and the control actions. We deploy the driving policy trained in the simulation directly to an autonomous vehicle platform and show, to the best of our knowledge, the first demonstration of the RL-based E2E autonomous in various real environments.	Proceedings Paper	2022	10.1109/IV51971.2022.9827374	[]	[]	-1	-1	-1
C	Xiong, Lu; Cao, Lei; Leng, Bo; Liu, Ming	Noncooperative-Game-Based Intelligent Vehicle Decision ethod fo Lane-Changing Interactive Behavior	2022	2022 34TH CHINESE CONTROL AND DECISION CONFERENCE, CCDC	Chinese Control and Decision Conference	28	34	In the future transportation, it is a foreseeable trend that human-driven vehicles and autonomous vehicles will coexist and interact on roads for a long time. how to integrate autonomous vehicles into human drivers' traffic ecology and make behavioral decisions and predictions in the complex environment considering vehicle-vehicle interaction is worthy of deep consideration. Therefore, the proposed game theoretic method is applied to automatic lane-changing scenarios. Then, the safety risk, ride comfort and traffic efficiency of vehicles in game are quantitatively evaluated as cost functions. Specifically, the driving maneuvers of interactive agents could be predicted by reasoning, and the optimal strategy is obtained by calculating Nash equilibrium. Finally, simulations are conducted using Commonroad under typical driving conditions. The results illustrate that our model could help autonomous vehicles make reasonable and explainable lane-changing decision when interacting with heterogeneous agents, demonstrating the feasibility and reliability of our proposed approach.	Proceedings Paper	2022	10.1109/CCDC55256.2022.10033451	[]	[]	-1	-1	-1
J	Wiberg, Viktor; Wallin, Erik; Nordfjell, Tomas; Servin, Martin	Control of Rough Terrain Vehicles Using Deep Reinforcement Learning	2022	IEEE ROBOTICS AND AUTOMATION LETTERS		390	397	We explore the potential to control terrain vehicles using deep reinforcement in scenarios where human operators and traditional control methods are inadequate. This letter presents a controller that perceives, plans, and successfully controls a 16-tonne forestry vehicle with two frame articulation joints, six wheels, and their actively articulated suspensions to traverse rough terrain. The carefully shaped reward signal promotes safe, environmental, and efficient driving, which leads to the emergence of unprecedented driving skills. We test learned skills in a virtual environment, including terrains reconstructed from high-density laser scans of forest sites. The controller displays the ability to handle obstructing obstacles, slopes up to 27 degrees, and a variety of natural terrains, all with limited wheel slip, smooth, and upright traversal with intelligent use of the active suspensions. The results confirm that deep reinforcement learning has the potential to enhance control of vehicles with complex dynamics and high-dimensional observation data compared to human operators or traditional control methods, especially in rough terrain.	Article	JAN 2022	10.1109/LRA.2021.3126904	[]	[]	-1	-1	-1
B	Zhang, Ethan	A Predictive-Prescriptive Safety Framework at Intersections in a Connected Vehicle Environment	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	You, Changxi; Lu, Jianbo; Filev, Dimitar; Tsiotras, Panagiotis	Autonomous Planning and Control for Intelligent Vehicles in Traffic	2020	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		2339	2349	"This paper addresses the trajectory planning problem for autonomous vehicles in traffic. We build a stochastic Markov decision process (MDP) model to represent the behaviors of the vehicles. This MDP model takes into account the road geometry and is able to reproduce more diverse driving styles. We introduce a new concept, namely, the ""dynamic cell,"" to dynamically modify the state of the traffic according to different vehicle velocities, driver intents (signals), and the sizes of the surrounding vehicles (i.e., truck, sedan, and so on). We then use Bezier curves to plan smooth paths for lane switching. The maximum curvature of the path is enforced via certain design parameters. By designing suitable reward functions, different desired driving styles of the intelligent vehicle can be achieved by solving a reinforcement learning problem. The desired driving behaviors (i.e., autonomous highway overtaking) are demonstrated with an in-house developed traffic simulator."	Article	JUN 2020	10.1109/TITS.2019.2918071	[]	[]	-1	-1	-1
J	Zhang, Z.; Huang, D.; Huang, C.; Hu, L.; Du, R.	TD3 Algorithm Improving and Lane-merging Strategy Learning for Autonomous Vehicles	2023	Journal of Mechanical Engineering		224	34	To enhance the comprehensive performance of automotive lane-merging, the Q-value estimation method of twin delayed deep deterministic policy gradient(TD3) algorithm and the reward function are improved. The automotive lane-merging model is formalized as the Markov decision process, and the influences of Q-value underestimated by TD3 algorithm on lane-merging strategy are analyzed. A Q-value estimation method based on weighted average of sample variance is proposed to enhance the Q-value estimation accuracy, when two Q-value estimation samples are obtained by performing Monte Carlo dropout on the dual target critic network. With giving priority to the completion of the lane-merging, a more perfect reward function is designed considering the safety, comfort and traffic efficiency. Based on the improved TD3 algorithm and the reward function, a lane-merging strategy of autonomous vehicles is learned and verified with BARK simulator. The results show that the improved TD3 algorithm significantly enhances the accuracy of Q-value estimation. Combined with the established reward function, the safety and ride comfort of lane-merging are improved while ensuring traffic efficiency. Â© 2023 Journal of Mechanical Engineering.	Journal Paper	2023	10.3901/JME.2023.08.224	[]	[]	-1	-1	-1
J	Zhang, Meng; Abbas-Turki, Abdeljalil; Mualla, Yazan; Koukam, Abderrafiaa; Tu, Xiaowei	Coordination Between Connected Automated Vehicles and Pedestrians to Improve Traffic Safety and Efficiency at Industrial Sites	2022	IEEE ACCESS		68029	68041	Transportation in controlled industrial sites provides a conducive environment for technologies of Connected Automated Vehicles (CAV). Recent studies show that safe and efficient road sharing between CAVs and pedestrians is challenging. Besides safety issues, a significant loss of time occurs when pedestrians cross a stream of CAVs. Currently, many techniques have been employed to improve the coordination between CAVs and pedestrians. They focus on pedestrian detection, display of the intention of CAVs, and cooperative collision avoidance. However, one of the most significant sources of information that the pedestrian uses for her/his decision-making is the speed profile of CAVs. This paper aims to provide a safe and efficient pedestrian crossing at industrial sites through communicative crossing behavior. To this end, a suitable speed profile of the CAV is designed by assuming that pedestrians and CAV play a cooperative game to move as close as possible to their desired speed. First, a system analysis is proposed to derive the optimal decision and trajectory for each agent. Then, Deep Reinforcement Learning (DRL) is used to control the longitudinal speed of the CAVs. Compared with Model Predictive Control approach, DRL allows coping with unforeseeable pedestrian behaviors (e.g. long reaction time, varying ideal speed, stop in the middle, etc.). Simulations and experiments with real human testers based on immersive hamlet are performed. Results show that the proposed speed profile outperforms significantly the collision avoidance approach.	Article	2022	10.1109/ACCESS.2022.3185734	[]	[]	-1	-1	-1
J	Hou, Xiaohui; Zhang, Junzhi; He, Chengkun; Ji, Yuan; Zhang, Junfeng; Han, Jinheng	Autonomous driving at the handling limit using residual reinforcement learning	2022	ADVANCED ENGINEERING INFORMATICS				While driving a vehicle safely at its handling limit is essential in autonomous vehicles in Level 5 autonomy, it is a very challenging task for current conventional methods. Therefore, this study proposes a novel controller of trajectory planning and motion control for autonomous driving through manifold corners at the handling limit to improve the speed and shorten the lap time of the vehicle. The proposed controller innovatively combines the advantages of conventional model-based control algorithm, model-free reinforcement learning algorithm, and prior expert knowledge, to improve the training efficiency for autonomous driving in extreme conditions. The reward shaping of this algorithm refers to the procedure and experience of race training of professional drivers in real time. After training on track maps that exhibit different levels of difficulty, the proposed controller implemented a superior strategy compared to the original reference trajectory, and can to other tougher maps based on the basic driving knowledge learned from the simpler map, which verifies its superiority and exten-sibility. We believe this technology can be further applied to daily life to expand the application scenarios and maneuvering envelopes of autonomous vehicles.	Article; Early Access		10.1016/j.aei.2022.101754	[]	[]	-1	-1	-1
J	Mavrogiannis, Angelos; Chandra, Rohan; Manocha, Dinesh	B-GAP: Behavior-Rich Simulation and Navigation for Autonomous Driving	2022	IEEE ROBOTICS AND AUTOMATION LETTERS		4718	4725	We address the problem of ego-vehicle navigation in dense simulated traffic environments populated by road agents with varying driver behaviors. Navigation in such environments is challenging due to unpredictability in agents' actions caused by their heterogeneous behaviors. We present a new simulation technique consisting of enriching existing traffic simulators with behavior-rich trajectories corresponding to varying levels of aggressiveness. We generate these trajectories with the help of a driver behavior modeling algorithm. We then use the enriched simulator to train a deep reinforcement learning (DRL) policy that consists of a set of high-level vehicle control commands and use this policy at test time to perform local navigation in dense traffic. Our policy implicitly models the interactions between traffic agents and computes safe trajectories for the ego-vehicle accounting for aggressive driver maneuvers such as overtaking, over-speeding, weaving, a nd sudden lane changes. Our enhanced behavior-rich simulator can be used for generating datasets that consist of trajectories corresponding to diverse driver behaviors and traffic densities, and our behavior-based navigation scheme can be combined with state-of-the-art navigation algorithms.	Article	APR 2022	10.1109/LRA.2022.3152594	[]	[]	-1	-1	-1
J	Siebinga, Olger; Zgonnikov, Arkady; Abbink, David	A Human Factors Approach to Validating Driver Models for Interaction-aware Automated Vehicles	2022	ACM TRANSACTIONS ON HUMAN-ROBOT INTERACTION				A major challenge for autonomous vehicles is interacting with other traffic participants safely and smoothly. A promising approach to handle such traffic interactions is equipping autonomous vehicles with interaction-aware controllers (IACs). These controllers predict how surrounding human drivers will respond to the autonomous vehicle's actions, based on a driver model. However, the predictive validity of driver models used in IACs is rarely validated, which can limit the interactive capabilities of IACs outside the simple simulated environments in which they are demonstrated. In this article, we argue that besides evaluating the interactive capabilities of IACs, their underlying driver models should be validated on natural human driving behavior. We propose a workflow for this validation that includes scenario-based data extraction and a two-stage (tactical/operational) evaluation procedure based on human factors literature. We demonstrate this workflow in a case study on an inverse-reinforcement-learning-based driver model replicated from an existing IAC. This model only showed the correct tactical behavior in 40% of the predictions. The model's operational behavior was inconsistent with observed human behavior. The case study illustrates that a principled evaluation workflow is useful and needed. We believe that our workflow will support the development of appropriate driver models for future automated vehicles.	Article	DEC 2022	10.1145/3538705	[]	[]	-1	-1	-1
J	Mazouchi, Majid; Nageshrao, Subramanya P. P.; Modares, Hamidreza	A Risk-Averse Preview-Based <i>Q</i> -Learning Algorithm: Application to Highway Driving of Autonomous Vehicles	2023	IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY		1803	1818	A risk-averse preview-based Q-learning planner is presented for navigation of autonomous vehicles (AVs). To this end, the multilane road ahead of a vehicle is represented by a finite-state nonstationary Markov decision process (MDP). A risk assessment unit module is then presented, which leverages the preview information provided by sensors along with a stochastic reachability module to assign reward values to the MDP states and update them as scenarios develop. A sampling-based risk averse preview-based Q-learning algorithm is finally developed, which generates samples using the preview information and reward function to learn risk-averse optimal planning strategies without actual interaction with the environment. The risk factor is imposed on the objective function to avoid fluctuation of the Q values, which can jeopardize the vehicle's safety and/or performance. The overall hybrid automaton model of the system is leveraged to develop a feasibility check unit module that detects unfeasible plans and enables the planner system to react proactively to the changes of the environment. Finally, to verify the efficiency of the presented algorithm, its implementation on two highway driving scenarios of an AV in a varying traffic density is considered.	Article; Early Access		10.1109/TCST.2023.3245824	[]	[]	-1	-1	-1
B	Wright, Matthew Abbott	Studies on Complex and Connected Vehicle Traffic Networks	2019						Dissertation/Thesis	Jan 01 2019		[]	[]	-1	-1	-1
J	Cao, Zhong; Xu, Shaobing; Jiao, Xinyu; Peng, Huei; Yang, Diange	Trustworthy safety improvement for autonomous driving using reinforcement learning	2022	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Reinforcement learning (RL) can learn from past failures and has the potential to provide selfimprovement ability and higher-level intelligence. However, the current RL algorithms still suffer from challenges in reliability, especially compared to the rule/model-based algorithms that are pre-engineered, human-input intensive, but widely used in autonomous vehicles. To take advantages of both the RL and rule-based algorithms, this work aims to design a decision-making framework that leverages RL and use an existing rule-based policy as its performance lower bound. In this way, the final policy remains the potential of self-learning, while guaranteeing a better system performance compared with the integrated rule-based policy. Such a decision making framework is called trustworthy improvement RL (TiRL). The basic idea is to make the RL policy iteration process synchronously estimate the given rule-based policy's value function. AV will then use the RL policy to drive only in the cases where the RL has learned a better policy, i.e., a higher policy value. This work takes highway safe driving as the case study. The results are obtained through more than 42,000 km driving in stochastic simulated traffic, and calibrated by naturalistic driving data. The TiRL planner is given two typical rule-based highway-driving policies for comparison. The results show that the TiRL can outperform the given arbitrary rule based driving policy. In summary, the proposed TiRL can leverage the learning-based method in stochastic and emergent scenarios, while having a trustworthy safety improvement from the existing rule-based policies.	Article; Early Access		10.1016/j.trc.2022.103656	[]	[]	-1	-1	-1
J	Pan Feng; Bao Hong	Research progress of automatic driving control technology based on reinforcement learning	2021	Journal of Image and Graphics		28	35	Research on fully automatic driving has been largely spurred by some important international challenges and competitions,such as the well-known Defense Advanced Research Projects Agency Grand Challenge held in 2005. Selfdriving cars and autonomous vehicles have migrated from laboratory development and testing conditions to driving on public roads. Self-driving cars are autonomous decision-making systems that process streams of observations coming from different on-board sources,such as cameras,radars,lidars,ultrasonic sensors,global positioning system units,and/or inertial sensors. The development of autonomous vehicles offers a decrease in road accidents and traffic congestions. Most driving scenarios can be simply solved with classical perception,path planning,and motion control methods. However,the remaining unsolved scenarios are corner cases where traditional methods fail. In the past decade,advances in the field of artificial intelligence (AI) and machine learning (ML) have greatly promoted the development of autonomous driving. Autonomous driving is a challenging application domain for ML . ML methods can be divided into supervised learning,unsupervised learning,and reinforcement learning (RL) . RL is a family of algorithms that allow agents to learn how to act in different situations. In other words,a map or a policy is established from situations (states) to actions to maximize a numerical reward signal. Most autonomous vehicles have a modular hierarchical structure and can be divided into four components or four layers,namely,perception,decision making,control,and actuator. RL is suitable for decision making and control in complex traffic scenarios to improve the safety and comfort of autonomous driving. Traditional controllers utilize an a priori model composed of fixed parameters. When robots or other autonomous systems are used in complex environments,such as driving,traditional controllers cannot foresee every possible situation that the system has to cope with. An RL controller is a learning controller and uses training information to learn their models over time. With every gathered batch of training data,the approximation of the true system model becomes accurate. Deep neural networks have been applied as function approximators for RL agents,thereby allowing agents to generalize knowledge to new unseen situations,along with new algorithms for problems with continuous state and action spaces. This paper mainly introduces the current status and progress of the application of RL methods in autonomous driving control. This paper consists of five sections. The first section introduces the background of autonomous driving and some basic knowledge about ML and RL. The second section briefly describes the architecture of autonomous driving framework. The control layer is an important part of an autonomous vehicle and has always been a key area of autonomous driving technology research. The control system of autonomous driving mainly includes lateral control and longitudinal control,namely,steering control and velocity control. Lateral control deals with the path tracking problem,and longitudinal control deals with the problem of tracking the reference speed and keeping a safe distance from the preceding vehicle. The third section introduces the basic principles of RL methods and focuses on the current research status of RL in autonomous driving control. RL algorithms are based on Markov decision process and aim to learn mapping from situations to actions to maximize a scalar reward or reinforcement signal. RL is a new and extremely old topic in AI. It gradually became an active and identifiable area of ML in 1980 s. Q-learning is a widely used RL algorithm.	Review	2021		[]	[]	-1	-1	-1
J	Lin, Jie; Huang, Siqi; Zhang, Hanlin; Yang, Xinyu; Zhao, Peng	A Deep-Reinforcement-Learning-Based Computation Offloading With Mobile Vehicles in Vehicular Edge Computing	2023	IEEE INTERNET OF THINGS JOURNAL		15501	15514	Vehicular edge networks involve edge servers that are close to mobile devices to provide extra computation resource to complete the computation tasks of mobile devices with low latency and high reliability. Considerable efforts on computation offloading in vehicular edge networks have been developed to reduce the energy consumption and computation latency, in which roadside units (RSUs) are usually considered as the fixed edge servers (FESs). Nonetheless, the computation offloading with considering mobile vehicles as mobile edge servers (MESs) in vehicular edge networks still needs to be further investigated. To this end, in this article, we propose a Deep-Reinforcement-Learning-based computation offloading with mobile vehicles in vehicular edge computing, namely, Deep-Reinforcement-Learning-based computation offloading scheme (DRL-COMV), in which some vehicles (such as autonomous vehicle) are deployed and considered as the MESs that move in vehicular edge networks and cooperate with FESs to provide extra computation resource for mobile devices, in order to assist in completing the computation tasks of these mobile devices with great Quality of Experience (QoE) (i.e., low latency) for mobile devices. Particularly, the computation offloading model with considering both mobile and FESs is conducted to achieve the computation tasks offloading through vehicle-to-vehicle (V2V) communications, and a collaborative route planning is considered for these MESs to move in vehicular edge networks with objective of improving efficiency of computation offloading. Then, a Deep-Reinforcement-Learning approach with designing rational reward function is proposed to determine the effective computation offloading strategies for multiple mobile devices and multiple edge servers with objective of maximizing both QoE (i.e., low latency) for mobile devices. Through performance evaluations, our results show that our proposed DRL-COMV scheme can achieve a great convergence and stability. Additionally, our results also demonstrate that our DRL-COMV scheme also can achieve better both QoE and task offloading requests hit ratio for mobile devices in comparison with existing approaches (i.e., DDPG, IMOPSOQ, and GABDOS).	Article	SEPT 1 2023	10.1109/JIOT.2023.3264281	[]	[]	-1	-1	-1
J	Qian, Yubin; Feng, Song; Hu, Wenhao; Wang, Wanqiu	Obstacle avoidance planning of autonomous vehicles using deep reinforcement learning	2022	ADVANCES IN MECHANICAL ENGINEERING				Obstacle avoidance path planning in a dynamic circumstance is one of the fundamental problems of autonomous vehicles, counting optional maneuvers: emergency braking and active steering. This paper proposes emergency obstacle avoidance planning based on deep reinforcement learning (DRL), considering safety and comfort. Firstly, the vehicle emergency braking and lane change processes are analyzed in detail. A graded hazard index is defined to indicate the degree of the potential risk of the current vehicle movement. The longitudinal distance and lateral waypoint models are established, including the comfort deceleration and stability coefficient considerations. Simultaneously, a fuzzy PID controller is installed to track to satisfy the stability and feasibility of the path. Then, this paper proposes a DRL process to determine the obstacle avoidance plan. Mainly, multi-reward functions are designed for different collisions, corresponding penalties for longitudinal rear-end collisions, and lane-changing side collisions based on the safety distance, comfort reward, and safety reward. Apply a special DRL method-DQN to release the planning program. The difference is that the long and short-term memory (LSTM) layer is utilized to solve incomplete observations and improve the efficiency and stability of the algorithm in a dynamic environment. Once the policy is practiced, the vehicle can automatically perform the best obstacle avoidance maneuver in an emergency, improving driving safety. Finally, this paper builds a simulated environment in CARLA and is trained to evaluate the effectiveness of the proposed algorithm. The collision rate, safety distance difference, and total reward index indicate that the collision avoidance path is generated safely, and the lateral acceleration and longitudinal velocity satisfy the comfort requirements. Besides, the method proposed in this paper is compared with traditional DRL, which proves the beneficial performance in safety and efficiency.	Article	DEC 2022	10.1177/16878132221139661	[]	[]	-1	-1	-1
J	Masmoudi, Mehdi; Friji, Hamdi; Ghazzai, Hakim; Massoud, Yehia	A Reinforcement Learning Framework for Video Frame-Based Autonomous Car-Following	2021	IEEE OPEN JOURNAL OF INTELLIGENT TRANSPORTATION SYSTEMS		111	127	Car-following theory has received considerable attention as a core component of Intelligent Transportation Systems. However, its application to the emerging autonomous vehicles (AVs) remains an unexplored research area. AVs are designed to provide convenient and safe driving by avoiding accidents caused by human errors. They require advanced levels of recognition of other drivers' driving-style. With car-following models, AVs can use their built-in technology to understand the environment surrounding them and make real-time decisions to follow other vehicles. In this paper, we design an end-to-end car-following framework for AVs using automated object detection and navigation decision modules. The objective is to allow an AV to follow another vehicle based on Red Green Blue Depth (RGB-D) frames. We propose to employ a joint solution involving the You Look Once version 3 (YOLOv3) object detector to identify the leader vehicle and other obstacles and a reinforcement learning (RL) algorithm to navigate the self-driving vehicle. Two RL algorithms, namely Q-learning and Deep Q-learning have been investigated. Simulation results show the convergence of the developed models and investigate their efficiency in following the leader. It is shown that, with video frames only, promising results are achieved and that AVs can adopt a reasonable car-following behavior.	Article	2021	10.1109/OJITS.2021.3083201	[]	[]	-1	-1	-1
J	Wang, Mei; Ma, Chen; Li, Zhanli; Zhang, Siming; Li, Yuancheng	Alertness Estimation Using Connection Parameters of the Brain Network	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		25448	25457	Alertness mechanism of unmanned monitoring vehicles to environment is important. Especially, the vigilance modeling of underground security robots has a particularly significance because the underground is a dangerous environment. However, there is no a mature methodology for the alertness computation. In this work, four parts of the alertness estimation are focused. First, an autonomous robot alertness mechanism framework is proposed by using the deep reinforcement learning model of the human alertness mechanism. Second, a fast K-T filtering algorithm is developed to eliminate the multiple noises of the electroencephalograph (EEG) signals by the blind source separation and the adjustable Q factor wavelet transform. Third, the description problem of the directed interaction stability of the cortical EEG signals is solved by the ensemble empirical mode decomposition and the directional transfer function. Fourth, the human alertness estimation is explored by using the support vector regression of the dynamically spatial-temporal brain network connection parameters. Experiments show that, the mean square error and the determination coefficient of the explored alertness estimation are respectively 0.115 and 0.8337. Compared with the scalp EEG alertness estimation, it has a better performance because the mean square error is decreased by 0.0684, and the determination coefficient is increased by 0.023.	Article; Early Access		10.1109/TITS.2021.3124372	[]	[]	-1	-1	-1
J		Effective and Semantic Communication in Multi-Agent Reinforcement Learning	2021					My project focuses on designing goal-oriented communication frameworks and systems using machine learning optimisation techniques.The communication problem can be divided into 3 levels:1. Technical problem: How accurately can the symbols of communication be transmitted?2. Semantic problem: How precisely do the transmitted symbols convey the desired meaning?3. Effectiveness problem: How effectively does the received meaning affect conduct in the desired way?The leading prevailing paradigm is the technical problem perspective. Consider streaming visual media or guiding a remote-controlled rover. Current approaches to communications consider a layered strategy. First, the message (for instance video frame or a rover command) is mapped to a bit pattern to remove redundancy. Next, the compressed bit pattern is protected against channel distortion with an error-correcting code. Finally, the protected bits are mapped to channel symbols for transmission. After the transmission, the process is reversed at the receiver. Each step of this process has been extensively researched in the last 80 years. This method has a significant advantage - it allows for much simpler analysis at each stage. The problem is broken into subproblems that are more manageable to tackle.This approach is optimal for specific sources and long enough block codes - it cannot be beaten asymptotically. However, in general, this approach is not flawless. Communication is rarely the goal in itself. Instead, it is used to achieve some other end. Thus, the success of communication should be measured in the context of the overall objective. That is the main focus of my project - semantic and effective communication problems. The modular system fails to exploit the interactions, dependencies, and correlations between the steps. However, each of these stages is complex by itself. Thus, doing away with the modular approach is not feasible in a straightforward analytic manner. That is why the current advancements in statistical methods such as machine learning are especially promising. The ability to learn inductively from examples allows for joining the modules of communication into a single system. However, what constitutes the desired behaviour is not always apparent. Thus, another framework is introduced. Reinforcement learning is the set of algorithms that allows for learning complex behaviours through interactions with an environment. By introducing realistic communication channels, we can extend those methods to allow for learning of the communication schemes themselves jointly with the desired conduct. This framework can be extended to include multiple agents interacting and learning. This study is relevant to remote control problems, drone swarm navigation, coordinated autonomous vehicle driving, distributed learning, or industrial internet of things where the number of independent actors need to coordinate to achieve a common goal.Relevant EPSRC research areas: Artificial intelligence technologies, ICT networks and distributed systems, Digital signal processing, Statistics and applied probability.	Awarded Grant	Sep 30 2021		[]	[]	-1	-1	-1
B	Leu, Jessica En Shiuan	Designing Integrated Strategies for Modularized Robotic Systems in Uncertain Environments	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Maia, Francisco Alexandre LourenÃ§o	Hybrid Machine Learning/Simulation Approaches for Logistic Systems Optimization	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Du, Yuchuan; Chen, Jing; Zhao, Cong; Liu, Chenglong; Liao, Feixiong; Chan, Ching-Yao	Comfortable and energy-efficient speed control of autonomous vehicles on rough pavements using deep reinforcement learning	2022	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				Rough pavements cause ride discomfort and energy inefficiency for road vehicles. Existing methods to address these problems are time-consuming and not adaptive to changing driving conditions on rough pavements. With the development of sensor and communication technologies, crowdsourced road and dynamic traffic information become available for enhancing driving performance, particularly addressing the discomfort and inefficiency issues by controlling driving speeds. This study proposes a speed control framework on rough pavements, envisioning the operation of autonomous vehicles based on the crowdsourced data. We suggest the concept of 'maximum comfortable speed' for representing the vertical ride comfort of oncoming roads. A deep reinforcement learning (DRL) algorithm is designed to learn comfortable and energyefficient speed control strategies. The DRL-based speed control model is trained using realworld rough pavement data in Shanghai, China. The experimental results show that the vertical ride comfort, energy efficiency, and computation efficiency increase by 8.22%, 24.37%, and 94.38%, respectively, compared to an optimization-based speed control model. The results indicate that the proposed framework is effective for real-time speed controls of autonomous vehicles on rough pavements.	Article	JAN 2022	10.1016/j.trc.2021.103489	[]	[]	-1	-1	-1
B	Alqahtani, Mohammed	Integrated Energy Scheduling and Routing for a Network of Mobile Prosumers	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
B	Sousa, Bruno AntÃ³nio Rodrigues	Parallel, Angular and Perpendicular Parking for Autonomously Driven Vehicles	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
C	Fandango, Armando; Gutierrez, Alexander; Hoayun, Clive; Hurter, Jonathan; Reed, Dean	ARORA & NavSim: a simulator system for training autonomous agents with geospecific data	2022	ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING IN DEFENSE APPLICATIONS IV	Proceedings of SPIE			This study accompanies the initial public release of the software for ARORA, or A Realistic Open environment for Rapid Agent training, and marks a high point of several years of work for the mature and completely open ARORA simulator. The purpose of ARORA is to support the training of an autonomous agent for tasks associated with a large-scale and geospecific outdoor urban environment, including the task of navigation as a car. The study elaborates on the simulator's architecture, agent, and environment. For the environment, ARORA provides an improvement on similar simulators through an unconstrained geospecific environment with detailed semantic annotation. The agent is represented as a car available with four different options of physics fidelity. The agent also has sensors available: a pose sensor, a camera sensor, and a set of three proximity sensors. Future use cases from training extend to both civilians and militaries (including human training and wargaming), in terms of training autonomous agents in outdoor urban environments. The study also presents a brief description of NavSim, a Python-based companion tool. The purpose of NavSim is to connect to ARORA (or any other similar simulator) and train an agent using reinforcement-learning algorithms. The study also provides challenges in development and subsequent work-arounds and solutions. The goal of the ARORA & NavSim system is to provide communities with a high-fidelity, publicly available, free, and open-source system for training an autonomous agent as a car.	Proceedings Paper	2022	10.1117/12.2647733	[]	[]	-1	-1	-1
J	Ekren, Banu Y.; Arslan, Bartu	A reinforcement learning approach for transaction scheduling in a shuttle-based storage and retrieval system	2024	INTERNATIONAL TRANSACTIONS IN OPERATIONAL RESEARCH		274	295	With recent Industry 4.0 developments, companies tend to automate their industries. Warehousing companies also take part in this trend. A shuttle-based storage and retrieval system (SBS/RS) is an automated storage and retrieval system technology experiencing recent drastic market growth. This technology is mostly utilized in large distribution centers processing mini-loads. With the recent increase in e-commerce practices, fast delivery requirements with low volume orders have increased. SBS/RS provides ultrahigh-speed load handling due to having an excess amount of shuttles in the system. However, not only the physical design of an automated warehousing technology but also the design of operational system policies would help with fast handling targets. In this work, in an effort to increase the performance of an SBS/RS, we apply a machine learning (ML) (i.e., Q-learning) approach on a newly proposed tier-to-tier SBS/RS design, redesigned from a traditional tier-captive SBS/RS. The novelty of this paper is twofold: First, we propose a novel SBS/RS design where shuttles can travel between tiers in the system; second, due to the complexity of operation of shuttles in that newly proposed design, we implement an ML-based algorithm for transaction selection in that system. The ML-based solution is compared with traditional scheduling approaches: first-in-first-out and shortest process time (i.e., travel) scheduling rules. The results indicate that in most cases, the Q-learning approach performs better than the two static scheduling approaches.	Article; Early Access		10.1111/itor.13135	[]	[]	-1	-1	-1
J	Li, Duowei; Zhu, Feng; Wu, Jianping; Wong, Yiik Diew; Chen, Tianyi	Managing mixed traffic at signalized intersections: An adaptive signal control and CAV coordination system based on deep reinforcement learning	2024	EXPERT SYSTEMS WITH APPLICATIONS				Managing the mixed traffic involving connected and autonomous vehicles (CAVs) and human-driven vehicles (HVs) at a signalized intersection has become a concern of researchers. However, the performances of most existing control methods are limited, especially when CAV penetration rate is low, since they fail to make a better trade-off between safety and operational efficiency for both CAVs and HVs. To this end, this study proposes a deep reinforcement learning (DRL) powered control system for the mixed traffic at signalized intersections, which aims to optimize operational efficiency of both CAVs and HVs while assuring safety and reducing interference on HVs' driving habits. The system adopts an adaptive traffic signal control strategy and an efficient CAV control policy with a passing rule proposed as a link in between. The traffic signal control strategy allows traffic light to adaptively adjust its phase and duration based on real-time traffic information, while the CAV control policy permits the CAVs meeting certain safety constraints to form platoons and pass through the intersection in a coordinated manner regardless of traffic signals. As an efficient DRL algorithm, Deep Q-Network (DQN) is adopted to adaptively control the signals and implement CAV coordination. The proposed system is examined on Simulation of Urban Mobility (SUMO), given different CAV penetration rates and traffic conditions. It is found that the proposed system not only outperforms the state-of-the-art control methods on reducing travel time and fuel consumption under low CAV penetration rate, but also enlarges its advantages with the increase of CAV penetration rate. In certain traffic scenarios, the proposed system can even achieve a maximum reduction of travel time by 37.33% and fuel consumption by 15.95%, in comparison to the existing method with the best performance. Besides, to some extent, the comparisons between the performances of CAVs and HVs demonstrate certain benefits of introducing CAVs into the mixed traffic.	Article; Early Access		10.1016/j.eswa.2023.121959	[]	[]	-1	-1	-1
J		EAGER: Real-Time: Formal Reinforcement Learning Methods for the Design of Safety-critical Autonomous Systems	2019					This EArly-Concept Grant for Exploratory Research (EAGER) project takes a clean-slate first-principles approach to the design of safety-critical autonomous systems by integrating formal methods and reinforcement learning from data. Several recent high-profile traffic incidents involving semi-autonomous vehicles have raised questions about whether current artificial intelligence (AI)-centered methods can ever lead us to Level 4 or 5 autonomy, i.e., to the realization of fully-autonomous vehicles with performance equivalent to a human driver in all driving scenarios. On the other hand, approaches rooted in formal methods for verification and synthesis can provide safety guarantees but have difficulty in efficiently reasoning about uncertainty and the correctness of data-driven models. This project will combine these two, seemingly incompatible, paradigms for designing autonomous systems. It will use model-free reinforcement learning algorithms to learn from semi-autonomous vehicle driving data. It will adopt model-based methods for system design, verification, and synthesis to offer provably safe operation in highly uncertain scenarios. An AutoDrive testbed will be set up where human driving data from scaled vehicular models will be leveraged to infer safe control policies using imitation and inverse reinforcement learning algorithms. The research is relevant to the science of intelligent autonomous transportation systems with significant societal implications. The experimental testbed will be used to provide hands-on research experience to undergraduate students and for K-12 outreach efforts.<br/><br/>In particular, the project will develop a framework for optimal control synthesis for safety and performance specification expressed in signal temporal logic. It will then incorporate vehicular and pedestrian kinematics in non-deterministic/probabilistic  transition models specified via probabilistic computation tree logic. Finally, it will develop formal reinforcement learning methods for partially observed dynamic models subject to safety specifications and complex temporal goals by learning from traces of safe human drivers. One key technical contribution of the project will be development of new formal reinforcement learning methods that may be useful in a broad array of applications wherein we must synthesize optimal controllers that satisfy certain safety specifications by learning from data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	Awarded Grant	Apr 01 2019		[]	[]	-1	-1	-1
B	Lin, Theresa	Modeling of Human Decision Making via Direct and Optimization-based Methods for Semi-Autonomous Systems	2015						Dissertation/Thesis	Jan 01 2015		[]	[]	-1	-1	-1
J	Wang, Shuo; Fujii, Hideki; Yoshimura, Shinobu	Generating merging strategies for connected autonomous vehicles based on spatiotemporal information extraction module and deep reinforcement learning	2022	PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS				A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs. Deep reinforcement learning (DRL) is a promising approach to address this challenge. It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need compliance of HDVs. However, the fusion of CAV sensing information is non-trivial. Traditional DRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment. Aimed at tackling these issues, this study proposes a DRL framework named spatiotemporal deep Q network (STDQN), by integrating a double deep Q network (DDQN) and a spatiotemporal information extraction module. A long-short term memory neural network with an attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information. In addition, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as the connectivity of multiple CAVs in a global range. Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems. Experimental results prove that as compared to baseline DRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort. The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving.(c) 2022 Elsevier B.V. All rights reserved.	Article; Early Access		10.1016/j.physa.2022.128172	[]	[]	-1	-1	-1
J		CAREER: Online Learning-based Underwater Acoustic Communications and Networking	2017					Underwater acoustic communication networks are the enabling technologies for unmanned, in situ, and real-time aquatic monitoring in a wide range of applications, such as scientific studies, pollution detection, offshore exploration, and tactical surveillance. The lifespan of underwater systems varies from a few years to decades, while the spatiotemporal dynamics of underwater acoustic environments at multiple scales pose grand challenges to efficient and reliable acoustic data transmission. The objective of this project is to develop a fundamental and systematic online-learning-based framework for underwater acoustic communications and networking, where the underwater acoustic system 1) models and predicts the long-term dynamics of the acoustic environment, and 2) proactively adapts its communication and networking strategy to the dynamics of the environment, thereby maximizing the long-term system performance in the aspects of energy efficiency, spectrum efficiency, and transmission reliability. Through explicit learning about its environment, the proposed framework will allow harmonious co-existence with other acoustic systems, including marine animals, to achieve eco-friendly operation. This project's research will be integrated with education through summer youth K-12 outreach, curriculum development, undergraduate and graduate student training that will be particularly tailored to females and underrepresented minorities, and collaboration with an underwater robotics team from the local Dollar Bay High School. These activities are designed to motivate and better train rural, female, minority, and economically disadvantaged students to pursue STEM careers.<br/><br/><br/>This project tackles fundamental challenges in online-learning-based underwater acoustic communications and networking by innovating across three interrelated domains. First, novel signal processing and sparse learning techniques will be developed to model and predict the large-scale dynamics and the statistical distribution of small-scale fading of underwater acoustic environments, including the acoustic transmission loss, ambient soundscape, and statistical characterization of external (anthropogenic and marine animal) acoustic sources. Second, an optimization framework will be developed, based on the acoustic environment prediction, for joint transmission power control, link scheduling, node-cooperative routing, and autonomous vehicle mobility control to achieve high network utility and harmonious coexistence with other acoustic systems. Third, the acoustic environment exploration-exploitation tradeoff will be tackled in the Bayesian reinforcement learning framework, which will provide a principled approach to weighing the immediate reward of a communication and networking strategy and its associated long-term benefit of revealing the environment's dynamics. Leveraging the geographic advantage of Michigan Tech and the state-of-the-art facilities of Michigan Tech's Great Lakes Research Center, extensive field experiments will be conducted for acoustic measurement collection and for offline and online algorithm evaluation in a software-defined networking architecture. The methodologies and crosscutting techniques developed in this project can be applied to the design of intelligent radio-frequency communication networks.	Awarded Grant	Feb 15 2017		[]	[]	-1	-1	-1
J	Ma, Jichang; Xie, Hui; Song, Kang; Liu, Hao	Self-Optimizing Path Tracking Controller for Intelligent Vehicles Based on Reinforcement Learning	2022	SYMMETRY-BASEL				The path tracking control system is a crucial component for autonomous vehicles; it is challenging to realize accurate tracking control when approaching a wide range of uncertain situations and dynamic environments, particularly when such control must perform as well as, or better than, human drivers. While many methods provide state-of-the-art tracking performance, they tend to emphasize constant PID control parameters, calibrated by human experience, to improve tracking accuracy. A detailed analysis shows that PID controllers inefficiently reduce the lateral error under various conditions, such as complex trajectories and variable speed. In addition, intelligent driving vehicles are highly non-linear objects, and high-fidelity models are unavailable in most autonomous systems. As for the model-based controller (MPC or LQR), the complex modeling process may increase the computational burden. With that in mind, a self-optimizing, path tracking controller structure, based on reinforcement learning, is proposed. For the lateral control of the vehicle, a steering method based on the fusion of the reinforcement learning and traditional PID controllers is designed to adapt to various tracking scenarios. According to the pre-defined path geometry and the real-time status of the vehicle, the interactive learning mechanism, based on an RL framework (actor-critic-a symmetric network structure), can realize the online optimization of PID control parameters in order to better deal with the tracking error under complex trajectories and dynamic changes of vehicle model parameters. The adaptive performance of velocity changes was also considered in the tracking process. The proposed controlling approach was tested in different path tracking scenarios, both the driving simulator platforms and on-site vehicle experiments have verified the effects of our proposed self-optimizing controller. The results show that the approach can adaptively change the weights of PID to maintain a tracking error (simulation: within +/- 0.071 m; realistic vehicle: within +/- 0.272 m) and steering wheel vibration standard deviations (simulation: within +/- 0.04 degrees; realistic vehicle: within +/- 80.69 degrees); additionally, it can adapt to high-speed simulation scenarios (the maximum speed is above 100 km/h and the average speed through curves is 63-76 km/h).	Article	JAN 2022	10.3390/sym14010031	[]	[]	-1	-1	-1
J	Liu, Yongyang; Zhou, Anye; Wang, Yu; Peeta, Srinivas	Proactive longitudinal control to preclude disruptive lane changes of human-driven vehicles in mixed-flow traffic	2023	CONTROL ENGINEERING PRACTICE				Connected and autonomous vehicles (CAVs) can be leveraged to enable cooperative platooning control to alleviate traffic oscillations. However, prior to a pure CAV environment, CAVs and human-driven vehicles (HDVs) will coexist on roads, creating a mixed-flow traffic environment. Mixed-flow traffic introduces key challenges for CAV operations due to potential lane changes by HDVs in adjacent lanes, which can cause stop-and-go waves and traffic oscillations. An understanding of the interactions between CAVs and HDVs in the lane-change process can be leveraged to use CAVs to proactively preclude disruptive lane changes by HDVs. This study proposes a deep reinforcement learning-based proactive longitudinal control strategy (PLCS) for CAVs to counteract disruptive HDV lane-change behaviors that can induce disturbances, and to preserve the smoothness of traffic flow in the CAV platooning control process. In it, a Transformer-based lane-change traffic condition predictor is constructed to predict whether an HDV will likely perform a disruptive lane change under the ambient traffic conditions. If no disruptive lane change is predicted, an extended intelligent driver model is activated for the CAV to perform smooth car-following behavior under cooperative CAV platooning control. If a disruptive lane change is predicted, a rainbow deep Q-network (RDQN)-based lane-change preclusion model is proposed through which the CAV can alter the lane-change traffic condition to preclude the HDV's lane change. Results from numerical experiments suggest that a CAV controlled by the PLCS is effective in reducing disruptive lane-change maneuvers by an HDV in its vicinity, and can improve string stability performance in mixed-flow traffic. Further, the effectiveness of the PLCS is illustrated under different lane-change scenarios, CAV control setups, and HDV driver types.	Article; Early Access		10.1016/j.conengprac.2023.105522	[]	[]	-1	-1	-1
J	Shi, Haotian; Zhou, Yang; Wu, Keshu; Chen, Sikai; Ran, Bin; Nie, Qinghui	Physics-informed deep reinforcement learning-based integrated two-dimensional car-following control strategy for connected automated vehicles	2023	KNOWLEDGE-BASED SYSTEMS				Connected automated vehicles (CAVs) are broadly recognized as next-generation transformative transportation technologies having great potential to improve traffic safety, efficiency, and stability. Efficiently controlling CAVs on two-dimensional curvilinear roadways to follow preceding vehicles is denoted as the two-dimensional car-following process, which is highly critical; this process is challenging to implement owing to the complexity and varied nature of driving environments. This study proposes an innovative integrated two-dimensional control strategy for CAVs based on deep reinforcement learning (DRL), which efficiently regulates the two-dimensional car-following process of CAVs in terms of both stability-wise longitudinal control performance and accurate lateral path-tracking performance. Within the control framework, each CAV can receive the surrounding information from downstream vehicles and roadway geometry based on vehicle-to-everything (V2X) communication. To better utilize this information, we designed a physics-informed DRL state fusion approach and reward function, which efficiently embeds prior physics knowledge and borrows the merits of the equilibrium and consensus concepts from the control theory. Given the physics-informed information, the DRL-based controller outputs the integrated control instructions for both longitudinal and lateral control. For training, we constructed a roadway with a set of varying curvatures and em-bedded the ground-truth vehicle trajectory datasets to more effectively capture the realistic variations in the roadway geometry and driving environment. To facilitate value function approximation and enhance the policy iteration process in training, the distributed proximal policy optimization (DPPO) algorithm was applied, owing to its balanced performance. A series of simulated experiments were conducted to validate the controller's lateral control accuracy and stability-wise oscillation dampening performance in diverse traffic scenarios, including extreme ones.(c) 2023 Elsevier B.V. All rights reserved.	Article; Early Access		10.1016/j.knosys.2023.110485	[]	[]	-1	-1	-1
B	Lu, Yantao	Human Activity Recognition from Egocentric Videos and Robustness Analysis of Deep Neural Networks	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J		CAREER: Systematic Approach for Extensively (SAfEly) Testing and Verifying the Security of Connected and Autonomous Vehicle	2022					This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).&lt;br/&gt;&lt;br/&gt;The potential benefits of connected autonomous vehicles (CAV) are numerous, and society is expecting that this technology will increase the quality of everyday life and follow through on its promises. However, to be effective, they must be tested to demonstrate a standard level of safety and security. The complex and interconnected nature of the transportation system makes the task of testing and verification exceedingly difficult, raising serious concerns regarding their safety and security. It, thus, calls for new problem formulation and a novel systematic approach for the task of CAV testing and verification. The existing testing solutions use ad-hoc methods, such as miles driven, to demonstrate some indication of safety, often assuming that the CAV's perception of the surrounding environment is comprehensive and ideal. However, no fundamental structure has been developed to demonstrate the security of CAV products. This CAREER proposal models the transportation system as a networked control system providing a novel resiliency metric enabling the testing resiliency of CAVs. In addition, it utilizes the prior developed verification framework to formulate the testing and verification process as a centralized feedback control system enabling the development of a novel attack generator. The expected outcomes of this project would pave the way towards safely testing CAVs, directly impacting the future of this technology and related standards, ultimately eliminating crash-related fatalities and saving lives. The research findings can be further implemented for all networked control systems, such as high-assurance military systems and autonomous systems ranging from unmanned aerial vehicles to power systems. The educational purpose of the project is to expand students', particularly underrepresented and women minorities, awareness of CAV security by designing fully integrated educational modules and demonstrations. We plan to include the following activities to serve the need for rural and largely economically distressed regions: (i) develop after school online STEM curriculum adjusted for primary, High-school, and college students; (ii) provide workshops for educators and industrial partners as their professional development activities; (iii) involve underrepresented undergraduate and college students through research for undergraduate experience and internship program; (iv) develop an undergraduate and an advanced graduate courses.&lt;br/&gt;&lt;br/&gt;This CAREER project addresses the problem of testing and verification for the security of CAVs. The importance of the security of CAVs has been recognized in the existing literature and has motivated the development of several detection and compensation algorithms to ensure safety under faults, failures, and attacks. However, not much effort is invested in the task of CAVs testing and verification. This CAREER project illustrates that the current approaches are insufficient to safely verify the security of CAVs in a realistic environment, suffering from the lack of a metric that is dynamic-dependent to measure the system resiliency. We describe a research plan where a transportation system is modeled as a networked control system where roads, pedestrians, vehicles, and traffic signs (due to their dynamic behavior) are modeled as agents, interacting with each other using sensors and communication networks. The new perspective allows us to propose a novel resiliency metric to be used alongside the safety metric to develop reinforcement learning-based controllers for testing CAVs' security. As there are infinite types of faults and attacks, the proposed controller formulates the effects of attacks rather than focusing on specific types, easing the process of fault and attack generation. This project is expected to advance the area of testing and verification of CAVs by (i) Introducing a novel perspective using the concept of networked control systems enabling the development of a unique data stream generator utilizing reinforcement learning to generate attacks by modeling the testing process as a feedback control system where minimizing safety and security is the desired objective and (ii) Developing a unique experimental platform enriched with the power of mixed reality (MR) and vehicle-in-the-loop (ViL) to test the security of CAVs safely.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	Awarded Grant	Feb 15 2022		[]	[]	-1	-1	-1
P	KUUTTI S; PORAV H; UPCROFT B; NEWMAN P	Method for generating new adversarial scenario            involving autonomous vehicle and agent, involves            performing reinforcement learning to train agent using            proxy of autonomous vehicle software stack in            reinforcement learning environment						NOVELTY - The method involves performing reinforcement learning to train the agent using a proxy of an autonomous vehicle software stack in a reinforcement learning environment to generate episodes. The episodes each represents an adversarial scenario terminating in failure of the proxy of the autonomous vehicle software stack. A descriptors (20) are generated based on the or each episode. The descriptors are stored in a database. The descriptors clustered for the or each episode, and the stored descriptor comprises the cluster of descriptors stored in the database. A new descriptor generated by moving away from the cluster of descriptors in a descriptor space. USE - Computer-based method for generating trajectories of actors. ADVANTAGE - The method involves simulating a first scenario comprising an environment having an ego-vehicle, where set of actors is provided with a first actor and a set of objects is provided, and thus enables to reduce the risk to the AV and occupants of the AV. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer based method of generating an agent from a scenario involving an autonomous vehicle; anda transitory computer readable storage medium storing program for generating trajectories of actors. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the scenario of an ego-vehicle.10Autonomous vehicle12Vehicle14Pedestrian20Descriptor24Computer				[]	[]	-1	-1	-1
P	KUUTTI S; PORAV H; UPCROFT B; NEWMAN P	Method for generating new adversarial scenario            involving autonomous vehicle and agent, involves            performing reinforcement learning to train agent using            autonomous vehicle software stack in reinforcement            learning environment to generate episodes						NOVELTY - The method involves performing reinforcement learning to train the agent using an autonomous vehicle software stack in a reinforcement learning environment to generate episodes, the episodes each representing an adversarial scenario terminating in a failure of the autonomous vehicle software stack. A descriptors (20) are generated based on the or each episode. The descriptors are stored in a database. The descriptors clustered for the or each episode, and the stored descriptors which comprises the cluster of descriptors stored in the database. A new descriptor is generated by moving away from the cluster of descriptors in a descriptor space. USE - Computer-based method for generating new adversarial scenario involving autonomous vehicle and agent. ADVANTAGE - The method involves simulating a first scenario comprising an environment having an ego-vehicle, where set of actors is provided with a first actor and a set of objects is provided, and thus enables to reduce the risk to the AV and occupants of the AV. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer implemented method of generating an agent from a scenario involving an autonomous vehicle; anda transitory computer readable medium storing program for generating an agent from a scenario involving an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the scenario of an ego-vehicle.10Autonomous vehicle12Vehicle14Pedestrian16Sidewalk20Descriptor				[]	[]	-1	-1	-1
B	Ghazanfari, Behzad	Machine Learning-Based Decision Making in Autonomous Systems	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
P	RAJKUMAR M; GORTHI R; DEVI S G; RAO T N; SRINIVASARAO P; GONUGUNTLA S	System for providing navigation of self-driving            car in highways, has reinforcement learning framework            implemented within processors to enable autonomous            vehicle to learn and adapt navigation strategy						NOVELTY - The system has an autonomous vehicle equipped with sensors, processors and control mechanisms. A reinforcement learning framework is implemented within the processors to enable the autonomous vehicle to learn and adapt a navigation strategy based on real-time sensory data and environmental conditions. The sensors are provided with cameras, lidars, radars, and global positioning systems for providing the autonomous vehicle with multi-modal data for environment perception. The reinforcement learning framework employs deep reinforcement learning algorithms to optimize navigation decisions including route planning, obstacle avoidance, and trajectory control. USE - System for providing navigation of an autonomous vehicle (claimed) i.e. self-driving car, in complex urban environments i.e. highways. ADVANTAGE - The system can navigate bustling city streets, thus ensuring safe, efficient, and reliable transportation in complex and challenging urban settings. The system defines a set of rewards and penalties that guide agent's decision-making process such that rewards can be associated with safe and efficient actions such as obeying traffic rules and avoiding collisions and penalties can be imposed for risky behaviors or traffic violations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) a method for providing autonomous vehicle navigation in complex urban environments using a reinforcement learning framework; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for providing navigation of an autonomous vehicle in complex urban environments.				[]	[]	-1	-1	-1
P	WONGPIROMSARN T; COLLIN A; BELTA C; TEBBENS R D; HELOU B; BEIJBOM O O; KORING A; BERTA C; DUINTJER T R; HERAU B; OLOFF B O; BASSAM H; DOUINTIER T B; BELTACALIN; ANN C	Method for scoring trajectories of autonomous            vehicle through given traffic scenario using e.g.            linear support vector machine, involves planning            trajectory in environment using augmented route            planner, and operating vehicle along planned trajectory            using control circuit of vehicle						NOVELTY - The method (1000) involves generating a set of trajectories for a vehicle operating in an environment using processors (1001), where each trajectory is associated with a traffic scenario. A reasonableness score for each trajectory in the set of trajectories is predicted (1002) using the processors, where the reasonableness score is obtained from a machine learning model that is trained using input obtained from a set of human annotators, and a loss function that penalizes predictions of reasonableness scores that violate a rulebook structure. A route planner of the vehicle is augmented using the predicted reasonableness scores for the trajectories using the processors. A trajectory is planned in the environment using the augmented route planner by the processors. The vehicle is operated along the planned trajectory using a control circuit of the vehicle. USE - Method for scoring trajectories of an autonomous vehicle through a given traffic scenario using a machine learning model such as linear support vector machine, neural network and convolutional neural network trained on images (all claimed), to predict reasonableness scores for the trajectories. ADVANTAGE - The method enables using the machine learning model to predict reasonableness scores for autonomous vehicle trajectories for given traffic scenario, so that the predicted scores can be used to tune a route planner and performance of the route planner, compare two AV stacks, and compare reinforcement learning and any other desired application in efficient manner. DETAILED DESCRIPTION - The loss function is a hinge or slack loss function. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for scoring trajectories of an autonomous vehicle.Method for scoring trajectories of autonomous vehicle through given traffic scenario (1000)Generating set of trajectories for vehicle operating in environment (1001)Predicting reasonableness score for each trajectory in set of trajectories (1002)Using predicted reasonableness scores (1003)				[]	[]	-1	-1	-1
P	KRYSTEK P; KWATRA S; WILSON J; BAYSINGER B	Method for implementing intelligent driving            characteristic adjustment for autonomous vehicles by a            processor involves determining a user experience            satisfaction level for one or more users during a            journey within an autonomous vehicle						NOVELTY - The method (700) involves determining a user experience satisfaction level for one or more users during a journey within an autonomous vehicle according to historical user experience satisfaction levels, one or more user profiles, one or more contextual factors, or a combination (704). Dynamically adjusting one or more performance characteristics of the autonomous vehicle for the one or more users according to the historical user experience satisfaction levels, the one or more user profiles, if the user experience satisfaction level is less than a predetermined threshold (706). Determining a risk threshold for the cluster of users and use reinforcement learning to dynamically adjust the one or more performance characteristics based on a global satisfaction level of the cluster of users. USE - Method for implementing intelligent driving characteristic adjustment for autonomous vehicles by a processor. ADVANTAGE - Method for implementing intelligent driving characteristic adjustment for autonomous vehicles by a processor can allow cloud computing environment to offer infrastructure, platforms and/or software as services for which a cloud consumer does not need to maintain resources on a local computing device. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for implementing intelligent driving characteristic adjustment for autonomous vehicles; and(2) a computer program product. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for implementing intelligent driving characteristic adjustment for autonomous vehicles by a processor.Method (700)Determining a user experience satisfaction level for one or more users during a journey within an autonomous vehicle according to historical user experience satisfaction levels, one or more user profiles, one or more contextual factors, or a combination (704)Dynamically adjusting one or more performance characteristics of the autonomous vehicle for the one or more users according to the historical user experience satisfaction levels, the one or more user profiles, if the user experience satisfaction level is less than a predetermined threshold (706)				[]	[]	-1	-1	-1
P	HALDER B	Method for managing autonomous vehicle application            operations with reinforcement learning, involves            communicating interruptible command from vehicle safety            manager to action integrity module to determine final            action for autonomous vehicle						NOVELTY - The method involves providing an observed state of an autonomous vehicle as a feedback to a reinforcement learning (RL) model-agent that decides a next action for the autonomous vehicle. The observed state is provided to an epistemic uncertainty check (EUC) module to determine that that the input-state distribution varies from a training-state distribution based on a specified distance value between the input-state distributions from a training-state distribution, where the EUC module outputs an RL model confidence factor (RLMCF). The RLMCF is communicated from the RL model-agent and the interruptible command is communicated from a vehicle safety manager (VSM) (102) to an action integrity module (AIM) to determine a final action for the autonomous vehicle. USE - Method for managing autonomous vehicle application operations with RL. ADVANTAGE - The method enables providing robust and safe handling of RL model decision in real autonomous vehicle application. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computerized system for managing autonomous vehicle application operations with RL. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an autonomous vehicle management system.VSM (102)Applications (104)Middleware (106)Operating system (108)Sensors (110)				[]	[]	-1	-1	-1
P	ENGLARD B; GANDHI G; MAHESHWARI P	Non-transitory computer-readable medium storing            program for controlling autonomous vehicle, includes            instruction for generating grid path through            environment is based on cost maps by motion            planner						NOVELTY - The non-transitory computer-readable medium includes instruction for generating a perception component (506) and prediction component configured to receive sensor data (502). An observed occupancy grid (508) indicative of which cells are currently occupied in a two-dimensional representation of an environment through the autonomous vehicle is moving based on the received sensor data. The navigation data is provided for guiding the autonomous vehicle through the environment towards a destination by mapping component (530). The several predicted occupancy grids and the navigation data are generated based on the observed occupancy grid by cost map generation component. A grid path through the environment is generated based on multiple cost maps by motion planner and generate decisions for maneuvering the autonomous vehicle toward the destination based on the grid path. USE - Non-transitory computer-readable medium storing program for controlling autonomous vehicle (claimed) using cost maps. ADVANTAGE - The performance e.g. safety and efficiency of the autonomous vehicle can be improved if the candidate decisions generated by the self-driving control architecture (SDCA) reflects a greater level of diversity. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method of controlling an autonomous vehicle; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an self-driving control architecture using a motion planner that is trained with reinforcement learning.Sensor data (502)Perception component (506)Occupancy grid (508)Mapping component (530)Control signals (546)				[]	[]	-1	-1	-1
P	HOEL C; LAINE L	Method for providing reinforcement learning (RL)            agent for decision-making to be used in controlling            autonomous vehicle, involves initiating additional            training in which RL agent interacts with second            environment including autonomous vehicle						NOVELTY - The method (100) involves interacting an RL agent with a first environment including the autonomous vehicle, in multiple training sessions (110-1 to 110-K). Each training session includes a different initial value and yielding a state-action value function dependent on state and action. An uncertainty evaluation (114) is performed on the basis of a variability measure for multiple state-action value functions evaluated for multiple state-action pairs corresponding to possible decisions by the trained RL agent. The RL agent interacts with a second environment including the autonomous vehicle, in an additional training (116). The second environment differs from the first environment by an increased exposure to a subset of state-action pairs for which the variability measure indicates a relatively higher uncertainty. USE - Method for providing a reinforcement learning agent for decision-making to be used in controlling an autonomous vehicle. ADVANTAGE - The method effectively provides the reinforcement learning agent for decision-making to be used in controlling the autonomous vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an arrangement for controlling an autonomous vehicle;(2) a computer program; and(3) a data carrier carrying the computer program. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for providing a reinforcement learning agent for decision-making to be used in controlling an autonomous vehicle.Reinforcement learning method (100)Training sessions (110-1 to 110-K)Uncertainty evaluation (114)Additional training (116)				[]	[]	-1	-1	-1
P	HUH K S; MIN K S; HAYOUNGKIM	Method for controlling autonomous vehicle using            deep reinforcement learning and driver assistance            system, involves determining action for vehicle control            using sensor and image data received from deep            reinforcement learning algorithm						NOVELTY - The method involves receiving (S110) the measured sensor data and captured image data with deep reinforcement learning algorithm. An action is determined (S120) for vehicle control by using the sensor data and the image data received from the deep reinforcement learning algorithm. A vehicle is controlled (S130) by selecting a driver assistance system (DAS) according to the determined behavior. The input sensor data and the image data are purified respectively. The connected data is formed by connecting the refined sensor data and the image data. A Q value is obtained by inputting the connected data into a fully connected layer of the deep reinforcement learning algorithm. USE - Method for controlling autonomous vehicle using deep reinforcement learning and driver assistance system. ADVANTAGE - The deep reinforcement learning stably controls an autonomous vehicle by determining an appropriate driver assistance system for various situation through an algorithm that determines the optimal behavior using deep reinforcement learning and a control method of an autonomous vehicle using a driver assistance system. The path of the control of the autonomous vehicle is planned. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an autonomous vehicle control device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for controlling autonomous vehicle using deep reinforcement learning and driver assistance system. (Drawing includes non-English language text)Step for receiving input of measured sensor data and captured image data with deep reinforcement learning algorithm (S110)Step for determining an action for vehicle control using the sensor data and the image data received from the deep reinforcement learning algorithm (S120)Step for controlling a vehicle by selecting a DAS according to the determined behavior (S130)				[]	[]	-1	-1	-1
P	KWON S D; LEE Y S; JIYEONG H; JU S	Method for personalizing autonomous driving            algorithm for enhancing satisfaction of specific user's            autonomous driving experience, involves setting driving            mode of autonomous vehicle to manual mode by            personalization device						NOVELTY - The method involves setting a driving mode of an autonomous vehicle to a manual mode by personalization device, and acquiring manual control data of the autonomous vehicle under situations according to an operation of a specific user. Automatic control data is generated by an autonomous driving module, and a personalization device is provided with a fitting module. An autonomous driving algorithm loaded in the autonomous driving module is set with reference to the first to Nth manual control data and the first to Nth automatic control data to the specific user. A determination is made whether instantaneous rate of change in speed or direction of the autonomous vehicle according to the input of the raw manual control data to the autonomous vehicle is equal to or greater than a first threshold. USE - The method is useful for personalizing an autonomous driving algorithm for enhancing the satisfaction of the specific user's autonomous driving experience. ADVANTAGE - The satisfaction of the user's autonomous driving experience is enhanced. The autonomous driving algorithm is personalized by performing inverse reinforcement learning with driving of a specific user as an optimal policy. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for personalizing the autonomous driving algorithm for enhancing satisfaction of specific user's autonomous driving experience. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for personalizing an autonomous driving algorithm for enhancing the satisfaction of a specific user's autonomous driving experience (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	HUN B S	Apparatus for improving traffic based on            reinforcement learning at non-signal intersection for            autonomous vehicle cluster operation, has reinforcement            learning library environment construction unit that            optimizes traffic control with multi-agent deep            rein						NOVELTY - The device has a simulation execution unit (100) that builds a simulation environment using simulation of urban mobility (SUMO) and delivers the data obtained from the speed, position, and sensor of the autonomous vehicle to a flow application unit (200). A reinforcement learning library environment construction unit (300) optimizes traffic control with multi-agent deep reinforcement learning using the flow. A result file generation unit transmits the obtained data from speed, location and sensor to the flow environment. A simulation initialization unit sets the simulation environment including human driver definition. A vehicle control module (23) controls the vehicle and delivers control information to the execution unit. A data sampling unit (32) samples data to be learned. An update unit receives the simulation state from an execution unit and updates the simulation status. A policy optimizer maximizes the reward of reinforcement learning for behavior. USE - The apparatus is useful for improving traffic based on reinforcement learning at non-signal intersection for autonomous vehicle cluster operation. ADVANTAGE - The self-driving vehicle platooning learning improves non-signal crossing traffic and ensures safety. The reward of reinforcement learning for behavior is maximized. The driving behavior of autonomous vehicles is learned in mixed traffic flow situation where clustered autonomous vehicles and human driver vehicles are mixed. The average travel speed in fully autonomous vehicle environment is improved. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for the method for reinforcement learning-based traffic improvement at non-signal intersections for group operation of autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of apparatus for improving traffic based on reinforcement learning at non-signal intersection for autonomous vehicle cluster operation (Drawing includes non-English language text).Simulation initialization unit (20)Vehicle control module (23)Data sampling unit (32)Simulation execution unit (100)Flow application unit (200)Reinforcement learning library environment construction unit (300)				[]	[]	-1	-1	-1
P	NISHI T	Method for configuring autonomous operation            capability for vehicle, involves providing            reinforcement learning module with actor-critic module            to reduce active environment scanning by multiple            sensors as vehicle operates autonomously						NOVELTY - The method involves producing a state value function for an autonomous vehicle capability in relation to a first dataset by a processor, where the state value function includes a present state, a next state, and a state cost. A second dataset relating to a portion of vehicles is identified by the processor. A policy control gain for the autonomous vehicle capability is optimized in relation to the second dataset by the processor. The autonomous vehicle capability is operable to generate an autonomous vehicle action (124) for progressing to a next state based on the state value function in cooperation with the policy control gain and the autonomous vehicle capability. A reinforcement learning (RL) module (312) is provided with an actor-critic module (402) to reduce active environment scanning by multiple sensors as the vehicle operates autonomously. USE - Method for configuring an autonomous operation capability for a vehicle. Uses include but are not limited to a sports vehicle, a suburban utility vehicle, an outdoor vehicle, a pickup lorry, a passenger vehicle, a recreational vehicle, and a motorized land vehicle. ADVANTAGE - The method enables operating an autonomous vehicle dataset module to provide the reinforcement leaning module with the ability to provide continuous state and action spaces that engage in machine learning based on less system knowledge and without requiring active environment sensing and/or exploration, so that the passive dynamics data of the first dataset relating to the category, or model, of the vehicle are honed by the second dataset as relating to the state value functions and policies for improving reinforced learning efficiency with respect to improved implementation while reducing active scanning required for autonomous operations. The method enables operating an actor module to generate a prediction of an action of the autonomous vehicle capability based on an adaptive policy generated from a two-stage hierarchical configuration of a first and/or passive environment dynamic dataset and a second and/or control dynamics dataset when the RL module includes an actor-critic module. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vehicle control unit. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a reinforcement learning module.Autonomous vehicle action (124)RL module (312)Autonomous vehicle dataset module (314)Actor-critic module (402)Reinforcement feedback signal (404)				[]	[]	-1	-1	-1
P	AL Q M H; QI X; CLIFFORD D H	Method for training an autonomous vehicle,            involves storing real world data that includes a            sequence of images of a road environment and generating            the sequence of images on the basis of a vehicle that            traverses the road environment						NOVELTY - The method involves storing real world data that includes a sequence of images of a road environment in a data storage device. The sequence of images is generated on the basis of a vehicle that traverses the road environment. The sequence of images is processed with a deep reinforcement learning agent in an offline simulation environment. The deep reinforcement learning agent is associated with a control feature of the autonomous vehicle to obtain an optimized set of control policies. The autonomous vehicle is trained on the basis of the optimized set of control polices. A first image is obtained from the sequence of images and processes the first image with the deep reinforcement learning agent to obtain an action. USE - Method for training an autonomous vehicle. ADVANTAGE - Improves the training process by no longer relying on synthesized simulation environment data. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system that includes a data storage device; and(2) an autonomous vehicle that include multiple sensors. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the Method for training an autonomous vehicle.				[]	[]	-1	-1	-1
P	CHOI Y	Method for predicting optimal speed trajectory of autonomous vehicle, involves generating virtual vehicle model by virtual vehicle learning unit using vehicle driving data collected from actual autonomous vehicle						NOVELTY - The method involves generating a virtual vehicle model using vehicle driving data collected from an actual autonomous vehicle by the virtual vehicle learning unit. The driving data of the virtual vehicle model driving in the virtual environment using the generated virtual vehicle model input is predicted and outputted by a virtual environment agent. A speed trajectory is predicted by a controller agent in which target speed values are connected point by point using driving data of the virtual vehicle. A virtual environment model is generated by modeling the virtual environment. A compensation value is used as a value for learning the controller agent by a reinforcement learning algorithm. USE - Method for predicting optimal speed trajectory of the autonomous vehicle. ADVANTAGE - The dynamic characteristics of the autonomous vehicle are learned through the learning of two agents generated by artificial neural network and reinforcement learning, and the speed at which the experiential and inferential elements of the route are reflected. Stable autonomous driving is possible even for driving paths similar to learning data. The learning and driving in various modes are possible through a simple parameter change of the compensation value. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for predicting an optimal speed trajectory of an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for predicting the optimal speed trajectory of the autonomous vehicle. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	FAUST A; DEVIN M; CHEN Y J; MORLEY F; FURMAN V; FUERTES P C A	Method for implementing interactive simulated            autonomous vehicle agent for e.g. self-driving cars,            involves generating experience tuple, and providing            experience tuple as input to reinforcement learning            system for autonomous vehicle						NOVELTY - The method involves generating an immediate quality value for a predicted environment of an autonomous vehicle and an action as input to a context-specific quality model that generates immediate quality values that are specific to a particular driving context. An experience tuple is generated (350), where the experience tuple comprises initial environment observation, candidate action, and immediate quality value. The experience tuple is provided as input to a reinforcement learning system (360) for the autonomous vehicle. USE - Method for implementing an interactive simulated autonomous vehicle agent for self-driving cars, boats, and aircraft using a computer. Uses include but are not limited to mobile telephone, personal digital assistant (PDA), mobile audio or video player, game console, global positioning system (GPS) receiver and Universal Serial Bus (USB) flash drive device. ADVANTAGE - The method enables generating sufficient number of reliable and realistic experience tuples in a reasonable amount of time for different driving contexts efficiently. The method enables allowing a vehicle behavior model to be trained with automatically generated training data to reduce the amount of human-labeled data to be collected to reduce amount of time required to build and train the system. The method enables improving safety of learning driving maneuvers without putting a physical vehicle or humans at risk for learning to drive in dangerous situations or environments, thus ensuring the vehicle to perform smooth and safe action transitions. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a system for implementing an interactive autonomous vehicle agent(2) a computer program product comprising a set of instructions for implementing an interactive autonomous vehicle agent. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating an experience tuple for an interactive autonomous vehicle agent in a driving context.Step for receiving request to generate experience tuple for vehicle in driving context (310)Step for receiving candidate action and initial environment observation (320)Step for using context-specific quality model to compute immediate quality value for candidate action in initial environment (340)Step for generating experience tuple using initial environment observation, candidate action and computed immediate quality value (350)Step for providing experience tuple as input to reinforcement learning system for autonomous vehicle (360)				[]	[]	-1	-1	-1
P	WEDEKIND D	Method for adapting driving behavior of autonomous            vehicle, involves training model to carry out driving            maneuver in traffic situation, training execution of            maneuver to optimize traffic flow, and carrying out            driving maneuver by autonomous vehicle based on model            depending on flow						NOVELTY - The method involves defining a traffic situation with a driving maneuver to be carried out by an autonomous vehicle (217). A model is trained to carry out the driving maneuver in the traffic situation, where the model is based on a traffic flow related to the situation is taken into account and evaluated. An execution of the maneuver is trained in such a way that the traffic flow is optimized according to the evaluation. The maneuver is carried out in the situation by the autonomous vehicle based on the model depending on the flow, where the model uses reinforcement learning to particularly reward executions of maneuver. USE - Method for adapting driving behavior of autonomous vehicle. ADVANTAGE - The method enables avoiding excessively defensive driving behavior of the autonomous vehicle, which can lead to unnecessary driving delays and driving impairments of other vehicles. The method allows the model to be trained with data recorded in reality, so that the model can use reinforcement learning to reward executions of driving maneuvers that are in reality carried out by non-autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a street scene at an intersection.200Street scene201Intersection202First road204First lane217Autonomous vehicle				[]	[]	-1	-1	-1
P	FUERTES P C A; FURMAN V; MORLEY F; CHEN Y J; DEVIN M; FAUST A	Method for detecting nearby objects by using            on-board sensors for autonomous vehicles, such as cars,            involves selecting action to be taken by autonomous            vehicle in initial environment from enumerated set of            candidate actions						NOVELTY - The method involves receiving an initial environment observation representing an initial environment of an autonomous vehicle when the autonomous vehicle is in a particular driving context. An action to be taken is selected by the autonomous vehicle in the initial environment from an enumerated set of candidate actions using a reinforcement learning system for the autonomous vehicle. A predicted environment observation representing a predicted environment of the autonomous vehicle is generated after the selected candidate action is taken by the autonomous vehicle in the initial environment. The initial environment observation and the selected candidate action is provided as input to a vehicle behavior model neural network trained to generate predicted environment observations. USE - Method for detecting nearby objects by using on-board sensors for autonomous vehicles, such as cars, boats, and aircraft. ADVANTAGE - Detection system is allowed to efficiently generate a sufficient number of reliable and realistic experience tuples in a reasonable amount of time for many multiple driving contexts. Quality metric engine can use a comfort feature that quantifies the level of driver comfort for the candidate action in the predicted environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for detecting nearby objects by using on-board sensors for autonomous vehicles;(2) a computer program product for detecting nearby objects by using on-board sensors for autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of method for detecting nearby objects by using on-board sensors for autonomous vehicles.Receiving request to generate an experience tuple for a vehicle in a particular driving context (310)Receiving candidate action and initial environment observation (320)Using vehicle behavior model to compute predicted environment observation for the candidate action in the initial environment (330)Generating an experience tuple using the initial environment observation, the candidate action, and the computed immediate quality value (350)Providing the experience tuple as input to a reinforcement learning system for the autonomous vehicle (360)				[]	[]	-1	-1	-1
P	ISELE D; NAKHAEI S A; FUJIMURA K	Method for controlling an autonomous vehicle,            involves collecting scenario information from one or            more sensors mounted on a vehicle, and determining a            high-level option for a fixed time horizon based on the            scenario information						NOVELTY - The method involves collecting scenario information from one or more sensors mounted on a vehicle (102), and determining a high-level option for a fixed time horizon based on the scenario information. A prediction algorithm is applied to the high-level option masking undesired low-level behaviors for completing the high-level option where a collision is predicted to occur. A restricted subspace of low-level behaviors is evaluated through a reinforcement learning system. USE - Method for controlling an autonomous vehicle. ADVANTAGE - The increased penalty for a timeout reduces the number of unsuccessful trials. Improved autonomous driving is ensured. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a vehicle for autonomous driving; and(2) a computer-readable medium storing computer executable code for autonomously controlling a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an operating environment of a reinforcement learning system for autonomous driving.Vehicle (102)Autonomous driving system (110)Electronic control unit (112)Processor (114)Memory (116)				[]	[]	-1	-1	-1
P	SHALEV-SHWARTZ S; SHASHUA A; SHAMMAH S	Navigation system for host autonomous vehicle            using reinforcement learning techniques, has processing            device programmed to cause adjustment of navigational            actuator of host vehicle in response to determined            navigational action for host vehicle						NOVELTY - The system has a processing device (110) programmed to analyze multiple images to identify a first object and a second object in the environment of a host vehicle. A first predefined navigational constraint implicated by the first object and a second predefined navigational constraint implicated by the second object are determined, where the first predefined navigational constraint and the second predefined navigational constraint are not satisfied. The second predefined navigational constraint has a priority higher than the first predefined navigational constraint. A navigational action for the host vehicle satisfying the second predefined navigational constraint is determined, but not satisfying the first predefined navigational constraint. An adjustment of a navigational actuator of the host vehicle is caused in response to the determined navigational action for the host vehicle. USE - For navigating an autonomous vehicle using reinforcement learning techniques. ADVANTAGE - The navigation system allows implementation of portions of the policy manually which ensures the safety of the policy, and implementation of other portions of the policy using reinforcement learning techniques which enables adaptivity to many scenarios, a human-like balance between defensive/aggressive behavior, and a human-like negotiation with other drivers. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method of navigating autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a side view representation of a vehicle including a system.Navigation system (100)Processing device (110)Image capture device (122)Navigate vehicle (200)				[]	[]	-1	-1	-1
P	ZHANG H; ZHANG Y; CHEN G	Method for controlling autonomous vehicle based on            deep prediction network and deep reinforcement            learning, involves defining upper-level discrete            controller corresponding to control signal at bottom of            vehicle						NOVELTY - The method involves defining an upper-level discrete controller corresponding to a control signal at the bottom of the vehicle. The depth prediction network is built based on double-depth network of the decoder-decoder framework to initialize the weights of each network. The deep reinforcement learning training is conducted on the controlled vehicle. The weight of the network is iteratively updated until the reward value obtained by the controlled vehicle in one round of driving behavior reaches the preset level or the number of training rounds reaches the preset value. The trained deep prediction network and double deep network is deployed to the controlled vehicle. The position and speed of the vehicle is predicted in front of the controlled vehicle through the deep prediction network. USE - Method for controlling an autonomous vehicle based on a deep prediction network and deep reinforcement learning. ADVANTAGE - The method converts the track of the historical vehicle into image data, which can represent the influence of the vehicle interaction between the vehicle track prediction, reduces the whole model parameter, saves the training and calculation cost, compared with the individual vehicle equipped with prediction network, and reduces the calculation cost. The method introduces a channel attention mechanism in the prediction network of encoder-decoder frame, enhances the attention of the model influencing the characteristic of the track prediction and improves the prediction precision. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for the automatic driving vehicle control method based on depth prediction network and depth reinforcement learning to claim. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for controlling autonomous vehicle based on deep prediction network and deep reinforcement learning. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	KWON M	Method for controlling road flow in road            information system using reinforcement learning-based            autonomous vehicle, involves receiving location and            speed of self-driving vehicles and non-autonomous            vehicles as status information at predetermined time            intervals						NOVELTY - The method involves receiving (S10) location and speed of self-driving vehicles and non-autonomous vehicles as status information at predetermined time intervals from a road information system using a selected deep reinforcement learning algorithm. An action of the self-driving vehicle is selected (S20), which is the strength of stepping on the accelerator or brake, based on the state information. A compensation value is derived (S30) for an action of an autonomous vehicle based on a target speed to be reached and the speeds of autonomous vehicles and non-autonomous vehicles. A policy is updated (S40) based on the compensation value. The learned self-driving car is executed together with the non-self-driving cars in the road information system based on the learning result and the location and speed status information of the real-time self-driving car and non-self-driving cars. USE - Method for controlling road flow in road information system using reinforcement learning-based autonomous vehicle e.g. self-driving car. ADVANTAGE - The road flow is smoothly induced by inducing all vehicles on the road to drive close to the target speed by using the self-driving car based on the deep reinforcement learning algorithm. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer-readable storage medium storing program for controlling road flow in road information system using reinforcement learning-based autonomous vehicle; anda road flow control device in a road information system using a deep reinforcement learning-based autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the autonomous driving learning step in the road flow control method in the road information system using the deep reinforcement learning-based autonomous vehicle. (Drawing includes non-English language text)S10Step for receiving location and speed of self-driving vehicles and non-autonomous vehicles as status information at predetermined time intervals from a road information system using a selected deep reinforcement learning algorithmS20Step for selecting the action of the self-driving vehicle, which is the strength of stepping on the accelerator or brake, based on the state informationS30Step for deriving the compensation value for an action of an autonomous vehicle based on a target speed to be reached and the speeds of autonomous vehicles and non-autonomous vehiclesS40Step for updating the policy based on the compensation valueS50Step for terminating the learning, when the predetermined time elapses				[]	[]	-1	-1	-1
P	YANG M; LIU X; LI Z	Deep reinforcement learning based autonomous            vehicle driving behavior decision-making method,            involves calculating final action behavior value of            vehicle by learning structure if action behavior in the            experience pool is not found						NOVELTY - The method involves obtaining current environment state around an autonomous vehicle. Action behavior of the autonomous vehicle in an experience pool is selected and outputted according to the current environment state and current behavior state of the autonomous vehicle. Final action behavior value of the autonomous vehicle is calculated by deep reinforcement learning structure if action behavior corresponding to current environment state in the experience pool is not found. Environmental information of road is received through an red green blue (RGB) camera. Information of obscured objects in the road is received through an infrared camera. Environmental sensing detection is performed on the environmental information. USE - Deep reinforcement learning based autonomous vehicle driving behavior decision-making method. ADVANTAGE - The method enables utilizing the RGB camera, the infrared camera and solid-state lidars to obtain current environmental status, which reduces sensor usage, forming an experience pool by imitating and learning driving experience of human drivers according to human driving habits to solve coexistence problem of manned and unmanned vehicles on the road and improving safety performance. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep reinforcement learning based autonomous vehicle driving behavior decision-making method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	JAFARI T S R; GUPTA P; MEHDI S B; PALANISAMI P; PALANISAMY P	Method for performing lane change by autonomous            vehicle, involves evaluating lane data, map data,            vehicle data and participant data, and controlling            vehicle by processor to perform lane change based on            lane change action						NOVELTY - The method involves determining a desired lane change by a processor. Lane change action is determined by the processor based on enhanced learning process and rule-based process. Lane data, map data, vehicle data and participant data are evaluated. A vehicle is controlled by the processor to perform lane change based on the lane change action, where the lane change action includes a gap between two vehicles on a road and an identifier performs a timing of the lane change. The lane change action is determined based on a reinforcement learning to satisfy a rule-based constraint. USE - Method for performing lane change by an autonomous vehicle. ADVANTAGE - The system controls the vehicle to perform lane change by the processor based on the lane action in an effective manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for performing lane change by an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating of a method for performing lane change by an autonomous vehicle.				[]	[]	-1	-1	-1
P	ENGLARD B; MAHESHWARI P; KHILARI S C; RAMEZANI V R	Non-transitory computer-readable medium for            implementing self-driving control architecture for            controlling e.g. car, has set of instructions for            utilizing values of variables to generate decisions for            maneuvering autonomous vehicle						NOVELTY - The medium has a set of instructions for utilizing signals descriptive of a current state of an environment and signals descriptive of a set of predicted future states of the environment to set values of a set of independent variables in objective equation, where the objective equation includes a set of terms that each correspond to different one of a set of driving objectives over a finite time horizon. Values of the set of dependent variables in the objective equation are determined by solving the objective equation subject to a set of constraints. The determined values of the dependent variables are utilized to generate decisions for maneuvering an autonomous vehicle (400) toward a destination. USE - Non-transitory computer-readable medium for implementing a self-driving control architecture for controlling an autonomous vehicle (claimed) i.e. car (from drawings). ADVANTAGE - The medium enables improving performance of the autonomous vehicle if candidate decisions generated by a self-driving control architecture reflect greater level of diversity. The medium enables utilizing a computing system to provide the generated decisions to operational subsystems so as to realize effectuate maneuver of the autonomous vehicle in accordance with the generated decisions. The medium enables training an arbitration model by using reinforcement learning for completely avoiding safety violations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for controlling an autonomous vehicle(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a top view of an autonomous vehicle.Autonomous vehicle (400)Accelerator (440)Brakes (442)Vehicle engine (444)Steering mechanism (446)				[]	[]	-1	-1	-1
P	PATHAK S; NADKARNI V J; BAG S	System for controlling an autonomous vehicle,            comprises first sensor for detecting environment            characteristic, where driver characteristic input            device is configured to receive driver characteristic            corresponding to driving style of driver						NOVELTY - The system comprises a first sensor for detecting an environment characteristic, where a driver characteristic input device is configured to receive a driver characteristic corresponding to a driving style of a driver. A controller includes a reinforcement learning adaptive cruise control that is in communication with the first sensor and the driver characteristic input device. The reinforcement learning adaptive cruise control is configured to determine a target behavior for the vehicle based on the environment characteristic and the driver characteristic, and selectively controls the vehicle based on the target behavior. USE - System for controlling a vehicle, such as an autonomous vehicle. ADVANTAGE - Ensures to control the vehicle to follow an other vehicle at a constant velocity, while maintaining a safe distance between the vehicle and the other vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a longitudinal control apparatus for controlling a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of a learning-based adaptive cruise control.				[]	[]	-1	-1	-1
P	ZHAO Y; XUAN Z; MA K; SUN X; LIU L; LIN W	Method for managing speed control of autonomous            vehicle using human driving mode, involves comparing            vehicle control command with driving behavior standard            and verifying or modifying vehicle control command            according to comparison, and causing autonomous vehicle            to execute vehicle control command						NOVELTY - The method involves receiving a vehicle control command before controlling an autonomous vehicle to execute a vehicle control command. A vehicle control command with a driving behavior standard and verifying or modifying the vehicle control command is compared according to a comparison. The autonomous vehicle is used to execute the vehicle control command in accordance with the verified or modified vehicle control command. A driving behavior criteria is determined by training a reinforcement learning process comprising training by simulation to generate a data from the driving behavior criteria. The data is compared with data corresponding to human driving behavior during simulation. USE - Method for managing speed control of autonomous vehicle using human driving mode. ADVANTAGE - The method involves generating data corresponding to a desired human driving behavior, and training a human driving model module using an enhanced learning process, and ensures simple and efficient managing of the speed control of the autonomous vehicle using human driving mode. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:1. a device; and2. a computer-readable storage medium comprising a set of instructions for managing speed control of autonomous vehicle using human driving mode. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for managing speed control of autonomous vehicle using human driving mode (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	NAGESHRAO S; JALES C B S; FILEV D P	System for interpreting data from a reinforcement            learning agent controller to control autonomous            vehicle, comprises a memory for storing instructions            that are executed by the processor that include            calculating multiple state-action values						NOVELTY - The data interpreting system (10) comprises a processor (20), and a memory (22) for storing instructions that are executed by the processor. The instructions include calculating multiple state-action values based on sensor data representing an observed state through a deep neural network. A variety of linear models is generated that map the variety of state-action values to the sensor data. The processor is programmed to generate multiple linear models using an Evolving Takagi Sugeno model. USE - System for interpreting data from a reinforcement learning agent controller to control an autonomous vehicle. ADVANTAGE - Ensures to optimize a speed of the vehicle in relation to the lead vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block representation of a system for interpreting data from a reinforcement learning agent controller to control an autonomous vehicle. (Drawing includes non-English language text).Data interpreting system (10)Reinforcement learning agent controller (12)Fuzzy control (14)Processor (20)Memory (22)				[]	[]	-1	-1	-1
P	LIU Y; OGUCHI K; QI X	System for learning optimal driving behavior for            autonomous vehicle i.e. car, has first stage training            module for training feature learning network during            first training stage, and second stage training module            for training decision action network during second            training stage						NOVELTY - The system has feature learning network for receiving sensor data from a vehicle as input and outputting spatial temporal feature embeddings. Decision action network receives the spatial temporal feature embeddings as input and outputs optimal driving policy for the vehicle. A first stage training module trains the feature learning network during first training stage by object detection loss. A second stage training module trains the decision action network during second training stage by reinforcement learning. Spatial feature learning network receives the sensor data as input and outputs spatial feature embeddings. USE - System for learning optimal driving behavior for an autonomous vehicle i.e. car (from drawings). ADVANTAGE - The system can provide an improved learning system for autonomous vehicles to learn optimal driving decisions and policy, and allows the decision action network to receive the spatial temporal feature embeddings as input and output the optimal driving policy for the vehicle, thus determining optimal driving behavior of the vehicle in crowded driving conditions where occlusions are present, and hence improving system performance and extendibility. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for learning optimal driving behavior for an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system for learning optimal driving behavior for autonomous vehicle i.e. car.100Autonomous vehicle optimal driving behavior learning system102Receiver vehicle104Transmitter vehicle106Road				[]	[]	-1	-1	-1
P	WONGPIROMSARN T; COLLIN A; BELTA C; TEBBENS R D; HELOU B; BEIJBOM O O	Method for scoring trajectories for autonomous            vehicles, involves identifying trajectory with            inadequate performance by augmented route planner based            on metric associated with predicted reasonableness            scores, and changing parameters of model						NOVELTY - The scoring method involves augmenting the route planner of a first vehicle using predicted reasonableness scores for trajectories by using one or more processors. The reasonableness scores are predicted by a trained machine learning model with parameters determined using a loss function that penalizes predictions of the scores that violate a rule book structure. One or more processors plan trajectories in an environment using the augmented route planner. One or more processors identify one trajectory with inadequate performance by the augmented route planner based on a first metric associated with the predicted reasonableness scores. One or more processors change the parameters of the augmented route planner in response to the identified trajectory. USE - Scoring method for trajectories of autonomous vehicles and/or other objects. ADVANTAGE - The method enables using a machine learning model to predict reasonableness scores for autonomous vehicle trajectories for a given traffic scenario, so that the predicted scores can be used to efficiently tune a route planner and performance of the route planner, compare two an autonomous vehicle stacks, and compare reinforcement learning and any other desired application. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a scoring system; (2) a non-transitory storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of an autonomous vehicle.Autonomous vehicle (100)Stereo video camera (122)Database (134)Server (136)Environment (190)				[]	[]	-1	-1	-1
P	XIE J; WANG J; HUA G; HUANG Z	Autonomous vehicle lane keeping method based on            TD3 algorithm modified by exploration strategy,            involves collecting lot of training data through            interaction with environment and using data to learn,            updating algorithm and finally converging to optimal            strategy						NOVELTY - The method involves calculating input automatic driving vehicle state and sensor information. In the training stage of the double-delay depth certainty strategy gradient algorithm, the noise is added to the action output by the double-delay depth certainty strategy gradient algorithm by using an Ornstein-Ulnbeck process for fully exploring a state space. The noise of the Ornstein-Ulnbeck process is subjected to weighted correction based on a path tracking method, so that invalid exploration of an automatic driving vehicle in the training process is reduced. Many experiments are carried out on a TORCS simulation platform. An automatic driving vehicle is guided by a double-delay depth certainty strategy gradient algorithm improved by an exploration strategy. Multiple training data are collected through interaction with the environment. The data are used for learning, the algorithm is updated, and finally the optimal strategy is converged. USE - Autonomous vehicle lane keeping method based on a TD3 algorithm modified by exploration strategy in field of deep reinforcement learning and automatic driving. ADVANTAGE - Improves the quality of the training samples obtained in the interaction process of an unmanned vehicle, and improves the performance of the algorithm. The exploration of the unmanned vehicle is biased to the correct direction, the proportion of low-efficiency samples in an experience playback pool is reduced, the algorithm is finally converged more quickly, and the strategy has better expressiveness. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of the autonomous vehicle lane keeping method based on a TD3 algorithm modified by exploration strategy.				[]	[]	-1	-1	-1
P	PARK T H; KIM M	Method for determining lane change of autonomous            driving vehicle using reinforcement learning in            vehicle-only road environment, involves receiving            vehicle information of surrounding vehicles through            vehicle to everything communication						NOVELTY - The method involves receiving (S101) vehicle information of surrounding vehicles through vehicle to everything (V2X) communication. The received vehicle information is used (S103) to select an important vehicle, which is a neighboring vehicle that has the greatest influence in determining a lane change of the autonomous vehicle. A lane change probability of the important vehicle is calculated (S105). The lane change probability is added (S107) to vehicle information. The pre-processing necessary for reinforcement learning is performed (S109) through a pre-processing network on the information obtained by adding the lane probability to the vehicle information. The reinforcement learning is performed (S111) by adding information about the autonomous driving vehicle to the information preprocessed in the preprocessing network, and a lane change determination result is outputted. USE - Method for determining lane change of autonomous vehicle using reinforcement learning in vehicle-only road environment. ADVANTAGE - The method enables determining the lane change of the autonomous vehicle using reinforcement learning in an automobile-only road environment so as to ensure real-time performance and flexibly respond flexibly to movement change of other vehicles. The method enables exhibiting better performance even in a road environment in which lanes change is performed by adding direct characteristic information on lane changes. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer-readable recording medium storing program for determining lane change of autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for determining a lane change of an autonomous vehicle using reinforcement learning in a vehicle-only road environment. (Drawing includes non-English language text)S101Step for receiving vehicle information of surrounding vehiclesS103Step for using received vehicle information to select important vehicleS105Step for calculating lane change probability of important vehicleS107Step for adding lane change probability to vehicle informationS109Step for performing pre-processing necessary for reinforcement learningS111Step for performing reinforcement learning by adding information about autonomous driving vehicle to information preprocessed in preprocessing network				[]	[]	-1	-1	-1
P	LEE D	Reinforcement learning-based agent for collision avoidance and autonomous driving of autonomous vehicle, has operating environment that is provided in which speed control command is assigned by processing action value of learning model						NOVELTY - The agent has a sensor fusion model (100) that creates a state memory-based state variable by fusion of multi-sensor data. An artificial neural network-based reinforcement learning model (200) outputs a behavior value for motion control of an autonomous moving object based on the state memory-based state variable of the sensor fusion model. An operating environment (300) is provided in which a speed control command is assigned by processing the action value of the artificial neural network-based reinforcement learning model. A multi-sensor (110) is provided with two sensors and generates state data by sensing surrounding objects. USE - Reinforcement learning-based agent for collision avoidance and autonomous driving of autonomous vehicle (claimed). ADVANTAGE - The performance is improved more and more because users train themselves. The behavior value directly outputs to the operating environment to exhibit simple collision avoidance and autonomous driving performance of the autonomous vehicle, and trained to improve collision avoidance and autonomous driving performance. The agent efficiently reduces the processing time by using the depth sensor and the ultrasonic sensor at the same time because the ultrasonic sensor estimates the area of pixels in the depth map image. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a reinforcement learning-based agent. (Drawing includes non-English language text)Sensor fusion model (100)Multi-sensor (110)Artificial neural network-based reinforcement learning model (200)Operating environment (300)Speed value conversion unit (310)				[]	[]	-1	-1	-1
P	KWON M; LEE D	Method for solving congestion using deep            reinforcement learning-based autonomous driving            vehicle, involves operating autonomous vehicle based on            learned policy to determine behavior of autonomous            vehicle						NOVELTY - The method involves selecting an algorithm and a reward function from among multiple deep reinforcement learning for self-driving vehicle learning in an environment of a round road in which autonomous vehicles and non-autonomous vehicles operate. A deep neural network structure is determined according to the selected deep reinforcement learning algorithm. The selected deep reinforcement learning algorithm, autonomous driving is used based on state information and compensation information including the speed of the autonomous driving vehicle and the relative speed and relative position between the autonomous driving vehicle and the observable vehicle at each predetermined time learning a policy in which the vehicle speed is closest to constant speed driving. The autonomous vehicle is operated based on the learned policy to determine the behavior of the autonomous vehicle. USE - Method for solving congestion using deep reinforcement learning-based self-driving vehicle. ADVANTAGE - The decision-making model that controls the flow of roads in a vehicle-dense environment is provided. The algorithm for the most efficient driving is selected and applied by comparing and analyzing the driving patterns and performance of autonomous vehicles learned through each deep reinforcement learning algorithm. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a computer-readable storage medium storing program for solving congestion; anda device for solving congestion. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a stop-and-go wave phenomenon in a circular road that is an autonomous driving environment. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	HU Y; NAKHAEI S A; TOMIZUKA M; FUJIMURA K	Method for performing interaction-aware decision            making using simulated autonomous vehicle, involves            generating multi-goal, multi-agent, multi-stage and            interaction-aware decision making network policy based            on neural networks						NOVELTY - The method involves training a first agent based on a first policy gradient and training a first critic based on a first loss function to learn goals in a single-agent environment, where the first agent is associated with a first agent neural network and the first critic is associated with a first critic neural network. A number of agents are trained based on the first policy gradient and training a second policy gradient and a second critic based on the first loss function and a second loss function to learn the goals in a multi-agent environment including the first agent and the number of agents using a Markov game to instantiate a second agent neural network, where each agent is associated with a driver type indicative of a level of cooperation for the respective agent. A multi-goal, a multi-agent, a multi-stage and an interaction-aware decision making network policy are generated based on the first agent neural network and the second agent neural network. USE - Method for performing interaction-aware decision making using a vehicle (claimed) i.e. simulated autonomous vehicle. ADVANTAGE - The method enables providing a multi-agent curriculum and distribution can be constructed to ensure sufficient training on difficult goal combinations that require cooperation, along with easier combinations for maintaining agent's ability to act toward goal. The method enables using Q-values associated with remaining subset of actions considered by the traffic simulator during simulation, thus mitigating amount of processing power and/or computing resources utilized during simulation and training of the autonomous vehicle in autonomous vehicle policy generation. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for interaction-aware decision making. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for performing cooperative multi-goal, multi-agent multi-stage reinforcement learning.Vehicle (170)Vehicle communication interface (172)Storage device (174)Controller (176)Vehicle system (178)				[]	[]	-1	-1	-1
P	NIU Q; WANG H; CAO R; LI S; SHI W; XU J; YU Z; WANG J; LIU S	Multi-autonomous vehicle (AUV) collaborative            underwater data acquisition method, involves            continuously correcting moving track, and collecting            data of key node based on data collection method of            matrix complement type						NOVELTY - The method involves setting underwater acoustic sensor nodes on an underwater cloth. Different sensing areas are formed. An AUV collecting sensing area is selected. A path of multi-AUV information collection is planned based on a deep reinforcement learning (DQN) method. A reward function is designed according to an information value. A moving track in a multi- AUV sailing process is continuously corrected. Data of a key node is collected based on the data collection method of a matrix complement type. USE - Multi-autonomous vehicle (AUV) collaborative underwater data acquisition method based on deep reinforcement learning (DQN) and matrix completion used in the field of marine data collection. ADVANTAGE - The method enables setting the underwater acoustic sensor node in the ocean area according to the correlation and timeliness between the node generating data and application needed data, calculating the value of each area generating information, and for subsequent AUV path planning, which greatly reduces the loss of the data value, planning multiple AUV information collecting path by using depth reinforcement learning method, and considering underwater acoustical sensor node drift caused by ocean current impact, continuously correcting AUV navigation track in the AUV driving process, and designing a data acquisition method of matrix compensation type, making AUV only need to collect data generated by key node, reducing the workload of data collection. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the multi-autonomous vehicle (AUV) collaborative underwater data acquisition method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	SONG S; CIOCARLIE M	System for selecting action to be taken by            reinforcement learning agent e.g. robot, in            environment, has hardware processor coupled to memory,            where processor determines that variance meets            threshold, and requests identification of action to be            taken by reinforcement learning agent						NOVELTY - The system has a hardware processor (502) coupled to a memory (504), where the processor determines a first variance for a first state of an environment, where the variance is based on reinforcement learning. The processor determines that the variance meets a threshold, requests an identification of a first action to be taken by a reinforcement learning agent from a human, receives the identification of the first action, and causes the first action to be taken by the agent in response to determining that the first variance meets the threshold, where the agent is one of an autonomous vehicle and a robot. The processor determines a second variance for a second state of the environment, and selects a second action based on the reinforcement learning policy. USE - System for selecting an action to be taken by a reinforcement learning agent e.g. autonomous vehicle and robot (all claimed), in an environment. ADVANTAGE - The system allows the reinforcement learning agent to be able to always interact with an environment optimally, provides an efficient way to select actions to be taken by the agent in a deterministic environment, and allows the agent to learn behavior of the environment efficiently, thus improving performance of the agent. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a method for selecting an action to be taken by a reinforcement learning agent in an environment; (2) a non-transitory computer-readable medium comprising a set of instructions for selecting an action to be taken by a reinforcement learning agent. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for selecting an action to be taken by a reinforcement learning agent.502hardware processor504Memory506input device controller508Input device514communication interfaces				[]	[]	-1	-1	-1
P	LI R; ZHAO Y; LI X; WEI H; XU Z; ZHANG Y	Method for controlling automatic driving vehicle,            involves returning to step of obtaining real-time            traffic environment information of automatic driving            vehicle during driving process at current moment if            automatic driving is not ended						NOVELTY - The method involves obtaining (S101) real-time traffic environment information of an autonomous vehicle during a driving process at a current moment. Real-time traffic environment information is mapped (S102) based on a preset mapping relationship to obtain mapped traffic environment information. A target deep reinforcement learning model is adjusted (S103) based on a pre-stored existing deep reinforcement learning model and mapped traffic environment information. A determination is made (S104) on whether to end the automatic driving. The method is returned (S105) to the step of obtaining the real-time traffic environment information of the automatic driving vehicle during a driving process at a current moment if the automatic driving is not ended in which the mapping relationship includes the mapping relationship between the real-time traffic environment information and the existing traffic environment information of the existing deep reinforcement learning model. USE - Method for controlling automatic driving vehicle. ADVANTAGE - The method avoided adjusting the target deep reinforcement learning model from scratch, speeds up the decision-making efficiency of the target deep reinforcement learning model, and performs fast and stable automatic driving. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an automatic driving system;(2) an automatic driving device; and(3) a computer readable storage medium storing program for performing automatic driving. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an automatic driving method. (Drawing includes non-English language text)Step for obtaining real-time traffic environment information of an autonomous vehicle during a driving process at a current moment (S101)Step for mapping real-time traffic environment information based on a preset mapping relationship to obtain mapped traffic environment information (S102)Step for adjusting target deep reinforcement learning model based on a pre-stored existing deep reinforcement learning model and mapped traffic environment information (S103)Step for determining whether to end the automatic driving (S104)Step for returning to step of obtaining the real-time traffic environment information of automatic driving vehicle during driving process at current moment if automatic driving is not ended (S105)				[]	[]	-1	-1	-1
P	HOEL C; LAINE L	Method for controlling autonomous vehicle using            reinforcement learning agent, involves providing set of            training sessions in which reinforcement learning agent            interacts with environment including autonomous            vehicle						NOVELTY - The method involves providing a reinforcement learning (RL) agent (320) for decision-making to be used in controlling an autonomous vehicle (299). A set of training sessions is provided in which the RL agent interacts with an environment including the autonomous vehicle, where each training session includes a different initial value, and yields a state-action quantile function dependent on states and actions. A tentative decision relating to control of the vehicle is executed in dependence of two estimated uncertainties. The RL agent is provided with a neural network, where the training sessions employ an implicit quantile network. USE - Method for controlling an autonomous vehicle i.e. car using a reinforcement learning agent. ADVANTAGE - The aleatoric and epistemic uncertainty of outputs of the decision-making agent can be assessed effectively. The need for additional training of the RL agent is assessed based on aleatoric and epidemiologic uncertainty. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a method for providing a reinforcement learning; andan arrangement for controlling an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of an arrangement for controlling an autonomous vehicle.299Autonomous vehicle314Vehicle control interface320RL agent322First uncertainty estimator324Second uncertainty estimator				[]	[]	-1	-1	-1
P	FENG Y; ZHAO X; HUI F; JING S	Lane change trajectory planning method for            autonomous vehicle, involves calculating vehicle state            of lane changing vehicle at each time point in changing            process by using vehicle transverse and longitudinal            discretization kinematics model through optimal            sequential acceleration decision information						NOVELTY - The method involves acquiring vehicle information. The optimal lane changing decision of the lane changing vehicles at the current moment is solved, according to vehicle information, a game lane changing decision model of lane changing vehicles and surrounding vehicles, a game income function considering safety and timeliness and a game income matrix. An automatic driving vehicle lane changing track planning model based on deep reinforcement learning is utilized to obtain optimal sequential acceleration decision information in the whole lane changing process, according to the optimal lane changing decision. The vehicle state of the lane changing vehicle at each time point in the lane changing process by using the vehicle transverse and longitudinal discretization kinematics model through the optimal sequential acceleration decision information is calculated, and the lane changing track of the lane changing vehicle is obtained according to the vehicle state of the lane changing vehicle. USE - Lane change trajectory planning method for autonomous vehicle. ADVANTAGE - The method completes the track changing planning of the automatic driving vehicle under the condition of considering safety, high efficiency, comfort and fuel economy. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a lane change trajectory planning system for autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the lane change trajectory planning method for autonomous vehicle. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	SHANKAR M; RAJPUT S	Method for training recurrent neural network for            e.g. autonomous car, utilizing reinforcement learning            model, involves training recurrent neural network based            on randomly selected sets of training representations            and driving states						NOVELTY - The method involves providing a recurrent neural network (RNN) configured to receive representations of surroundings of vehicles (T1). Training data comprising sets of training representations of surroundings of vehicles and corresponding driving states is provided (T2) to derive a ground truth. The RNN is trained (T3) based on randomly selected sets of training representations and corresponding driving states from the training data, where the sets of training representations of surroundings of vehicles are recorded during real drives of real vehicles, and the corresponding driving states are recorded during the respective real drives of the real vehicles. USE - Method for training a RNN for an autonomous vehicle i.e. autonomous car, utilizing a reinforcement learning model by a data processing system (claimed). ADVANTAGE - The method enables optimizing filters or kernels through an automated learning model, thus improving performance of the RNN in an efficient manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a computer program for storing a set of instructions for training a RNN for an autonomous vehicle utilizing a reinforcement learning model by a data processing system; anda computer-readable medium for storing a set of instructions for training a RNN for an autonomous vehicle utilizing a reinforcement learning model by a data processing system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for training a RNN for an autonomous vehicle utilizing a reinforcement learning model by a data processing system.T1Step for providing RNN configured to receive representations of surroundings of vehiclesT2Step for providing training data comprising sets of training representations of surroundings of vehicles and corresponding driving states to derive ground truthT3Step for training RNN based on randomly selected sets of training representations and corresponding driving states from training data				[]	[]	-1	-1	-1
P	RAIKO P T; LEVIHN M	Method for evaluating varying-size action spaces            for autonomous vehicles using neural network-based            reinforcement learning models, involves transmitting            motion-control directives to implement particular            action of set to subsystem of vehicle						NOVELTY - The method involves identifying a first set of actions corresponding to a first state of an environment of a first vehicle. A first encoding of the first action and a second encoding of the second action are generated, where in the first encoding, the first target lane segment is indicated by a first color. The respective estimated value metric associated with individual actions of the first set is determined using a plurality of instances of a first machine learning model. An input data set of a first instance of the first machine learning model comprises the first encoding, and an input data set of a second instance of the first machine learning model comprises the second encoding. The motion-control directives to implement a particular action of the first set are transmitted to a motion-control subsystem of the first vehicle. The particular action is selected from the first set based on its estimated value metric. USE - Method for evaluating varying-size action spaces for autonomous vehicles using neural network-based reinforcement learning models. ADVANTAGE - The method involves determining a representation of the current state of the environment of an autonomous or partially-autonomous vehicle at various points in time during a journey, and identifying a corresponding set of feasible or proposed actions, and thus enables to make timely and reasonable decisions regarding an autonomous vehicle's trajectory in the context of unpredictable behaviors of other entities and incomplete or noisy data about static and dynamic components of the vehicle's environment remains a significant challenge. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following;1. System for evaluating varying-size action spaces for autonomous vehicles; and2. Non-transitory computer-accessible storage media storing program for evaluating varying-size action spaces for autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram illustrating the decision making scenario for an autonomous vehicle.Origin (215)Entrance ramp (234)Exit ramp (235)Autonomous vehicle (250)Autonomous vehicle motion planning problem characteristics (261)				[]	[]	-1	-1	-1
P	SAXENA D M; BAE S; NAKHAEI S A; FUJIMURA K	System for generating model-free reinforcement            learning policy using autonomous vehicle e.g. car, has            simulator that is implemented via processor and memory            for building policy based on simulated traffic scenario            using actor-critic network						NOVELTY - The system (100) has a processor (102) and a memory (104). A simulator (108) is implemented via the processor and the memory for generating a simulated traffic scenario including lanes, an ego-vehicle (170), a dead end position, and traffic participants. The dead end position is a position by which a lane change for the ego-vehicle is desired. The simulated traffic scenario is associated with an occupancy map, a relative velocity map, a relative displacement map, and a relative heading map at each time step within the simulated traffic scenario. The ego-vehicle and the traffic participants are modeled using a kinematic bicycle model. A policy is built based on the simulated traffic scenario using an actor-critic network. The policy is implemented on an autonomous vehicle. USE - System for generating model-free reinforcement learning policy using autonomous vehicle e.g. car, truck, van, minivan, sport utility vehicle (SUV), motorcycle, scooter, boat, personal watercraft, and aircraft, battery electric vehicle (BEV) and plug-in hybrid electric vehicle. ADVANTAGE - The controller is enabled to autonomously drive the vehicle around based on the policy network and to make autonomous driving decisions according to the generating a model-free reinforcement learning policy which occurred within the simulator, as the policy network is indicative of the policies or decisions which should be made based on the training or the simulation. The policies target the states and actions specifically in each repetition cycle, testing and improving the accuracy of the policy, to improve the quality of the model. The reinforcement is applied by continually re-running or re-executing the learning process based on the results of prior learning, effectively updating an old policy with a newer policy to learn from the results and to improve the policy. The policy is built in a model free fashion to mitigate capturing interactions from each traffic participant at each time step, which reduces the associated computational cost for the system. The simulation enables the policy to learn to repeatedly probe into a target road lane while finding a safe spot to move into. The size of the action space is greatly increased to achieve smooth behaviors with high enough fidelity via discrete control, which makes discrete control methods intractable. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for generating the model-free reinforcement learning policy;(2) an autonomous vehicle implementing a model-free reinforcement learning policy. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the system for model-free reinforcement learning.System for model-free reinforcement learning (100)Processor (102)Memory (104)Simulator (108)Vehicle (170)				[]	[]	-1	-1	-1
P	KIM K; KIM Y; KIM I; KIM H; NAM W; BOO S; SUNG M; YEO D; RYU W; JANG T; JEONG K; JE H; CHO H; KIMKYEHYUN; KIM Y J; KIM I S; KIMHAGKYUNG; NAM W H; BOO S H; SUNG M C; RYU W J; JANG T U; JEONG K J; JE H M; CHO H J; JIN G; JIN R; JIN Y; JIN H; NAN Y; FU S; CHENG M; LV D; LIU Y; ZHANG T; ZHENG J; ZHU H; ZHAO H	Method for efficient resource allocation in            autonomous driving by reinforcement learning of            autonomous vehicle involves adjusting at least portion            of parameters used for neural network operation by            referring to reward by computing device						NOVELTY - The method involves enabling computing device to perform at least one neural network operation by referring to the attention sensor data to calculate one or more attention scores (210), and acquire at least one video data taken by at least portion of one or more cameras installed on the autonomous vehicle (200) by referring to the attention scores and generate at least one decision data for the autonomous driving by referring to the video data. The computing device operates autonomous vehicle by referring to the decision data to acquire at least one circumstance data representing change of circumstance of the autonomous vehicle in operation, and to generate at least one reward by referring to the circumstance data. The computing device adjusts at least portion of one or more parameters used for the neural network operation by referring to the reward. USE - Method for efficient resource allocation in autonomous driving by reinforcement learning of autonomous vehicle. ADVANTAGE - Performs resource allocation efficiently in autonomous driving by reinforcement learning which reduces power consumption of the autonomous vehicle. Provides virtual space where the autonomous vehicle optimizes the resource allocation by the reinforcement learning which reduces potential risks in the reinforcement learning. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing device for efficient resource allocation in autonomous driving by reinforcement learning of autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing is the schematic view illustrating attention scores calculated for performing the efficient resource allocation.Autonomous vehicle (200)Attention scores (210)Panoramic image (220)Specific directions (222,223)Region of interest panoramic image (230)				[]	[]	-1	-1	-1
P	HOPPE S; LOU Z	Method for automatically influencing actuator such            as particular robot, machine, partially autonomous            vehicle or tool, involves defining upper confidence            bound as function of empirical unit and variance over            distribution of advantages						NOVELTY - The method involves providing (300) a state of an actuator or an environment of the actuator by an exploration strategy for learning a policy. An action for the automated influencing of the actuator depending on the state is defined (308) by the policy. A state value is defined as an expected value for a sum of rewards that are achieved when the policy is followed based on the state. A state action value is defined as an expected value for a sum of rewards that are achieved if any action is first carried out in the state and then the policy is carried out. The action that maximizes an empirical unit over a distribution is defined by the policy for the state. A state that locally maximizes an upper confidence bound is specified by the exploration strategy. The upper confidence bound is defined as a function of an empirical unit and a variance over the distribution of multiple advantages. USE - Method for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool. ADVANTAGE - The influencing of an actuator by reinforcement learning is improved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a device for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool; and(2) a computer program product having instructions for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating am method for automatically influencing actuator such as particular robot, machine, partially autonomous vehicle or tool.Step for providing a state of an actuator or an environment of the actuator by an exploration strategy for learning a policy (300)Step for moving actuator moved to a waypoint (302)Step for determining whether a collision of the actuator with an obstacle in the vicinity of the actuator occurs when moving from one waypoint to a next waypoint in the series (304)Step for judging whether the next waypoint can be reached when moving from one waypoint to a next waypoint in the series (306)Step for defining action for the automated influencing of the actuator depending on the state by the policy (308)				[]	[]	-1	-1	-1
P	NISHI T	Controlling method for autonomous vehicle,            involves applying passive actor-critic reinforcement            learning method to passively-collected data related to            vehicle operation, and Z-value is estimated using            linearized version of bellman equation						NOVELTY - The controlling method involves applying the passive actor-critic reinforcement learning method to passively-collected data related to the vehicle operation. A control policy is adapted for the vehicle to perform the vehicle operation. The control policy is configured for controlling the vehicle to merge the vehicle midway between the second vehicle and third vehicle. The Z-value is estimated using a linearized version of a bellman equation. The average cost is estimated under the optimal policy. USE - Controlling method for an autonomous vehicle, such as a hybrid vehicle. ADVANTAGE - The control policy is adapted for the vehicle to perform the vehicle operation with a minimum expected cumulative cost, and ensures allowing the communications between the nearby vehicles. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for optimizing the control policy for performing the vehicle operation; and(2) a computing system for optimizing he control policy for autonomously controlling the vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of the controlling method.Computing system (14)Communications interfaces (16)Steering system (18)Throttle system (20)Braking system (22)				[]	[]	-1	-1	-1
P	HOTSON G; MOOSAEI M; NARIYAMBUT MURALI V; JAIN J J; NARIYAMBUT M V	Method for operating autonomous vehicle, involves            receiving inputs from passenger, which is followed by            receiving sensor data, and updating control logic            according to inputs and sensor data to obtain updated            control logic						NOVELTY - The autonomous vehicle operation method involves receiving (304) multiple inputs from a passenger of an autonomous vehicle. The sensor data is received (306) from the autonomous vehicle. The control logic of the autonomous vehicle is updated (310) according to the inputs and the sensor data to obtain updated control logic. USE - Method for operating an autonomous vehicle. ADVANTAGE - The deep reinforcement learning models are trained based on the feedback to promote actions that were rated highly by the passenger and uneventful ride and reduce occurrence of actions that are present in lowly rated rides or flagged as anomalies by the passenger. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for operating autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for updating control logic based on passenger feedback.Presenting interface (302)Receiving multiple inputs (304)Receiving sensor data (306)Training model per feedback (308)Updating control logic (310)				[]	[]	-1	-1	-1
P	KIM K; KIM Y; KIM H; NAM W; BOO S; SUNG M; SHIN D; YEO D; RYU W; LEE M; LEE H; JANG T; JEONG K; JE H; CHO H; KIMKYEHYUN; KIM Y J; KIMHAGKYUNG; NAM W H; BOO S H; SUNG M C; SHIN D S; RYU W J; LEE M C; SOO L H; JANG T U; JEONG K J; JE H M; CHO H J; JIN G; JIN R; JIN H; NAN Y; FU S; CHENG M; SHEN D; LV D; LIU Y; LI M; LI J; ZHANG T; ZHENG J; ZHU H; ZHAO H; KIM I S	Learning method for use with an autonomous vehicle            for supporting a Reinforcement Learning (RL) by using            human driving data as training data, to perform a            personalized path planning by a learning device,            involves learning device						NOVELTY - The method involves learning device. If actual circumstance vectors and information on actual actions performed at timings corresponding to the actual circumstance vectors by referring to actual circumstances, corresponding there to included in each of driving trajectories of the subject driver are acquired, a process of instructing an adjustment reward network is performed, which is built to operate as an adjustment reward function, to generate each of first adjustment rewards corresponding to each of the actual actions performed at each of the timings. The learning device instructs a first loss layer to generate one adjustment reward loss by referring to each of first personalized rewards and the actual prospective values, and to perform backpropagation by referring to the adjustment reward loss, to learn portion of parameters of the adjustment reward network. USE - Learning method for use with an autonomous vehicle for supporting a Reinforcement Learning by using human driving data as training data, to perform a personalized path planning by a learning device (claimed). ADVANTAGE - Provides a learning method for supporting a Reinforcement Learning algorithm by using human driving data as training data, to provide a personalized path planning, and then to provide a satisfied driving experience to passengers of an autonomous vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a testing method for using at least one personalized reward function to train a personalized Reinforcement Learning agent; and(2) a learning device for acquiring at least one personalized reward function, used for performing a Reinforcement Learning algorithm. DESCRIPTION Of DRAWING(S) - The drawing shows a block representation of a configuration of a learning device performing a learning method for supporting a Reinforcement Learning by using human driving data as training data, to perform a personalized path planning.Learning device (100)Communication portion (110)Memory (115)Processor (120)Adjustment reward network (130)				[]	[]	-1	-1	-1
P	HOEL C; LAINE L	Control method for autonomous vehicle using            reinforcement learning (RL) agent involves executing            tentative decision in dependence of estimated            uncertainty to control autonomous vehicle						NOVELTY - The method (100) involves interacting RL agent with an environment including the autonomous vehicle provided in several training sessions (110-1-110-K). Each training session is provided with a different initial value and a state-action value function Qk(s, a) dependent on state and action is yielded. The decision-making (112) is made in which the RL agent outputs tentative decision relating to control of the autonomous vehicle. Uncertainty estimation (114) is made on the basis of a variability measure for several state-action value functions evaluated for a state-action pair corresponding to each of the tentative decisions. The tentative decision is executed in dependence of the estimated uncertainty and vehicle control (116). USE - Method of controlling an autonomous vehicles such as trucks, buses, construction equipment, mining equipment and other heavy equipment operating in public or non-public traffic, using a RL agent to make decisions such as when to change lanes on a highway, or whether to stop or go at an intersection. ADVANTAGE - The method enables providing a safety criterion that determines whether the trained decision-making agent is confident enough about a particular decision, so that the agent can be overridden by a safety-oriented fallback decision in the negative case. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an arrangement for controlling an autonomous vehicle; and(2) a computer program for controlling an autonomous vehicle using a RL agent. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method.Method (100)Several training sessions (110-1-110-K)Step for decision-making in which the RL agent outputs tentative decision relating to control of the autonomous vehicle (112)Step for estimating uncertainty on the basis of a variability measure for several state-action value functions evaluated for a state-action pair corresponding to each of the tentative decisions (114)Step for executing tentative decision in dependence of the estimated uncertainty and vehicle control (116)				[]	[]	-1	-1	-1
P	FELIP L J; ALVAREZ I J; ELLI M S; GONZALEZ A D I; TUREK J; ALVAREZ I; GONZALEZ A D; FELIP L J F	Map generation system for use with autonomous            vehicle, has analyzer that is configured to analyze            vector-field map to identify vectors of several cells            exceeding predetermined threshold values						NOVELTY - The map generation system has a data aggregator (305) that is configured to aggregate data received from autonomous vehicles (AVs) to generate aggregated data. A vector-field generator is configured to generate a vector-field map including several cells based on the aggregated data each cell having a corresponding vector. An analyzer is configured to analyze the vector-field map to identify vectors of the several cells exceeding predetermined threshold values. An analyzed signal is generated corresponding to the identified vectors and provide the analyzed signal to the AVs. USE - Map generation system for use with autonomous vehicle (claimed). ADVANTAGE - Since the MGS is configured to analyze the sum of magnitudes of vectors during traffic light transitions to adapt traffic control configurations, the overall accelerations in the vicinity is reduced. Since the machine learning model can be executed by a computing system, the performance of a specific task is improved. The accuracy is improved by providing positive or negative feedback to the reinforcement learning models. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory computer-readable storage medium storing a program for generating and analyzing acceleration-based vector field maps; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of map generation system.Controller (210)Data aggregator (305)Spatio-temporal vector field generator (310)Frequency analyzer (315)Magnitude analyzer (320)				[]	[]	-1	-1	-1
P	WONGPIROMSARN T; COLLIN A; BELTA C; TEBBENS R D; HELOU B; BEIJBOM O O	Method for scoring trajectories for autonomous            vehicles, involves planning trajectory in environment            using augmented route planner, and operating vehicle            along planned trajectory using control circuit of            vehicle						NOVELTY - The method involves generating a set of trajectories for a vehicle operating in an environment, where each trajectory is associated with a traffic scenario. A reasonableness score is predicted for each trajectory, where the score is obtained from a machine learning model that is trained using input obtained from human annotators. A route planner of the vehicle (100) is augmented using the predicted scores for the trajectories. A trajectory in the environment is planned using the augmented planner. The vehicle is operated along the planned trajectory using a control circuit of a vehicle. A set of realizations of traffic scenarios is obtained. USE - Method for scoring trajectories for autonomous vehicles (claimed) and/or other objects. ADVANTAGE - The method enables using a machine learning model to predict reasonableness scores for AV trajectories for a given traffic scenario, so that the predicted scores can be used to tune a route planner and performance of the route planner, compare two AV stacks, and compare reinforcement learning and any other desired application in an efficient manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an autonomous vehicle.Autonomous vehicle (100)Steering control (102)Brakes (103)Server (136)Communication device (140)				[]	[]	-1	-1	-1
P	CAMPOS M L E; DE LA GUARDIA G R; GUZMAN L A K; GOMEZ G D; PARRA V J I; CAMPOS MACIAS L E; DE LA GUARDIA GONZALEZ R; GUZMAN LEGUEL A K; GOMEZ GUTIERREZ D; PARRA VILCHIS J I	Event filtering device for filtering multiple            event data, has processor that filters filter event            data, such that light frequency associated with filter            event data is not substantially same as reference            signal frequency						NOVELTY - The device has processor (102) that determines a reference signal frequency associated with a transmitted light. An event data associated with a change in a light intensity of a pixel is received, such that the light intensity is associated with a light frequency. A select event data is identified from multiple event data, such that the light frequency associated with the select event data is substantially the same as the reference signal frequency. A filter event data is filtered from multiple event data, such that the light frequency associated with the filter event data is not substantially the same as the reference signal frequency. USE - Event filtering device for filtering multiple event data. ADVANTAGE - The reinforcement learning models can include positive or negative feedback to improve accuracy. The safety driving model includes a mathematical model for safety assurance that enables identification and performance of proper responses to dangerous situations such that self-perpetrated accidents can be avoided. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for filtering multiple event data; and(2) a device for calculating time to contact for autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram illustrating the autonomous vehicle.Vehicle (100)Processor (102)Memory (104)Antenna system (106)Measurement device (116)				[]	[]	-1	-1	-1
P	NAGESHRAO S; TSENG H E; FILEV D P; BAKER R L; CRUISE C; DAEHLER L; MOHAN S; KUSARI A	Process for performing drive adaptive learning of            autonomous or semi-autonomous vehicle based on input of            vehicle sensor data, involves entering vehicle sensor            data into neural network, where first neural network            includes security agent						NOVELTY - The process involves entering vehicle sensor data into a first neural network, where first neural network includes a security agent that determines a likelihood of unsafe vehicle operation. First neural network is adjusted at variety of times by using a periodically re-trained deep reinforcement learning agent with second neural network. The vehicle (110) is operated based on a vehicle action output from the first neural network. The vehicle sensor data is input by inputting a color video image into the first neural network. The probabilities of unsafe vehicle operation is determined by the security agent. USE - Process for performing drive adaptive learning of autonomous or semi-autonomous vehicle i.e land based vehicle, such as passenger car or light truck based on input of vehicle sensor data. ADVANTAGE - The deep neural network can be trained to enter vehicle transition states in response to sensor data input, which improves the driving of the vehicle by using the security agent block. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing system that includes a computer programmed to determine vehicle action based on the input of vehicle sensor data. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a traffic infrastructure system. (Drawing includes non-English language text).Vehicle (110)Infrastructure interface (111)Computing device (115)Sensor (116)Server computer (120)				[]	[]	-1	-1	-1
P	RUSSELL A K; GEBRE M R; KHILARI S C; BURBANK I P; ENGLARD B; RAMEZANI V R; MAHESHWARI P	Method for determining scan pattern according to            which sensor equipped with scanner scans field of            regard, involves applying optimization scheme to            multiple objective functions to generate scan            pattern						NOVELTY - The method involves obtaining a set of objective functions by processing hardware, where each objective function specifies a cost for a respective property of a scan pattern expressed in terms of operational parameters of a scanner. An optimization scheme is applied to the objective functions to generate the scan pattern by the processing hardware. A field of regard (FOR) (120A) is scanned according to the generated scan pattern. The objective functions include a velocity objective function specifying the cost for velocity at which the scanner scans a scan dimension of the FOR, and the velocity function generates higher cost for higher velocity. USE - Method for determining a scan pattern by which a sensor scans a FOR by using light detection and ranging (lidar) system (claimed) in an autonomous vehicle. Uses include but are not limited to car, automobile, motor vehicle, truck, bus, van, trailer, off-road vehicle, farm vehicle, lawn mower, construction equipment, golf cart, taxi, motorcycle, scooter, bicycle, skateboard, train, snowmobile, watercraft and spacecraft ADVANTAGE - The method enables determining the scan pattern in view of certain constrains of the scanning system, such as a maximum velocity or acceleration, and applying reinforcement learning techniques to account for higher-level goals, such as collision avoidance, when generating a scan pattern, thus accurately detecting and predicting lane boundary locations during autonomous vehicle operation. The method allows a control architecture to scan one or more regions of interest in a dense manner, thus yielding higher resolution. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of cognitive light detection and ranging configured to optimize multiple objective functions to generate a pattern for scanning the environment in an autonomous vehicle.Cognitive lidar (100)Control architecture (102)Vehicle (104)Vehicle environment (110)FOR (120A)				[]	[]	-1	-1	-1
P	ILIEVSKI M; SMART M; TIEU K; NARANG A; KUMAVAT A; NARANG G	Method for context-aware decision making of            autonomous agent in autonomous vehicle field, involves            determining trajectory for autonomous agent based on            first output and operating autonomous agent based on            trajectory						NOVELTY - The method involves determining a characterization of an environment of an autonomous agent. The characterization is mapped to a learned model from a set of multiple learned models. A first output is produced by using the learned model. A trajectory is determined for the autonomous agent based on the output. The autonomous agent is operated based on the trajectory. A predetermined set of contexts is labeled on a map, where the context is selected based on map and a pose of the agent. Each of the learned models is trained by adopting an inverse reinforcement learning algorithm. The action is selected from an action space defined by the model. USE - Method for context-aware decision making of an autonomous agent in the autonomous vehicle field. Uses include but are not limited to an automobile such as car, driverless car, bus, shuttle, taxi, ride-share vehicle, truck, semi-truck, etc., a watercraft such as boat, water taxi, etc., an aerial vehicle such as plane, helicopter, drone, etc., and a terrestrial vehicle such as two-wheeled vehicle, bike, motorcycle, scooter, etc. ADVANTAGE - The method enables utilizing a cloud computing as a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. The method allows a cloud consumer to unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider, and automatically and rapidly scale out and rapidly released to quickly scale in. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for context-aware decision making of an autonomous agent in the autonomous vehicle field.				[]	[]	-1	-1	-1
P	GOTO T; GOTO K	Learning device for use in autonomous vehicle, has            reward deriver that derives reward for action of            vehicle on basis of individual rewards and planner that            performs reinforcement learning that optimizes reward            derived by reward deriver						NOVELTY - The learning device (300) comprises a planner (310) that is configured to generate information indicating an action of a vehicle. A reward deriver (360) is configured to derive several individual rewards obtained by evaluating each of several pieces of information to be evaluated, which include feedback information obtained from a simulator or an actual environment by inputting information based on the information indicating the action of the vehicle to the simulator (400). The reward deriver is configured to derive a reward for the action of the vehicle on the basis of several individual rewards. The planner performs reinforcement learning that optimizes the reward derived by the reward deriver. USE - Learning device for use in autonomous vehicle e.g. two-wheeled vehicle, three-wheeled vehicle and four-wheeled vehicle. ADVANTAGE - The learning device inputs the various pieces of information to the simulator through the process of the control operator, and efficiently acquires the feedback information. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a learning method of autonomous vehicle; and(2) a computer-readable non-transitory storage medium storing program for causing computer to learn for use in autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the learning device for use in autonomous vehicle.Learning device (300)Planner (310)Control operator (320)Reward deriver (360)Simulator (400)				[]	[]	-1	-1	-1
P	MOZE M; GUILLEMARD F; AIOUN F; ZHAO H; SUN R; HU S	Method for trajectory of vehicle, involves selecting particular trajectory from set that mimicks human follow in initial situation and generates set containing admissible trajectories for predicted vehicle from initial situation						NOVELTY - The method involves generating a set containing admissible trajectories for predicted vehicles from an initial situation. A particular trajectory from the set that mimics the one human follows in the same initial situation is selected. The admissible trajectories are generated in a Frenet frame, that is set at the initial position of the predicted vehicle and the Frenet frame is fixed during the whole trajectory prediction and is always relative to the predicted vehicle initial position. USE - Method for trajectory of vehicle used in autonomous driving. Can also be used for implementing prediction system for trajectory of vehicle (claimed). ADVANTAGE - The prediction method for the trajectory planning of an autonomous vehicle based on inverse reinforcement learning, predicts the trajectory of human-driven vehicles during driving. The trajectory of the predicted vehicle using the Frenet Frame minimizes the acceleration and make the velocity, acceleration, and jerk have continuity. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a prediction system for the trajectory of a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic view of the generated trajectories and human driving trajectory.				[]	[]	-1	-1	-1
P	ZHANG K; ELWART S; STIMPSON A	Method for controlling autonomous vehicle (AV)            e.g. electric power train, involves generating            parameter tunings based on reinforcement learning            method, and updating AV controller based on parameter            tunings						NOVELTY - The method involves constructing a plant model based on a design of experiments (DOE) test matrix. A controller simulation is performed based on the constructed plant model. The performance data is generated based on the controller simulation. An unsupervised learning method is performed to identify regimes. A reinforcement learning method is performed based on the regimes. The parameter tunings are generated based on the reinforcement learning method. An AV controller is updated based on the parameter tunings. The DOE test matrix includes an entry radius, a curve radius, an exit radius, an entry length, a curve length, an exit length, an entry speed, a curve speed, an exit speed and direction. The DOE test matrix is replicated to refine the plant model. USE - Method for controlling autonomous vehicle (AV) such as electric power train. ADVANTAGE - The controller system communicates with the object detection system and the navigation system to operate a steering/acceleration profile for the host vehicle to avoid the potential collisions with other vehicles or objects. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a vehicle control system for controlling AV. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of a vehicle control system.Vehicle control system (5000)Controller (5130)Location device (5140)Image device (5150)Deep learning module (5240)				[]	[]	-1	-1	-1
P	LYU C; WU J	Deep reinforcement learning model training method            for autonomous vehicle, involves minimizing loss            function of policy network, where loss function            comprises autonomous guidance component and human            guidance component						NOVELTY - The method involves minimizing a loss function of a policy network and the loss function of the policy network that comprises a human guidance component. An autonomous guidance component is provided zero when the state information is indicative of input of a human input signal at a machine. A value network is configured to estimate the value function based on the Bellman equation. A first value network is paired with a second value network and each value network having the same architecture for reducing or preventing overestimation. Each value network is coupled to a target value network and the policy network is coupled to a target policy network. USE - Method of training deep reinforcement learning model for autonomous control of machine i.e. autonomous vehicle (AV). ADVANTAGE - The method effectively trains the deep reinforcement learning model for autonomous control of the machine, thus improving the data-processing efficiency to ensure that human guidance-based deep reinforcement training (DRL) algorithms are feasible in practice, and reducing the requirements for human participants in human-guided DRL algorithms. The method improves the performance of DRL agents, and ensures the quality of the data collected and achieves an ideal improvement in performance. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a method for autonomous control of a machine;a system for training a deep reinforcement learning model for autonomous control of machine;a system for autonomous control of machine; anda non-transitory storage comprising machine-readable instructions for autonomous driving system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the proposed method with real-time human guidance.102Agent104Environment106Output action108State transition116Human guidance				[]	[]	-1	-1	-1
P	ZHANG H; MA J; JIN Y; LIU P; MA H	Multi-agent reinforcement learning based            self-driving vehicle control method, involves using            optimal strategy obtained by multi-agent deep            reinforcement learning decision-making algorithm as            control input of autonomous vehicle						NOVELTY - The method involves setting the cooperation and alliance methods of connected automated vehicle (CAVs). The vehicle is driving at different positions on different roads, and the observation area is within the set area. The set area size is let to be l*n, in which l represents the length of the area, and n represents the width of the area. The CAVs multi-agent subsystem is constructed. The multi-agent subsystem area is divided, based on vehicle to vehicle (V2V) communication and field of view threshold. The CAVs multi-agent deep reinforcement learning decision-making algorithm is designed. The CAV decision-making algorithm of multi-agent deep reinforcement learning is an end-to-end decision-making architecture, including an input layer, a neural network layer, an output layer and an environment interaction layer. The optimal strategy obtained by the multi-agent deep reinforcement learning decision-making algorithm is used as the control input of the autonomous vehicle. USE - Multi-agent reinforcement learning based self-driving vehicle control method for artificial intelligence (AI) field of automatic driving. ADVANTAGE - The method enables ensuring safety and efficiency on a high speed trunk lane and ensuring safety of a ramp merging comfort level, so that efficiency is guaranteed in a scene of fully automatic driving vehicle, thus improving quality of observation space and shortening resource utilization rate by using V2V communication technology to define CAV learning area, and hence improving vehicle safety, comfortable and efficient driving. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of CAV decision-making algorithm framework based on the rolling time domain of multi-agent reinforcement learning based self-driving vehicle control method. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	YANG J H; YANG J	Apparatus for controlling driving speed of            vehicle, has camera for acquiring image in front of            vehicle, and sensor acquiring driving speed of            vehicle						NOVELTY - The apparatus has a camera for acquiring an image in front of a vehicle. A sensor acquires a driving speed of the vehicle. A first neural network calculates a compensation steering angle based on comparing a driving steering angle with a calculated compensation steering angle. A second neural network sets a vehicle speed based on comparison of the compensation steering angles with a threshold value, where the driving angle comprises steering angle information collected when the vehicle is driven, and the calculated steering angle comprises the steering angle information learned by receiving the image and the driving speed. USE - Apparatus for controlling driving speed of a vehicle i.e. automobile during path following control using a conventional camera sensor. Uses include but are not limited to a lane departure warning system (LDWS), a lane keeping assist system (LKAS), a blind side warning (BSD), a smart cruise control (SCC) and an automatic emergency braking system (AEB). ADVANTAGE - The apparatus allows an autonomous vehicle to reach a destination without a driver's manipulation of a steering wheel, an accelerator pedal, and a brake. The apparatus utilizes an advanced driver assistance system (ADAS) that provides convenience and safety to drivers, and predicts a front road environment by using map information, and provides appropriate control and convenience services. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for controlling driving speed of a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an apparatus for controlling driving speed of a vehicle.Vehicle (100)Data storage (101)Sensor module (103)Convolutional Neural Network (105)Reinforcement Learning Neural Network (107)ongitudinal Controller (109)ransverse Controller (111)				[]	[]	-1	-1	-1
P	FUJIMURA K; KOCHENDERFER M; NAKHAEI S A; ISELE D F; BOUTON M	Method for reinforcement learning with iterative            reasoning, involves providing level-0 policy and            desired reasoning level, and training level-2 agent            based on level-1 first agent and second agent and            deriving level-two policy						NOVELTY - The method involves providing a level-0 policy and a desired reasoning level n, and populating a training environment with a set of agents. A level-1 agent is trained based on the agents, where the agents follow an intelligent driver model (IDM) for longitudinal maneuvers. The training environment is populated with the agents associated with lane-change and lane-keeping behaviors, respectively. A set of level-2 agents is trained, and a set-2 policy is derived. A state associated with one of the agents includes a longitudinal position, a lateral position, longitudinal velocity, and lateral velocity. USE - Method for facilitating reinforcement learning with iterative reasoning for autonomous vehicles. Uses include but are not limited to cars, trucks, vans, minivans, sports utility vehicle, motorcycles, scooters, boats, personal watercraft, and aircraft. Can also be used in battery electric vehicles (BEV) and plug-in hybrid electric vehicles (PHEV). ADVANTAGE - The method enables training the level-2 agent based on the first agent and the second agent and deriving a level-1 policy, and populating the training environment with the first and second agents associated with a first behavior and a second behavior, so that the method enables providing a reinforcement learning with iterative reasoning in an effective manner. The method allows the autonomous vehicle to achieve the maneuver within a limited time and distance in an efficient manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for: a system for reinforcement learning with iterative reasoning. DESCRIPTION Of DRAWING(S) - The drawing shows a component block diagram of a network architecture associated with a system for reinforcement learning with iterative reasoning.Network architecture (100)Ego features (102)Vehicle features (104)Convolutional network (110)Layer (122)				[]	[]	-1	-1	-1
P	FAN H; KONG Q; XIA Z; LIU C; CHEN Y; ZHU F	Computer-based method for generating motion            planning cost function for autonomous driving vehicle            (ADV), involves selecting highest ranked trajectory to            control ADV autonomously according to highest ranked            trajectory						NOVELTY - The method involves collecting information for a driving environment surrounding the autonomous driving vehicle (101) (ADV) using sensors of the ADV. Sample trajectories are generated from a trajectory sample space for the driving environment. A reward is determined based on a reward model for each of the sample trajectories. The reward model is generated using a rank based conditional inverse reinforcement learning algorithm. The sample trajectories are ranked based on the determined rewards. A highest ranked trajectory is determined based on the ranking. The highest ranked trajectory is selected to control the ADV autonomously according to the highest ranked trajectory. USE - Computer-based method for generating motion planning cost function for autonomous driving vehicle (ADV). ADVANTAGE - The collision avoidance system automatically selects the maneuver that is both available and maximizes the safety of occupants of the autonomous vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory machine-readable medium storing program for generating motion planning cost function for autonomous driving vehicle; and(2) a computer-based method to train a rewards model for an autonomous driving vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a networked system.Autonomous driving vehicle (101)Control system (111)Wireless communication system (112)User interface system (113)Sensor system (115)				[]	[]	-1	-1	-1
P	LEE D	Method for operating reinforcement learning-based            agent using state memory-based artificial neural            network, involves output behavior values into speed            control commands in operating environment and assigns            state memory-based state variables						NOVELTY - The method involves generating (S10) a state memory-based state variable by fusing multi-sensor data by the sensor fusion model. The artificial neural network reinforcement learning model receives (S20) state memory-based state variables and outputs state memory-based state variables as behavior values for motion control of autonomous vehicles. The artificial neural network reinforcement learning model converts (S30) the output behavior values into speed control commands in the operating environment and assigns state memory-based state variables. USE - Method for operating reinforcement learning-based agent using state memory-based artificial neural network for collision avoidance and autonomous driving of autonomous vehicles. ADVANTAGE - The performance is improved more and more because you train yourself. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for operating a reinforcement learning-based agent using a state memory-based artificial neural network for collision avoidance and autonomous driving of an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method of operating a reinforcement learning-based agent using a state memory-based artificial neural network. (Drawing includes non-English language text)Step for generating a state memory-based state variable by fusing multi-sensor data by the sensor fusion model (S10)Step for receiving artificial neural network reinforcement learning model state memory-based state variables and outputs state memory-based state variables as behavior values for motion control of autonomous vehicles (S20)Step for converting the output behavior values into speed control commands in the operating environment and assigns state memory-based state variables (S30)				[]	[]	-1	-1	-1
P	ANANDARAM H; RACHAPUDI V; RAJEYYAGARI S; BATCHA R R; RAMALINGAM V; RAGUPATHI T; SARAVANAN D; NATHAN V; RAJU G D; DEWANGAN N	Method for detecting and recognizing real-time            object using adaptive deep learning framework for e.g.            surveillance application, involves integrating            convolutional neural networks, recurrent neural            networks and reinforcement learning mechanisms						NOVELTY - The method involves integrating convolutional neural networks, recurrent neural networks and reinforcement learning mechanisms. The parallel processing techniques are used to enable rapid analysis of large data sets for allowing real-time responses in time-critical applications. Deployment in small-scale and large-scale applications ranging from home security to city-wide management systems is enabled. Continuous learning and refinement of internal algorithms is permitted. USE - Method for detecting and recognizing a real-time object using an adaptive deep learning framework. Uses include but are not limited to autonomous vehicle i.e. self-driving car, robotics application, security system, industrial automation application, surveillance application and medical diagnostics application. ADVANTAGE - The method enables providing accurate and immediate object detection and recognition. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a real-time object detection and recognition system. DESCRIPTION Of DRAWING(S) - The drawing shows a functional block diagram of a real-time object detection and recognition system.				[]	[]	-1	-1	-1
P	PHAN T; BADITHELA A	Method for generating trajectory for autonomous            vehicle by updating neural network of motion planner            for detecting building present in vehicle surrounding            environment, involves updating machine learning model            using processor to generate trajectory of vehicle by            applying multiple feature weights						NOVELTY - The method involves training a machine learning model to generate a trajectory for a vehicle using a processor, where the training generating a first trained machine learning model having multiple first feature weights. The machine learning model is trained to generate the correct trajectory training using the processor based on a first counterexample for which the first trained machine learning model fails to generate a correct trajectory for the vehicle, where the training generating a second trained machine learning model having multiple second feature weights. Multiple third feature weights are determined using the processor by updating multiple first feature weights based on multiple second feature weights. The machine learning model is updated using the processor to generate the trajectory of the vehicle by applying multiple third feature weights. USE - Method for generating a trajectory for vehicles e.g. an autonomous vehicle (AV) by updating a machine learning model e.g. a neural network (claimed) of a motion planner for detecting objects present in the vehicle surrounding environment. Uses include but are not limited to pedestrians, a cyclist, a structure e.g. a building, traffic signs and a fire hydrant, other vehicles e.g. cars, bicycles, and buses, and curbs. ADVANTAGE - The method enables generating the trajectory to avoid collisions between the vehicle and the objects present in the surrounding environment, and operating the vehicle in accordance with other desirable characteristics such as path length, ride quality, required travel time, observance of traffic rules, and adherence to driving practices. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a system for generating a trajectory for vehicles; (2) a non-transitory storage media storing instructions for generating the trajectory for vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the process of training a machine learning model through inverse reinforcement learning (IRL).				[]	[]	-1	-1	-1
P	KARR R; LEE R	Storage system, has storage controllers for            delivering message to destination authority of one of            storage controllers responsive to achieving level of            redundancy for redundant copies of metadata regarding            message						NOVELTY - The system (100) has storage controllers that are configured to initiate an action based on redundant copies of metadata, such that a source authority of one of multiple storage controllers receives a message, records the message redundantly throughout multiple storage controllers, and delivers the message to a destination authority of a further one of the storage controllers responsive to achieving a level of redundancy for the redundant copies of the metadata regarding the message. Multiple storage controllers comprises a zoned storage drive. USE - Storage system for artificial intelligence (AI) application used in predictive maintenance in manufacturing and related fields, healthcare applications such as patient data and risk analytics, retail and marketing deployments such as search advertising, social media advertising, supply chains solutions, fintech solutions such as business analytics and reporting tools, operational deployments such as real-time analytics tools, application performance management tools, and information technology infrastructure management tools. Can also be used in deep learning solution, deep reinforcement learning solution, artificial general intelligence solution, autonomous vehicle, cognitive computing solution, commercial unmanned aerial vehicle, conversational user interface, enterprise taxonomy, ontology management solution, machine learning solution, smart dust, smart robot, and smart workplace. ADVANTAGE - The method enables providing enhanced features or take advantage of unique aspects of Flash (RTM: Computer multimedia application) and other solid-state memory in an efficient manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a storage cluster; and(2) a method for storing data. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the first example system for data storage.Storage system (100)Storage array (102A)Storage array controller (110A)Local area network (160)Power distribution bus (172)				[]	[]	-1	-1	-1
P	KIM K; KIM Y; KIM H; NAM W; BOO S; SUNG M; SHIN D; YEO D; RYU W; LEE M; LEE H; JANG T; JEONG K; JE H; CHO H; KIMKYEHYUN; KIM Y J; KIMHAGKYUNG; NAM W H; BOO S H; SUNG M C; SHIN D S; RYU W J; LEE M C; SOO L H; JANG T U; JEONG K J; JE H M; CHO H J; JIN G; JIN R; JIN H; NAN Y; FU S; CHENG M; SHEN D; LV D; LIU Y; LI M; LI J; ZHANG T; ZHENG J; ZHU H; ZHAO H; KIM I S	Method for an autonomous vehicle involves a            computing device acquiring multiple circumstance image            on surroundings of a subject vehicle, through a            panorama view sensor installed on the subject vehicle            and computing device instructing a CNN						NOVELTY - The method involves computing device which acquired multiple circumstance image on surrounded of a subject vehicle, through a panorama view sensor which is installed on the subject vehicle. The computed device which is instructed a Convolutional Neural Network (CNN)(130) to apply at CNN operation to the circumstance image.The initial object information and initial confidence information is generated on the circumstance image. The re-detection process is performed N times in computed device so that N-th adjusted object information is generated. The final object information is generated by referred to the initial object information and K is an integer from 2 to N, and N is the number of the re-detection process to be performed by determination of the Reinforcement Learning (RL) agent. USE - Method for an autonomous vehicle. ADVANTAGE - The performance of the autonomous driving is maintained, computing power is reduced and more accurately object is detected. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing device for achieving better performance. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a computing device.Convolutional Neural Network (130)Fc Layer (132)Region Proposal Network (140)				[]	[]	-1	-1	-1
P	VAN HOOF H; SCHMITT F; WOEHLKE J G	Method for controlling agent, such as robot is            autonomous vehicle, involves controlling agent in            accordance with control signals derived from evaluation            of control actions output by neural network in response            to input						NOVELTY - The method involves obtaining (301) numerical values of a first set of state variables and a second set of state variables. The numerical values of the first set of state variables together with the numerical values of the second set of variables represent a current full state of the agent. The numerical values of the first set of state variables represent a current partial state of the robot. A state value prior is determined (302) for potential subsequent partial states following the current partial state. An input has a local crop of the state value prior and the numerical values of the second set of state variables representing together is supplied (303) with the numerical values of the first set of state variables. The current full state to a neural network configured to output an evaluation of control actions. The agent is controlled (304) with control signals derived from an evaluation of control actions output by the neural network in response to the input. USE - Method for controlling an agent, such as robot is an autonomous vehicle with legs or tracks or other kind of propulsion system, such as a deep sea or Mars rover. ADVANTAGE - The method enables utilizing a combination of a planning algorithm that guides reinforcement learning to improve data-efficiency. The method allows the agent to learn to perform desired behaviors with respect to a task specification, which controls action to take to reach a goal location in a robotic navigation scenario. The neural network can efficiently train to determine the control action evaluations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a controller configured to control an agent; and(b) a non-transitory computer-readable medium for storing a computer program for controlling an agent. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the method.301Obtaining numerical values of a first and a second set of state variables302Determining a state value prior303Supplying local crop of the state value prior and the numerical values of the first and second set of state variables304Controlling the agent				[]	[]	-1	-1	-1
P	YOU C	Training method of autonomous driving strategy            using vehicle, involves configuring reinforcement            learning methods in training environment to obtain            optimal strategy for reinforcement learning models            based on state set, action set						NOVELTY - The method involves determining (601) the state set of the reinforcement learning model. The state set is set to represent the availability of target environment areas of the autonomous vehicle. The action set of the reinforcement learning model is determined (602). The action set represented the driving action of the autonomous vehicle. The state transition of the reinforcement learning model is determined (603). The reinforcement learning methods in the training environment is configured (605) to obtain the optimal strategy for reinforcement learning models based on the state set, action set, state transition and revenue function. The training environment is set to specify the probability of the obstacle performing each action in the action set. USE - Training method of autonomous driving strategy using vehicle (claimed). ADVANTAGE - The method enables realizing automatic driving strategy training process in a simple manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method of autonomous driving; and(2) a automatic driving device. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating training method of autonomous driving strategy using vehicle. (Drawing includes non-English language text)Step for determining the state set of the reinforcement learning model (601)Step for determining action set of the reinforcement learning model (602)Step for determining state transition of the reinforcement learning model (603)Step for determining the revenue function of the reinforcement learning model (604)Step for configuring reinforcement learning methods in the training environment (605)				[]	[]	-1	-1	-1
P	INAM R; HATA A; TERRA A I	Method for risk management for autonomous device            using reinforcement learning-based risk management            node, involves initiating sending control parameter to            autonomous device to control action of autonomous            device						NOVELTY - The method involves determining (601) state parameters from a representation of an environment that includes object, an autonomous device, and a set of safety zones for the autonomous device relative to the object. A reward value is determined (603) for the autonomous device based on evaluating a risk of a hazard with the object based on the determined state parameters and current location and current speed of the autonomous device relative to a safety zone from the set of safety zones. A control parameter is determined (605) for controlling action of the autonomous device based on the determined reward value. The sending the control parameter to the autonomous device is initiated (607) to control action of the autonomous device. The control parameter is dynamically adapted to reduce the risk of hazard with the object based on reinforcement learning feedback from the reward value. USE - Method for risk management for autonomous device such as robot and autonomous vehicle using reinforcement learning-based risk management node (claimed). ADVANTAGE - By enabling development of the more robust system can enhance safety of operation of autonomous devices while dynamically adapting to machine learned experiences during operation of the autonomous devices. The risk management node incorporates the current environment around autonomous device and state parameter of autonomous device in the improved manner in contrast to, using the predefined speed for the safety bubble. The risk management node can formulate the state and reward to minimize or reduce the potential risk. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a risk management node; and(2) a computer program product for risk management for autonomous device using reinforcement learning-based risk management node. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating the method for risk management for autonomous device using reinforcement learning-based risk management node.Step for determining state parameters (601)Step for determining reward value (603)Step for determining control parameter (605)Step for initiating the sending the control parameter to the autonomous device (607)				[]	[]	-1	-1	-1
P	WANG D; MEENAKSHI SUNDARAM A; KOTHARI R A; ROETTELER M H; KAPOOR A; MEENAKSHI S A	Computing device for training reinforcement            learning model, has processor receiving measurement            results in response to superposition queries from            quantum coprocessor, and updating policy function of            learning model based on results						NOVELTY - The computing device (10) has a processor (12) that is configured to transmit instructions to encode a Markov decision process (MDP) model as a quantum oracle to a quantum coprocessor (20). The processor is configured to train a reinforcement learning model (50). The processor is configured to transmit several superposition queries to the quantum oracle encoded at the quantum coprocessor. The processor is configured to receive the measurement results in response to the superposition queries from the quantum coprocessor. The processor is configured to update a policy function of the reinforcement learning model based on the measurement results. The measurement results include an estimated optimal Q-function, an estimated optimal value function or an estimated optimal policy function for the reinforcement learning model. The quantum coprocessor is configured to compute the estimated optimal Q-function, the estimated optimal value function or the estimated optimal policy function. USE - Computing device such as personal computer, server computer, tablet computer, home-entertainment computer, network computing device, gaming device, mobile computing device, smartphone, smart wristwatch and head mounted augmented reality device for training agents to play board games or online video games with humans or other artificial intelligence (AI). ADVANTAGE - The method effectively trains the reinforcement learning model in the computing device, thus improving the performance of the computing system. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for training agents to play board games or online video games used with computing device. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the computing device when a reinforcement learning model configured to control an autonomous vehicle is generated.Computing device (10)Processor (12)Memory (14)Input device (16)Output device (18)Quantum coprocessor (20)Reinforcement learning model (50)				[]	[]	-1	-1	-1
P	DOU L; SUN J; MEI D; XU Y	Identification and self-learning collaborative            control method of unmanned vehicle cluster system under            unknown model, used in e.g. civilian field, involves            finding optimal control strategy through policy            iterative learning algorithm to achieve collaborative            tracking control of unmanned vehicle						NOVELTY - The method involves establishing a multi-unmanned vehicle system model, in which the multi-unmanned vehicle cluster system consists of N follower unmanned vehicles and a leader unmanned vehicle, communicating through a directed graph topology. The system data collected by each autonomous vehicle is used to identify and reconstruct the system model through reinforcement learning and data-driven methods. A distributed collaborative controller is designed for reinforcement learning and the corresponding cost function, based on the identified and reconstructed system model. The optimal control strategy is found through the policy iterative learning algorithm to achieve the optimal collaborative tracking control of the follower unmanned vehicle on the leader unmanned vehicle, under the designed reinforcement learning algorithm. USE - Identification and self-learning collaborative control method of unmanned vehicle cluster system under unknown model, used in military and civilian fields such as collaborative rescue, collaborative search and rescue, collaborative reconnaissance, and collaborative strike. ADVANTAGE - The method enables designing an optimal cooperative control solution based on enhanced learning, and providing an online self-adaptive learning cooperative control algorithm depending on online state information and input information to learn the optimal control strategy through Lyapunov stability analysis method, thus ensuring that the unmanned vehicle realizes the optimal tracking of the pilotage unmanned vehicle. The method completely releases the requirement of depending on vehicle dynamics in the traditional tracking control protocol, and realizes the efficient cooperative path tracking control. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the identification and self-learning collaborative control method of the unmanned vehicle cluster system under unknown model. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	HWANG S B; LEE K; KUM D; SUK K; LEE K B	Method for changing lane of e.g. bus, by lane            change system in global vehicle industry, involves            outputting path and velocity plan corresponding to            input driving state information of vehicle by using            lane change algorithm						NOVELTY - The method involves receiving driving state information of a vehicle. A path and a velocity plan corresponding to input driving state information of the vehicle is output by using a lane change algorithm, where the driving state information of the vehicle comprises state information of the vehicle comprising driving information of the vehicle and situation information for recognizing a situation of a surrounding vehicle on basis of the vehicle. The vehicle is controlled by transitioning from a ready stage to an approaching stage to adjust a longitudinal position for a retrieved gap, and bringing the vehicle to approach the retrieved gap in the transitioned approaching stage. USE - Method for changing a lane of an autonomous vehicle i.e. passenger car such as sedan or sports utility vehicle, lorry, and bus, by a lane change system (claimed), in a global vehicle industry. ADVANTAGE - Safe and consistent lane change is performed by dividing lane change into stages based on a finite state machine and setting a transition criterion for characteristics for each step. A lane change success rate is maximized without sacrificing safety of the vehicle because a deep reinforcement learning-based methodology is used when the vehicle performs the lane change. Commercialization of the vehicle is advanced by securing lane change performance of the vehicle in an essential lane change section in addition to a non-essential lane change case. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view illustrating a position and movement of a vehicle when the vehicle performs a lane change.				[]	[]	-1	-1	-1
P	TREMBLAY J; FOX D; LEE M; FLORENSA C; RATLIFF N D; GARG A; TOZETO R F; RAMOS F T	Method for training robots to perform task,            involves moving robot to perform task under control of            second method that uses information generated by second            perception system as result of determining that robot            is in region						NOVELTY - The method involves moving a robot to be within a region under a control of a first method using a physical model based on information from a first perceptual system (106). An uncertainty of the information generated by the first perception system is determined. The determination is made that the robot is in the region based on the uncertainty. the robot is moved to perform a task under the control of a second method that uses information generated by a second perception system (108) as a result of determining that the robot is in the region. USE - Method for training robots to perform task for controlling autonomous vehicle such as passenger vehicle such as car, truck, bus, semi-tractor trailer truck, airplane and robotic vehicle. ADVANTAGE - The strengths of a model-based method (MBM) is combined with the strengths of a model-free method (MFM). The MBM is leveraged to provide efficient movement in an open space environment. The MBM is used to operate a robot in a room where collisions with the environment or people can easily be avoided. The model-based guideline is used toopen to navigate while avoiding obstacles, and an reinforcement learning (RL) algorithm and model-free guideline of architecture are used. The real world learning of a close fitting pen insertion task is achieved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer system;(2) a computer-readable medium storing program for training robots to perform task; and(3) a processor. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the method for training robots to perform task.Hole (104)First perception system (106)Second perception system (108)Uncertainty area (112)Model-free guideline (114)				[]	[]	-1	-1	-1
P	JIAO X; DENG N; ZHOU W; CAO C; YANG D	Reliable learning-type automatic driving            decision-making method for automatic driving vehicle,            involves selecting decision with high value in both            learning decision and interpretable decision as final            reliable learning decision action						NOVELTY - The method involves constructing interpretable decisions based on predetermined decision problems. The learned decision training is guided by the interpretable decision. The learning decision is trained by the decision problem. The learning decision with a high-value decision value function is obtained. The decision with high value in both the learning decision and the interpretable decision is selected as the final reliable learning decision action. USE - Reliable learning-type automatic driving decision-making method for automatic driving vehicle. ADVANTAGE - The method adjusts the value function evaluation process of the enhanced learning decision and makes the decision value function of the final generating strategy higher than a certain interpretive driving strategy so as to realize the reliability guarantee for the learning decision of the autonomous vehicle. The method can fully exert the decision-making ability of reinforcement learning for clear targets in a highly uncertain environment, and guarantee the lower bound of performance through interpretability strategies in order to ensure the high reliability of autonomous vehicles and to realize reliable learning decisions of autonomous vehicles. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a reliable learning-type automatic driving decision-making system; (2) a computer-readable storage medium storing programs for learning-type automatic driving decision-making for automatic driving vehicle; (3) a computing device.				[]	[]	-1	-1	-1
P	TANG G; CAI Y; ZHANG W; CHENG D	Reinforcement learning test method for autonomous driving, involves obtaining automatic driving reference control data under automatic driving scene failure scene through interactive feedback learning of enhanced learning module and virtual environment						NOVELTY - The method involves driving an automatic driving vehicle to perform a failure scene driving test in a specific scene. An automatic driving system of the vehicle is used for collecting environment data and bicycle data. A virtual environment is constructed according to the collected environmental data and the bicycle data to simulate vehicle driving information and the environment information in the real environment. Automatic driving reference control data is obtained under the automatic driving scene failure scene through an interactive feedback learning of an enhanced learning module and the virtual environment. USE - Reinforcement learning test method for autonomous driving. ADVANTAGE - The method: utilizes the real failure scene to construct a virtual environment; inputs the virtual failure scene data into the reinforcement learning framework; can train security policies for specific scene; improves the safety of autonomous vehicles in this accident scene; and achieves the goal of reducing autonomous vehicle accidents. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a reinforcement learning autonomous driving test system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for reinforcement learning test for autonomous driving (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	SILVER D; QUAN J; SCHAUL T	Method for controlling e.g. robot in simulated            environment to perform reinforcement learning tasks by            computing device, involves causing agent to perform            action specified by processing output of current            observation data						NOVELTY - The method involves receiving current observation data characterizing current state of an environment (104), and processing the current observation data using a neural network (110) to generate an output that specifies an action to be performed by an agent (102) based on the current observation data, where the neural network is trained through a reinforcement learning module using respective values of an expected learning progress measure for piece of experience data. The agent is caused to perform the action specified by a processing output of the current observation data. USE - Method for controlling an agent e.g. mechanical agent such as robot, autonomous vehicle, or semi-autonomous vehicle, and electronic agent, in an environment e.g. real-world environment and simulated environment (all claimed), to perform reinforcement learning tasks by a computing device. Uses include but are not limited to a mobile phone, a tablet computer, a notebook computer, a music player, an electronic book reader, a laptop or desktop computer, a personal digital assistant (PDA), a smartphone, a game console, a global positioning system (GPS) receiver, and a portable storage device i.e. universal serial bus (USB) flash drive. ADVANTAGE - The method enables improving speed of training the neural network for selecting the actions to be performed by the agents by reducing amount of training data needed to effectively train the neural network, while reducing amount of computing resources necessary for training the neural network. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system for controlling an agent in an environment to perform a task;(2) a non-transitory computer storage medium comprising instructions for controlling an agent in an environment to perform a task. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a reinforcement learning system.102Agent104Environment110Neural network120Training engine130Replay memory				[]	[]	-1	-1	-1
P	YU X; YANG X; SUN W; XUE X; LI Z; GAO H	Cross-sensor transfer learning based indoor            monocular navigation method for mobile robot e.g.            unmanned aerial vehicle, involves performing navigation            of robot according to heading angle at current moment,            and determining reward function						NOVELTY - The method involves acquiring simulation single-line laser radar data of a mobile robot in a simulation model, where the simulation model is built in a Webots open source simulation environment. A laser radar monocular vision navigation model is determined according to a heading angle of the mobile robot at same moment and monocular camera data at corresponding moment and by using a Resnet18 network and a pre-trained YOLO v3 network. Navigation of the robot is performed according to the heading angle at the current moment. Reward function is determined. USE - Cross-sensor transfer learning based indoor monocular navigation method for a mobile robot such as unmanned aerial vehicle (UAV) and autonomous vehicle. ADVANTAGE - The method enables improving accuracy of mobile robot navigation with a monocular camera, acquiring a navigation angle of the mobile robot carrying the monocular cameras through monocular image data and improving navigation accuracy of the robot. The method enables obtaining the stable autonomous navigation model by using a virtual single-line laser radar as a sensor in a simulation environment based on deep deterministic policy gradient (DDPG) reinforcement learning method. The method enables binding real environment data collected by a single line laser radar and a monoclonal camera frame by frame. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a cross-sensor transfer learning based indoor monocular navigation system.				[]	[]	-1	-1	-1
P	PENROD D C; DIKEMAN J M; GLENN J L; GERTISER K M; ROUTH S; LAWS S; RUTH S; GERTESER K M; DICKEMAN J M; PENROD D	Method for capturing die temperature data of            integrated circuit, involves obtaining temperature data            from thermal sensing device associated with die of IC,            and storing temperature data in storage component of            IC						NOVELTY - The method involves self-initializing an integrated circuit (IC) (25) by asserting a reset signal for a reset period, in response to an application of power to the IC. The reset signal is deasserted in an expiration of the reset period. Temperature data is obtained from a thermal sensing device associated with a die (27) of the IC, and the temperature data is stored in a storage component of the IC. A built-in self test (BIST) is conducted responsive to deassert the reset signal. A temperature read request is received from a requesting device. USE - Method for capturing die temperature data of an integrated circuit (IC) (claimed) used in vehicle e.g. autonomous or semi-autonomous vehicle, and automotive applications such as engine control units, power driving systems and antilock brake systems. Uses include but are not limited to desktop computers, laptop computers, mobile computing devices, tablet computing device, home appliances, stereos and medical devices. ADVANTAGE - The performance, accuracy and/or longevity of components in an IC depends on operating temperature. The self-heating of the IC die due to operation of self-heat components in the IC results in actual temperatures at locations within the IC being significantly higher than the reference temperature. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an IC. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an IC to receive power from a battery of the vehicle.IC (25)Die (27)Reset logic (RL) (30)Analog-to-digital converter (ADC) (60)Storage component (70)				[]	[]	-1	-1	-1
P	OSTROVSKI G; DABNEY W C	Method for selecting action i.e. action to control            navigation e.g. steering of e.g. autonomous vehicle,            involves selecting action from set of possible actions            to be performed by agent using measures of central            tendency for actions						NOVELTY - The method (500) involves processing action, current observation, and a probability value using a quantile function network including a set of network parameters, where the quantile function network is a neural network configured to process the action, the current observation, and the probability value in accordance with current values of the network parameters to generate a network output that indicates an estimated quantile value for the probability value. Measures of central tendency of the estimated quantile values generated by the quantile function network are determined (510). Action from a set of possible actions to be performed by an agent is selected (512) in response to the current observation using the measures of central tendency for the actions. USE - Method for selecting an action i.e. action to control navigation e.g. steering, and braking and/or acceleration of a vehicle e.g. autonomous or semi-autonomous land or air or sea vehicle, to be performed by a reinforcement learning agent interacting with an environment using a quantile function network. ADVANTAGE - The method enables training the agent in a quick manner so as to enable a system to consume less computational resources during training of the agent, and allowing the agent to control actions in an environment to increase efficiency by reducing resource usage and environmental impact of operations in the environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a system for selecting an action to be performed by a reinforcement learning agent interacting with an environment(2) a computer readable storage medium for storing a set of instructions for selecting an action to be performed by a reinforcement learning agent interacting with an environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for selecting an action to be performed by a reinforcement learning agent.Method for selecting action to be performed by reinforcement learning agent (500)Step for sampling probability value (504)Step for transforming probability value using distortion function (506)Step for generating estimated quantile values (508)Step for determining measures of central tendency of estimated quantile values generated by quantile function network (510)Step for selecting action from set of possible actions to be performed by agent (512)				[]	[]	-1	-1	-1
P	SHALEV-SHWARTZ S; SHASHUA A; STEIN G; SHAMMAH S	System for navigating host vehicle, has processing            device for determining navigational action for            execution by host vehicle and causing adjustment of            navigational actuator of host vehicle in response to            determined action for vehicle						NOVELTY - The system has a processing device for receiving a set of images representative of an environment of a host vehicle from a camera. The processing device analyzes the set of images to identify a navigational state associated with the host vehicle and obtains an indicator of an activity of an occupant of the host vehicle from a host vehicle component associated with an interior of the host vehicle. The processing device determines a navigational action for execution by the host vehicle and causes adjustment of a navigational actuator of the host vehicle in response to the determined navigational action for the host vehicle. USE - System for navigating a host vehicle (claimed). ADVANTAGE - The system provides a smoother ride for a user to improve user experiences while allowing for efficient navigation based on activity of occupants. The system allows implementation of parts of policy manually, which can ensure safety of the policy, and implementation of other parts of the policy using reinforcement learning technologies, which can enable adaptivity to many scenarios, a human-like balance between defensive/aggressive behavior, and a human-like negotiation with other drivers. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) an autonomous vehicle(2) a method for navigating a host vehicle(3) a host vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a representative image captured of an environment of a host vehicle, for determining a facing direction of a pedestrian.Scene (2900)Anticipated travel direction (2910)Pedestrian (2920)Region (2930)				[]	[]	-1	-1	-1
P	ZHANG L; JING Y; CUI T	Method for autonomous driving rule learning based            on deep reinforcement learning, involves learning            driving rules for autonomous driving vehicles to            acquires position and speed of connected vehicles in            road network						NOVELTY - The method involves providing autonomous driving vehicle in vehicle queue vehicle-to-vehicle communication during the driving process. The position and speed of the connected vehicle in the road network are obtained. The autonomous driving vehicle is needed to adopt driving behaviors according to the driving status of the connected vehicle. The defined driving behavior of the autonomous driving vehicle is the acceleration of the vehicle. The speed of the autonomous driving vehicle updates the movement state. The basic goal of autonomous vehicle driving is to dissipate stop-and-stop waves in the road network. The acceleration threshold of autonomous driving vehicles is set to accel threshold. The driving strategy model of autonomous driving vehicle selects multi-layer perceptron (MLP). The driving rules of autonomous driving vehicle are learned. The probability value of driving behavior is outputted through the driving strategy model of autonomous driving vehicle. USE - Autonomous driving rule learning based on deep reinforcement learning. ADVANTAGE - The utilization of deep reinforcement learning improves the autonomous decision-making ability of vehicles. The driver of the non-standard operation and error operation influence of the running safety of the automobile is reduced, which improves the driving safety of a vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrates the implementation method for autonomous driving rule learning based on deep reinforcement learning. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	ZADOROJNIY A; MASIN M; MASSING M	Method used for automated explanation of actions            of reinforcement learning in industrial automation,            involves calculating occupation measures for            state-action pairs, based on probabilities, and            receiving selection of action of interest						NOVELTY - The method involves operating a hardware processor for automatically identifying features that drive a reinforcement learning model to recommend an action of interest. The features are identified based on occupation measures of state-action pairs associated with the reinforcement learning model. The reinforcement learning model is fitted (202) to generate a policy. The probabilities of the state-action pairs are calculated (204), based on the policy. The occupation measures are calculated (206) for the state-action pairs, based on the probabilities. A selection of the action of interest is received (208). The predefined threshold is a predefined number of state-action pairs which have the highest occupation measures. USE - Method for automated explanation of actions of reinforcement learning used in robotic, industrial automation, autonomous vehicle, automated medical diagnosis and treatment, computer game, algorithmic trading, etc. ADVANTAGE - The action of interest which is selected is facilitated by presenting a list of actions of the particular model to the user, from which the user can conveniently choose. The explanation of actions recommended by a reinforcement learning model is efficiently and accurately provided, by adapting occupation measures to the reinforcement learning domain. Many machine learning algorithms make black-box predictions and decisions, so that users are prevented from learning from the insight covertly gathered by these algorithms. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:1. a system for automated explanation of reinforcement learning actions;and2. a computer program product for automated explanation of reinforcement learning actions. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for automated explanation of reinforcement learning actions.Method for automated explanation of reinforcement learning actions (200)Step for fitting reinforcement learning model (202)Step for calculating probabilities of the state-action pairs (204)Step for calculating occupation measures (206)Step for receiving selection of the action of interest (208)				[]	[]	-1	-1	-1
P	KREIDIEH A R; ZEIYNALI F Y; OGUCHI K; FARID Y Z	System for facilitating traffic-flow regulation            for i.e. car, through centralized lateral flow control,            has processor for executing machine-readable            instructions to receive aggregated macroscopic traffic            state information from section manager which            communicates with multiple connected vehicles						NOVELTY - The system has a memory (210) located for storing machine-readable instructions. A processor (205) is located for executing the machine-readable instructions to receive aggregated macroscopic traffic state information from a section manager (110) which communicates with multiple connected vehicles in a section of a roadway, process the aggregated macroscopic traffic state information using a reinforcement-learning-based model to determine target lateral flows for two or multiple lanes of the roadway in the section of the roadway and transmit target lateral flows to the section manager, where the section manager converts the target lateral flows to lane-change actions and transmits the lane-change actions to the connected vehicles, the reinforcement-learning-based model is based on a Markov decision process and the connected vehicles are autonomous vehicles. USE - System for facilitating traffic-flow regulation for an autonomous vehicle or a network-enabled manually driven vehicle i.e. car, through centralized lateral flow control. ADVANTAGE - The system improves safety and efficiency of traffic flow when small percentage of the vehicles on the roadway are connected vehicles by anticipating and repositioning the vehicles based on potential downstream congestion. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a non-transitory computer-readable medium including instructions for facilitating traffic-flow regulation for a vehicle through centralized lateral flow control; and(2) a method for facilitating traffic-flow regulation for a vehicle through centralized lateral flow control. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a system for facilitating traffic-flow regulation for a vehicle through centralized lateral flow control.110Section manager205Processor210Memory215Input module225Output module				[]	[]	-1	-1	-1
P	ATOUI H; GONZALEZ BAUTISTA D; MAHTOUT I; GERARD D; MILANES M V; GONZALEZ B D; MONTERO MILANES V; MILANES V	Method for aiding driving of e.g. car, on road,            involves generating control response being optimized            with respect to current situation by reinforcement            learning of control response						NOVELTY - The method involves receiving a set of data (E1). The data received to determine a current situation of an autonomous motor vehicle is determined (E2). A control response to be provided with respect to the current situation is generated (E3). A command is sent (E4) to control the autonomous motor vehicle, where the command is a function of response to be provided. Another control response being optimized with respect to the current situation is generated (E5) by reinforcement learning of the latter response from control depending on quality information on the result of the command to control the autonomous motor vehicle. USE - Method for aiding driving of an autonomous motor vehicle (claimed) e.g. passenger vehicle such as car, lorry and motorcycle, on a road. Can also be used for a semi-autonomous motor vehicle and a bus. ADVANTAGE - The method enables effectively assisting driving of the autonomous motor vehicle on the road, and improving control of the autonomous vehicle on the road. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) a device for assisting driving of an autonomous motor vehicle on a road;(2) a computer program product comprising a set of instructions for assisting driving of an autonomous motor vehicle on a road; and(3) an autonomous motor vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for aiding driving of an autonomous motor vehicle on a road.Step for receiving set of data (E1)Step for determining data received to determine a current situation of an autonomous motor vehicle (E2)Step for generating control response to be provided with respect to the current situation (E3)Step for sending command to control autonomous motor vehicle (E4)Step for generating another control response being optimized with respect to current situation (E5)				[]	[]	-1	-1	-1
P	OGUCHI K; ZEIYNALI F Y; KREIDIEH A R	System for coordinated vehicle lane assignment,            has memory for storing machine-readable instructions            executed by the processor to receive target lateral            flows for multiple lanes of roadway in section of            roadway that includes multiple connected vehicles, from            locality manager						NOVELTY - The system has a processor. A memory is used for storing machine-readable instructions executed by the processor to receive target lateral flows for multiple lanes (150) of a roadway (130) in a section of the roadway that includes multiple connected vehicles, from a locality manager. The target lateral flows are converted to a target number of connected vehicles N. A set of N connected vehicles, whose ranked distances (190) from a following vehicle in a target lane are greatest among the one or more connected vehicles in the section of the roadway, is selected for lane change when a direction of lane change is uniform among the set of N connected vehicles. The lane-change actions are transmitted to the set of N connected vehicles. USE - System for coordinated lane assignment to a vehicle, such as an autonomous vehicle (claimed). ADVANTAGE - By anticipating and repositioning vehicles in response to potential downstream congestion, such systems can greatly improve the safety and efficiency of traffic flow, even when only a small percentage of the vehicles on the roadway are connected vehicles. The coordinated lane-assignment strategies present promising strategies for improving the flow of vehicular traffic to alleviate traffic congestion. The system can be applied to both connected (network-enabled) manually driven vehicles and to connected autonomous vehicles. Setting y to zero results in RL module focusing entirely on improving the performance of the connected vehicles. The same model can then use all sampled information, thus improving the efficiency of the learning procedure. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a non-transitory computer-readable medium for coordinated vehicle lane assignment and storing instructions; and(2) a method for coordinated vehicle lane assignment. DESCRIPTION Of DRAWING(S) - The drawing shows an architecture diagram of a hierarchical traffic-flow regulation system.110But locality manager130Roadway159 Lanes170Legacy non-connected vehicles190Whose ranked distances				[]	[]	-1	-1	-1
P	SYED A A; BACHET A E	System for operating fleet of vehicles, has            assignment unit that is configured to apply algorithm            to request batch to assign each of user requests to            respective vehicle of fleet						NOVELTY - The system (100) has a receiving unit (110) for receiving a set of user requests (UR) for transportation of users to respective destinations. A batching decisions controller unit (120) batches the set of UR based on one of an origin and a destination of each user to form a request batch. An assignment unit (130) applies an algorithm to the request batch to assign each of the user requests to a respective vehicle of a fleet of vehicles. The controller unit includes a Reinforcement Learning-trained neural network for batching the UR. The system navigates and controls the vehicles according to determined travel paths. USE - System for operating a fleet of vehicles. ADVANTAGE - The efficient vehicle utilization and vehicle routing can be provided, thus reducing vehicle emissions, such as carbon dioxide emission. The processing times and the use of processing resources can be reduced. The overall complexity of the vehicle routing is reduced. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for operating a fleet of vehicle; and(2) a machine readable medium for operating a fleet of vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system for operating a fleet of vehicle.Autonomous vehicle (10)Method for operating a fleet of vehicle (100)Receiving Unit (110)Batchin decision controller unit (120)Assignment unit (130)				[]	[]	-1	-1	-1
P	ZHANG D; YU C; LIU Y; ZHANG Q	Reinforcement learning based automatic driving            ship active fault-tolerant path tracking control            method, involves establishing overall control law of            automatic driving ship, so that state of non-linear            dynamic model tracks state of nominal model						NOVELTY - The method involves establishing a non-linear dynamic model of a three-degree-of-freedom model based on a mechanical module of an autonomous vehicle (ASV). A maneuvering module describes longitudinal, lateral and bow movements of the ASV under multiple external forces and torques from a propeller and a rudder. A nominal model is obtained based on the nonlinear dynamics model. An overall control law of an ASV is established such that a state of the non-linear dynamic model can track the state of a nominal model. A basic path tracking control law for ensuring basic tracking performance is determined. An error-tolerant robust control law is determined based on intensive learning for compensating system uncertainty and sensor faults. The sensor fault uses FDI detection and estimates size. USE - Reinforcement learning based automatic driving ship active fault-tolerant path tracking control method used for performing dangerous, complex and expensive maritime tasks e.g. global shipping, environmental monitoring and resource exploration. ADVANTAGE - The method enables reducing the dependence of the fault-tolerant control design on the model information by using the non-model characteristic of the intensified learning, improving the learning efficiency, and reducing the dependence on the sensor fault estimation precision. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a reinforcement learning based automatic driving ship active fault-tolerant path tracking control method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	NEELAKANTAM N; SCHULTZ G	Method for orchestrating virtual storage system,            involves resuming cloud-based storage system            periodically and based on recovery objectives,            including refreshing copy of dataset that is maintained            by storage system						NOVELTY - The method involves receiving recovery objectives associated with a dataset that is stored in a primary storage system. A cloud-based storage system (604) is created. The cloud-based storage system is suspended. The cloud based storage system is periodically resumed based on the recovery objectives. A copy of the dataset is refreshed that is maintained by the cloud based system. Hosts in the primary storage system are configured to utilize the cloud-based storage system upon a failure of the primary storage system. A performance level associated with components of the cloud-based storage system is decreased. USE - Method for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system for various application. Uses include but are not limited to deep learning solutions, deep reinforcement learning solutions, artificial general intelligence solution, autonomous vehicle, cognitive computing solution, commercial unmanned aerial vehicle (UAV) or drone, conversational user interface, enterprise taxonomy, ontology management solution, machine learning solution, smart robot and smart workplace. ADVANTAGE - The method enables utilizing cloud computing as a model of service delivery for enabling convenient on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. The method enables allowing cloud computing environment to offer infrastructure, platforms and/or software as services for which a cloud consumer not need to maintain resources on a local computing device. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) an apparatus for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system;(2) a computer program product comprising a set of instructions for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustarting a method for orchestrating a virtual storage system for managing disaster recovery to cloud computing environment in a cloud-based storage system.Cloud computing environment (602)cloud-based storage system (604)Receiving request to write data to cloud-based storage system (606)Deduplicating data (608)Compressing data (610)				[]	[]	-1	-1	-1
P	PONULAK F	Method for performing credit assignment for            artificial spiking network for e.g. autonomous vehicle,            involves determining credit based on relating network            output, and adjusting learning parameter associated            with unit based on credit						NOVELTY - The method involves operating a spiking neuron network (106) comprising a spiking neuron (106-1) in accordance with reinforcement learning process capable of generating a network/unit output (112). A credit is determined based on relating the network output to contribution of a unit of a set of units. A learning parameter (130) associated with the unit is adjusted based on the credit, where the contribution of the unit is determined based on eligibility associated with the unit. The credit is determined for the unit based on a unit input (102), the unit output and a unit state. USE - Method for performing credit assignment for an artificial spiking network for a robotic apparatus (claimed) e.g. medical robot apparatus such as surgical robot apparatus and rover robot apparatus, autonomous vehicle, unmanned air vehicle, underwater vehicle, smart appliance such as ROOMBA (RTM: autonomous robotic vacuum cleaner), and robotic toys, used for painting or welding and for control applications e.g. HVAC applications and electromechanical devices applications, and games applications. Can also be used for machine vision applications, pattern detection applications, pattern recognition applications, object classification applications, signal filtering applications, data segmentation applications, data compression applications, data mining applications, optimization and scheduling applications and complex mapping applications. ADVANTAGE - The method enables providing implementation of reinforcement learning for large populations of neurons in an efficient manner. The method enables providing faster and more precise learning, thus reducing operational costs associated with operating learning networks due to a shorter amount of time that can be required to arrive at stable solution. The method enables controlling faster processes and learning precision performance in a reliable manner. DETAILED DESCRIPTION - The learning parameter is synaptic weight. INDEPENDENT CLAIMS are also included for the following:(1) a computer-implemented method for operating a set of data interfaces in a computerized network(2) a method for operating a neural network(3) a method for enhancing the learning performance of a neural network(4) a computerized robotic system(5) a robotic apparatus for performing accelerated learning performance. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an adaptive controller comprising a spiking neuron network operable in accordance with reinforcement learning process.Unit input (102)Spiking neuron network (106)Spiking neuron (106-1)Unit/network output (112)Learning parameter (130)				[]	[]	-1	-1	-1
P	HEE L W; KIM E; LEE J H	Robot logistics game device for providing            programming education for artificial intelligence-based            machine learning and coding to induce students to            actively participate in class, has administrator device            providing user environment for inputting result						NOVELTY - The robot logistics game device comprises a game board that is provided with a central area and a border area surrounding the center area. The border area is divided into four areas. The former area displays a color, where the latter area displays another color. The third area displays third color. An obstructing device is configured to move along a circular path. A first control device is provided for controlling a first game device. A second control device controls a second game device. An output unit outputs a manipulation program and a manipulation model building program. A data storage unit stores a steering model. An input unit inputs a corresponding user's command. An interface unit is connected with a corresponding game device, where an output unit is configured for outputting the manipulation program. The first sub-block of the former color and the third sub block of the third color are displayed together with a quick response (QR) code. USE - Robot logistics game device for providing programming education for artificial intelligence-based machine learning and coding to induce students to actively participate in class. ADVANTAGE - The device provides programming education for coding easily and efficiently through the robot logistics game so as to induce students to actively participate in a class, so that the students can have fun with AI, and understanding of underlying machine learning and coding principles can be supported. The device gradually applies reinforcement learning by correcting errors occurred in process of repeating certain actions to solve a problem, and supports students in learning concept of an autonomous vehicle lane recognition system. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of robot logistics game device for providing programming education for artificial intelligence-based machine learning.Game device (10)Game board (100)Block group (110)Moving unit (120)Control device (130)				[]	[]	-1	-1	-1
P	CHAO M; KOSE C N; ROSALES R; KOSE CIHANGIR N	Device for triggering vehicular action of vehicle,            has processor that identifies features in subsets of            image data and determines state of occupants based on            tracked changes from states and trigger vehicular            action based on determined state						NOVELTY - The device has a processor that is configured to identify features in subsets of image data detailing the occupants. The processor is configured to track changes over time of features over subsets of image data. The processor is configured to determine a state of the occupants based on the tracked changes from states. The processor is configured to trigger the vehicular action based on the determined state. The segments corresponds to a respective subset of subsets of image data, and each segment comprises a value corresponding to each of features. USE - Device for triggering vehicular action of vehicle such as automobile, aquatic vehicle, sub-aquatic vehicle and autonomous vehicle, for use in driving safety system. ADVANTAGE - The reinforcement learning models include positive or negative feedback to improve accuracy. A safety driving model or include a mathematical model for safety assurance that enables identification and performance of proper responses to dangerous situations such that self-perpetrated accidents are avoided. Since the order and magnitude of change of a motion are very critical in differentiating specific actions, the mechanisms and schemes provided are provide high accuracy and efficiency in identifying a state of a driver and passengers and using this information for a safety driving model. The device helps to capture the temporal information between selected frames using values of feature and to understand a direction of a motion together with the magnitude of change. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method to trigger vehicular action based on monitoring occupants of vehicle; and(2) a non-transitory computer readable media storing program for trigger vehicular action based on monitoring occupants of vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of driving safety system.Camera (702)Spatio-temporal system (704)Estimator (706)Driving model (708)Safety response (710)				[]	[]	-1	-1	-1
P	HUANG Y; YUAN K; YANG S; WANG L; CHEN H	Scene self-adaptive decision planning method for            autonomous driving vehicle, involves outputting            steering wheel angle and accelerator opening to            actuator to realize function of autonomous            driving						NOVELTY - The method involves obtaining dynamic traffic information of an autonomous vehicle to be planned. The dynamic traffic information is input into pre-trained decision-end output layer network to obtain a lane change decision. The dynamic traffic information is input into the pre-trained output layer network of planning end, and adjustment results of model predictive control (MPC) parameters used for trajectory planning of a vehicle are obtained under current working conditions. The lane change decision and MPC parameter adjustment results are input into a pre-trained horizontal and vertical progressive trajectory planning network, and a steering wheel angle and accelerator opening are output to an actuator to realize a function of autonomous driving. USE - Scene self-adaptive decision planning method for autonomous driving vehicle. ADVANTAGE - The method enables utilizing a reinforcement learning framework to rationally and fully use dynamic traffic information provided by a sensing system through construction of reinforcement learning, so that the reinforcement learning neural network can be trained in an efficient manner. The method allows a decision-making end output layer network in a training process at current time of the vehicle state information and the dynamic information as a state, lane or lane keeping as action, thus improving safety and comfort of the automatic driving automobile, driving efficiency as a reward, and driving efficiency. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an electronic device for using scene self-adaptive decision planning device for autonomous driving vehicle; and(2) a non-transitory machine-readable storage medium storing program for using scene self-adaptive decision planning device for autonomous driving vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an algorithm architecture. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	GHOSH S; LINCOLN P D; RAMAMURTHY B S	Method for using machine learning to create model            that improves operation of controller of e.g. laptop,            involves providing model as trusted model that improves            operation of system controller by enabling controller            to perform system action						NOVELTY - The method involves modifying a model (108) or input data (104) or a reward function (110) of the model, and re-training the model using one of the model, the input data or the reward function when a determination is made that the model does not satisfy a trust-related constraint (114). The model is provided as a trusted model that improves operation of a computer system controller (150) by enabling the computer system controller to perform a system action (152) within predetermined guarantees when the determination is made that the model satisfies the trust-related constraint. USE - Method for using machine learning to create a trusted model that improves operation of a controller of an autonomous computer system e.g. personal computer such as desktop, laptop, tablet, smart phone and wearable or body-mounted device, server and enterprise computer system, for life-critical or mission-critical applications. Uses include but are not limited to self-driving cars, cyber security applications, surgical robotics and aircrafts. ADVANTAGE - The method enables improving reliability of a computer system operation through the use of machine learning that provides guarantees of operation through the trusted model. The method allows the trusted model provided to a controller by a machine learning system to make the controller more trustworthy, thus making the system more reliable. The method enables facilitating maximum likelihood inverse reinforcement learning process by using a maximum likelihood estimation to a problem of inverse reinforcement learning to identify unknown reward functions from traces of initial input data. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a machine learning system for creating a trusted model that improves operation of a computer system controller(2) a non-transitory computer readable medium comprising a set of instructions for using machine learning to create a trusted model that improves operation of an autonomous vehicle controller. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an environment of a computing system including components for a machine learning system that uses a trusted model.Input data (104)Model (108)Reward function (110)Trust-related constraint (114)Computer system controller (150)System action (152)				[]	[]	-1	-1	-1
P	XIA H; REN D; BAI Y; ZHANG T; JIA Q; LI K	Strategy transfer method for reinforcement            learning, involves executing next iteration process            based on updated first prediction model and updated            first policy information until sample data output by            first prediction model matches sample data at target            time T'						NOVELTY - The method involves obtaining (201) the first sample data at multiple simulation moments, based on the simulation scenario of the autonomous driving scenario. The first strategy information and the first prediction model are determined (202), based on multiple sets of first sample data. The following steps are iteratively executed (203) to train the first prediction model and the first policy information, based on the first policy information, the first prediction model and the automatic driving scenario. The second sample data at the target time T' in the autonomous driving scenario is obtained, in any iterative process, based on the first strategy information of this iterative process. The next iteration process is executed based on the updated first prediction model and the updated first policy information until the sample data output by the first prediction model matches the sample data at the target time T'. USE - Strategy transfer method for reinforcement learning applied in scenarios such as autonomous driving and intelligent workshop scheduling to conduct strategic planning in scenarios based on reinforcement learning. ADVANTAGE - The action to be performed by the autonomous vehicle is accurately predicted in the autonomous driving scenario, so that better results are achieved after executing the action, by migrating the prediction model and strategy information in the simulation scenario to the autonomous driving scenario through this method. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following: (1) a computer device; and (2) a computer readable storage medium storing program for strategy transfer for reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the strategy transfer method for reinforcement learning. (Drawing includes non-English language text)201Step for obtaining the first sample data at multiple simulation moments202Step for determining the first strategy information and the first prediction model203Step for training the first prediction model and the first policy information				[]	[]	-1	-1	-1
P	PALANISAMY P; MUDALIGE U P; CHEN Y; DOLAN J M; MUELLING K; PALANIZAMY P; MUDARIGUE U P; MILLING K	Method for learning lane-change policies through            actor-critic network architecture, involves generating            spatial context vector at spatial attention module, and            processing combined context vector to generate            hierarchical actions						NOVELTY - The method involves processing image data received from an environment to learn the lane-change policies as a set of hierarchical actions through an actor network (110) over time. The action values are predicted through an action value function at a critic network (120). The learned importance weights are applied to each of the relevant regions of the image data to add importance to the relevant regions of the image data at the spatial attention module. A spatial context vector is generated at the spatial attention module (140). The spatial context vector is processed to learn temporal attention weights to be applied to past frames of image data to indicate relative importance in deciding which lane-change policy to select at a temporal attention module (160) of the actor network. A combined context vector is generated at the temporal attention module. The combined context vector is processed to generate the set of hierarchical actions through fully connected layer. USE - Method for learning lane-change policies through actor-critic network architecture. ADVANTAGE - The two streams of temporal attention and spatial attention boost the performance in deep reinforcement learning (DRL). The hierarchical action structure of sub-policies for lane changing behavior allows a vehicle to perform safe and efficient lane-changes which is a crucial feature for delivering fully autonomous vehicles on road. The total number of parameters in the network is reduced for more efficient training and testing. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an actor-critic network system; and(2) an autonomous vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows the block diagram of a DRL system.DRL system (100)Actor network (110)Critic network (120)Spatial attention module (140)Temporal attention module (160)				[]	[]	-1	-1	-1
P	CHENTANEZ N; MUELLER-FISCHER M; MACKLIN M; MAKOVIICHUK V; JESCHKE S	Method for imitating reference object from motion            capture (MOCAP) video clip using physics simulator and            neural network (NN), involves adjusting movement of            target object using stability threshold and movement            agent						NOVELTY - The method involves tracking a reference object (306) using a movement agent of a target object (305). A movement of the target object is adjusted using a stability threshold and the movement agent. The movement agent utilizes an output from the NN to provide modifications to the movement. The output is indicative of joint torques, joint positionings, applied forces, or proportional derivative (PD) controller gain parameters. The output from the NN is generated utilizing simulation environment data, motion interference parameters, or target object information. The NN is one of a tracking NN or a recovery NN. USE - Method for imitating reference object from MOCAP video clip using physics simulator and NN. ADVANTAGE - The method can use the deep reinforcement learning technique combined with the physics-based MOCAP methods to allow the simulator to generate the higher quality video portion where the movements of the object or character appear smooth and natural. The motion correctors can be used to help determine how the human can react and move in various situations so that the autonomous vehicle can select the action that could minimize the damage and harm to the human. The recovery agent can be utilized to provide the NN parameters to bring the target object back to the position that satisfies the stability threshold using information from NN. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a video generator system for imitating reference object from MOCAP video clip using physics simulator and NN; and(2) a computer program product for imitating reference object from MOCAP video clip using physics simulator and NN. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of the target object movement agent process.Agent process (300)Target object (305)Reference object (306)Tracking samples (310a-310e,320a-320e,330a-330e)				[]	[]	-1	-1	-1
P	SEO S; KIM S; YOON H; KIM C	Uncertainty conditional deep reinforcement            learning device, has uncertainty estimation unit            estimating epistemological uncertainty and feature            vector, and behavior estimation unit estimating            behavior for reward based on feature vector						NOVELTY - The device has a feature extraction unit (100) that extracts a state as input for a feature vector. An uncertainty estimation unit (200) estimates epistemological uncertainty and the feature vector based on epistemic uncertainty and action. A behavior estimation unit (300) estimates behavior for a reward based on the state and the feature vector. An encoder outputs the state of the behavior. The uncertainty estimation unit utilizes a Monte-Carlo dropout method to determine the state, applies Monte-Carlo dropout to the encoder, and utilizes the variance of Monte Carlo samples as an uncertainty indicator. USE - Uncertainty conditional deep reinforcement learning device for an autonomous vehicle i.e. autonomous car. ADVANTAGE - The device checks whether the input state is well trained in the deep reinforcement learning model and determines appropriate action by considering uncertainty of the input condition. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a uncertainty conditional deep reinforcement learning method; anda computer-readable recording medium comprising a set of instructions for realizing uncertainty conditional deep reinforcement learning process. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a uncertainty conditional deep reinforcement learning device (Drawing includes non-English language text).100Feature extraction unit200Uncertainty estimation unit300Behavior estimation unit				[]	[]	-1	-1	-1
P	SCHAEFER M	Method for reinforcement learning for carrying out            control and/or regulation task of entity such as            mechanical or electrical component, involves changing            strategy of strategy module by correction factor            determined by divergence						NOVELTY - The method involves determining modeled values for the properties and parameters of an entity (10) from a learning reinforcement module (400) to generate a second probability distribution for the respective properties and parameters. The first calculation results for the first probability distribution are generated by using mathematical functions and/or statistical methods. The second calculation results for the second probability distribution are generated by using mathematical functions and/or statistical methods. A divergence between the first calculation results and the second calculation results is determined. The strategy of a strategy module (470) is changed by a correction factor (550) determined by the divergence. USE - Method for reinforcement learning for carrying out control and/or regulation task of entity such as mechanical, electrical, electronic, mechatronic or hydraulic component in many areas of technology such as mechanical engineering and automotive technology. Uses include but are not limited to entity designed as larger unit including further components and assemblies such as motor vehicle with internal combustion engine or electric motor, autonomous vehicle, agricultural vehicle such as combine harvester, robot for use in production or services, watercraft or flying object such as airplane or drone, system or assembly such as braking system, drive train, electric motor or internal combustion engine for motor vehicle, or energy generation system such as photovoltaic system or a wind turbine, or a production system for producing a product, such as production facility for manufacture of motor vehicle, scientific analysis instrument, household appliance or medical device for diagnosing and supporting bodily functions, transportation system, supply chain system, stock market, energy supply system, and computer simulation, computer game, augmented reality, or another virtual object, and etc. ADVANTAGE - The possibilities for improving reinforcement learning in order to be able to carry out control and regulation tasks with high level of reliability, security, accuracy and efficient use of computing capacity is created. The system is able to carry out control and regulation tasks based on reinforcement learning more optimally and thus to reduce costs and the consumption of resources. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a system for reinforcement learning for carrying out control and/or regulation tasks of an entity; and(2) computer program product for reinforcement learning for carrying out control and/or regulation tasks of an entity. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram illustrating the system for reinforcement learning for carrying out control and/or regulation task of entity.10Entity100Reinforcement learning system400Learning reinforcement module470Strategy module550Correction factor				[]	[]	-1	-1	-1
P	SATTI J R; GREWAL A S; SHAHRIARI M	Method for implementing acceleration and            deceleration requests in behaviour-based adaptive            cruise control (ACC), involves processing set of            results with driver behaviour data determined by            reinforcement learning to correlate control actions            with driver behaviour data						NOVELTY - The method involves executing adaptive cruise control to obtain a set of vehicle inputs about the operating environment and current operations of a host vehicle (10). A target vehicle operating in the vicinity of the host vehicle is identified. A set of target vehicle parameters about the target vehicle derived from sensed inputs is quantified. A state estimate of the host vehicle and the target vehicle is modelled by generating a set of velocity calculations and torque calculations about the vehicle. A set of results is generated from the reward function based on one or more modelled state estimates of the host vehicle and the target vehicle. The set of results are processed with driver behaviour data determined by the reinforcement learning to correlate one or more control actions with the driver behaviour data. USE - Method for implementing acceleration and deceleration requests in behaviour-based adaptive cruise control (ACC) using gain learning (RL) based on driving style of driver of vehicle such as motorcycle, truck, sport utility vehicle, recreational vehicle, watercraft, and airplane. ADVANTAGE - The behaviour of the driver can be assessed and target vehicle behavior can be detected to train intelligent model for adaptive cruise control function that correlates to a driver's driving style when operating a vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for implementing adaptive cruise control. DESCRIPTION Of DRAWING(S) - The drawing shows a functional block diagram of the autonomous or semi-autonomous vehicle with control system that controls vehicle actions based on the use of neural network for driver behaviour in vehicle control system.Vehicle (10)Transmission system (22)Braking system (26)Actuator system (30)Vehicle control system (100)				[]	[]	-1	-1	-1
P	SUN Y; KING S; MA O	System for controlling interaction of apparatus            with e.g. self-closing door, for facilitating            navigation of e.g. machines, to perform wide range of            tasks in e.g. commercial settings, has door control            apparatus arranged on vehicle and provided with            controller, and control arm including camera						NOVELTY - The system has a door control apparatus (100) arranged on a vehicle (102) and provided with a controller. A frame includes a top frame portion and a bottom frame portion. A door control arm is integrated as a portion of the frame and moved vertically between the top frame portion and the bottom frame portion. A door control component includes a door handle contact protrusion with a curve-shaped end portion. The door handle contact protrusion extends perpendicularly relative to the frame. A door interaction arm is arranged on a surface of the frame, and extends perpendicularly relative to the frame. The door control arm includes a camera. The frame is formed by aluminum material. USE - System for controlling interaction of an apparatus with a door e.g. self-closing door, for facilitating navigation of a vehicle e.g. robot, autonomous vehicle and machines, to perform wide range of tasks in residential and commercial settings. Uses include but are not limited to manufacturing and machining complex components for automobiles, computers, surveilling surroundings, cleaning floors, inspecting areas of interest and delivering items from cleaning rooms. ADVANTAGE - The door control apparatus is mounted on the vehicle to autonomously unlock and open the self-closing door so as to effectively facilitate navigation of the vehicle. The controller of the door control apparatus can utilize the door control arm and sub-components of the door control arm so as to unlatch the door and pull or push the door to configure the door in a slightly open state and utilize one of two door-holding bars to hold the door to open for enabling navigation the vehicle through the door in an efficient and autonomous manner. The system utilizes a camera to transfer images to the controller so as to analyze the images and/or live video stream and facilitate movement of the vehicle and the door control apparatus toward the door. The controller can be trained on a reinforcement learning-based model to determine a set of actions that enable the door control apparatus to efficiently and effectively recharge a power supply component. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of a door control apparatus mounted on a vehicle.100Door control apparatus102Vehicle				[]	[]	-1	-1	-1
P	LIN W; LIU L; SUN X; MA K; XUAN Z; ZHAO Y	Vehicle speed control system for autonomous            vehicles, has vehicle speed control module that            determines if proposed vehicle speed control command            conforms to desired human driving behaviors by use of            human driving model module						NOVELTY - The system (201) has a data processor (171) and a vehicle speed control module (200). The vehicle speed control module is configured to perform a vehicle speed control command validation operation for an autonomous vehicle. The vehicle speed control command validation operation generates data corresponding to desired human driving behaviors, trains a human driving model module (175) using a reinforcement learning process and the desired human driving behaviors. The vehicle speed control module receives a proposed vehicle speed control command (210), determines if the proposed vehicle speed control command conforms to the desired human driving behaviors by use of the human driving model module, and validates or modifies the proposed vehicle speed control command based on the determination. USE - Vehicle speed control system for autonomous vehicles. ADVANTAGE - The system uses the real-time generated trajectory and vehicle motion control command signal to safely and efficiently navigate the vehicle through a real world driving environment, while avoiding obstacles and safely controlling the vehicle. The speed control module and the human driving model module serve to enable the modeling and modification of the vehicle speed control command for the vehicle based on a comparison of a proposed vehicle speed control command with corresponding normal human driving behavior data maintained by the human driving model module. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for managing speed control of autonomous vehicles; and(2) a non-transitory machine-useable storage medium that comprises instructions for performing a method for managing speed control of autonomous vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows the block diagram of the vehicle speed control system.Data processor (171)Human driving model module (175)Vehicle speed control module (200)Vehicle speed control system (201)Proposed vehicle speed control command (210)				[]	[]	-1	-1	-1
P	MILTON S	Non-transitory machine-readable medium for            performing vehicle data analytics on            remotely-controlled vehicles, semi-autonomous vehicles            and fully autonomous vehicle, involves providing second            feedback indicator to operator based on second            value						NOVELTY - The medium have set of instructions for providing a first feedback feedback indicator to a vehicle operator based on a first action value by using a vehicle. Multiple sensor measurements is performed, while the first feedback indicator is provided to the vehicle operator. Multiple sensor measurements is associated with the vehicle by using multiple vehicle sensors or multiple roadside sensors. A reward value is determined based on the target vehicle state and multiple sensor measurements. A second action value is determined based on reward value based on policy by using a deep reinforcement learning system, and the policy comprises mapping of a possible state to perform possible action. A second feedback indicator is provided to the operator based on second value. USE - Non-transitory machine-readable medium for performing vehicle data analytics on remotely-controlled vehicles, semi-autonomous vehicles and fully autonomous vehicles. ADVANTAGE - The medium have set of instructions for computationally performing operations on devices of a vehicle by using a computing layer, thus quickly performing searching process to a subset of available tiles corresponding to the data model through the entire map, and calculating significant amount of time or computational resources, so that noise can be injected during process, thus improving accuracy of training operation, and hence increasing security and privacy of data during FL training operation. The medium have set of instructions for creating application to perform neural network operations across multiple parameters, thus allowing deeper understanding of the latent space of various previously disparate records or data such as vehicle sensor data, operator profile data, road network graph data, and online profile data. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a first computing environment.First vehicle (102)Vehicle sensor (104)Wireless network interface (105)Onboard computing device (106)Vehicle agent (108)Anomalous object (110)Base station (120)Local computer data center (122)Local computing agent (128)				[]	[]	-1	-1	-1
P	SCHMITT F; VAN HOOF H; WOEHLKE J G; WOLCK J G; SCHMIDT F; WOEHLKE H V; VAN H H	Method for controlling a robot, involves receiving            an indication of a target configuration to be reached            from an initial configuration of the robot, and            determining a coarse-scale value map by value            iteration						NOVELTY - The method (300) involves receiving (301) an indication of a target configuration to be reached from an initial configuration of the robot. The coarse-scale value map is determined (302) by value iteration. The transition probabilities are determined using a transition probability model mapping coarse-scale states and coarse-scale actions to transition probabilities for coarse-scale states. The fine-scale sub-goal from the coarse-scale value map is determined. The fine-scale control actions are performed (305) to reach the determined fine-scale sub-goal by an actuator of the robot. The sensor data is obtained to determine the fine-scale states reached as a result of performing the fine-scale control actions for each fine-scale state of the resulting sequence of fine-scale states of the robot. The next coarse-scale state of the sequence of coarse-scale states is determined (306) from the last fine-scale state of the sequence of fine-scale states. USE - Method for controlling robot such as autonomous vehicle. ADVANTAGE - The method allows controlling a robot by training with less observation data since only a transition probability model needs to be learned for the high-level planner as opposed to learning the value iteration procedure itself end-to-end with the low-level continuous control policy via reinforcement learning, which is a more difficult learning task. The method is computationally more efficient than approaches learning a value iteration model (such as HiDe) since no backpropagation of gradients through the planner is necessary. The data-efficiency of RL algorithms is high in case the provided reward signal is very sparse. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a robot controller;(2) a computer program for controlling robot; and(3) a computer-readable medium storing program for controlling robot. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart of a method for controlling robot.Method for controlling robot (300)Step for receiving indication of a target configuration to be reached from an initial configuration of the robot (301)Step for determining a coarse-scale value map by value iteration (302)Step for performing fine-scale control actions to reach the determined fine-scale sub-goal by an actuator of the robot (305)Step for determining the next coarse-scale state of the sequence of coarse-scale states from the last fine-scale state of the sequence of fine-scale states (306)				[]	[]	-1	-1	-1
J	Liu, Hao; Kiumarsi, Bahare; Kartal, Yusuf; Taha Koru, Ahmet; Modares, Hamidreza; Lewis, Frank L.	Reinforcement Learning Applications in Unmanned Vehicle Control: A Comprehensive Overview	2023	UNMANNED SYSTEMS		17	26	This paper briefly reviews the dynamics and the control architectures of unmanned vehicles; reinforcement learning (RL) in optimal control theory; and RL-based applications in unmanned vehicles. Nonlinearities and uncertainties in the dynamics of unmanned vehicles (e.g. aerial, underwater, and tailsitter vehicles) pose critical challenges to their control systems. Solving Hamilton-Jacobi-Bellman (HJB) equations to find optimal controllers becomes difficult in the presence of nonlinearities, uncertainties, and actuator faults. Therefore, RL-based approaches are widely used in unmanned vehicle systems to solve the HJB equations. To this end, they learn the optimal solutions by using online data measured along the system trajectories. This approach is very practical in partially or completely model-free optimal control design and optimal fault-tolerant control design for unmanned vehicle systems.	Article; Early Access		10.1142/S2301385023310027	[]	[]	-1	-1	-1
C	Zhang, Kai; Wang, Guile; Hu, Jinwen; Xu, Zhao; Guo, Chubing	Path Planning Technology of Unmanned Vehicle Based on Improved Deep Reinforcement Learning	2021	2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)	Chinese Control Conference	8392	8397	"As the basic problem of unmanned vehicle navigation control, path planning has been widely studied. Reinforcement learning (RL) has been found an effective way of path optimization for the highly nonlinear and unmodeled dynamics. However, the RL based methods suffer from the ""dimension disaster"" under the high-dimension state spaces. In this paper, the path planning of an unmanned vehicle with collision avoidance is considered, and an improved Deep Q-Network (DQN) algorithm is proposed to reduce the computation load in the high-dimension state space. First, the states, actions and rewards are determined based on the task requirement, and a smoothing function is defined as an additional penalty term to modify the basic reward function. Then, the two-dimension grid of the state space is mapped to a gray image, which is applied as the input of a neural network, i.e., the Q-Network. Finally, simulation results show that the modified DQN algorithm is more stable and the fluctuation frequency is significantly reduced."	Proceedings Paper	2021		[]	[]	-1	-1	-1
C	Marvi, Zahra; Kiumarsi, Bahare	Safe Off-policy Reinforcement Learning Using Barrier Functions	2020	2020 AMERICAN CONTROL CONFERENCE (ACC)	Proceedings of the American Control Conference	2176	2181	This paper presents a safe off-policy reinforcement learning (RL) scheme to design optimal controllers for systems with uncertain dynamics. The utility function for which its optimization achieves a desired behavior is augmented with a control barrier function (CBF) candidate providing a platform for merging safety planning and optimal control design. A damping factor is included in the CBF providing a design tool to specify the relative importance of performance and safety. As one of the main contributions of this paper, it is shown that by iterative approximation of the value function, the safety properties of CBF are certified which bridges the broad capability of barrier functions into the learning-based approaches. Then, the safety of control system is proved accordingly. Stability and optimality of the control system in a safe condition are verified. Afterward, an off-policy RL algorithm is used to obtain the safe and optimal controller without requiring full knowledge about the system dynamics. The efficiency of the proposed method is demonstrated on the lane changing as an automotive control problem.	Proceedings Paper	2020	10.23919/acc45564.2020.9147584	[]	[]	-1	-1	-1
J	Ma, Chengdong; Liu, Jianan; He, Saichao; Hong, Wenjing; Shi, Jia	Confrontation and Obstacle-Avoidance of Unmanned Vehicles Based on Progressive Reinforcement Learning	2023	IEEE ACCESS		50398	50411	The core technique of unmanned vehicle systems is the autonomous maneuvering decision, which not only determines the applications of unmanned vehicles but also is the critical technique many countries are competing to develop. Reinforcement Learning (RL) is the potential design method for autonomous maneuvering decision-making systems. Nevertheless, in the face of complex decision-making tasks, it is still challenging to master the optimal policy due to the low learning efficiency caused by the complex environment, high dimensional state, and sparse reward. Inspired by the human learning process from simple to complex, we propose a novel progressive deep RL algorithm for policy optimization in unmanned autonomous decision-making systems in this paper. The proposed algorithm divides the training of the autonomous maneuvering decision into a sequence of curricula with learning tasks from simple to complex. Finally, through the self-play stage, the iterative optimization of the policy is realized. Furthermore, the confrontation environment with two unmanned vehicles with obstacles is analyzed and modeled. Finally, the simulation leads to the one-to-one adversarial tasks demonstrate the effectiveness and applicability of the proposed design algorithm.	Article	2023	10.1109/ACCESS.2023.3278597	[]	[]	-1	-1	-1
C	Mason, Federico; Drago, Matteo; Zugno, Tommaso; Giordani, Marco; Boban, Mate; Zorzi, Michele	A Reinforcement Learning Framework for PQoS in a Teleoperated Driving Scenario	2022	2022 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC)	IEEE Wireless Communications and Networking Conference	114	119	In recent years, autonomous networks have been designed with Predictive Quality of Service (PQoS) in mind, as a means for applications operating in the industrial and/or automotive sectors to predict unanticipated Quality of Service (QoS) changes and react accordingly. In this context, Reinforcement Learning (RL) has come out as a promising approach to perform accurate predictions, and optimize the efficiency and adaptability of wireless networks. Along these lines, in this paper we propose the design of a new entity, integrated at the RAN level that implements PQoS functionalities with the support of an RL framework. Specifically, we focus on the design of the reward function of the learning agent, able to convert QoS estimates into appropriate countermeasures if QoS requirements are not satisfied. We demonstrate via ns-3 simulations that our approach achieves better results in terms of QoS and Quality of Experience (QoE) performance of end users in a teleoperated driving scenario.	Proceedings Paper	2022	10.1109/WCNC51071.2022.9771590	[]	[]	-1	-1	-1
J	Xu Guoyan; Zong Xiaopeng; Yu Guizhen; Su Hongjie	A Research on Intelligent Obstacle Avoidance of Unmanned Vehicle Based on DDPG Algorithm	2019	Automotive Engineering		206	212	An intelligent obstacle avoidance scheme for unmanned vehicle based on reinforcement learning is proposed in this paper. In view of that the movement of unmanned vehicle must meet both interior and exterior constraints,including vehicle dynamics constraints and traffic rule constraints and its output must be continuous, which the traditional reinforcement learning cannot assure,an improved deep deterministic policy gradient algorithm is proposed to tackle continuous motion space issue and achieve the continuous output of steering wheel angle and acceleration. Multi-source sensor data fusion is adopted to fulfill the state input of unmanned vehicle obstacle avoidance algorithm and both interior and exterior constraints are added to make output motion more reasonable and effective. Finally a simulation is conducted on the open-source simulation platform TORCS and the effectiveness and robustness of the algorithm verified.	Article	2019		[]	[]	-1	-1	-1
J	Mukherjee, Sayak; Perez-Rapela, Daniel; Forman, Jason L.; Panzer, Matthew B.	Generating Human Arm Kinematics Using Reinforcement Learning to Train Active Muscle Behavior in Automotive Research	2022	JOURNAL OF BIOMECHANICAL ENGINEERING-TRANSACTIONS OF THE ASME				Computational human body models (HBMs) are important tools for predicting human biomechanical responses under automotive crash environments. In many scenarios, the prediction of the occupant response will be improved by incorporating active muscle control into the HBMs to generate biofidelic kinematics during different vehicle maneuvers. In this study, we have proposed an approach to develop an active muscle controller based on reinforcement learning (RL). The RL muscle activation control (RL-MAC) approach is a shift from using traditional closed-loop feedback controllers, which can mimic accurate active muscle behavior under a limited range of loading conditions for which the controller has been tuned. Conversely, the RL-MAC uses an iterative training approach to generate active muscle forces for desired joint motion and is analogous to how a child develops gross motor skills. In this study, the ability of a deep deterministic policy gradient (DDPG) RL controller to generate accurate human kinematics is demonstrated using a multibody model of the human arm. The arm model was trained to perform goal-directed elbow rotation by activating the responsible muscles and investigated using two recruitment schemes: as independent muscles or as antagonistic muscle groups. Simulations with the trained controller show that the arm can move to the target position in the presence or absence of externally applied loads. The RL-MAC trained under constant external loads was able to maintain the desired elbow joint angle under a simplified automotive impact scenario, implying the robustness of the motor control approach.	Article	DEC 1 2022	10.1115/1.4055680	[]	[]	-1	-1	-1
J	Hu, B.; Chen, J.; Lin, Y.; Tan, S.	Vehicle Following Hybrid Control Algorithm Based on DRL and PID in Intelligent Network Environment	2022	SAE Technical Papers		2022	01-7113	Deep reinforcement learning (DRL) has not been widely used in the engineering field yet because RL needs to be learned through 'trial and error', which makes the application of this kind of algorithm in real physical environment more difficult, and it is impossible to carry out 'trial and error' learning on real vehicles. By analyzing the motion state of the vehicle in the car following mode, the algorithm that combined traditional longitudinal motion control with DRL improves the safety of RL in the real physical environment and the poor adaptability of the traditional longitudinal motion control algorithm. In this paper, the longitudinal motion of the unmanned vehicle is taken as the research object, and the PID algorithm is combined with the Deep Deterministic Policy Gradient (DDPG) algorithm to control the longitudinal motion of the unmanned vehicle. The research results show that the longitudinal motion control hybrid algorithm performed better than the single PID algorithm or DDPG algorithm in the vehicle following control. This strategy establishes the relationship between the vehicle longitudinal control and the state of both the ego vehicle and the front vehicle, while also considers the randomness of the front vehicle motion in the iterative learning process, which improves the safety, comfort and following performance.	Conference Proceedings	2022	10.4271/2022-01-7113	[]	[]	-1	-1	-1
J	Evdokimova, T.S.; Sinodkin, A.A.; Tiurikov, M.I.; Fedosova, L.O.; Kalyashina, A.V.	Developing an unmanned vehicle local trajectory using a reinforcement learning algorithm	2021	IOP Conference Series: Materials Science and Engineering		012015 (6 pp.)	012015 (6 pp.)	This article describes the algorithm development for constructing a local trajectory for an unmanned vehicle or for implementation in an ADAS system using the reinforcement learning method. A special part is dedicated to reinforcement learning. One of the methods that is best suitable for the task conditions will also be implemented. This method will allow bypassing obstacles and reaching the specified short target points.	Conference Paper; Journal Paper	2021	10.1088/1757-899X/1086/1/012015	[]	[]	-1	-1	-1
C	Zhai, Weitong; Wang, Xiangrong; Greco, Maria S.; Gini, Fulvio	Reinforcement Learning based Integrated Sensing and Communication for Automotive MIMO Radar	2023	2023 IEEE RADAR CONFERENCE, RADARCONF23				Integrated sensing and communication (ISAC) is a promising technique in vehicular transportation thanks to its substantial gains in size, cost, power consumption, electromagnetic compatibility and spectrum congestion. In this paper, we propose a reinforcement learning (RL) based ISAC system with a multi-input-multi-output (MIMO) automotive radar. The target sensing and downlink communication are separately performed by dividing the transmit antennas into two non-overlapping but interweaving subarrays. We first design a RL framework to adaptively allocate the proper number of transmit antennas for the two subarrays under any unknown environment. The training is performed in the metrics of Cramer-Rao Bound (CRB) of direction of arrival (DOA) estimation for sensing and receive signal-to-noise (SNR) for communications, respectively. We proceed to propose a co-design method to jointly optimize the configurations of the two subarrays to further enhance the sensing accuracy with a constrained communication quality. The resultant problem is converted into the convex form via convex relaxation. Simulations are provided to demonstrate the adaptability and effectiveness of the proposed RL based ISAC system under the unkown environment.	Proceedings Paper	2023	10.1109/RADARCONF2351548.2023.10149653	[]	[]	-1	-1	-1
J	Egan, Daniel; Zhu, Qilun; Prucka, Robert	A Review of Reinforcement Learning-Based Powertrain Controllers: Effects of Agent Selection for Mixed-Continuity Control and Reward Formulation	2023	ENERGIES				One major cost of improving the automotive fuel economy while simultaneously reducing tailpipe emissions is increased powertrain complexity. This complexity has consequently increased the resources (both time and money) needed to develop such powertrains. Powertrain performance is heavily influenced by the quality of the controller/calibration. Since traditional control development processes are becoming resource-intensive, better alternate methods are worth pursuing. Recently, reinforcement learning (RL), a machine learning technique, has proven capable of creating optimal controllers for complex systems. The model-free nature of RL has the potential to streamline the control development process, possibly reducing the time and money required. This article reviews the impact of choices in two areas on the performance of RL-based powertrain controllers to provide a better awareness of their benefits and consequences. First, we examine how RL algorithm action continuities and control-actuator continuities are matched, via native operation or conversion. Secondly, we discuss the formulation of the reward function. RL is able to optimize control policies defined by a wide spectrum of reward functions, including some functions that are difficult to implement with other techniques. RL action and control-actuator continuity matching affects the ability of the RL-based controller to understand and operate the powertrain while the reward function defines optimal behavior. Finally, opportunities for future RL-based powertrain control development are identified and discussed.	Review	APR 2023	10.3390/en16083450	[]	[]	-1	-1	-1
P	XU G; ZONG X; YU G	Reinforcement learning-based obstacle-avoiding            system for unmanned vehicle, has executing part for            realizing obstacle-avoidant function by rotating            unmanned vehicle through calculated steering wheel            angle						NOVELTY - The system has a system main body provided with a sensing part, a decision part, a controlling part and an implementing part. The sensing part detects obstacle area using single laser radar. Determination is made to check an obstacle for an unmanned vehicle, if unmanned vehicle requires control, the controlling part controls state information input through reinforcement learning model, where the model is obtained through trial and error to converge to steady state. The executing part realizes obstacle-avoidant function by rotating unmanned vehicle through calculated steering wheel angle. USE - Reinforcement learning-based obstacle-avoiding system for unmanned vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a reinforcement learning-based obstacle-avoiding method for unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a reinforcement learning-based obstacle-avoiding method for unmanned vehicle. '(Drawing includes non-English language text)'				[]	[]	-1	-1	-1
B	Zhai, W.; Wang, X.; Greco, M.S.; Gini, F.	Reinforcement Learning based Integrated Sensing and Communication for Automotive MIMO Radar	2023	2023 IEEE Radar Conference (RadarConf23)		1	6	Integrated sensing and communication (ISAC) is a promising technique in vehicular transportation thanks to its substantial gains in size, cost, power consumption, electromag-netic compatibility and spectrum congestion. In this paper, we propose a reinforcement learning (RL) based ISAC system with a multi-input-multi-output (MIMO) automotive radar. The target sensing and downlink communication are separately performed by dividing the transmit antennas into two non-overlapping but interweaving subarrays. We first design a RL framework to adaptively allocate the proper number of transmit antennas for the two subarrays under any unknown environment. The training is performed in the metrics of Cramer-Rao Bound (CRB) of direction of arrival (DOA) estimation for sensing and receive signal-to-noise (SNR) for communications, respectively. We proceed to propose a co-design method to jointly optimize the configurations of the two subarrays to further enhance the sensing accuracy with a constrained communication quality. The resultant problem is converted into the convex form via convex relaxation. Simulations are provided to demonstrate the adaptability and effectiveness of the proposed RL based ISAC system under the unkown environment.	Conference Paper	2023	10.1109/RadarConf2351548.2023.10149653	[]	[]	-1	-1	-1
J	Jeong, Min S.; Jang, Jin H.; Lee, Eun S.	Optimal IPT Core Design for Wireless Electric Vehicles by Reinforcement Learning	2023	IEEE TRANSACTIONS ON POWER ELECTRONICS		13262	13272	In this article, optimal inductive power transfer (IPT) core structures for wireless electric vehicle (WEV), which can be derived by optimal reinforcement learning (RL) algorithms, are newly proposed. Because the IPT cannot be theoretically analyzed to find a maximum value of mutual inductance for the optimal core structure design, intuitive and iterative process based on finite element method analysis are usually implemented. This conventional method, however, is not preferred due to numerous possible combinations and computation times. For this reason, RL algorithms are designed to optimize nonlinear system design, enabling the WEV IPT to be efficiently designed with high mutual inductance, even in the presence of severe misalignment conditions. Contrary to the conventional RL algorithm for the IPT core design, the proposed RL algorithm can follow higher mutual inductance by shorter episodes; hence, 50% of computation time reduction and 2% of maximum mutual inductance were achieved. A prototype of WEV IPT system designed by the proposed RL algorithm was fabricated, satisfying the standard J2954 of the society of automotive engineers for WPT3/Z3 case. As a result, it is found that the proposed WEV IPT can be manufactured, considering the desired number of cores for reasonable cost and weight of the vehicle assembly.	Article	NOV 2023	10.1109/TPEL.2023.3297740	[]	[]	-1	-1	-1
P	JIANG H; CHEN C; MA B; LOU M; LU J	Unmanned vehicle reinforcement learning training            environment construction method involves disconnecting            simulation environment model from reinforcement            learning module when simulation environment model is            closed						NOVELTY - The method involves using the real on-board camera of the real unmanned vehicle to collect real scene pictures as the real domain data set. A CycleGAN network is established, the real domain data set and the enhanced simulation domain data set are used as the input of the two generators in the CycleGAN network respectively. The API interface is established between the simulation environment model and the reinforcement learning module. The simulation environment model is connected to the reinforcement learning module, and the trained CycleGAN model is loaded when the simulation environment is loaded. The real simulation environment picture is collected as the input of the CycleGAN model, the simulated real scene picture is obtained, and the simulated real scene picture is used as the input of the reinforcement learning module for intensive training. When the simulation environment model is closed, the simulation environment model is disconnected from the reinforcement learning module. USE - Unmanned vehicle reinforcement learning training environment construction method. ADVANTAGE - The unmanned vehicle camera collects real scene pictures in reality. Since the simulated real scene pictures input by the reinforcement learning algorithm during training are very similar to the real scene pictures, the trained algorithm can be directly transferred or transferred to the real scene after fine-tuning. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an unmanned vehicle reinforcement learning training system based on the construction method. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an unmanned vehicle reinforcement learning training environment construction method. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	YU X; FENG P; TIAN Y; LUO J	Simulation verification system for            virtual-real-combined reinforcement learning algorithm            in robot or multi-robot system or providing from            sensing to control end to end strategy, has simulation            unmanned vehicle/machine replaced by mirror image            unmanned vehicle/machine						NOVELTY - The system has a simulation platform provided with a simulation unmanned vehicle/machine as an intelligent body and a mirror image unmanned machine. An object platform is provided with an action capturing system, an obstacle mirror image, an object unmanned vehicle/machine and a related communication device. A data range is processed to be consistent with a data format and range obtained from a real object. The simulation unmanned vehicle/machine is replaced by a mirror image unmanned vehicle/machine. A mirror image unmanned vehicle/machine operation data to obtain a virtual reality algorithm transplantation effect. USE - Simulation verification system for virtual-real-combined reinforcement learning algorithm in robot or multi-robot system or providing from sensing to control end to end strategy. ADVANTAGE - The simulation verification method of virtual-real combined reinforcement learning algorithm fully verifies the feasibility and reliability of the algorithm in the actual complex environment, and has low cost. The virtual combination of the reinforcement learning for simulating verification method, strong real-time performance, experiment fidelity and high reliability. The simulation environment for training reinforcement learning butt joint with the actual intelligent vehicle physical model, real time mapping of real vehicle in the simulation environment, the state observation in simulation environment is consistent with actual Intelligent vehicle, finally transferring simulation environment-based training-based reinforcement learning strategy to actual intelligent vehicles, finishing the complex task. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the simulation verification system for virtual-real-combined reinforcement learning algorithm.				[]	[]	-1	-1	-1
P	LIU H; LIU C; HAN L; ZHAN Z; BEI W	Reinforcement learning based unmanned vehicle            obstacle avoiding method, involves utilizing penalty            functions and evaluation functions to score multiple            groups of trajectories and evaluate obstacle avoidance            performance of trajectories						NOVELTY - The method involves training a dynamic window obstacle avoidance model using a reinforcement learning algorithm under pre-obtained environmental constraints to obtain a prediction window. The environment constraints are utilized to indicate shape and size of each obstacle in the area in front of an unmanned vehicle. Distance between each obstacle and the unmanned vehicle is determined. A kinematics model is constructed. Judgment is made to check whether the prediction window is a dynamic window after the position is adjusted according to the environmental constraints. A set of trajectories is generated for each set of sampling speeds based on the prediction window and the speed sampling constraints. Multiple sets of speeds are sampled. The speed sampling constraint is pre-built based on the driving state. USE - Reinforcement learning based unmanned vehicle obstacle avoiding method. ADVANTAGE - The method enables improving obstacle-avoiding performance of the unmanned vehicle and realizing active obstacle avoidance of the unmanned vehicle in an effective manner. DETAILED DESCRIPTION - Pre-built obstacle avoidance penalty functions and evaluation functions are utilized to score multiple groups of the trajectories and evaluate obstacle avoidance performance of each group of the trajectories. A trajectory with the highest score is selected from each group of the trajectories as the target trajectory. Sampling speed corresponding to the target trajectory is outputted to a control system of the unmanned vehicle. The unmanned vehicle corresponding to the target trajectory is driven according to the sampling speed. INDEPENDENT CLAIMS are also included for the following:(1) a reinforcement learning based unmanned vehicle obstacle avoiding device; and(2) a computer-readable storage medium comprises a set of instructions for avoiding unmanned vehicle obstacle based on reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a reinforcement learning based unmanned vehicle obstacle avoiding method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Pathak, Shashank; Bag, Suvam; Nadkarni, Vijay	A Generalised Method for Adaptive Longitudinal Control Using Reinforcement Learning	2019	INTELLIGENT AUTONOMOUS SYSTEMS 15, IAS-15	Advances in Intelligent Systems and Computing	464	479	Adaptive cruise control (ACC) seeks intelligent and adaptive methods for longitudinal control of the cars. Since more than a decade, high-end cars have been equipped with ACC typically through carefully designed model-based controllers. Unlike the traditional ACC, we propose a reinforcement learning based approach - RL-ACC. We present the RL-ACC and its experimental results from the automotive-grade car simulators. Thus, we obtain a controller which requires minimal domain knowledge, is intuitive in its design, can accommodate uncertainties, can mimic human-like behaviour and may enable human-trust in the automated system. All these aspects are crucial for a fully autonomous car and we believe reinforcement learning based ACC is a step towards that direction.	Proceedings Paper	2019	10.1007/978-3-030-01370-7_37	[]	[]	-1	-1	-1
J	Pengfei Liu; Yimin Liu; Tianyao Huang; Yuxiang Lu; Xiqin Wang	Cognitive Radar Using Reinforcement Learning in Automotive Applications [arXiv]	2019	arXiv		11 pp.	11 pp.	The concept of cognitive radar (CR) enables radar systems to achieve intelligent adaption to a changeable environment with feedback facility from receiver to transmitter. However, the implementation of CR in a fast-changing environment usually requires a well-known environmental model. In our work, we stress the learning ability of CR in an unknown environment using a combination of CR and reinforcement learning (RL), called RL-CR. Less or no model of the environment is required. We also apply the general RL-CR to a specific problem of automotive radar spectrum allocation to mitigate mutual interference. Using RL-CR, each vehicle can autonomously choose a frequency subband according to its own observation of the environment. Since radar's single observation is quite limited compared to the overall information of the environment, a long short-term memory (LSTM) network is utilized so that radar can decide the next transmitted subband by aggregating its observations over time. Compared with centralized spectrum allocation approaches, our approach has the advantage of reducing communication between vehicles and the control center. It also outperforms some other distributive frequency subband selecting policies in reducing interference under certain circumstances.	Journal Paper	24 April 2019		[]	[]	-1	-1	-1
P	CHEN J; LI Z; SUN J	Unmanned aerial vehicle detection track planning            method based on depth reinforcement learning, involves            establishing detection track optimization problem of            unmanned vehicle according to Markov Decision process            model						NOVELTY - The unmanned aerial vehicle detection track planning method involves constructing a Markov decision process model of unmanned aerial vehicle detection trajectory planning. A detection track optimization problem of an unmanned vehicle is established according to the Markov Decision process model. An unmanned vehicle detection tracking optimization problem is designed to design reinforcement learning to a solving algorithm. The unmanned vehicle observation is inputted to design the reinforcement learning. The solving algorithm is used to obtain the unmanned vehicle inspection trajectory based on the deep reinforcement learning based on a planning strategy. USE - Unmanned aerial vehicle detection track planning method based on depth reinforcement learning. ADVANTAGE - The method realizes the trajectory planning of the unmanned aerial vehicle detection signal field under the condition that the distribution of the unmanned aerial vehicle dynamics model and the signal field to be detected is completely unknown, obtains sufficient information at the shortest time and reaches the predetermined target, and has high practical value. DESCRIPTION Of DRAWING(S) - The drawing shows a graphical representation of unmanned aerial vehicle detection track planning method based on depth reinforcement learning.				[]	[]	-1	-1	-1
P	NATARAJAN V; ACHARYA G; M R; BAXI A S; K G A; VINCENT S M; NATARAJAN; BAXI A; SHAGAYA V	Apparatus for constructing complex assembly of            objects by robot using reinforcement learning action            primitives, has movement manager for commanding robot            to construct physical assembly of objects based on            sequences of action primitives						NOVELTY - The apparatus has a construction manager for determining sequences of reinforcement learning (RL) action primitives based on object location goals and associated assembly goals determined for respective ones of objects depicted in an imaged assembly of objects. A movement manager commands a robot (102) to construct a physical assembly of objects based on the sequences of RL action primitives, where the physical assembly of objects corresponds to the imaged assembly of objects. An object-to-goal mapper generates an object-to-goal map based on the object location goals, the associated assembly goals, and initial locations of physical objects to be included in the physical assembly of objects. USE - Apparatus for constructing a complex assembly of objects by a robot using RL action primitives in different applications. Uses include but are not limited to smart retail and warehouse logistics applications, smart manufacturing assembly lines, automotive component fabrication and assembly applications and consumer electronics fabrication and assembly applications. ADVANTAGE - The apparatus provides an RL-based approach that utilizes relatively simple and non-complex RL action primitives that are combinable into a myriad of configurations and/or sequences. The apparatus constructs the complex assembly of objects by the robots using the RL action primitives corresponding to relatively simple robotic actions and/or robotic movements. The apparatus reduces tedious and labor-intensive process of manually programming the robots to perform complex assembly tasks. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a non-transitory computer-readable storage medium for storing a set of instructions for constructing a complex assembly of objects by a robot using RL action primitives(2) a method for constructing a complex assembly of objects by a robot using RL action primitives. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an environment in which a robot is implemented to construct a complex assembly of objects using RL action primitives.Robot (102)Computing device (104)Robotic arm (108)Display (112)Assembly picture (126)				[]	[]	-1	-1	-1
B	Ahadi-Sarkani, A.; Elmalaki, S.	ADAS-RL: Adaptive Vector Scaling Reinforcement Learning For Human-in-the-Loop Lane Departure Warning	2021	CPHS21: Proceedings of the First International Workshop on Cyber-Physical-Human System Design and Implementation		13	18	Multiple sensory modalities are fast becoming a key instrument in the future of the automotive industry. Collision avoidance, lane departure warning, and self-parking are examples of Advanced Driver Assistance Systems (ADAS) that are becoming possible with the adoption of more sensors. Moreover, thanks to the recent advances in mobile computing and wearable devices, the driver is now equipped with advanced sensory systems. This rich sensory environment paves the way to integrate the human factor into the loop of computation of ADAS to provide a personalized experience. In this paper, we introduce ADAS-RL, a Reinforcement Learning based algorithm that integrates the behavior and reactions of the driver with the vehicle context to continuously adapt and tune the warning interventions of Lane Departure Warning System (LDW). We validated ADAS-RL against human drivers using CARLA simulator. Our evaluation shows a significant enhancement in the driver experience compared to the standard LDW systems. ADAS-RL shows the ability to track the changes in driving behavior and adapt the frequency of warnings allowing drivers to stay within a reasonable distance (around 1.75m) from lane markings with a significant decrease in the false warnings.	Conference Paper	18 May 2021	10.1145/3458648.3460008	[]	[]	-1	-1	-1
B	Chen, Y.; Zhang, Y.; Chen, J.; Zhao, J.; Li, K.; Wang, L.	Deep Reinforcement Learning Algorithm and Simulation Verification Analysis for Automatic Control of Unmanned Vehicles	2023	Man-Machine-Environment System Engineering: Proceedings of the 22nd International Conference on MMESE. Lecture Notes in Electrical Engineering (941)		279	86	This study conducted research mainly on the proven applicability of controlling the unmanned vehicle using a deep reinforcement learning algorithm and relative performance improvements. In specific, this study chose the AirSim platform developed by Microsoft as the simulation environment and conducted simulations mainly in the indoor parking lot Unreal 4 environment. In the simulations, the deep reinforcement learning method applied is Deep Q Networks for its effectiveness as well as simplicity. To improve the performance of the trained network, object detection methodology YOLO v3 is applied as the detection algorithm for the unmanned vehicle, and the network is improved using the output of object detection as its input to accelerate the training process. The implementation of the algorithms has efficiently proven the feasibility of using deep reinforcement learning agents for the unmanned vehicle in the project and the implementation of effective object detection.	Conference Paper	2023	10.1007/978-981-19-4786-5_39	[]	[]	-1	-1	-1
J	Man Xunyu; Liu Yuansheng; Qi Han; Yan Chao; Yang Rujin	Autonomous Navigation Exploration and Map Construction for Unmanned Vehicles in Unknown Environments	2023	Automobile Technology		34	40	For the problem that the autonomous navigation exploration algorithm is easy to fall into the local area,this paper proposed an exploration algorithm combining sampling and deep reinforcement learning.First,the Long-Short-Term Memory (LSTM) network was used locally to obtain the historical pose information of the unmanned vehicle to avoid repeated exploration of the explored area;secondly,the optimal action of the deep reinforcement learning strategy was used to output using deep reinforcement learning and the reward function was designed to encourage the unmanned vehicle to fully explore the unknown area;Finally,the horizontal movement factor of the unmanned vehicle was considered to generate a global exploration path conforming to its current attitude by solving the Asymmetric Travel Salesman Problem (ATSP).In the 2 000 s mine tunnel simulation environment,compared with the Technologies for Autonomous Robot Exploration (TARE) algorithm,the proposed algorithm increased the exploration area by 346.3 m~2 and reduced the total driving distance by 209.4 m;in the real scene test,the exploration algorithm completed the exploration of the underground garage with an area of 3 444.3 m~2 and returned to the starting point in 1 014 s and built the environment map.	Article	2023		[]	[]	-1	-1	-1
J	Han, Wei; Guo, Fang; Su, Xichao	A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem	2019	ALGORITHMS				The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP). To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper. HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed. On this basis, the MDP framework is established. The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL. Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution. For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory. Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch. The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.	Article	NOV 2019	10.3390/a12110222	[]	[]	-1	-1	-1
B	Wu, X.; Wedernikow, E.; Nitsche, C.; Huber, M.F.	Towards Optimal Energy Management Strategy for Hybrid Electric Vehicle with Reinforcement Learning	2023	2023 IEEE Intelligent Vehicles Symposium (IV)		1	7	In recent years, the development of Artificial Intelligence (AI) has shown tremendous potential in diverse areas. Among them, reinforcement learning (RL) has proven to be an effective solution for learning intelligent control strategies. As an inevitable trend for mitigating climate change, hybrid electric vehicles (HEVs) rely on efficient energy management strategies (EMS) to minimize energy consumption. Many researchers have employed RL to learn optimal EMS for specific vehicle models. However, most of these models tend to be complex and proprietary, making them unsuitable for broad applicability. This paper presents a novel framework, in which we implement and integrate RL-based EMS with the open-source vehicle simulation tool called FASTSim. The learned RL-based EMSs are evaluated on various vehicle models using different test drive cycles and prove to be effective in improving energy efficiency.	Conference Paper	2023	10.1109/IV55152.2023.10186787	[]	[]	-1	-1	-1
C	Moradi, Mehrdad; Oakes, Bentley James; Saraoglu, Mustafa; Morozov, Andrey; Janschek, Klaus; Denil, Joachim	Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection	2020	50TH ANNUAL IEEE/IFIP INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS WORKSHOPS (DSN-W 2020)		102	109	Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry. Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262. Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms. In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults. During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior. The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values. In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection. The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.	Proceedings Paper	2020	10.1109/DSN-W50199.2020.00028	[]	[]	-1	-1	-1
P	WANG X; ZHANG Y; ZOU L; LI B	Global path planning method for unmanned vehicles,            involves generating motion path of unmanned vehicle,            according to evaluation index of path planning result            output by deep enhanced neural network						NOVELTY - The global path planning method involves establishing (S110) describing the sequence decision-making process of unmanned vehicle path planning through a reinforcement learning method. Unmanned vehicle status, environmental status described using map pictures, and evaluation indicators of route planning results. The map picture and the state of the unmanned vehicle in the current task scene is inputted (S120) into the deep reinforcement learning neural network after training. The motion path of the unmanned vehicle is generated (S130), according to the evaluation index of the path planning result output by the deep enhanced neural network. USE - Global path planning method for unmanned vehicles. ADVANTAGE - The global path planning method identifies the environmental information in the scene through the map picture. The map features are extracted through deep neural network simplifies the modeling process of map scenes. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for unmanned vehicle global path planning device. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating global path planning method for unmanned vehicles. (Drawing includes non-English language text)Step for establishing describing sequence decision-making process of unmanned vehicle path planning through reinforcement learning method (S110)Step for inputting map picture and state of unmanned vehicle in current task scene into deep reinforcement learning neural network after training (S120)Step for generating motion path of unmanned vehicle, according to evaluation index of path planning result output by deep enhanced neural network (S130)				[]	[]	-1	-1	-1
P	LI X; WANG Z; LI G; ZHOU Y; LU P; GENG S; HE B	Method for planning motion of robot based on            digital twinning and reinforcement learning, involves            obtaining route and linear velocity and angular            velocity information for controlling unmanned vehicle            movement						NOVELTY - The method involves constructing a virtual scene corresponding to real scene by using digital twinning technology. The virtual scene is constructed for performing training process in reinforcement learning. Training in the virtual scene is performed to obtain a local path planning and obstacle avoidance algorithm. The real scene and the virtual scene are used to map for finishing motion planning task of an unmanned vehicle in the real scene and updating the virtual scene. Unmanned aerial vehicle observation information of the virtual space and the real space is combined after finishing training. Route and linear velocity and angular velocity information is obtained for controlling unmanned vehicle movement. USE - Method for planning motion of a robot based on digital twinning and reinforcement learning. Uses include but are not limited to product design, product manufacturing, medical analysis and engineering construction fields. ADVANTAGE - The method enables combining the digital twinning and depth reinforcement learning and training the unmanned vehicle to realize autonomous navigation algorithm. The information obtained by unmanned vehicle assembling sensor is used for updating the virtual scene, and the position and speed of unmanned vehicle in real scene can be monitored online in real-time through virtual scene and state information. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for planning motion of a robot based on digital twinning and reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	DUAN Z; REN Y; YANG T; ZHANG Z; LV X	Method for planning path of unmanned vehicle based            on reinforcement learning layered double-sensing            domain, involves realizing path planning of unmanned            vehicle based on reinforcement learning of target            hierarchical dual perception domain						NOVELTY - The method involves reading two-dimensional pixel map of an unmanned vehicle driving area. Coordinate conversion and global alignment process is performed to obtain a world map for path planning. An obstacle position is extracted from the world map. A current unmanned vehicle position state and a target position state are obtained. A passable path is obtained according to the global map search. A safety area corridor is obtained along the passable paths. A sub-target point sequence is obtained by using a target layering process according to a safety area corridors. The corresponding data of the unmanned vehicle to be planned is inputted, and the model output get the planned smooth path. The path planning of the unmanned vehicle based on the reinforcement learning of the target hierarchical dual perception domain is realized. USE - Method for planning path of unmanned vehicle based on reinforcement learning layered double-sensing domain. ADVANTAGE - The method is set to adapt to the map of different dimensions, and avoids the dimensional disaster in the training, the calculation efficiency is high, applicability is strong, and meets the actual requirement of real-time path planning. By using the target layering method to make the sub-target layer at the middle portion of the safety area corridor, avoiding the situation that the path is close to the barrier due to the search algorithm. The obstacle sensing domain and the target discovery domain double sensing domain reduce observation input, decoupling the obstacle sensing and target discovery separately. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating the process for planning path of unmanned vehicle based on reinforcement learning layered double-sensing domain. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
C	Fang, Xing; Zhang, Qichao; Gao, Yinfeng; Zhao, Dongbin	Offline Reinforcement Learning for Autonomous Driving with Real World Driving Data	2022	2022 IEEE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	3417	3422	Since traditional reinforcement learning (RL) approaches need active online interaction with the environment, previous works are mainly investigated in the simulation environment rather than the real world environment, especially for safety-critical applications. Offline RL has recently emerged as a promising data-driven learning paradigm to learn a policy from offline dataset directly. It seems that offline RL is well suited for autonomous driving, as it is feasible to collect offline naturalized driving dataset. However, it remains unclear how to deploy offline RL with real world driving dataset only including observation data, and whether current offline RL algorithms work well to learn a driving policy than imitation learning? In this paper, we provide an offline RL benchmark for autonomous driving including the dataset, baselines, and a data driven simulator1. First, we summarize and introduce the popular offline RL baseline methods. Then, we construct an offline RL dataset for the car following task based on the real world driving dataset INTERACTION. A data driven simulator is applied to obtain augmented data and test the driving policy. Further, we deploy four popular offline algorithms and analyze their performances under different datasets including real world driving data and augmented data. Finally, related conclusions and discussions are given to analyze the critical challenge for offline RL in autonomous driving.	Proceedings Paper	2022	10.1109/ITSC55140.2022.9922100	[]	[]	-1	-1	-1
P	GUO D; ZHOU Z; YU Y; ZHANG Z	Automatic driving decision system based on maximum            entropy layered reinforcement learning, is configured            to be trained based on the maximum entropy layered            reinforcement learning method to obtain automatic            driving strategies						NOVELTY - The automatic driving decision-making system is configured to be trained based on the maximum entropy layered reinforcement learning method to obtain automatic driving strategies adapted to different road conditions and automatic selection strategies for driving strategies. The car automatically selects a reasonable driving strategy according to the road conditions. The system constructs an unmanned vehicle simulation control environment to train the automatic driving strategy. The system includes the driving strategy value network module, which is used for evaluating the driving strategy, a driving strategy selection policy network module used for unmanned vehicles to select driving strategies according to their own state, and a driving strategy group network module for unmanned vehicles to determine driving actions according to the selected driving strategy and its own state. USE - Automatic driving decision-making system based on maximum entropy layered reinforcement learning. ADVANTAGE - The automatic driving decision system ensures that the automatic driving strategy learning cost is reduced and the learning efficiency of the strategy is improved. The learning efficiency is improved and the trial error cost is also reduced. The unmanned vehicle can effectively learn multiple driving strategies for automatic driving decision and automatically selecting the optimal driving strategy for driving according to the unmanned vehicle state, which is suitable for unmanned vehicle training in the automatic driving environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a training method of automatic driving strategy based on maximum entropy layered reinforcement learning; (2) an automatic driving decision method based on maximum entropy layered reinforcement learning; (3) a computer device; (4) a computer-readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows an unmanned vehicle automatic driving decision flow chart for the automatic driving decision system based on maximum entropy layered reinforcement learning. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Seowoo Jang; Namwoo Kang	Generative Design by Reinforcement Learning: Maximizing Diversity of Topology Optimized Designs [arXiv]	2020	arXiv		18 pp.	18 pp.	Generative design is a design exploration process in which a large number of structurally optimal designs are generated in parallel by diversifying parameters of the topology optimization while fulfilling certain constraints. Recently, data-driven generative design has gained much attention due to its integration with artificial intelligence (AI) technologies. When generating new designs through a generative approach, one of the important evaluation factors is diversity. In general, the problem definition of topology optimization is diversified by varying the force and boundary conditions, and the diversity of the generated designs is influenced by such parameter combinations. This study proposes a reinforcement learning (RL) based generative design process with reward functions maximizing the diversity of the designs. We formulate the generative design as a sequential problem of finding optimal parameter level values according to a given initial design. Proximal Policy Optimization (PPO) was applied as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. This study also proposes the use of a deep neural network to instantly generate new designs without the topology optimization process, thus reducing the large computational burdens required by reinforcement learning. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.	Journal Paper	17 Aug. 2020		[]	[]	-1	-1	-1
C	Wang, Yiquan; Wang, Jingguo; Yang, Yu; Li, Zhaodong; Zhao, Xijun	An End-to-End Deep Reinforcement Learning Model Based on Proximal Policy Optimization Algorithm for Autonomous Driving of Off-Road Vehicle	2023	PROCEEDINGS OF 2022 INTERNATIONAL CONFERENCE ON AUTONOMOUS UNMANNED SYSTEMS, ICAUS 2022	Lecture Notes in Electrical Engineering	2692	2704	Most conventional unmanned vehicle control algorithms require human adjustment of parameters and design of precise rules, thus failing to adapt quickly to multiple situations when facing complex environments in the wild. To address these problems, this paper adopts an end-to-end deep reinforcement learning model based on proximal policy optimization algorithm to control the steering, speed and braking of an unmanned vehicle, allowing it to autonomously learn motion control strategies from perceptionmap in un-known environments. A novel environment simulator which contains variable passable areas and obstacles is also proposed to support agents to achieve target reward. The proposed agent model has been proved to receive the highest reward over SAC and has the ability to overcome the complexity of the wild environment generated by the simulator.	Proceedings Paper	2023	10.1007/978-981-99-0479-2_248	[]	[]	-1	-1	-1
B	Luo, Z.; Zhou, J.; Wen, G.	Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee	2022	2022 13th Asian Control Conference (ASCC)		1893	8	It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique. Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee. By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained. Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast. Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.	Conference Paper	2022	10.23919/ASCC56756.2022.9828057	[]	[]	-1	-1	-1
P	WANG M; CHEN S; SONG W; WANG K	Method for realizing unmanned vehicle formation on            motorways based on multi-agent reinforcement learning,            involves carrying out trajectory planning according to            each vehicle so that each vehicle performs specified            action in action decision						NOVELTY - The method involves obtaining environmental information as observation input in a trained Q-MIX network to obtain action decisions of each unmanned vehicle to realize the formation. A training environment is initialized. Environmental information of the training environment is inputted as observations into the Q-MIX network to obtain an action decision of each unmanned vehicle. Trajectory planning according to each unmanned vehicle is carried out based on the action decision so that each unmanned vehicle performs specified action in the action decision. Corresponding reward value is obtained when each unmanned vehicle performs the specified action. Judgment is made to check whether distance between any two unmanned vehicles is greater than set threshold or collision occurs. Average speed of all unmanned vehicles is determined. Lateral displacement and longitudinal displacement of the unmanned vehicle are determined. USE - Method for realizing unmanned vehicle formation on motorways based on multi-agent reinforcement learning. ADVANTAGE - The method enables utilizing multi-intelligent body reinforcement learning process for changing strategy, combining S-T diagram track optimization process, calculating accurate control quantity and increasing control constraint and improving safety of the unmanned vehicle. DETAILED DESCRIPTION - A mean square error loss function corresponding to each unmanned vehicle is constructed according to sum of reward value. The Q-MIX network is updated according to the mean square error loss function. The Q-MIX network is resumed with the updated Q-MIX network until the set number of repetitions is reached. A final Q-MIX network is obtained. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for realizing unmanned vehicle formation on motorways based on multi-agent reinforcement learning. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
B	Shouqing Lu	Automatic Tracking Method of Unmanned Vehicle Trajectory Based on Reinforcement Learning	2021	AIAM2021: 2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture		1596	9	Up to now, unmanned driving is still a challenging research field in academia at home and abroad. The vehicle trajectory tracking technology is a very critical and urgent link, because it provides important information for intelligent traffic monitoring. The reinforcement learning method is an important method for learning in an unknown environment. In the field of artificial intelligence machine learning, reinforcement learning research has made great progress in theory, algorithm and application, and has become a current hotspot in research. Unmanned vehicle trajectory tracking is one of the key technologies in the field of unmanned driving research. It uses built-in sensors to perceive the environment, uses trajectory planning algorithms to generate the required path in real time, and the decision system selects the best path. Finally, the built-in path tracking controller implement it. This article mainly adopts the experimental analysis method to discuss how to break through the problem of automatic trajectory tracking technology in the support of enhanced learning by unmanned vehicles, and compare and analyze the expected yaw rate and actual yaw rate and frequency of the target vehicle. According to the experimental research results, the expected yaw rate and actual yaw rate of the unmanned vehicle trajectory automatic tracking test are relatively close, and the test system in this test has a certain tracking effect.	Conference Paper	2021	10.1145/3495018.3495448	[]	[]	-1	-1	-1
P	WANG Y; HUANG G; LI Z; HOU M; YUAN X	Method for performing depth reinforcement learning            for unmanned vehicle path planning in three-dimensional            large size terrain environment, involves using current            planning point as center to expand outwards for ten            unit step lengths						NOVELTY - The method involves inputting global map observation information to a dynamic global channel. Local map observing information is inputted to the dynamic local channel. Energy consumption and driving time of an unmanned vehicle are evaluated by a multi-target reward function. Historical empirical data reaching a target position is extracted from an experience buffer pool as a training set. Three-dimensional map data is stored in a three-dimensional large-scale terrain environment. A depth reinforcement learning search strategy is adopted based on prior experience rebroadcast. A current planning point is used as a center to expand outwards for ten unit step lengths. USE - Method for performing high-efficient depth reinforcement learning for unmanned vehicle path planning in a three-dimensional large size terrain environment. Uses include but are not limited to video game, robot, intelligent driving and recommendation system. ADVANTAGE - The method enables inputting the global map observation information to the dynamic global channel, and thus ensuring simple and efficient learning process of depth reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for performing high-efficient depth reinforcement learning for unmanned vehicle path planning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	WANG X; SHEN S; WANG L; HUANG Y; QIN X	Method for controlling formation of multi-unmanned            vehicle based on depth reinforcement learning            technology, involves providing formation generating            strategy and formation holding strategy for            multi-unmanned vehicle long distance formation            according to multiunmanned vehicle global            planning						NOVELTY - The method involves obtaining state information of each unmanned vehicle, where the state information comprises coordinate, yaw angle, linear speed, angular speed and front 180 degrees in an obstacle and a distance of the unmanned vehicle. An action of each unmanned vehicle currently needed to execute is calculated according to a policy network. A reward value and the state information of the next time of each unmanned vehicle currently needed to be executed are obtained, where all actions of the unmanned vehicle need to be executed form a combined action. A sample from an experience buffer pool is read according to an evaluation network. A minimum loss function is calculated. USE - Method for controlling a formation of a multi-unmanned vehicle based on a depth reinforcement learning technology. ADVANTAGE - The method enables improving a success rate of the multiunmanned vehicle in a dynamic obstacle environment by training a neural network, so that the multi-unmanned vehicle can reach an expected formation of a global stability. DETAILED DESCRIPTION - A parameter of the target strategy network and the target evaluation network is updated to optimize a multi-unmanned vehicle formation control strategy, where the multi-unmanned vehicle formation control strategy comprises a formation generating strategy and a formation holding strategy, the formation generating strategy makes all unmanned vehicle tracking to a target point, and the formation holding strategy corrects the unmanned vehicle track, which deviates from the unmanned vehicle. A multi-unmanned vehicle overall planning is performed according to a soft actor critic (SAC) algorithm. The formation generating strategy and the formation holding strategy are provided for multi-unmanned vehicle long distance formation according to a multiunmanned vehicle global planning.An INDEPENDENT CLAIM is included for a system for controlling a formation of a multi-unmanned vehicle based on depth reinforcement learning technology. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for controlling a formation of a multi-unmanned vehicle based on a depth reinforcement learning technology. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	ZHOU P; WANG J; ZHANG S; LI Z; LIANG H	Multi-element characteristic fusion automatic            driving course reinforcement learning training method            for use in automatic driving of vehicle i.e. car,            involves generating current optimal trajectory of            unmanned vehicle in real time according to obtained            optimal driving behavior						NOVELTY - The method involves collecting (S1) vehicle surrounding environmental information by using a vehicle-mounted sensor according to a training task for global path planning. A traffic state information vector is obtained. A local occupied grid image of a vehicle current position is obtained based on a Fleiner coordinate system. An action space and reward function of an action decision reinforcement learning building depth reinforcement learning backbone network model are designed (S2). A current optimal trajectory of an unmanned vehicle is generated (S4) in real time according to the obtained optimal driving behavior. USE - Multi-element characteristic fusion automatic driving course reinforcement learning training method for use in automatic driving of a vehicle i.e. car. ADVANTAGE - The method makes the automatic driving vehicle automatically learn the optimal driving strategy by data driving so as to finish the navigation task in the urban structured road environment. The multi-resolution trajectory planning algorithm based on sampling, generates the current optimal trajectory of the unmanned vehicle in real time according to the obtained optimal driving behavior. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the multi-element characteristic fusion automatic driving course reinforcement learning training method for use in automatic driving of a vehicle. (Drawing includes non-English language text).S1Step for collecting vehicle surrounding environmental information by using vehicle-mounted sensor according to training task for global path planningS2Step for designing action space and reward function of action decision reinforcement learning building depth reinforcement learning backbone network modelS3Step for inputting inputting extracted multivariate characteristic into constructed depth reinforcement learning backbone network modelS4Step for generating current optimal trajectory of unmanned vehicle in real time according to obtained optimal driving behavior				[]	[]	-1	-1	-1
J	Yun, Lingxiang; Wang, Di; Li, Lin	Explainable multi-agent deep reinforcement learning for real-time demand response towards sustainable manufacturing	2023	APPLIED ENERGY				The demand response (DR) plays a significant role in manufacturing system energy management and sustainable industrial development. Current literature on DR management for manufacturing systems has mostly focused on day-ahead production scheduling, whose effectiveness is limited due to the lack of flexibility to control the production line in real time. The development of reinforcement learning (RL) possesses huge potential for realtime production control to address the flexibility issue. However, since production is the top priority for any manufacturing system, a trustful and explainable RL for manufacturing system energy management that can ensure the production requirements is necessary for this application. This study proposes an explainable multiagent deep RL method, where the analytical manufacturing system model is applied to decompose the systemlevel energy management objective and production requirement to the agent level. Based on the decomposed task, the agent can then form a safe action subset that is interpretable to achieve the original system-level production requirement while learning to reduce energy costs under DR. The proposed RL method, which is referred to as decomposed multi-agent deep Q-network (DMADQN), is applied to control a section of an automotive assembly line using one year of DR electricity price data to validate its performance. Results show that the proposed method ensures the achievement of the production requirement while providing better DR energy management performance in both RL training and testing phases. In addition, the proposed approach can outperform the day-ahead scheduling approach and save up to an additional 30.7% of energy costs under dynamic DR.	Article; Early Access		10.1016/j.apenergy.2023.121324	[]	[]	-1	-1	-1
P	CHEN J; WANG G; LI B; SUN J	Social navigation method for mobile robots based            on reinforcement learning, involves finishing social            navigation of mobile robot after training depth            reinforcement learning network frame as actual            navigation of optimal strategy						NOVELTY - The method involves constructing a model of the social navigation problem. A deep reinforcement learning network framework is constructed according to the model of the social navigation problem. The deep reinforcement learning network framework constructed is trained. The deep reinforcement learning network framework trained is used as the optimal strategy for actual navigation to complete the social navigation of mobile robots. The deep reinforcement learning network framework includes an interaction module, historical feature extraction module, pooling module and planning module. USE - Social navigation method for mobile robots used in unmanned distribution and other fields, based on reinforcement learning in electronic game. ADVANTAGE - The method enables providing a more accurate model for an unmanned vehicle sensing an actual crowd environment under same experimental condition, so that the algorithm can make the unmanned vehicle to effectively avoid the crowd and faster reach a target position. The method allows a mobile robot social navigation method based on reinforcement learning to comprehensively consider crowd time and space feature, thus improving efficiency of the social navigation algorithm. The planning module plans a feasible path according to unmanned vehicle state and the extracted crowd characteristic, thus pre-training the model by simulating learning of expert experience, and hence accelerating model convergence speed. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the construction method of mobile robot and crowd spatio-temporal map.				[]	[]	-1	-1	-1
P	GUO Q; MA W; LAN W; XU R; FENG X	Method for diagnosing fault of unmanned vehicle            based on depth reinforcement learning and expert            knowledge base, involves circulating step except            initial parameter configuration until convergence or            reaches maximum training wheel number						NOVELTY - The method involves randomly dividing a training sample into a training set and a test set. The training set fault data is inputted as a state space. A fault type of the training set is input as an action space, discount rate, learning rate and corrupt heart rate. A Q network parameter is initialized. A parameter of a target Q network is initialized until a maximum training wheel number is reached. A gradient descent algorithm is performed on a loss function. A neural network parameter and a target network parameter are updated. A test set is utilized to model test to obtain a deep reinforcement learning knowledge system. USE - Method for performing fault diagnosis of an unmanned vehicle based on depth reinforcement learning and expert knowledge base. Uses include but are not limited to an automatic driving unmanned vehicle, a computer-driving unmanned vehicle and a vehicle capable of detecting and identifying a road and an obstacle. ADVANTAGE - The method enables updating the knowledge system and explanation library, so as to effectively improve the detection range of the fault, thus reducing the occurrence of safety accident caused by the fault of the unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the unmanned vehicle fault diagnosis method based on depth reinforcement learning expert knowledge base. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Marvi, Zahra; Kiumarsi, Bahare	Safe reinforcement learning: A control barrier function optimization approach	2021	INTERNATIONAL JOURNAL OF ROBUST AND NONLINEAR CONTROL		1923	1940	This article presents a learning-based barrier certified method to learn safe optimal controllers that guarantee operation of safety-critical systems within their safe regions while providing an optimal performance. The cost function that encodes the designer's objectives is augmented with a control barrier function (CBF) to ensure safety and optimality. A damping coefficient is incorporated into the CBF which specifies the trade-off between safety and optimality. The proposed formulation provides a look-ahead and proactive safety planning and results in a smooth transition of states within the feasible set. That is, instead of applying an optimal controller and intervening with it only if the safety constraints are violated, the safety is planned and optimized along with the performance to minimize the intervention with the optimal controller. It is shown that addition of the CBF into the cost function does not affect the stability and optimality of the designed controller within the safe region. This formulation enables us to find the optimal safe solution iteratively. An off-policy reinforcement learning (RL) algorithm is then employed to find a safe optimal policy without requiring the complete knowledge about the system dynamics, while satisfies the safety constraints. The efficacy of the proposed safe RL control design approach is demonstrated on the lane keeping as an automotive control problem.	Article; Early Access		10.1002/rnc.5132	[]	[]	-1	-1	-1
P	IN C T; WON J	Apparatus for determining path of e.g. UAV, in            airspace by using reinforcement learning, has airspace            path inference unit predicting flight path of vehicle            through reinforcement learning of inference of agent            node at vector point nodes						NOVELTY - The apparatus (1) has space vector point nodes for representing environmental elements of an airspace of an unmanned vehicle as an environment for reinforcement learning. A starting node corresponding to a starting point of the unmanned vehicle and an arrival point are set in the space vectors point nodes. An airspace environment recognition unit (10) sets an arrival node. An agent node setting unit (21) is configured to set agent nodes that perform reinforcement learning at the space vector points. A flight path of the vehicle is predicted by an airspace path inference unit through reinforcement learning of a flight path inference of the agent node. USE - Apparatus for determining a path of an unmanned mobile vehicle i.e. UAV, in the airspace by using reinforcement learning. ADVANTAGE - The apparatus determines and expresses optimal flight path avoiding obstacles according to 3D buildings and terrain in space through an automatic path determination process using a reinforcement learning technique in a space vector point node environment in an airspace expressed in 4 dimensions. The apparatus is convenient to select and determine routes individually on a 2D or 3D map to determine the flight route of the unmanned vehicle, so that difficulties in performing tasks that require direct visual observation and judgment, and consequently unpredictable accidents are reduced. The apparatus allows a user to easily obtain a safe route for the unmanned vehicle by setting starting and ending points while solving problems such as yagi and lack of automation. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the apparatus for determining the path of the unmanned mobile vehicle (Drawing includes non-English language text).1Apparatus for determining path of unmanned mobile vehicle10Airspace environment recognition unit13Node unit14Start node setting unit21Agent node setting unit				[]	[]	-1	-1	-1
J	Raio, Stephen; Corder, Kevin; Parker, Travis W.; Shearer, Gregory G.; Edwards, Joshua S.; Thogaripally, Manik R.; Park, Song J.; Nelson, Frederica F.	Reinforcement Learning as a Path to Autonomous Intelligent Cyber-Defense Agents in Vehicle Platforms	2023	APPLIED SCIENCES-BASEL				Technological advancement of vehicle platforms exposes opportunities for new attack paths and vulnerabilities. Static cyber defenses can help mitigate certain attacks, but those attacks must generally be known ahead of time, and the cyber defenses must be hand-crafted by experts. This research explores reinforcement learning (RL) as a path to achieve autonomous, intelligent cyber defense of vehicle control networks-namely, the controller area network (CAN) bus. We train an RL agent for the CAN bus using Toyota's Portable Automotive Security Testbed with Adaptability (PASTA). We then apply the U.S. Army Combat Capabilities Development Command (DEVCOM) Army Research Laboratory's methodology for quantitative measurement of cyber resilience to assess the agent's effect on the vehicle testbed in a contested cyberspace environment. Despite all defenses having similar traditional performance measures, our RL agent averaged a 90% cyber resilience measurement during drive cycles executed on hardware versus 41% for a naive static timing defense and 98% for the bespoke timing-based defense. Our results also show that an RL-based agent can detect and block injection attacks on a vehicle CAN bus in a laboratory environment with greater cyber resilience than prior learning approaches (1% for convolutional networks and 0% for recurrent networks). With further research, we believe there is potential for using RL in the autonomous intelligent cyber defense agent concept.	Article	NOV 2023	10.3390/app132111621	[]	[]	-1	-1	-1
J	Zhang, Li; Li, Zhao; Ren, Huali; Yu, Xiao; Ma, Yuxi; Zhang, Quanxin	Knowledge graph and behavior portrait of intelligent attack against path planning	2022	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS		7110	7123	The broad application of artificial intelligence (AI) shows more and more vulnerabilities. Adversaries have more opportunities to attack AI systems. For example, unmanned vehicles may be interfered with by adversaries in path planning, resulting in unmanned vehicles being unable to move according to the planned route, and even serious safety problems. On the other side, the portrait technology can extract highly refined characteristics of different attack strategies, so that unmanned vehicles can defend themselves based on the characteristics of each attack. Existing research lacks intelligent attack research on path planning in the field of unmanned vehicles, and lacks portraits of attack behaviors in this scenario. This paper combines multiagent reinforcement learning technology, time-series segmentation clustering technology, and knowledge graph technology to study the portrait technology of adversary intelligent attack behavior in the field of unmanned vehicle path planning. First, the simulation results of unmanned vehicle path planning are obtained, and the steps of adversary attack behavior are extracted by using Toeplitz inverse covariance-based clustering time-series segmentation cluster technology. Second, the knowledge graph is used to save the attack strategy, so as to form the attack behavior portrait of unmanned vehicle path planning. The test on the Neo4j platform shows that our method is universal, can effectively describe the attack steps for unmanned vehicle path planning, and provides the basis for attack detection to establish the defense system of unmanned vehicles.	Article; Early Access		10.1002/int.22874	[]	[]	-1	-1	-1
J	Voogd, K.; Allamaa, J.P.; Alonso-mora, J.; Son, T.D.	Reinforcement Learning from Simulation to Real World Autonomous Driving using Digital Twin [arXiv]	2022	arXiv				Reinforcement learning (RL) is a promising solution for autonomous vehicles to deal with complex and uncertain traffic environments. The RL training process is however expensive, unsafe, and time consuming. Algorithms are often developed first in simulation and then transferred to the real world, leading to a common sim2real challenge that performance decreases when the domain changes. In this paper, we propose a transfer learning process to minimize the gap by exploiting digital twin technology, relying on a systematic and simultaneous combination of virtual and real world data coming from vehicle dynamics and traffic scenarios. The model and testing environment are evolved from model, hardware to vehicle in the loop and proving ground testing stages, similar to standard development cycle in automotive industry. In particular, we also integrate other transfer learning techniques such as domain randomization and adaptation in each stage. The simulation and real data are gradually incorporated to accelerate and make the transfer learning process more robust. The proposed RL methodology is applied to develop a path following steering controller for an autonomous electric vehicle. After learning and deploying the real-time RL control policy on the vehicle, we obtained satisfactory and safe control performance already from the first deployment, demonstrating the advantages of the proposed digital twin based learning process.	Journal Paper	27 Nov. 2022		[]	[]	-1	-1	-1
J	Nantogma, Sulemana; Zhang, Shangyan; Yu, Xuewei; An, Xuyang; Xu, Yang	Multi-USV Dynamic Navigation and Target Capture: A Guided Multi-Agent Reinforcement Learning Approach	2023	ELECTRONICS				Autonomous unmanned systems have become an attractive vehicle for a myriad of military and civilian applications. This can be partly attributed to their ability to bring payloads for utility, sensing, and other uses for various applications autonomously. However, a key challenge in realizing autonomous unmanned systems is the ability to perform complex group missions, which require coordination and collaboration among multiple platforms. This paper presents a cooperative navigating task approach that enables multiple unmanned surface vehicles (multi-USV) to autonomously capture a maneuvering target while avoiding both static and dynamic obstacles. The approach adopts a hybrid multi-agent deep reinforcement learning framework that leverages heuristic mechanisms to guide the group mission learning of the vehicles. Specifically, the proposed framework consists of two stages. In the first stage, navigation subgoal sets are generated based on expert knowledge, and a goal selection heuristic model based on the immune network model is used to select navigation targets during training. Next, the selected goals' executions are learned using actor-critic proximal policy optimization. The simulation results with multi-USV target capture show that the proposed approach is capable of abstracting and guiding the unmanned vehicle group coordination learning and achieving a generally optimized mission execution.	Article	APR 2023	10.3390/electronics12071523	[]	[]	-1	-1	-1
J	Luo, Jie; Wang, Zhong-Xun; Pan, Kang-Lu	Reliable Path Planning Algorithm Based on Improved Artificial Potential Field Method	2022	IEEE ACCESS		108276	108284	"In order to solve the ""minimum trap"" of Artificial Potential Field and the limitation of traditional path planning algorithm in dynamic obstacle environment, a path planning algorithm based on improved artificial potential field is proposed. Firstly, a virtual potential field detection circle model (VPFDCM) with adjustable radius is proposed to detect the ""minimum trap"" formed by the repulsion field of obstacles in advance. And the motion model of unmanned vehicle is established. Combined with the improved reinforcement learning algorithm based on Long Short-Term Memory(LSTM), the radius of virtual potential field detection circle is adjusted to achieve effective avoidance of dynamic obstacles. The reliable online collision free path planning of unmanned vehicle in semi closed dynamic obstacle environment is realized. Finally, the reliability and robustness of the algorithm are verified by MATLAB simulation. The simulation results show that the improved artificial potential field can effectively solve the problem of unmanned vehicle falling into the ""minimum trap"" and improve the reliability of unmanned vehicle movement. Compared with the traditional artificial potential field method, the improved artificial potential field method can achieve more than 90% success rate in obstacle avoidance."	Article	2022	10.1109/ACCESS.2022.3212741	[]	[]	-1	-1	-1
J	Yoon, Yeomyung; Lee, Hojeong; Kim, Hyogon	Deep reinforcement learning-based dual-mode congestion control for cellular V2X environments	2023	ELECTRONICS LETTERS				The Society of Automotive Engineers (SAE) J2945/1 standard for Dedicated Short Range Communication (DSRC) environmentutilizes transmit (Tx) power control and rate control elements for the periodic BSM transmissions, which are intended to work in a complementary manner. An equivalent standard for the cellular vehicle-to-everything (C-V2X) communication environment is J3161/1, but it eliminates Tx power control and uses only rate control. However, the consequence is the degraded update delay of neighbouring vehicles' kinematics, potentially undermineing driving safety. In this Letter, the authors propose to retain the dual-mode control in the C-V2X environment and find a policy through reinforcement learning (RL) to adjust the rate control function to maintain synergy. Moreover, the authors can extract the RL-created policy from the neural network so that it can be explicitly specified in the standard, and downloaded and used more conveniently by vehicles. Finally, the RL-generated policy achieves a better packet delivery frequency than J2945/1 or J3161/1.Current V2X standards typically specify human-designed heuristic control algorithms. Their drawbacks are frequently discovered later by more thorough simulation experiments that were not done during standardization. This Letter shows that AI-generated control can be automatically obtained while running simulation, and better performing than heuristic algorithms, with the case of congestion control. Moreover, the policy can be extracted from the neural network and explicitly specified in the standard and used by vehicles instead of the neural network.image	Article	OCT 2023	10.1049/ell2.12984	[]	[]	-1	-1	-1
J	Zielinski, Kallil M. C.; Hendges, Lucas, V; Florindo, Joao B.; Lopes, Yuri K.; Ribeiro, Richardson; Teixeira, Marcelo; Casanova, Dalcimar	Flexible control of Discrete Event Systems using environment simulation and Reinforcement Learning	2021	APPLIED SOFT COMPUTING				Discrete Event Systems (DESs) are classically modeled as Finite State Machines (FSMs), and controlled in a maximally permissive, controllable, and nonblocking way using Supervisory Control Theory (SCT). While SCT is powerful to orchestrate events of DESs, it fail to process events whose control is based on probabilistic assumptions. In this research, we show that some events can be approached as usual in SCT, while others can be processed using Artificial Intelligence. We present a tool to convert SCT controllers into Reinforcement Learning (RL) simulation environments, from where they become suitable for intelligent processing. Then, we propose a RL-based approach that recognizes the context under which the selected set of stochastic events occur, and treats them accordingly, aiming to find suitable decision making as complement to deterministic outcomes of the SCT. The result is an efficient combination of safe and flexible control, which tends to maximize performance for a class of DES that evolves probabilistically. Two RL algorithms are tested, State-Action-Reward-State-Action (SARSA) and N-step SARSA, over a flexible automotive plant control. Results suggest a performance improvement 9 times higher when using the proposed combination in comparison with non-intelligent decisions. (C) 2021 Elsevier B.V. All rights reserved.	Article; Early Access		10.1016/j.asoc.2021.107714	[]	[]	-1	-1	-1
J	Xue, Lei; Ma, Bei; Liu, Jian; Mu, Chaoxu; Wunsch, Donald C.	Extended Kalman Filter Based Resilient Formation Tracking Control of Multiple Unmanned Vehicles via Game-Theoretical Reinforcement Learning	2023	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		2307	2318	In this paper, we discuss the resilient formation tracking control problem of multiple unmanned vehicles (MUV). A dynamic leader-follower distributed control structure is utilized to optimize the performance of the formation tracking. For the follower of the MUV, the leader is a cooperative unmanned vehicle, and the target of formation tracking is a non-cooperative unmanned vehicle with a nonlinear trajectory. Therefore, an extended Kalman filter (EKF) observer is designed to estimate the state of the target. Then the leader of the MUV is adjusted dynamically according to the state of the target. In order to describe the interactions between the follower and dynamic leader, a Stackelberg game model is constructed to handle the hierarchical decision problems. At the lower layer, each follower responds by observing the leader's strategy, and the potential game is used to prove a Nash equilibrium among all followers. At the upper layer, the dynamic leader makes decisions depending on the response of all followers to reaching the Stackelberg equilibrium. Moreover, the Stackelberg-Nash equilibrium of the designed game theoretical model is proven. A novel reinforcement learning-based algorithm is designed to achieve the Stackelberg-Nash equilibrium of the game. Finally, the effectiveness of the method is verified by a variety of formation tracking simulation experiments.	Article	MAR 2023	10.1109/TIV.2023.3237790	[]	[]	-1	-1	-1
P	CHEN L; GAO S; CUI L; WANG T	Underwater unmanned vehicle safety opportunity            routing method on reinforcement learning by using            computer device, involves using underwater unmanned            aircraft to primary filter node, and setting            state-action value updating function						NOVELTY - The method involves using an underwater unmanned aircraft to primary filter node in a communication range. A trust evaluation model is established according to the node of primary filter. The node of the preliminary filter is evaluated by using the trust evaluation model. An evaluation element of evaluation model is composed of two parts of direct trust value DTValue and indirect trust value ITValue. An element input fuzzy logic system is evaluated to obtain evaluation node comprehensive trust value. The node comprehensive trust value updated to a meeting node trust value dynamic table is evaluated. A strengthening learning is used to perform routing selection. A state-action value updating function and a reward function are set. USE - The method is useful for underwater unmanned vehicle safety opportunity routing on reinforcement learning by using a computer device (claimed). ADVANTAGE - The method: improves the network performance of underwater unmanned vehicle networking; reduces the delay of underwater information transmission; increases the delivery rate of messages; and realizes safety and high efficiency of information transmission of unmanned aerial vehicle. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an underwater unmanned vehicle safety opportunity routing device on reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows an implementation process of underwater unmanned vehicle safety opportunity routing method on reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	HAO M; WU H; WANG Y; WANG J; JIAO R	Method for controlling intelligent unmanned aerial            vehicle formation maintaining based on deep            reinforcement learning, involves using            proportional-integral-derivative cascade controller to            receive control instruction to operate unmanned vehicle            for completing formation and maintenance						NOVELTY - The method involves establishing an unmanned aerial vehicle flying dynamics model and a kinematics model according to flying mechanics principle. An unmanned vehicle relative motion model is designed according to a virtual long machine topology structure. A proportional-integral-derivative (PID) cascade controller of a stable-attitude-track of an unmanned vehicle is designed. Markov decision process of a MAPPO intelligent body of the unmanned vehicle is designed. Input of an intelligent agent is provided with a state space. Output of the intelligent agent is contained with an unmanned vehicle control instruction. The PID cascade controller is used to receive the control instruction to operate the unmanned vehicle for completing formation and maintenance. USE - Method for controlling an intelligent unmanned aerial vehicle formation maintaining based on deep reinforcement learning in different fields e.g. robot, game, finance and traffic. ADVANTAGE - The method enables improving the intelligence, robustness and accuracy of the unmanned aerial vehicle formation. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram illustrating a method for controlling an intelligent unmanned aerial vehicle formation maintaining based on deep reinforcement learning. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	DAI B; ZHAO D; XIAO L; NIE Y; SHANG E; WANG X	Depth reinforcement learning and A-star search            algorithm based method for planning path of unmanned            vehicles, involves driving unmanned vehicle in off-road            environment and collecting three-dimensional point            cloud data, and utilizing path traveled by unmanned            vehicle as path planning result						NOVELTY - The method involves driving an unmanned vehicle in an off-road environment and collecting three-dimensional (3D) point cloud data. A path traveled by the unmanned vehicle is recorded. Multiple grid obstacle maps are generated from the point cloud data. Target points are generated in the current grid map from the path traveled by the unmanned vehicle. The grid obstacle maps are utilized to represent a simulation environment constructed by the rectangle of the unmanned vehicle. An A-star algorithm is utilized to generate a guiding path from each obstacle map. The guidance path is utilized to design a reward function. Data augmentation and curriculum learning are realized to train a unmanned vehicle agent in the simulation environment using a deep neural network. The path traveled by the unmanned vehicle is utilized as a path planning result of the corresponding obstacle map. USE - Depth reinforcement learning and A-star search algorithm based method for planning path of unmanned vehicles. ADVANTAGE - The method enables realizing path planning of unmanned vehicles of different off-road scenes and different sizes. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a depth reinforcement learning and A-star search algorithm based method for planning path of unmanned vehicles. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Yao, Jialing; Ge, Zhen	Path-Tracking Control Strategy of Unmanned Vehicle Based on DDPG Algorithm	2022	SENSORS				This paper proposes a deep reinforcement learning (DRL)-based algorithm in the path-tracking controller of an unmanned vehicle to autonomously learn the path-tracking capability of the vehicle by interacting with the CARLA environment. To solve the problem of the high estimation of the Q-value of the DDPG algorithm and slow training speed, the controller adopts the deep deterministic policy gradient algorithm of the double critic network (DCN-DDPG), obtains the trained model through offline learning, and sends control commands to the unmanned vehicle to make the vehicle drive according to the determined route. This method aimed to address the problem of unmanned-vehicle path tracking. This paper proposes a Markov decision process model, including the design of state, action-and-reward value functions, and trained the control strategy in the CARLA simulator Town04 urban scene. The tracking task was completed under various working conditions, and its tracking effect was compared with the original DDPG algorithm, model predictive control (MPC), and pure pursuit. It was verified that the designed control strategy has good environmental adaptability, speed adaptability, and tracking performance.	Article	OCT 2022	10.3390/s22207881	[]	[]	-1	-1	-1
J	Jang, Seowoo; Yoo, Soyoung; Kang, Namwoo	Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs	2022	COMPUTER-AIDED DESIGN				Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention. (c) 2022 Elsevier Ltd. All rights reserved.	Article; Early Access		10.1016/j.cad.2022.103225	[]	[]	-1	-1	-1
J	Levin, J.; Correll, R.; Ide, T.; Suzuki, T.; Saito, T.; Arai, A.	Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes [arXiv]	2024	Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes [arXiv]				Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number of trucks and nodes, and then embedded into a large supply chain to yield solutions for larger numbers of trucks and nodes. We test our approach on a real supply chain environment arising in the operations of Japanese automotive parts manufacturer Aisin Corporation, and find that our algorithm outperforms Aisin's previous best solution.	Preprint	2024		[]	[]	-1	-1	-1
P	SUN L; QIN W; ZHAI J; DI J	deep learning based reinforcement vehicle speed            decision method, involves constructing deep            reinforcement learning structure of Actor-Critic frame,            and performing tracking of vehicle at low speed running            under city congestion condition						NOVELTY - The method involves receiving position, speed and acceleration information of front and rear unmanned vehicle as an environment state by using a network in real-time. deep reinforcement learning structure of an Actor-Critic frame is constructed by using environment state and current state of the unmanned vehicle. Training process is performed on the deep reinforcement learning structure to update a Critic network parameter and an Actor network parameter such that certain safe distance is established between the unmanned vehicle and front and rear vehicles. Automatic tracking of the unmanned vehicle is performed at low speed running under city congestion condition. USE - deep learning based reinforcement vehicle speed decision method. ADVANTAGE - The method enables increasing comfort degree of driving, improving traffic safety and smoothness of traffic lane and decision process. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based reinforcement vehicle speed decision method. '(Drawing includes non-English language text)'				[]	[]	-1	-1	-1
P	YANG L; WANG Y; REN F; LIU J; WANG L	Deep reinforcement learning based intelligent            unmanned vehicle driving end-to-end decision-making            method, involves constructing deep reinforcement            network, and outputting value of action network when            target network operation is completed						NOVELTY - The method involves constructing and training a deep reinforcement learning network, where the deep reinforcement learning network comprises an activity network, an Eval action network and a Target Actor network. Training or Eval action network action values are output when target action network operation is completed. Current time environment state is received by the Eval action network. Discrete space action is output. Multiple groups of samples are adjusted from a playback experience pool, where the current time environment state comprises front road environment state and vehicle self state. The front road environment state is a front road characteristic code, where the vehicle state comprises a vehicle driving speed, steering wheel angle, accelerator pedal opening degree and brake pedal opening degree. USE - Deep reinforcement learning based intelligent unmanned vehicle driving end-to-end decision-making method. ADVANTAGE - The method enables requiring less environment data so as to effectively reduce cost, constructing a deep strengthening learning network with high learning efficiency, and increasing training speed and improving exploration efficiency of the intelligent body. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep reinforcement learning based intelligent unmanned vehicle driving end-to-end decision-making method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Huang Yuzhou; Wang Lisong; Qin Xiaolin	Bi-level Path Planning Method for Unmanned Vehicle Based on Deep Reinforcement Learning	2023	Computer Science		194	204	With the wide application of intelligent unmanned vehicles,intelligent navigation,path planning and obstacle avoidance technology have become important research contents.This paper proposes model-free deep reinforcement learning algorithms DDPG and SAC,which use environmental information to track to the target point,avoid static and dynamic obstacles,and can be generally suitable for different environments.Through the combination of global planning and local obstacle avoidance,it solves the path planning problem with better globality and robustness,solves the obstacle avoidance problem with better dynamicity and generalization,and shortens the iteration time.In the network training stage,PID,A* and other traditional algorithms are combined to improve the convergence speed and stability of the method.Finally,a variety of experimental scenarios such as navigation and obstacle avoidance are designed in the robot operating system ROS and the simulation program gazebo.Simulation results verify the reliability of the proposed approach,which takes the global and dynamic nature of the problem into account and optimizes the generated paths and time efficiency.	Article	2023		[]	[]	-1	-1	-1
J	Yudin, D. A.; Skrynnik, A.; Krishtopik, A.; Belkin, I; Panov, A., I	Object Detection with Deep Neural Networks for Reinforcement Learning in the Task of Autonomous Vehicles Path Planning at the Intersection	2019	OPTICAL MEMORY AND NEURAL NETWORKS		283	295	Among a number of problems in the behavior planning of an unmanned vehicle the central one is movement in difficult areas. In particular, such areas are intersections at which direct interaction with other road agents takes place. In our work, we offer a new approach to train of the intelligent agent that simulates the behavior of an unmanned vehicle, based on the integration of reinforcement learning and computer vision. Using full visual information about the road intersection obtained from aerial photographs, it is studied automatic detection the relative positions of all road agents with various architectures of deep neural networks (YOLOv3, Faster R-CNN, RetinaNet, Cascade R-CNN, Mask R-CNN, Cascade Mask R-CNN). The possibilities of estimation of the vehicle orientation angle based on a convolutional neural network are also investigated. Obtained additional features are used in the modern effective reinforcement learning methods of Soft Actor Critic and Rainbow, which allows to accelerate the convergence of its learning process. To demonstrate the operation of the developed system, an intersection simulator was developed, at which a number of model experiments were carried out.	Article	OCT 2019	10.3103/S1060992X19040118	[]	[]	-1	-1	-1
B	Qiao, Zhiqian	Reinforcement Learning for Behavior Planning of Autonomous Vehicles in Urban Scenarios	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
C	Biswas, Atriya; Anselma, Pier G.; Emadi, Ali	Real-Time Optimal Energy Management of Electrified Powertrains with Reinforcement Learning	2019	2019 IEEE TRANSPORTATION ELECTRIFICATION CONFERENCE AND EXPO (ITEC)	IEEE Transportation Electrification Conference and Expo			Reinforcement learning (RL) algorithm is employed in solving energy management problem for electrified powertrain in real-world driving scenarios and the application process is streamlined. A near-global optimal control policy is articulated for the energy management system (EMS) using Q-learning algorithm which is real-time implementable. The core of the EMS is an updating optimal control policy in the form of a changing look-up table comprising near-global optimal action value function (Q-values) corresponding to all feasible state-action combinations. Using the updating control policy, the EMS can optimally decide power-split between electric machines (EMs) and internal combustion engine (ICE) in real-world driving situations.	Proceedings Paper	2019	10.1109/itec.2019.8790482	[]	[]	-1	-1	-1
J	Liu, Pengfei; Liu, Yimin; Huang, Tianyao; Lu, Yuxiang; Wang, Xiqin	Decentralized Automotive Radar Spectrum Allocation to Avoid Mutual Interference Using Reinforcement Learning	2021	IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS		190	205	Nowadays, mutual interference among automotive radars has become a problem of wide concern. In this article, a decentralized spectrum allocation approach is presented to avoid mutual interference among automotive radars. Although decentralized spectrum allocation has been extensively studied in cognitive radio sensor networks, two challenges are observed for automotive sensors using radar. First, the allocation approach should be dynamic as all radars are mounted on moving vehicles. Second, each radar does not communicate with the others so it has quite limited information. A machine learning technique, reinforcement learning, is utilized because it can learn a decision-making policy in an unknown dynamic environment. As a single radar observation is incomplete, a long short-term memory recurrent network is used to aggregate radar observations through time so that each radar can learn to choose a frequency subband by combining both the present and past observations. Simulation experiments are conducted to compare the proposed approach with other common spectrum allocation methods such as the random and myopic policy, indicating that our approach outperforms the others.	Article	FEB 2021	10.1109/TAES.2020.3011869	[]	[]	-1	-1	-1
J	Wang, Y.; Chen, Q.; Ma, H.; Jiang, Y.	Research on intelligent combat decision making based on deep reinforcement learning	2023	Proceedings of SPIE		128032D (9 pp.)	128032D (9 pp.)	With the operational advantages of unmanned combat platforms in modern war gradually appearing, the research of unmanned combat platforms has become the focus of all circles. In order to realize intelligent and autonomous unmanned operation in a real sense, a combat mission computer based on AI development board was proposed to be built as the control core of unmanned vehicles, simulate the operational mobility situation diagram of unmanned vehicles, and use the deep reinforcement learning network DQN to establish angle and distance decision-making network, so as to realize intelligent mobility decision-making of unmanned vehicles. The experiment verified that the unmanned vehicle can maneuver to the target area autonomously, which proved that the deep reinforcement learning network can realize the feasibility of platform autonomous and intelligent decision-making, and provided a feasible technical approach and theoretical support for the construction of combat mission computer to realize intelligent, autonomous and unmanned combat in a real sense.	Conference Paper; Journal Paper	2023	10.1117/12.3009215	[]	[]	-1	-1	-1
P	HUANG Z; QU Z	Deep reinforcement learning based unmanned layered            motion decision control method, involves controlling            target driving behavior of unmanned vehicle to finish            target driving behavior when meta-action instruction is            changed						NOVELTY - The method involves receiving a specific driving action instruction by a meta-action decision-making layer. A series of meta-action instruction is output to a vehicle control layer. A basic task of the vehicle control layer is performed to maintain a lane keeping driving state of an unmanned vehicle. A vehicle running state is changed according to the meta-action instruction when the meta-action instruction is received. A target driving behavior of the unmanned vehicle is controlled to finish the target driving behavior when the meta-action instruction is changed. USE - Deep reinforcement learning based unmanned layered motion decision control method. ADVANTAGE - The method enables avoiding discrete decision problem by abstract decomposition of driving behavior and environmental factor analysis. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a deep reinforcement learning based unmanned layered motion decision control method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
B	Huang, Zhe	Distributed Reinforcement Learning for Autonomous Driving	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Chen, J.; Shi, Z.; Wang, X.; Wu, J.	UBER: An Unreal Engine Based Simulation Platform with Extensibility and Real-time Capability*	2023	2023 IEEE International Conference on Real-time Computing and Robotics (RCAR)		322	7	In this paper, we propose a real-time platform called UBER, which stands for Unreal Engine Based simulation platform with Extensibility and Real-time capability. It provides a visualization way to the online tests of unmanned vehicle simulation with Unreal Engine 5 (UE5). By building a TCP communication module to exchange message between two ends in UBER, we successfully solve the problem that no code script can be operated in blueprints of UE5. Excellent physics components in UE5 makes simulation visualization more real and beautiful compared with other Reinforcement Learning environments, and by accessing the python scripts outside of blueprints, various of elements, such as scenario components and environmental settings, etc., can be easily modified. Furthermore, a series of reinforcement learning and formation control algorithms are implemented on UBER to show its extensibility and real-time capability, proving that our TCP communication method brings forth a good solution to the problem that UE5 does not support python scripts within blueprint.	Conference Paper	2023	10.1109/RCAR58764.2023.10249599	[]	[]	-1	-1	-1
P	HU W; XU Q; ZHANG Y; WANG G	Deep reinforcement learning based ground unmanned            vehicle autonomous driving method, involves analyzing            driving environment image by learning neural network,            and executing vehicle action by control module until            reaching ending state						NOVELTY - The method involves obtaining a driving environment image and a semantic segmentation environment image. Pixel information in the driving environment image is analyzed. A standard reference path is automatically generated according to a road pixel. The driving environment image is pre-processed. The pre-processed driving environment image is conveyed into a deep reinforcement learning neural network. The driving environment image is analyzed by the learning neural network for predicting vehicle action to obtain the intelligent decision result. The pre-processed driving environment image, the predicted vehicle action, a driving state score and a current driving state are stored in an experience pool module. Reverse propagation is performed to the learning neural network. Neural network parameter is adjusted. Control instruction is transmitted to a vehicle model control module. The vehicle action is executed by the vehicle model control module until reaching an ending state. USE - Deep reinforcement learning based ground unmanned vehicle autonomous driving method. ADVANTAGE - The method enables obtaining the ideal actual driving track under the complex road condition and different vehicle models. The method enables realizing environment perception-intelligent decision-local path planning-vehicle action, simplifying the calculation process, reducing the calculation amount, and improving the precision of the calculation, performing the unmanned monitoring training for deep reinforcement learning neural network for autonomous driving. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep reinforcement learning based ground unmanned vehicle autonomous driving system.				[]	[]	-1	-1	-1
P	ZHENG X; LI S; CHEN T; LUO X	Method for controlling constrained hybrid vehicle            formation based on deep reinforcement learning            strategy, involves executing, circularly reciprocating            and completing autonomous training of hybrid vehicle            formation longitudinal queue model under safety            constraint						NOVELTY - The method involves establishing a third-order nonlinear dynamics model of a manned vehicle and a third-order nonlinear engine model of an unmanned vehicle. An updated actor-reviewer network is taken as a target network. Weight parameters of the target network are updated by adopting deterministic strategy gradient to obtain an optimal action value function. Optimal control strategy is input into the unmanned vehicle of a hybrid vehicle formation longitudinal queue model. A state of the unmanned vehicle of the hybrid vehicle formation longitudinal queue model is updated. Autonomous training of the hybrid vehicle formation longitudinal queue model is executed, circularly reciprocated and completed under safety constraint. USE - Method for controlling constrained hybrid vehicle formation based on a deep reinforcement learning strategy. ADVANTAGE - The method enables inputting the optimal control strategy into the unmanned vehicle of the hybrid vehicle formation longitudinal queue model, and updating the state, thus solving problem of autonomous training of the hybrid vehicle formation on premise of safety. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating a method for controlling constrained hybrid vehicle formation based on a deep reinforcement learning strategy.				[]	[]	-1	-1	-1
B	Nagasai, L.S.; Sriprasath, V.J.; SajithVariyar, V.V.; Sowmya, V.; Aniketh, K.; Sarath, T.V.; Soman, K.P.	Electric Vehicle Steering Design and Automated Control Using CNN and Reinforcement Learning	2021	Soft Computing and Signal Processing. Proceedings of 3rd ICSCSP 2020. Advances in Intelligent Systems and Computing (AISC 1325)		513	23	Autonomous vehicles are one of the engrossing technological trends in the present automotive industry. These vehicles enticed substantial attention in industry as well as in academia. With the rising trend in research and development of autonomous vehicles, it is important to keep in mind the safety, control, and cost effectiveness of the system. The cost and implementation of self-driving technologies hinder the development of similar systems in academia and research. In this paper, we are mainly focused on developing a vision system, an automated steering system in an electric vehicle platform for academia and research. The developed system has a provision to incorporate deep learningConvolutional neural network (CNN) and reinforcement learning (RL) for automated steering control. The proposed automated steering model uses end-to-end learning and reinforcement learning for predicting the steering angles with at most 85% accuracy and control the steering.	Conference Paper	2021	10.1007/978-981-33-6912-2_46	[]	[]	-1	-1	-1
B	Huang, J.; Tan, Q.; Ma, J.; Han, L.	Path planning method using dyna-q algorithm under complex urban environment	2022	2022 China Automation Congress (CAC)		6776	81	Path planning and obstacle avoidance problems are now the focus of robotics research. This paper uses the Dyna-Q reinforcement learning algorithm to implement an obstacle avoidance and a path planning algorithm for unmanned ground vehicle(UGV) under urban environment. Using the reinforcement learning algorithm, we calculate the waypoints of the unmanned vehicle and achieve obstacle avoidance tasks and path planning using a vector field. Finally, we use a PID controller on unmanned aerial vehicle (UAV) to realize the air-ground collaboration task. The algorithms and the agents' modeling in this paper are implemented in the lab's simulation platform.	Conference Paper	2022	10.1109/CAC57257.2022.10054800	[]	[]	-1	-1	-1
P	ZANG Z; LV C; ZUO Y; WEI L; LI Z; GONG J	Ground unmanned vehicle chassis movement and            target strike cooperative control method, involves            using sensor information of unmanned vehicle on ground            as input, using trained enhanced learning parameter            model for cooperative control						NOVELTY - The method involves building a simulation scene corresponding to a real vehicle environment. A strengthening learning parameter model is built. The strengthening learning parameters model is provided with a full connection layer, a state value network, an action value network and a dropout network. The reinforcement learning parameters are trained and tested by using the simulation scenario to obtain a trained reinforcement learning model. The sensor information of the unmanned vehicle on the ground is used as an input. The trained reinforced learning parameters is used for cooperative control of the movement and target attack of the ground unmanned vehicle chassis. USE - Ground unmanned vehicle chassis movement and target striking cooperative control method for ground unmanned vehicle i.e., unmanned mining vehicle, unmanned fire truck, and unmanned special vehicle. ADVANTAGE - The method realizes ground unmanned vehicle chassis movement and target striking collaborative control, realizes cooperation of an autonomous mobile module and an autonomous task module, shortens finishing time of task and improves task execution effect. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a ground unmanned vehicle chassis movement and target striking collaborative control system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating a ground unmanned vehicle chassis movement and target striking cooperative control method (Drawing includes non-English language text).Specifically implementing step (101)				[]	[]	-1	-1	-1
P	YU Y; ZHAN D; ZHOU Z; YU F; CHEN X; LUO F; ZHANG Y; GUAN C	Method for keeping unmanned lane keeping based on            maximum entropy reinforcement learning framework,            involves selecting action with largest probability            value for unmanned vehicle to execute for trained            strategy model according to action probability            distribution outputted by network in use phase						NOVELTY - The method involves creating an unmanned vehicle simulation environment. An interaction is made with an environment, and a sample data is collected and stored in a buffer pool by an unmanned vehicle. A random strategy is used to sample from the buffer pool to update a state value function network, action value function network and strategy network. A soft update method is used to update a target state value function network. The collecting to using steps are repeated until the strategy network is about to converge. An entropy coefficient is set in an optimization objective of the state value network to zero and a training is continued until the strategy network converges completely. An action with a largest probability value is selected for the unmanned vehicle to execute for the trained strategy model according to an action probability distribution outputted by the network in the use phase. USE - Method for keeping unmanned lane keeping based on maximum entropy reinforcement learning framework. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the frame of method for keeping unmanned lane keeping based on maximum entropy reinforcement learning framework. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	GAO M	Method for controlling unmanned vehicle by            electronic device, involves mapping area of target            vehicle with grid map, and obtaining control            instruction of target vehicle by input deep            pre-training reinforcement learning network						NOVELTY - The method involves mapping an area of a target vehicle with a grid map. A position of the target vehicle is located in the grid map. State information of the area of the target vehicle is obtained. A control instruction of the target vehicle is obtained by an input deep pre-training reinforcement learning network. The target vehicle is controlled according to the control instruction, where the control instruction comprises a forward command or brake command. The area of the target vehicle is divided into a set of grid units. USE - Method for controlling an unmanned vehicle by an electronic device (claimed). ADVANTAGE - The method enables controlling the unmanned vehicle in safe and efficient manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) an unmanned vehicle control device(2) an electronic device(3) a computer-readable storage medium storing a set of instructions for controlling an unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for controlling an unmanned vehicle by an electronic device. '(Drawing includes non-English language text)'				[]	[]	-1	-1	-1
J	Lee, Sanghoon; Kim, Jinyoung; Wi, Gwangjin; Won, Yuchang; Eun, Yongsoon; Park, Kyung-Joon	Deep Reinforcement Learning-Driven Scheduling in Multijob Serial Lines: A Case Study in Automotive Parts Assembly	2023	IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS				Multijob production (MJP) is a class of flexible manufacturing systems, which produces different products within the same production system. MJP is widely used in product assembly, and efficient MJP scheduling is crucial for productivity. Most of the existing MJP scheduling methods are inefficient for multijob serial lines with practical constraints. We propose a deep reinforcement learning (DRL)-driven scheduling framework for multijob serial lines by properly considering the practical constraints of identical machines, finite buffers, machine breakdown, and delayed reward. We analyze the starvation and the blockage time, and derive a DRL-driven scheduling strategy to reduce the blockage time and balance the loads. We validate the proposed framework by using real-world factory data collected over six months from a tier-one vendor of a world top-three automobile company. Our case study shows that the proposed scheduling framework improves the average throughput by 24.2% compared with the conventional approach.	Article; Early Access		10.1109/TII.2023.3292538	[]	[]	-1	-1	-1
P	HAN J; WU X; REN L; CHEN J; ZENG X; ZHANG F; YANG C; LIU J	Training method for unmanned ship track generation            model based on graph neural network and deep            reinforcement learning, involves giving priority to            selecting experiences with high sampling probability to            train unmanned boat track generation model						NOVELTY - The training method involves building a track generation scene, which includes unmanned boats, moving obstacles and target locations. The environment in the scene, the actions performed by the unmanned boat, rewards, and the environment are set after the unmanned boat performs the actions as parameters of the deep reinforcement learning method. A memory pool is constructed with a fixed capacity to store experience and experience sampling probability. The experience includes the environment at a certain moment, the actions performed by the unmanned vehicle, rewards and the environment after the unmanned vehicle performs the action. An unmanned ship track generation model is constructed and trained. The priority is given to selecting experiences with high sampling probability to train the unmanned boat track generation model. USE - Training method for an unmanned ship track generation model based on graph neural network and deep reinforcement learning. ADVANTAGE - The method effectively improves the success rate of track planning. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a unmanned boat track generation method based on graph neural network and deep reinforcement learning;(2) a training system of unmanned boat track generation model based on graph neural network and deep reinforcement learning;(3) an unmanned boat track generation system based on graph neural network and deep reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a training method for unmanned ship track generation model based on graph neural network and deep reinforcement learning. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Dinh-Son Vu; Alsmadi, A.	Trajectory Planning of a CableBased Parallel Robot using Reinforcement Learning and Soft Actor-Critic	2020	WSEAS Transactions on Applied and Theoretical Mechanics		165	72	Industry 4.0 introduces the use of modular stations and better communication between agents to improve manufacturing efficiency and to lower the downtime between the customer and its final product. Among novel mechanisms that have a high potential in this new industrial paradigm are cable-suspended parallel robot (CSPR): their payload-to-mass ratio is high compared to their serial robot counterpart and their setup is quick compared to other types of parallel robots such as Gantry system, popular in the automotive industry but difficult to set up and to adapt while the production line changes. A CSPR can cover the workspace of a manufacturing hall and providing assistance to operators before they arrive at their workstation. One challenge is to generate the desired trajectories, so that the CSPR could move to the desired area. Reinforcement Learning (RL) is a branch of Artificial Intelligence where the agent interacts with an environment to maximize a reward function. This paper proposes the use of a RL algorithm called Soft Actor-Critic (SAC) to train a two degrees-of-freedom (DOFs) CSPR to perform pick-and-place trajectory. Even though the pick-and-place trajectory based on artificial intelligence has been an active research with serial robots, this technique has yet to be applied to parallel robots.	Journal Paper	2020	10.37394/232011.2020.15.19	[]	[]	-1	-1	-1
P	CHEN M; LI Y; LIU Q; LV S; XU Y; LIU Y	Attention model and depth reinforcement learning            based automatic unmanned vehicle driving            decision-making method, involves inputting current            state to decision module to obtain updated current            maximum probability executing driving action						NOVELTY - The method involves initializing an automatic driving environment. An observation sequence of a vehicle front camera is obtained through the driving environment. A sensing module is modeled by a building self-attention model and a long-short-time memory network. The sensing module is trained by an automatic encoder model. A Q-value of a decision module is calculated according to a depth deterministic strategy algorithm. The current state is input to the decision module to obtain updated current maximum probability executing driving action. Low-dimensional feature in the observation sequence is extracted. USE - Attention model and depth reinforcement learning based automatic unmanned vehicle driving decision-making method. ADVANTAGE - The method enables reducing dimension of observation data by using the sensing module, and improving utilization rate of the data sample by introducing priority experience playback process so as to improve training speed of algorithm and driving safety in a complex road environment. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram representing an attention model and depth reinforcement learning based automatic unmanned vehicle driving decision-making process. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	DING Z; DING D; WANG Y; WEI G; E G	Combined unmanned vehicle knowledge conversion            reinforcement learning method, involves recording            online learning experience to memory to expand case            library, and learning experience application in case            base according to different tasks						NOVELTY - The method involves establishing BP neural network mapping relationship between target tasks through task learning experience source to initialize mapping relationship of the target tasks. The task learning experience source is stored. Linear perceptron learning action mapping relationship between source domain and target domain is established by using a case-based reasoning mechanism. Online learning experience is recorded to a memory to expand a case library when learning the target tasks. An experience application is learned in a case base according to different tasks. USE - Combined unmanned vehicle knowledge conversion reinforcement learning method. ADVANTAGE - The method enables avoiding dimension disaster and improving unmanned vehicle self-skill learning speed and efficiency. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an unmanned vehicle self-skill learning method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a case-based reasoning mechanism. '(Drawing includes non-English language text)'				[]	[]	-1	-1	-1
P	ZHONG G; XIU C; MIAO C; XU S; BU X	Navigation method for unmanned vehicle based on deep reinforcement learning, involves obtaining first depth image with repeating size of minimum value and threshold value until minimum value is greater than set threshold value						NOVELTY - The navigation method involves obtaining a first depth image through an RGBD depth camera on an unmanned vehicle. A second depth image is obtained through a linear interpolation. A relative positioning of a starting point is calculated by a wheel speed milemeter of the unmanned vehicle when the minimum value is greater than a set threshold value. A Markov state space is constructed. A next action is randomly or determined according to a deep learning network. The minimum value and the threshold value are repeated until the minimum values are greater than the set threshold values. USE - Navigation method for unmanned vehicle based on deep reinforcement learning. ADVANTAGE - The method makes the network learning efficiency higher, the error convergence value is smaller, so that the obstacle avoiding effect of the unknown environment is better, and the collecting efficiency of the map is improved.				[]	[]	-1	-1	-1
P	HU H; ZHOU C; LI T; CHEN Y; ZHANG Y; YANG H; ZHOU Y; SHI D	Energy constraint multi-machine search method            based on depth reinforcement learning, involves            obtaining Double deep Q-network reinforcement learning            algorithm by training target network and playback            method						NOVELTY - The method involves obtaining a current view based on a mathematical model of an unmanned aerial vehicle constructed in advance. The current view is pre-processed and inputted to a Convolutional neural network (CNN) for feature extraction to obtain extracted feature. The extracted feature is input into a pre-trained Double deep Q-network (DDQN) reinforcement learning algorithm to obtain an action of the unmanned vehicle. Determination is made to check whether the action of unmanned vehicle is executable by a safety controller. The action of drone is determined. USE - Energy constraint multi-machine search method based on depth reinforcement learning for autonomous robot used in dangerous complex physical environment. Uses include but are not limited to planet detection, reconnaissance, rescue, mowing and cleaning. ADVANTAGE - The method providing a penalty function based on minimum distance, which can make an exploration system to ensure higher exploration efficiency and make the system to obtain higher return rate, so that the unmanned aerial vehicle can return take-off landing area before energy is exhausted while the exploration task is finished. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for the multi-machine search system based on energy constraint of depth reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of an energy constraint multi-machine search method based on depth reinforcement learning for autonomous robot used in dangerous complex physical environment. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
B	Yan, Yushu	Reinforcement Learning Based Decentralized Automotive Radar Spectrum Allocation	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J	He, Zichen; Dong, Lu; Sun, Changyin; Wang, Jiawei	Asynchronous Multithreading Reinforcement-Learning-Based Path Planning and Tracking for Unmanned Underwater Vehicle	2022	IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS		2757	2769	The underwater unmanned vehicle (UUV) is widely used in various marine operations, in which path planning and trajectory tracking are the critical technologies to achieve autonomous motion planning. Unlike previous research methods, this article proposes the asynchronous multithreading proximal policy optimization-based path planning (AMPPO-PP) and trajectory tracking (AMPPO-TT) algorithms and applies these two methods to different task scenarios of UUVs. Taking advantage of the AMPPO, the expensive online computational procedure is converted to an offline training process. The proposed algorithms enable the UUV to learn autonomous planning, tracking, and emergency obstacle avoiding. Besides, the algorithm architecture of the AMPPO-PP and the AMPPO-TT is described in detail. By refining the reward in each timestep and utilizing the reward-shaping trick, the reward sparsity is avoided. The goal-distance heuristic reward function is used to make the UUV explore more directionally. Various simulation environments are developed from simple to complex, along with multiple comparative experiments to verify the effectiveness of the proposed algorithms.	Article; Early Access		10.1109/TSMC.2021.3050960	[]	[]	-1	-1	-1
B	Wenbin Chen; Guo Xie; Wenjiang Ji; Rong Fei; Xinhong Hei; Siyu Li; Jialin Ma	Decision Making for Overtaking of Unmanned Vehicle Based on Deep Q-learning	2021	Proceedings of the 2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)		350	3	Overtaking decision under dynamic environment is one of the key research directions. Traditional methods are difficult to solve such a complex optimization decision-making problem. In recent years, reinforcement learning algorithm is developing continuously, which can be applied to solve the overtaking decision problem of unmanned vehicles. Reinforcement learning generates new data through the continuous interaction between agent and environment, and feeds back to the agent for corresponding learning behavior, so as to improve its own behavior strategy. In this paper, Deep Q-learning (DQN) is used to solve the overtaking decision-making problem on one-way through two lanes. A one-way through two lane traffic environment is built in Python to train and test the algorithm.	Conference Paper	2021	10.1109/DDCLS52934.2021.9455523	[]	[]	-1	-1	-1
J	Deng, Yingjie; Gong, Mingde; Ni, Tao	Double-channel event-triggered adaptive optimal control of active suspension systems	2022	NONLINEAR DYNAMICS		3435	3448	"An event-triggered adaptive fuzzy optimal control strategy is proposed for a quarter-car electromagnetic active suspension system, where the stiffness and the road input are unknown. The event-triggered mechanism is utilized in both sensor-to-controller (SC) and controller-to-actuator (CA) channels, such that communication saving is achieved in double channels. Two separate triggering conditions are constructed to guarantee optimal performance and stability. Via the reinforcement learning (RL) method, the critic-actor architecture of fuzzy logic systems (FLSs) is constructed to approximate the solution of Hamilton-Jacobi-Bellman (HJB) equation, where there are two critics with one actor. To overcome the ""jumps of virtual control laws"" (JVCL) problem arising in the backstepping-based ETC (see Deng et al. in ISA Trans. 117:28-39 (2021)), undetermined continuous virtual control laws are constructed for analysis. An event-triggered adaptive observer is fabricated to estimate the unknown road input. It is proved that all the estimating and tracking errors are semi-globally uniformly ultimately bounded (SGUUB). Simulation verifies the effectiveness of the proposed scheme."	Article; Early Access		10.1007/s11071-022-07360-3	[]	[]	-1	-1	-1
B	Gorodnichev, M.	On the Applicability of Reinforced Learning in the Route Selection Task of an Unmanned Vehicle	2023	2023 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO		1	9	This paper presents the process of describing and investigating methods for solving the optimal route search problem, choosing the means for developing a software module, including consideration of the required tools, description of the project structure, and features of the algorithm used. An introduction to the field of optimal route finding is given through the basic terms of graph theory, which is a leading framework for the field of reinforcement learning. The process of developing a learning environment is reviewed, starting with the selection of the environment and an explanation of its possible properties. Then the research of selected multi-agent learning algorithms is described in order to make a final comparison according to the most important criteria. Neural network architectures, hyperparameter learning tables and quality graphs of the learning models are plotted.	Conference Paper	2023	10.1109/SYNCHROINFO57872.2023.10178444	[]	[]	-1	-1	-1
J	Alomrani, Mhd Ali; Tushar, Mosaddek Hossain Kamal; Kundur, Deepa	Detecting State of Charge False Reporting Attacks via Reinforcement Learning Approach	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		10467	10476	The increased push for green transportation has been apparent to address the alarming increase in atmospheric CO2 levels, especially in the last five years. The success and popularity of Electric Vehicles (EVs) have led many carmakers to shift to developing clean cars in the next decade. Moreover, many countries around the globe have set aggressive EV target adoption numbers, with some even aiming to ban gasoline cars by 2050. Unlike their gasoline-based counterparts, EVs comprise many sensors, communication channels, and decision-making components vulnerable to cyberattacks. Hence, the unprecedented demand for EVs requires developing robust defenses against these increasingly sophisticated attacks. In particular, recently proposed cyberattacks demonstrate how malicious owners may mislead EV charging networks by sending false data to unlawfully receive higher charging priorities, congest charging schedules, and steal power. This paper proposes a learning-based detection model that can identify deceptive electric vehicles. The model is trained on an original dataset using real driving traces and a malicious dataset generated from a reinforcement learning agent. The Reinforcement Learning (RL) agent is trained to create intelligent and stealthy attacks that can evade simple detection rules while also giving a malicious EV high charging priority. We evaluate the effectiveness of the generated attacks compared to handcrafted attacks. Moreover, our detection model trained with RL-generated attacks displays greater robustness to intelligent and stealthy attacks.	Article; Early Access		10.1109/TITS.2023.3281476	[]	[]	-1	-1	-1
J	Loffredo, Alberto; May, Marvin Carl; Matta, Andrea; Lanza, Gisela	Reinforcement learning for sustainability enhancement of production lines	2023	JOURNAL OF INTELLIGENT MANUFACTURING				The importance of sustainability in industry is dramatically rising in recent years. Controlling machine states to achieve the best trade-off between production rate and energy demand is an effective method for improving the energy efficiency of production systems. This technique is referred to as energy-efficient control (EEC) and it triggers machines in a standby state with low power requests. Reinforcement Learning (RL) algorithms can be used to successfully control production systems without the requirement of prior knowledge about system parameters. Due to the difficulty in acquiring comprehensive information about system dynamics in real-world scenarios, this is considered an important factor. The goal of this work is to create a novel RL-based model to apply EEC to multi-stage production lines with parallel machine workstations without relying on full knowledge of the system dynamics. Numerical results confirm model benefits when applied to a real line from the automotive sector. Further experiments confirm the effectiveness and generality of the approach.	Article; Early Access		10.1007/s10845-023-02258-2	[]	[]	-1	-1	-1
P	HONG W; MA C; SHI J	Method for counteracting and avoiding obstacle of            e.g. military vehicle, with gradual depth reinforcement            learning, involves obtaining real-time decision of            executor of neural network training for avoiding human            carrier and obstacle						NOVELTY - The method involves using a progressive self-game SAC algorithm to self-adjust training course difficulty and executing a self-game process, generating decision data of an unmanned vehicle and putting the decision data into an experience playback pool to update different course learning data, averagely sampling latest data of the experience playback pool and updating a parameter of a criter neural network, and finishing a learning course (S4). Criter and neural network learning courses are made, real-time decision of an executor of neural network training is obtained for avoiding a human carrier and an obstacle (S5). USE - Method for counteracting and avoiding obstacle of an unmanned vehicle e.g. military vehicle and civil unmanned vehicle, with gradual depth reinforcement learning. ADVANTAGE - The method enables providing the progressive self-game SAC algorithm through progressive course learning, avoiding sparse appreciation, improving learning efficiency, improving generalization capability, and generalizing a static task to a random dynamic task. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for counteracting and avoiding obstacle of an unmanned vehicle with gradual depth reinforcement learning. (Drawing includes non-English language text).S1Step for solving modeling by longger process according to kinematics model of unmanned vehicleS2Step for simulating independent decision process of unmanned vehicle system through computer, and generating simulation data of unmanned vehicle motion process and decision behaviorS3Step for designing and optimizing form, size and number of criticizer of neural network progressive self-game SAC neural network, size and numberS4Step for using progressive self-game SAC algorithm to self-adjust training course difficulty and executing self-game process, generating decision data of unmanned vehicle and putting decision data into experience playback pool to update different course learning data, averagely sampling latest data of experience playback pool and updating parameter of criter neural network, and finishing learning courseS5Step for making criter and neural network learning courses, obtaining real-time decision of executor of neural network training for avoiding human carrier and obstacle				[]	[]	-1	-1	-1
J	Ding, Lige; Zhao, Dong; Cao, Mingzhe; Ma, Huadong	When Crowdsourcing Meets Unmanned Vehicles: Toward Cost-Effective Collaborative Urban Sensing via Deep Reinforcement Learning	2021	IEEE INTERNET OF THINGS JOURNAL		12150	12162	Mobile crowdsensing (MCS) and unmanned vehicle sensing (UVS) provide two complementary paradigms for large-scale urban sensing. Generally, MCS has a lower cost but often confronts sensing imbalance and even blind areas due to the limitation of human mobility, whereas UVS is often capable of completing more demanding tasks at the expense of limited energy supply and hardware cost. Thus, it is significant to investigate whether we could integrate the two paradigms for high-quality urban sensing in a cost-effective collaborative way. However, it is nontrivial due to complex and long-term optimization objectives, uncontrolled dynamics, and a large number of heterogeneous agents. To address the collaborative sensing problem, we propose an actor-critic-based heterogeneous collaborative reinforcement learning (HCRL) algorithm, which leverages several key ideas: local observation to handle expanded state space and extract the states of neighbor nodes, generalized model to avoid environment nonstationarity and ensure the scalability and stability of network, and proximal policy optimization to prevent the destructively large policy updates. Extensive simulations based on a mobility model and a realistic trace data set are conducted to confirm that HCRL outperforms the state-of-the-art baselines.	Article	AUG 1 2021	10.1109/JIOT.2021.3062569	[]	[]	-1	-1	-1
P	HUANG Z; QU Z	Deep reinforcement learning supporting            multi-driving behavior based unmanned vehicle movement            decision control method, involves resuming expected            driving speed that improving the front vehicle being            changed or vehicle speed						NOVELTY - The method involves decomposing a driving action into a key action decision and a key action execution. The driving action is induced to change a vehicle transverse and a longitudinal expected state when driving. An action instruction is introduced to realize decision control of supporting multiple driving actions. An upper layer of a model is provided with an independent action decision layer corresponding to the driving action. The action instruction is executed by a uniform vehicle control layer. An unmanned vehicle is resumed to an expected driving speed when a front vehicle is changed. USE - Deep reinforcement learning based multi-driving behavior supporting unmanned vehicle movement decision control method. ADVANTAGE - Method enables dividing driving action into key action decision and key action execution, realizing decision control of supporting multiple driving behavior, avoiding repeated modeling of the same task, reducing the complexity of the model, introducing depth enhanced learning algorithm, adopting intelligent body and the environment interactive learning mode, avoiding the problem of data difficult to obtain and complex data pre-processing work, solving the problem of movement decision control of multi-driving behavior and the problem of supporting multi-driving behavior based on the learning end to end movement decision control model, finishing different driving behavior according to corresponding upper behavior instruction, and realizing autonomous driving of the vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of deep reinforcement learning supporting a multi-driving behavior-based unmanned vehicle movement decision control method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Yu Ling-li; Shao Xuan-ya; Long Zi-wei; Wei Ya-dong; Zhou Kai-jun	Intelligent land vehicle model transfer trajectory planning method of deep reinforcement learning	2019	Control Theory & Applications		1409	22	Aiming at the problem of unmanned vehicles model automobiles tracking error and excessive dependence in the traditional motion planning, a method of unmanned vehicle path planning based on deep reinforcement learning model migration is proposed. First, an abstract model of the real environment is extracted. The model uses the deep deterministic policy gradient (DDPG) and the vehicle dynamics model to jointly train the enhanced learning model that approximates the optimal intelligent driving. Secondly, the actual scenario problem is migrated through the model migration strategy. In the virtual abstract model, the control and trajectory sequences are calculated according to the trained deep reinforcement learning model in the environment; then, the optimal trajectory sequence is selected according to the evaluation function in the real environment. The experimental results show that the proposed method can process the continuous input state and generate a continuously controlled corner control sequence to reduce the lateral tracking error. At the same time, the model can improve the generalization performance of the model and reduce the excessive dependence.	Journal Paper	Sept. 2019	10.7641/CTA.2018.80341	[]	[]	-1	-1	-1
J	Liu, Teng; Huo, Weiwei; Lu, Bing; Li, Jianwei	Reinforcement Learning-Based Co-Optimization of Adaptive Cruise Speed Control and Energy Management for Fuel Cell Vehicles	2024	ENERGY TECHNOLOGY				With the development of intelligent autodriving vehicles, the co-optimization of speed control and energy management under the insurance of safe and comfortable driving has become a vital issue. Herein, the adaptive cruise control scenario is discussed. A co-optimization method for speed control and energy management for fuel cell vehicles is suggested to delay the degradation of energy sources while preserving fuel cell efficiency. A reward function based on a reinforcement learning (RL) algorithm is developed to optimize the safety coefficient, comfortability, car-following efficiency, and economy at the speed control level. The RL agent learns to control vehicle speed while avoiding collisions and maximizing the cumulative rewards. To handle the problem of energy management, an adaptive equivalent consumption minimization strategy, which takes into account the deterioration of energy sources, is implemented at the energy management level. The results indicate that the suggested method reduces the demand power by 1.7%, increases the lifetime of power sources, and reduces equivalent hydrogen consumption by 9.4% compared to the model predictive control.This study focuses on co-optimizing speed control and energy management for fuel cell vehicles in the adaptive cruise control scenario. A reinforcement learning algorithm optimizes safety, comfort, car-following efficiency, and economy. An equivalent consumption minimization strategy considers energy source degradation, resulting in reduced power demand, prolonged source lifespan, and decreased hydrogen consumption compared to model predictive control.image (c) 2023 WILEY-VCH GmbH	Article; Early Access		10.1002/ente.202300541	[]	[]	-1	-1	-1
P	DAI B; LIU Y; NIE Y; SHANG E; WANG X	Unmanned vehicle layered vehicle following control            method based on depth reinforcement learning, involves            training target recognition algorithm to identify            target vehicle and deploy target recognition algorithm            on following vehicle						NOVELTY - The method involves training the target recognition algorithm to identify the target vehicle and deploy the target recognition algorithm on the following vehicle. The coordinates of the target vehicle relative to the following vehicle based on the algorithm recognition results is calculated, and the kinematic model is used to calculate the distance between the two vehicles. The coordinates of the following vehicle are taken as the origin. The direction of the front of the vehicle is the y-axis to establish a right-handed coordinate system. The coordinates of the target car relative to the following car are (x, y). The action space is a binary group. The lower layer used the underlying control algorithm based on control theory to calculate the specific acceleration and angular velocity. USE - Unmanned vehicle layered vehicle following control method based on depth reinforcement learning. ADVANTAGE - The method enables providing a vehicle following control method of a real scene facing an unknown target vehicle kinematics model. The method ensures that the vehicle following intelligent body neural network model can be directly applied on the real vehicle without migration operation. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the unmanned vehicle layered vehicle following control method based on depth reinforcement learning. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
J	Xu, Wenchao; Guo, Song; Ma, Shiheng; Zhou, Haibo; Wu, Mingli; Zhuang, Weihua	Augmenting Drive-Thru Internet via Reinforcement Learning-Based Rate Adaptation	2020	IEEE INTERNET OF THINGS JOURNAL		3114	3123	Drive-thru Internet has been considered as an effective Internet access method for Internet of Vehicles (IoV). Through the opportunistic vehicle-to-roadside WiFi connection, it can provide high throughput performance with low communication cost for IoV applications, such as intelligent transportation system, automotive infotainment, etc. However, its usability is highly affected by a fundamental issue called rate adaptation (RA), which is to adjust the modulation and coding rate to adapt to the dynamic wireless channel between the vehicle and the roadside access point (AP). Conventional WiFi RA schemes are designed for indoor or quasistatic scenarios and do not account for the channel variations in drive-thru Internet. In this article, we study the limitation of applying existing RA schemes in drive-thru Internet and propose a reinforcement learning (RL)-based RA scheme to capture the potential channel variation patterns and efficiently select the rate for every vehicle's egress frame. Simulation results demonstrate that the proposed RA scheme outperforms the existing schemes in network throughput and that the efficiency of the learning model can be generalized under various conditions. The proposed RA method can provide useful inspirations for designing robust and scalable link adaptation protocols in IoV.	Article	APR 2020	10.1109/JIOT.2020.2965148	[]	[]	-1	-1	-1
J	Trilling, Jens; Schumacher, Axel; Zhou, Ming	Reinforcement learning based agents for improving layouts of automotive crash structures	2024	APPLIED INTELLIGENCE				The topology optimization of crash structures in automotive and aeronautical applications is challenging. Purely mathematical methods struggle due to the complexity of determining the sensitivities of the relevant objective functions and restrictions according to the design variables. For this reason, the Graph- and Heuristic-based Topology optimization (GHT) was developed, which controls the optimization process with rules derived from expert knowledge. In order to extend the collected expert rules, the use of reinforcement learning (RL) agents for deriving a new optimization rule is proposed in this paper. This heuristic is designed in such a way that it can be applied to many different models and load cases. An environment is introduced in which agents interact with a randomized graph to improve cells of the graph by inserting edges. The graph is derived from a structural frame model. Cells represent localized parts of the graph and delineate the areas where agents can insert edges. A newly developed shape preservation metric is presented to evaluate the performance of topology changes made by agents. This metric evaluates how much a cell has deformed by comparing its shape in the deformed and undeformed state. The training process of the agents is described and their performance is evaluated in the training environment. It is shown how the agents and the environment can be integrated as a new heuristic into the GHT. An optimization of the frame model and a vehicle rocker model with the enhanced GHT is carried out to assess its performance in practical optimizations.	Article; Early Access		10.1007/s10489-024-05276-6	[]	[]	-1	-1	-1
P	CAO J; LI P; LIANG D	Agent behavior tree generation method based on            reinforcement learning and genetic algorithm, involves            updating action space using retained parent behavior            tree and generated child behavior tree, and remaining            position of parent behavior tree unchanged, and filling            vacant position						NOVELTY - The method involves initializing (S1) behavior tree library. The action space is initialized (S2). The accumulated value estimate is reset (S3), and a table equal to the size of the action space is set. The reinforcement learning training is performed (S4). The reinforcement learning is used to perform a training iteration, and environmental information is taken as input, and the value estimate of the behavior tree in the action space is used as the output. Determination is made (S5) whether the behavior tree achieves the goal. The action space is updated (S10) using the retained parent behavior tree and the generated child behavior tree. The position of the parent behavior tree is remained unchanged, and the vacant position is filled by child behavior tree. USE - Intelligent portion agent behavior tree generation method based on reinforcement learning and genetic algorithm for use in application such as game intelligent agent, robot and unmanned vehicle. ADVANTAGE - The method enables providing a control strategy for automatically generating the action tree expression for multiple applications such as game intelligent agent, robot and unmanned vehicle, thus reducing cost. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an intelligent agent behaviour tree generation system based on enhanced learning and genetic algorithm. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating agent behavior tree generation method based on reinforcement learning and genetic algorithm. (Drawing includes non-English language text)S1Step for initializing behavior tree libraryS10Step for initializing action spaceS2Step for resetting accumulated value estimateS3Step for performing reinforcement learning trainingS4Step for determining whether behavior tree achieves goalS5Step for updating action space				[]	[]	-1	-1	-1
P	CHEN Q; XIA X; LI X	Method for scheduling an unmanned aerial vehicle            based on multi-intelligent body reinforcement learning,            involves updating MADDPG algorithm effectively strategy            in multi-agent environment, and optimal or near-optimal            unmanned plane position scheduling strategy in MDP            model is found						NOVELTY - The method involves obtaining environment information from a system, where the environment information comprises position information of an internet-of-things (IoT) node, task issuing condition, unmanned aerial vehicle resource and position information. Judgment is made to check whether there is a new task based on the new environment information. A task is distributed for an unmanned vehicle. A dispatching decision of the unmanned vehicle is performed. A next motion state of the drone is determined by using a multi-intelligent body reinforcement learning algorithm. The drone is used as an intelligent body in the multi-Intelligent structure reinforcement learning process. A real or simulation environment is continuously performed in the real or simulated environment. The MADDPG algorithm effectively updates the strategy in the multi-agent environment, and the optimal or near-optimal unmanned plane position scheduling strategy in the MDP model is found. USE - Method for scheduling an unmanned aerial vehicle based on multi-intelligent body reinforcement learning for mobile user equipments (UEs) in a short time. ADVANTAGE - The method optimizes the autonomy and efficiency of the individual unmanned aerial vehicle while ensuring the whole performance when the multiple unmanned aerial vehicles are dispatched according to the resource demand, and improves the system effect and the unmanned aerial device energy efficiency. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of method for scheduling unmanned aerial vehicle based on multi-intelligent body reinforcement learning. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	XU Z; HUANG Z	Unmanned decision and control method based on federal deep reinforcement learning, involves sending current model parameter to federal server which performs federal aggregation and then sends to each client						NOVELTY - The method involves issuing an initialized federal model to the client participating in the federal by the federal server. The required scene data is acquired from a scene and the scene data is processed into data dimensions input by a neural network by the client. The deep reinforcement learning training is carried out on the client participating in the federation locally according to a local data set by using a federation model to obtain a new client model. An aggregation request is initiated to a client regularly by a federal server, current model parameters are sent to the federal server the client, and federal aggregation is performed and then the federate aggregation to each client is issued by the federal server. USE - Federal deep reinforcement learning-based unmanned decision-making and control method. ADVANTAGE - The method ensures that the client data is not local, performing federal learning training, realizing the effect of decision and control of the unmanned vehicle in different scenes. The unmanned vehicle can finish the driving under different test scenes, and can keep the more stable speed and vehicle control. DESCRIPTION Of DRAWING(S) - The drawing shows an architecture of a federal deep reinforcement learning-based unmanned decision and control model. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	ZANG B; LI L; LONG L; JIANG C; ZHANG W; LI Y	Method for performing intelligent navigation of            multi-unmanned aerial vehicle based on deep            reinforcement learning, involves establishing            multi-frame four-rotor unmanned aerial vehicle model in            three-dimensional simulation environment						NOVELTY - The method involves establishing a multi-frame four-rotor unmanned aerial vehicle model in a three-dimensional simulation environment and generating an environment comprising an obstacle and a target point. Global observation information of multi-dimensional feature fusion of unmanned aerial vehicles, local observation information and discrete action space is set. A value evaluation index of unmanned vehicle state is defined based on reward function of Euclidean distance. A strategy network and a state value network are designed. A current execution of an action is determined according to local observing information of the unmanned vehicle. The current unmanned vehicle execution of the action score is evaluated according to the global observation information. A temporary experience pool is designed for storing interactive information. USE - Method for performing intelligent navigation of a multi-unmanned aerial vehicle based on deep reinforcement learning used in military field or civil field. ADVANTAGE - The method enables combining two-dimensional image information and one-dimensional state information input to reinforcement learning network training, so that the unmanned aerial vehicle can fully detect surrounding environment to make better action under specific state, and designing machine obstacle avoidance function in reward function, so as to realize better obstacle avoidance navigation effect. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a flight path corresponding to an unmanned aerial vehicle (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	HU W; XU Q; ZHANG Y; WANG G	Deep reinforcement learing based intelligent            ground unmanned vehicle decision method, involves            performing reverse propagation to deep reinforcement            learning decision network, and adjusting decision            network parameter until deep reinforcement network            convergence						NOVELTY - The method involves collecting vehicle information and environmental information around a vehicle. A deep reinforcement learning decision network is deeply enhanced to analyze and calculate the vehicle information. A driver driving characteristic expression is obtained through vehicle information to obtain a vehicle environment characteristic expression through the environmental information. An intelligent decision result is given to a current driving environment. A current driving state termination signal is stored in an experience pool module. An experience is extracted in the experience pool modules. A decision network parameter is adjusted until a decision network convergence process is performed. USE - Deep reinforcement learing based intelligent ground unmanned vehicle decision method. ADVANTAGE - The method simplifies calculation process, and reduces calculation amount, so that the real time is guaranteed, and avoids generating any disturbance in the whole training process to the driver, and correctly drives the vehicle by the driver to finish the training of the network. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep reinforcement learing based intelligent ground unmanned vehicle decision system.				[]	[]	-1	-1	-1
J	Kulmer, F.; Wolf, M.; Ramsauer, C.	Medium-term Capacity Management through Reinforcement Learning - Literature review and concept for an industrial pilot-application	2022	Procedia CIRP		1065	70	Empty storage shelves and long customer lead times due to a sharp rise in market demand from industries on the one hand (e.g., pharmaceutical products). On the other hand, increasing short-term working or unemployment due to a rapid decline in demand (e.g., automotive). Current supply and demand gaps caused by the COVID-19 pandemic remind us that successful competition in volatile business environments requires rapid adjustments of production capacities. Capacity management (CM) addresses these adjustments by adapting production capacity to market demand. Operations managers of flexible manufacturing systems can adjust the capacity by using various levers (e.g., overtime, used machines, ...). To guide these managers, decision support systems (DSS) exist for short-term CM (e.g., shop floor scheduling). However, due to complexity and runtime problems, the decision-making process for medium-term CM is usually carried out with low technical support. Increases in computing power and advances in algorithm performance over the past decades have enabled Machine Learning to solve ever more complex problems such as the aforementioned issues. Reinforcement Learning (RL) in particular has shown good performance in solving short-term CM problems when compared to humans or other established heuristics. In this work we review the current literature for CM using RL in flexible manufacturing systems. We identify an existing lack of knowledge within the overlap of medium-term CM and RL. However, good performance of RL in short-term CM indicates that an application in medium-term CM should be evaluated. In addition, we propose a concept of a method for medium-term CM based on RL to support operations managers in the decision-making process. The resulting DSS could have a significant impact on production performance, especially in terms of capacity adjustment speed. All rights reserved Elsevier.	Journal Paper	2022	10.1016/j.procir.2022.05.109	[]	[]	-1	-1	-1
P	FENG B; LIU W; YAN X; SHE H; DU Z; YE H; SHI Y; YANG X	Water-air amphibious unmanned aircraft path            planning method based on reinforcement learning            involves selecting amphibious unmanned aircraft            execution path planning task area, and finishing global            path planning according to different working            scenes						NOVELTY - The method involves selecting an amphibious unmanned aircraft execution path planning task area. An electronic chart is extracted corresponding to the area S of the data for three-dimensional environment modelling. A markov decision process (MDP) of amphibious unmanned aircraft path planning is constructed. A starting point and a target point are determined. A global path planning is completed according to the different working scenes of the amphibious unmanned vehicle based on a deep Q network (DQN) algorithm according to the MDP of the amphibious unmanned vehicle path planning. USE - The method is useful for planning water-air amphibious unmanned aircraft path based on reinforcement learning. ADVANTAGE - The method: improves the planning range to dozens of kilometers; considers the movement characteristics of amphibious unmanned aircraft; combines with the DQN algorithm to quickly find an optimal path that suits its working scene; and is simple and valid. DESCRIPTION Of DRAWING(S) - The drawing shows a logic step diagram of the water-air amphibious unmanned aircraft path planning method based on reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	YANG B; HUANG K; WANG L; YU Y	Method for optimizing unmanned aerial vehicle            auxiliary mobile edge computing resource, involves            solving optimization problem by deep reinforcement            learning algorithm to obtain optimal resource            distribution solution						NOVELTY - The method involves obtaining distribution condition of a user calculation task based on Poisson point process. An unmanned aerial vehicle is pre-deployed according to the distribution condition. The unmanned aerial vehicles are determined according to a determination of a coverage radius of the unmanned vehicle and an unmanned vehicle number and a position. A system model is constructed based on an unmanned aerial Vehicle auxiliary MEC system model. An optimization problem is obtained based on the system model of the drone-assisted MEC. An optimal resource distribution solution is obtained by a deep reinforcement learning algorithm. USE - Method for optimizing resource of an unmanned aerial vehicle auxiliary mobile edge computing (MEC) system based on NOMA in a communication field and strengthening learning technology field. ADVANTAGE - The method enables solving the optimization problem by a deep reinforcement learning algorithm to obtain an optimal resource distribution solution. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a method for optimizing resource of an unmanned aerial vehicle auxiliary MEC system based on NOMA (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	DENG F; WU X; HUANG K; HUANG J; LIU H; ZHANG X	Automatic amphibious unmanned vehicle mode            switching and ground obstacle avoidance training            method, involves obtaining trained decision neural            network when preset number of training period is not            reached						NOVELTY - The method involves constructing a training scenario based on a Gazebo simulator. The training scenario is provided with a road-air amphibious unmanned vehicle, an obstacle cone and a training scene of a wall. Current time sensing information is input into a decision neural network based on machine learning principle reinforcement learning. Judgment is made to check whether to switch a motion mode. An action instruction is sent to the Gazebos simulator. Motion-related data of each step is stored into an experience pool. Multiple groups of data are randomly extracted from the experience pool based on n-step time sequence difference method. A trained decision neural network is obtained. USE - Automatic amphibious unmanned vehicle mode switching and ground obstacle avoidance training method. ADVANTAGE - The method enables quickly providing sufficient and high-quality data based on simulation environment, reducing time for generating mature mode switching strategy, utilizing reinforcement learning algorithm to effectively eliminate problem that the state action value is high in reinforcement training, reducing estimation error of the state action value and starting training period by a training scene building module for obtaining the trained decision neural network. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a Automatic amphibious unmanned vehicle mode switching and ground obstacle avoidance training system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an automatic amphibious unmanned vehicle mode switching and ground obstacle avoidance training method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	SHEN Y; GAO Z; XIA Z; FENG L; WANG H	Method for controlling cooperative steering of            human-vehicle based on cooperation model reinforcement            learning corner weight distribution, involves            distributing weights of controller and driver model in            real-time manner						NOVELTY - The method involves distributing weight of a controller of an unmanned vehicle and a driver model in real-time manner by using a DQN intelligent body, and updating a policy network of the unmanned vehicle in real-time manner through a vehicle state and an evaluation network of the unmanned vehicle. Policy network iteration is finished after a certain iteration times, finishing training, and keeping network parameter of last updated policy network is not changed, where at this time, the evaluation network does not participate in the weight distribution process of the controller and the driver model. Weights of the controller and the driver model are distributed in real-time manner by the the DQN intelligent body through the last updated policy network. USE - Method for controlling cooperative steering of a human-vehicle based on cooperation model reinforcement learning corner weight distribution in an unmanned driving field in traffic transportation. ADVANTAGE - The method enables generating the intelligent body to satisfy expected standard in the iterative process to coordinate and distribute an output corner of the driver and the controller. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer readable storage medium for storing a set of instructions for controlling cooperative steering of a human-vehicle based on cooperation model reinforcement learning corner weight distribution. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a device for controlling cooperative steering of a human-vehicle. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Bragato, F.; Lotta, T.; Ventura, G.; Drago, M.; Mason, F.; Giordani, M.; Zorzi, M.	Towards Decentralized Predictive Quality of Service in Next-Generation Vehicular Networks [arXiv]	2023	arXiv				To ensure safety in teleoperated driving scenarios, communication between vehicles and remote drivers must satisfy strict latency and reliability requirements. In this context, Predictive Quality of Service (PQoS) was investigated as a tool to predict unanticipated degradation of the Quality of Service (QoS), and allow the network to react accordingly. In this work, we design a reinforcement learning (RL) agent to implement PQoS in vehicular networks. To do so, based on data gathered at the Radio Access Network (RAN) and/or the end vehicles, as well as QoS predictions, our framework is able to identify the optimal level of compression to send automotive data under low latency and reliability constraints. We consider different learning schemes, including centralized, fully-distributed, and federated learning. We demonstrate via ns-3 simulations that, while centralized learning generally outperforms any other solution, decentralized learning, and especially federated learning, offers a good trade-off between convergence time and reliability, with positive implications in terms of privacy and complexity.	Journal Paper	22 Feb. 2023		[]	[]	-1	-1	-1
P	YU H; FENG Y; OU J; LIU J; LIU Y	Method for smartly controlling automotive thermal            management based on adversarial reinforcement learning,            involves determining whether control agent and            adversarial agent have both converged, if that does not            converge						NOVELTY - The method involves collecting (S1) system environmental status data related to the thermal system in the vehicle. A thermal system model is established (S2) based on the system environmental status data obtained in S1. The initial state of the thermal system model is set (S3). The adversarial agent is used (S4) to give a random environmental disturbance d, and the control agent is used to give a control action a. A reinforcement learning algorithm is used (S6) to train the control agent based on the data set. Check whether the control agent and the adversarial agent have both converged is determined or not (S7), if that does not converge, return to S3. The process ends if convergence occurs. USE - Method for smartly controlling automotive thermal management based on adversarial reinforcement learning. ADVANTAGE - The method effectively deal with the problems of untimely control and unguaranteed comfort temperature, save energy under time-varying working conditions, and improve the control effect. The control smart agent has high robustness to the unknown disturbance in the real environment and reduces the risk of actual application DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a computer system; and (2) a computer readable storage medium storing program for smartly controlling automotive thermal management based on adversarial reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the method for smartly controlling automotive thermal management based on adversarial reinforcement learning. (Drawing includes non-English language text).S1Step for collecting system environmental status data related to the thermal system in the vehicleS2Step for establishing thermal system model based on the system environmental status dataS3Step for setting initial state of the thermal system modelS4Step for using adversarial agent to give a random environmental disturbance d, and the control agent is used to give a control action aS6Step for using a reinforcement learning algorithm to train the control agent based on the data setS7Step for determining whether the control agent and the adversarial agent have both converged, if that does not converge				[]	[]	-1	-1	-1
B	Wenwei Lin; Chen Zhu; Wenzhang Zhu; Sixin Shen	Charging Scheduling Strategies of Cooperated Car-hailing Operating Business for Electric Taxis	2021	2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)		407	13	With the popularity of electric vehicles (EVs), the replacement on the traditional fuel taxis to the electric taxis (e-taxis) has been gradually emerged in a creasing number of cities. Compared with the ordinary EVs, e-taxis require frequent charging due to their long daily mileage and high total power consumption. Additionally, the disorderly charging behaviors of EVs have caused congestion in charging stations (CSs) and have seriously affected the normal business of e-taxi drivers. This paper proposes a management architecture that combines the charging network and the car-hailing operating network of e-taxis. With the goal of minimizing the charging cost of e-taxis, a set of scheduling strategy which combines car-hailing operating business and charging plan is designed based on reinforcement learning (RL). The result shows that this strategy can effectively lower the charging cost of e-taxis by reducing the charging queue time in the CSs.	Conference Paper	2021	10.1109/ICWCSG53609.2021.00088	[]	[]	-1	-1	-1
B	Ko, Sangjin	Reinforcement Learning Based Decision Making for Self Driving & Shared Control Between Human Driver and Machine	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
B	Zhang, C.; Jing, G.; Zuo, S.; Su, M.; Liu, Q.	Inverse Reinforcement Learning in Automatic Driving Decision	2022	2022 2nd International Conference on Algorithms, High Performance Computing and Artificial Intelligence (AHPCAI)		701	4	With the urgent need of automatic driving on urban roads, autonomous unmanned system must complete the driving task considering safety, efficiency and comfort. For the planning and decision-making module, reinforcement learning can learn human strategies in a human-like manner. However, the reward function is difficult to be determined manually, and inverse reinforcement learning (IRL) can find a reasonable reward function that explains the human strategy. In this paper, the machine learning method on unmanned system is studied, and the IRL based on maximum entropy is introduced to learn the reward function. Experiments on the real-world nuScenes dataset is implemented by setting the features of reward function that conforms to urban environmental constraints. Finally, a reasonable reward function is obtained, which demonstrates the weights of the features can describe the trajectory of unmanned vehicle under the urban road.	Conference Paper	2022	10.1109/AHPCAI57455.2022.10087395	[]	[]	-1	-1	-1
C	Xu, Lifan; Zheng, Ruxin; Sun, Shunqiao	A DEEP REINFORCEMENT LEARNING APPROACH FOR INTEGRATED AUTOMOTIVE RADAR SENSING AND COMMUNICATION	2022	2022 IEEE 12TH SENSOR ARRAY AND MULTICHANNEL SIGNAL PROCESSING WORKSHOP (SAM)		316	320	We present a deep reinforcement learning approach to design an automotive radar system with integrated sensing and communication. In the proposed system, sparse transmit arrays with quantized phase shifter are used to carry out transmit beamforming to enhance the performance of both radar sensing and communication. Through interaction with environment, the automotive radar learns a reward that reflects the difference between mainlobe peak and the peak sidelobe level in radar sensing mode or communication user feedback in communication mode, and intelligently adjust its beamforming vector. The Wolpertinger policy based actioncritic network is introduced for beamforming vector learning, which solves the dimension curse due to huge beamforming action space.	Proceedings Paper	2022	10.1109/SAM53842.2022.9827815	[]	[]	-1	-1	-1
P	TAN L; LI B; WANG L; XIA J	Multi-unmanned aerial vehicle air charging and            task scheduling method based on deep reinforcement            learning e.g. used in mobile application, involves            calculating resource optimal allocation strategy of            unmanned aerial vehicle and optimal unloading decision            of user equipment						NOVELTY - The method involves obtaining a position of each user and a base station in an environment according to data collected by a third-party. A deployment position of an unmanned aerial vehicle group is initialized. A calculation resource of each unmanned vehicle is pre-set based on the optimization model of the unmanned vehicle group energy consumption minimum as an optimization target. An unloading decision of a user equipment is solved by using a differential evolution algorithm based on a current position of a current unmanned vehicle and an offload decision of the user equipment. An unmanned vehicle deployment strategy is optimized based on an un-install decision of an user equipment and a calculation resource allocation strategy of unmanned vehicle. An optimal resource optimal allocation strategy is calculated based on user equipment optimization decision when an absolute value of an energy consumption value is less than a preset threshold value. USE - Multi-unmanned aerial vehicle air charging and task scheduling method based on deep reinforcement learning used in mobile application, mobile online game, augmented reality enhancing and intelligent navigation. ADVANTAGE - The method involves constructing a multi-unmanned aerial vehicle group auxiliary edge calculation model, and presetting the calculation resource of each unmanned aerial vehicle, and thus enables improving the endurance ability of the unmanned aerial vehicles. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the multi-unmanned aerial vehicle air charging and task scheduling method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	YANG F; LIU Y; LIANG X; ZHANG L; LI M; XU Y; LIU Z; CHEN L; FENG Y; ZHANG Y	Method for calling unmanned aerial vehicle            knowledge model time sharing based on reinforcement            learning, involves using final evaluation value of            unmanned aerial vehicles knowledge model at different            time to perform time-sharing call						NOVELTY - The method involves obtaining multiple unmanned aerial vehicle knowledge models to be invoked. The unmanned vehicle knowledge model is provided with a cruise model, a reconnaissance model and a strike model. The cruise model is used to execute the task to the target area within the preset period. The environment of all unmanned vehicle information model timely feedback value is obtained. The accumulated discount feedback is obtained of all drone information models. The option strategy function of each drone information model is calculated according to the cumulative discount feedback and the multi-step time state transition probability of the drone knowledge model. A neural network is used as the evaluation function of the drones information model. An update formula of the evaluation functions is constructed according to option strategy functions. The update formula is utilized to update the neural network to obtain the updated neural network. USE - Method for performing time-sharing calling of an unmanned aerial vehicle knowledge model based on reinforcement learning. Uses include but are not limited to aerial photography, agriculture, express transportation, disaster rescue, wild animal observation, infectious disease monitoring, surveying and mapping, news report, power inspection, disaster relief and video shooting fields. ADVANTAGE - The method enables improving working efficiency of the unmanned aerial vehicle in the execution task. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an unmanned aerial vehicle knowledge model time-sharing calling device based on reinforcement learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method for performing time-sharing calling of an unmanned aerial vehicle knowledge model based on reinforcement learning. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Koch, Lucas; Roeser, Dennis; Badalian, Kevin; Lieb, Alexander; Andert, Jakob	Cloud-Based Reinforcement Learning in Automotive Control Function Development	2023	VEHICLES		914	930	Automotive control functions are becoming increasingly complex and their development is becoming more and more elaborate, leading to a strong need for automated solutions within the development process. Here, reinforcement learning offers a significant potential for function development to generate optimized control functions in an automated manner. Despite its successful deployment in a variety of control tasks, there is still a lack of standard tooling solutions for function development based on reinforcement learning in the automotive industry. To address this gap, we present a flexible framework that couples the conventional development process with an open-source reinforcement learning library. It features modular, physical models for relevant vehicle components, a co-simulation with a microscopic traffic simulation to generate realistic scenarios, and enables distributed and parallelized training. We demonstrate the effectiveness of our proposed method in a feasibility study to learn a control function for automated longitudinal control of an electric vehicle in an urban traffic scenario. The evolved control strategy produces a smooth trajectory with energy savings of up to 14%. The results highlight the great potential of reinforcement learning for automated control function development and prove the effectiveness of the proposed framework.	Article	SEP 2023	10.3390/vehicles5030050	[]	[]	-1	-1	-1
C	Biswas, Atriya; Anselma, Pier Giuseppe; Rathore, Aashit; Emadi, Ali	Comparison of Three Real-Time Implementable Energy Management Strategies for Multi-mode Electrified Powertrain	2020	2020 IEEE TRANSPORTATION ELECTRIFICATION CONFERENCE & EXPO (ITEC)	IEEE Transportation Electrification Conference and Expo	514	519	Three real-time implementable energy management system (EMS) strategies have been articulated for forward simulation vehicle model with an electrified powertrain. Rule-based strategy and equivalent consumption minimization strategy (ECMS) have been profoundly used as a competent real-time implementable EMS strategy for electrified powertrain. Rein-forcement learning (RL) is relatively new as a real-time EMS controller. All these three controllers have been articulated for model-in-the-loop (MIL) simulation. A comparison among state-of-the art RL-based controller, widely accredited ECMS, and rule-based control strategies is very crucial in order to analyze strengths and weaknesses of each of these strategies at the MIL and to make them apposite for the subsequent phases of utilitarian controller development.	Proceedings Paper	2020	10.1109/itec48692.2020.9161549	[]	[]	-1	-1	-1
P	WU S; WANG Z; HAO T; HOU Z	Automatic driving method for intersection scene            and related device, involves combining single-step            recurrent neural network in action neural network and            review neural network respectively based on            actor-critic reinforcement learning framework						NOVELTY - The method involves obtaining (S101) driving data of unmanned vehicles and environmental data of intersections. An action space is constructed (S102) according to the driving data. The state space according to the driving data and the environmental data is constructed (S103). The driving action of the unmanned vehicle is predicted (S104) based on the action space and the state space using intersection behavior model. The behavior of the unmanned vehicle at the intersection is controlled (S105) based on the driving action. The intersection behavior model is pre-built in the following manner. The single-step recurrent neural network is combined in the action neural network and the review neural network respectively based on the actor-critic reinforcement learning framework. USE - Automatic driving method for intersection scene and related device of unmanned vehicle using electronic device (claimed). ADVANTAGE - The automatic driving method provides a new solution for the behavioral decision-making of unmanned vehicle intersections, which effectively deals with various scenes at intersections. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an automatic driving device for intersection scene; and(2) a non-transitory storage medium storing program for automatic driving method. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the automatic driving method for intersection scene and related device. (Drawing includes non-English language text)Step for obtaining driving data of unmanned vehicles and environmental data of intersections (S101)Step for constructing action space (S102)Step for constructing state space according to driving data and environmental data (S103)Step for predicting driving action of unmanned vehicle (S104)Step for controlling behavior of unmanned vehicle at intersection (S105)				[]	[]	-1	-1	-1
B	Zhou, B.; Yi, J.; Zhang, X.	Learning to navigate on the rough terrain: A multi-modal deep reinforcement learning approach	2022	2022 IEEE 4th International Conference on Power, Intelligent Computing and Systems (ICPICS)		189	94	How to enable safe navigation of unmanned vehicles on complex and rough terrain is challenging and meaningful research. In this paper, we propose an end-to-end reinforcement learning local navigation method with multi-modal data fusion, which effectively combines the intrinsic perception, such as Inertial Measurement Unit (IMU) measurements, and the extrinsic perception, such as Three-dimensional (3D) point clouds and images, of an unmanned vehicle. A specific feature extraction network is constructed for each modal data, and the total network is effectively trained using a modal separation learning method. The experimental results show that the proposed method can effectively address various obstacles such as rough roads, vegetation obstacles, and water pool disturbances to achieve autonomous and safe navigation of unmanned vehicles in off-road scenarios.	Conference Paper	2022	10.1109/ICPICS55264.2022.9873725	[]	[]	-1	-1	-1
J	Panov, A., I	Simultaneous Learning and Planning in a Hierarchical Control System for a Cognitive Agent	2022	AUTOMATION AND REMOTE CONTROL		869	883	The tasks of behavior planning and decision-making learning in a dynamic environment are usually divided and considered separately in control systems for intelligent agents. A new unified hierarchical formulation of the problem of simultaneous learning and planning (SLAP) is proposed in the context of object-oriented reinforcement learning, and an architecture of a cognitive agent that solves this problem is described. A new algorithm for learning actions in a partially observed external environment is proposed using a reward signal, an object-oriented subject description of the states of the external environment, and dynamically updated action plans. The main properties and advantages of the proposed algorithm are considered, including the lack of a fixed cognitive cycle necessitating the separation of planning and learning subsystems in earlier algorithms and the ability to construct and update the model of interaction with the environment, thus increasing the learning efficiency. A theoretical justification of some provisions of this approach is given, a model example is proposed, and the principle of operation of a SLAP agent when driving an unmanned vehicle is demonstrated.	Article	JUN 2022	10.1134/S0005117922060054	[]	[]	-1	-1	-1
C	Yu, Lingli; Shao, Xuanya; Yan, Xiaoxin	Autonomous Overtaking Decision Making of Driverless Bus Based on Deep Q-learning Method	2017	2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017)		2267	2272	The autonomous overtaking maneuver is a valuable technology in unmanned vehicle field. However, overtaking is always perplexed by its security and time cost. Now, an autonomous overtaking decision making method based on deep Q-learning network is proposed in this paper, which employs a deep neural network(DNN) to learn Q function from action chosen to state transition. Based on the trained DNN, appropriate action is adopted in different environments for higher reward state. A series of experiments are performed to verify the effectiveness and robustness of our proposed approach for overtaking decision making based on deep Q-learning method. The results support that our approach achieves better security and lower time cost compared with traditional reinforcement learning methods.	Proceedings Paper	2017		[]	[]	-1	-1	-1
C	Kerbel, Lindsey; Ayalew, Beshah; Ivanco, Andrej; Loiselle, Keith	Residual Policy Learning for Powertrain Control	2022	IFAC PAPERSONLINE		111	116	Eco-driving strategies have been shown to provide significant reductions in fuel consumption. This paper outlines an active driver assistance approach that uses a residual policy learning (RPL) agent trained to provide residual actions to default power train controllers while balancing fuel consumption against other driver-accommodation objectives. Using previous experiences, our RPL agent learns improved traction torque and gear shifting residual policies to adapt the operation of the powertrain to variations and uncertainties in the environment. For comparison, we consider a traditional reinforcement learning (RL) agent trained from scratch. Both agents employ the off-policy Maximum A Posteriori Policy Optimization algorithm with an actor-critic architecture. By implementing on a simulated commercial vehicle in various car-following scenarios, we find that the RPL agent quickly learns significantly improved policies compared to a baseline source policy but in some measures not as good as those eventually possible with the RL agent trained from scratch. Copyright (c) 2022 The Authors. This is an open access article under the CC BY-NC-ND license	Proceedings Paper	2022	10.1016/j.ifaco1.2022.10.270	[]	[]	-1	-1	-1
J	Yan Hao; Liu Xiaozhu; Shi Ying	Lane-change Control for Unmanned Vehicle Based on REINFORCE Algorithm and Neural Network	2021	Journal of Transport Information and Safety		164	172	For lane change and overtaking of unmanned vehicles,the paper studies the lane change control strategy of unmanned vehicles based on the REINFORCE algorithm and neural network.The feedback,control input,and output limit requirement of the vehicle dynamics model are determined.The REINFORCE algorithm is used to design the structure of the neural network controller and the training plan of the controller.For too large data value and variance of the experience pool,a preprocessing method of the experience pool data is proposed to improve the controller training plan.Besides analyzing sparse reward distribution in the reinforcement learning process,a reward shaping solution based on logarithmic function is proposed combined with the running condition of unmanned vehicles.Compared with PID and LQR controllers,the experiment is carried out.The results show that the proposed control strategy has smaller maximum error compared with PID,with a safer lane-change process.The performance of the control strategy is similar to LQR,which proves its feasibility for the lane change control task of unmanned vehicles.Also,the execution time of the control strategy in different platforms is recorded to prove its real-time performance and feasibility in lightweight platforms.	Article	2021		[]	[]	-1	-1	-1
J	Li, Zhenyu; Zhou, Aiguo	RDDRL: a recurrent deduction deep reinforcement learning model for multimodal vision-robot navigation	2023	APPLIED INTELLIGENCE		23244	23270	Existing deep reinforcement learning-based mobile robot navigation relies largely on single-modal visual perception to perform local-scale navigation. However, multimodal visual fusion-based global navigation is still under technical exploration. Visual navigation necessitates that agents drive safely in structured, changing, and even unpredictable environments; otherwise, inappropriate operations may result in mission failure and even irreversible damage to life and property. We propose a recurrent deduction deep learning model (RDDRL) for multimodal vision-robot navigation to address these issues. We incorporate a recurrent reasoning mechanism (RRM) into the reinforcement learning model, which allows the agent to store memory, predict the future, and aid in policy learning. Specifically, the RRM first stores current observations and states by learning a parameterized environment model and then predicts future transitions. The RRM then performs a self-assessment on the predicted behavior and perceives the consequences of the current policy, producing a more reliable decision-making process. Furthermore, to obtain global-scale behavioral decision-making, information from scene recognition, semantic segmentation, and pose estimation are fused and used as partial observations of the RDDRL. A large number of simulated experiments based on CARLA scenarios, as well as test results in real-world scenarios, show that RDDRL outperforms state-of-the-art RL methods in terms of driving stability and safety. The results show that by training the agent, the collision rate in the global decision-making of the unmanned vehicle decreases from 0.2 % in the training state to 0.0 % in the test state.	Article; Early Access		10.1007/s10489-023-04754-7	[]	[]	-1	-1	-1
P	LI Z; LI H; WANG C; JIN L	Method for planning unmanned aerial vehicle            unknown environment path based on deep reinforcement            learning, involves building simulation environment of            unmanned aerial vehicle path planning, and performing            unmanned aerial vehicle route planning in actual            environment						NOVELTY - The method involves building a simulation environment of unmanned aerial vehicle path planning. A corresponding action space and reward function are designed according to flight characteristics of an unmanned vehicle and simulation environment. A proximal policy optimization (PPO) algorithm model is constructed with an actor-critic network. A self-attention module is added in the Actor-critic network structure. A route planning and obstacle avoidance training of the unmanned vehicle is performed based on the improved proximal policy optimization model. Unmanned aerial vehicle route planning is performed in the actual environment. USE - Method for planning path of an unmanned aerial vehicle i.e. four-rotor aerial vehicle, in an unknown environment based on deep reinforcement learning. ADVANTAGE - The method reduces the training time, improves the convergence speed of the network, and the network structure is stable, the parameter is small, and it is suitable for being deployed on small mobile device e.g. computing power computer, and improves autonomous path planning capability of the unmanned aerial vehicle in the unknown environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for planning unmanned aerial vehicle unknown environment path based on deep reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	JIANG J; MAO K; CUI H; WU Q; ZHU Q; ZHOU T; WANG L; HUANG Y	Method for performing combined search of unmanned            aerial vehicle and pan-tilt interference source based            on reinforcement learning, involves determining            vertical angle of time slots by tripod head until            unmanned vehicle flies to upper portion of interference            source						NOVELTY - The method involves dividing a time average of a search task into multiple time slots with the same length. The time slot is divided into a cloud stage and an unmanned aerial vehicle stage. A directional antenna is controlled by a tripod head. A vertical angle of the time slot tripod head is determined. A horizontal angle is determined as a flight direction. A fixed step length is determined to the flight direction to reach a position. The vertical angle of the time slots is determined by the tripod head until the unmanned vehicle flies to an upper portion of an interference source. USE - Method for performing combined search of an unmanned aerial vehicle and pan-tilt interference source based on reinforcement learning. ADVANTAGE - The method enables reducing multi-path effect, shadow fading and influence of complex ground environment, controlling the directional antenna by the tripod head to scan a measuring signal to avoid a use of a complex array antenna. DESCRIPTION Of DRAWING(S) - The drawing show a flow diagram illustrating a method for performing combined search of an unmanned aerial vehicle and pan-tilt interference source based on reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	WANG W; CAI C; YOU Z; LI X; SONG Q; ZHANG Y; FENG X; CHEN H; WEI W; CAI X	Method for generating underwater unmanned vehicle            track in complex dynamic environment, involves            utilizing multi-objective reinforcement learning            framework to adopt environment state in main scene,            unmanned underwater vehicle action and training samples            of reward function for training						NOVELTY - The method involves determining a main scene, a first sub-scene and a second sub-scene of track planning of an unmanned underwater vehicle (UUV). Environmental state of the two sub-scenes is input into a trained double-depth Q-network model with a priority experience playback mechanism to predict actions of the UUV in the two sub-scenes at the next moment. A trained multi-objective reinforcement learning framework based on integrated learning is utilized to predict two sub-scenes at the next moment. Predicted actions of the two sub-scenes at the next moment are selected through the trained multi-objective reinforcement learning framework based on ensemble learning with reference to the environmental state of the main scene, so as to prevent the underwater unmanned submersible from obstacles when there is a collision threat. USE - Method for generating UUV track in complex dynamic environment. Can also be used in military, civil and ocean fields. ADVANTAGE - The method enables utilizing a multi-objective reinforcement learning framework to judge the importance of two strategies of avoiding obstacles in different environmental states and going to a destination, thus improving the success rate of track planning and ensuring safe and efficient tracks. DETAILED DESCRIPTION - An optimal action is selected to realize planning of the track of the UUV. The multi-objective reinforcement learning framework is utilized to adopt environment state in the main scene, the UUV action and training samples of a reward function for training.INDEPENDENT CLAIMS are included for:(1) a system for generating UUV track in complex dynamic environment;(2) a computer readable storage medium comprises a set of instructions for generating UUV track in complex dynamic environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating underwater unmanned vehicle track in complex dynamic environment. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Zhang, Yuxiang; Gao, Bingzhao; Guo, Lulu; Guo, Hongyan; Chen, Hong	Adaptive Decision-Making for Automated Vehicles Under Roundabout Scenarios Using Optimization Embedded Reinforcement Learning	2021	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS		5526	5538	The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions. In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout. The promotion is the modified actor of the Actor-Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly. Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency. When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making. More notably, the modified actor matches human drivers' behaviors, macroscale behavior captures the human mind's jump, and medium-scale behaviors are preferentially adjusted through driving skills. To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network. In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time. To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly. The results show that the proposed method has high algorithm efficiency and better system performance.	Article	DEC 2021	10.1109/TNNLS.2020.3042981	[]	[]	-1	-1	-1
P	WANG H; LI Y; SUN X; LIU Q; ZHOU R; CAI Y	Navigation obstacle avoidance control method            combining path planning and reinforcement learning,            involves using improved dynamic window planning            algorithm to generate a plurality of path sampling            spaces, and obtaining optimal track of navigation            obstacle avoidance by using reinforcement            learning						NOVELTY - The method involves using a global planning algorithm to plan a passable path of an automatic driving vehicle according to a starting point and a target point in a driving task. Multiple path sampling spaces are generated by using an improved dynamic window planning algorithm (DWA). Each track is evaluated by using evaluation function. The evaluation function is provided with an included angle with the target position, whether there is an obstacle and a distance from the obstacle on the track. An optimal track of navigation obstacle avoidance is obtained by using reinforcement learning. USE - Navigation obstacle avoidance control method combining path planning and reinforcement learning. ADVANTAGE - The method combines the kinematics model of the vehicle with the interpretability, thus improving the safety in the vehicle navigation process. The method does not excessively depend on the model precision of the controlled object, thus greatly reducing the complexity of the solving process, and hence improving the real-time calculation efficiency of the automatic driving vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) Navigation obstacle control system combining path planning and reinforcement learning; and(2) Reinforcement learning network model applied to unmanned vehicle navigation obstacle-avoiding control system. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating of the navigation obstacle avoidance control method combining path planning and reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	LENG S; SI D; WANG L; ZHANG K	Migration learning based unmanned aerial vehicle            content caching decision method, involves putting            interactive data into buffer area to execute training            step to solve corresponding content caching decision            problem						NOVELTY - The method involves reducing content cache problem within service range by using a first unmanned aerial vehicle. Data is obtained from a buffer area to perform environment interaction process. Data training process is performed to establish a learning model to solve content caching problem. Cache decision problem is reduced by using an unmanned vehicle when the unmanned vehicle reaches adjacent area. Interactive data of a partial buffer area is transmitted to a second unmanned vehicle by using the first unmanned vehicle. Interactive data is processed and obtained by using the second unmanned vehicle. The interactive data is put into the buffer area to execute training step to solve corresponding content caching decision problem. The processed interactive data is stored in the buffer according to greedy strategy. USE - Migration learning based unmanned aerial vehicle content caching decision method. ADVANTAGE - The method enables determining cache content under constraint of self-caching capacity, optimizing the user to reduce content total time delay problem, considering network state dynamics, selecting enhanced learning algorithm to solve. The method enables transferring previous experience data of the unmanned machine to complete interaction, process, and reducing time to collect interactive data, calculating the resource, implementing faster convergence reinforcement learning algorithm, reducing actual problem, improving content decision efficiency. The method enables reducing content cache decision problem by using deep reinforcement learning (DRL) process, minimizing total content of the user to reduce service range acquisition time delay at same time, and satisfying user requirements and determining dynamic state. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a migration learning based unmanned aerial vehicle content caching decision method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Han, Yuqi	An adaptive obstacle avoidance algorithm of collaborative unmanned vehicles in dynamic scenes with monocular cameras	2021	OPTOELECTRONIC IMAGING AND MULTIMEDIA TECHNOLOGY VIII	Proceedings of SPIE			Monocular camera is widely used in robots and unmanned vehicles because it is low cost and easy to calibrate. However, the lacks of depth information hinders accurate estimation of the position and physical size of obstacles, which is specially important for a unmanned vehicle platform. To solve this problem, we propose a collaborative structure to accurately acquire the position of static or dynamic obstacles based on partial observations from multiple monocular cameras. After that, a reinforcement learning based obstacle avoidance algorithm is proposed for unmanned vehicles under unknown environments. Specifically, we discuss the influence of obstacles' moving orientations on the performance of obstacles adaptive avoidance. Simulation results verify the feasibility of the proposed approach.	Proceedings Paper	2021	10.1117/12.2602756	[]	[]	-1	-1	-1
J	Chen, Anthony Siming	Adaptive Optimal Control Via Reinforcement Learning : Theory and Its Application to Automotive Engine Systems	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Cui, Z.; Wang, Y.; Bian, N.; Chen, H.	Reward Machine Reinforcement Learning for Autonomous Highway Driving: An Unified Framework for Safety and Performance	2023	2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	6	Developing a safe and highly effective policy for autonomous vehicles (AVs) continues to pose a significant challenge in machine learning. In this study, we propose a novel reinforcement learning approach with reward machine. By tailoring the reward function to the specific needs of AVs in highway scenarios, we enable them to make more informed and efficient decisions. Our focus is on designing a reward function to formalize traffic rules, which is crucial for achieving safe and effective AV behavior on highways. To address this problem, we propose several innovative ideas that go beyond existing algorithmic techniques, specifically aimed at facilitating exploration and exploitation on different operations. To our knowledge, this is the first reinforcement learning algorithm that can integrate the safe distance with autonomous highway driving, aiming at the Vienna Convention on road traffic. Experimental results demonstrate the effectiveness of the proposed approach, which significantly improves AVs' safety and performance on highways.	Conference Paper	2023	10.1109/CVCI59596.2023.10397271	[]	[]	-1	-1	-1
B	Davila, Christian	On Achieving Acceptable Levels of Safety Risk in a Reinforcement Learning Environment	2023						Dissertation/Thesis	Jan 01 2023		[]	[]	-1	-1	-1
P	LIU G; LUO Y; SHENG J	Determining unmanned vehicle ethics behavior comprises obtaining barrier characteristic data on two lanes, inputting barrier characteristic data into ethics decision model, and determining ethics behavior of the unmanned vehicle						NOVELTY - Determining unmanned vehicle ethics behavior comprises obtaining barrier characteristic data on two lanes. The barrier characteristic data is inputted into an ethics decision model. An ethics behavior of an unmanned vehicle is determined. The ethics decision model is provided with multiple groups of the two lanes according to the barrier characteristic data. A statistical result of a forward excitation number of the barrier characteristic data is obtained. The ethics behavior of the unmanned vehicle is determined according to the statistical result of the positive excitation number corresponding to the obstacle feature data on the two lanes. USE - The method is useful for determining unmanned vehicle ethics behavior. ADVANTAGE - The method: realizes the ethics decision of deep reinforcement learning mode from bottom to top from the surrounding environment and people; and avoids partiality and discriminatory shortcomings in the process of human ethical decision-making. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:(1) system for determining unmanned vehicle ethics behavior; and(2) intelligent automobile. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for determining unmanned vehicle ethics behavior (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	GAO X; WAN K; YANG H; KANG P; GAN Z; BAI S; LI B	Unmanned aerial vehicle flight decision method            based on meta-reinforcement learning parallel training            algorithm, involves testing unmanned aerial vehicle            flight decision model based on meta reinforcement            learning algorithm, to evaluate flight decision            performance						NOVELTY - The method involves constructing an unmanned aerial vehicle flight control model. The unmanned aerial vehicle flight control rigid body model is used, in order to real-time resolve the position and posture information of the unmanned aerial vehicle. The state space of an unmanned vehicle flight decision is constructed according to a Markov decision process, an action space and a reward function. The multi-task experience pool is constructed for training sample data by a storing meta reinforcement learning algorithm. Different flight environment and unmanned aerial vehicle state are initialized, to realize unmanned aerial vehicle element reinforcement learning decision model by parallel training in multiple environments. The new flight environment and unmanned aerial vehicle state are initialized, and the unmanned aerial vehicle flight decision model is tested based on the meta reinforcement learning algorithm, to evaluate flight decision performance. USE - Method for unmanned aerial vehicle flight decision based on meta-reinforcement learning parallel training algorithm. ADVANTAGE - The method solves the problem that the generalization performance of the SAC algorithm is not enough by training the strategy in multiple environments, it can integrally optimize the unmanned aerial vehicle flight decision strategy, in the new environment, and it can be converged by the less step training. The generalization capability and universality of the strategy is effectively improved. DESCRIPTION Of DRAWING(S) - The diagram shows a schematic view illustrating the process for unmanned aerial vehicle flight decision based on meta-reinforcement learning parallel training algorithm. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	KOCHAR R	System for optimizing fuel and energy consumption            by automobile paint shop electro-deposition oven, has            fan operated based on corresponding optimal speed that            leads to optimization of energy consumption						NOVELTY - The system has a digital twin to simulate an electrodeposition (ED) oven for baking and curing a coated automobile component in an automobile paint shop. A reinforcement learning (RL) agent receives training using environment models derived from the digital twin of the ED oven. The RL agent learns control strategies through interaction with the environment models for generating policies, where each policy includes opening percentage of gas dampers and speed of fans e.g. recirculating fan. The fan is operated based on corresponding optimal speed that leads to optimization of energy consumption. USE - System for optimizing fuel and energy consumption by an automobile paint shop electro-deposition oven for baking and curing coated metal components. Uses include but are not limited to automotive, aerospace, and consumer goods. ADVANTAGE - The system optimizes fan speeds, damper positions, and fuel flow rates, and significantly reduces energy and fuel consumption. The system provides valuable insights into oven's operation, thus enabling continuous improvement and fine-tuning of a coating process. The system enhances efficiency, and drives innovation in process management. The system provides more responsive and efficient operation, and is capable of quickly addressing deviations from optimal conditions. The system continuously monitors and analyzes system's performance so as to predict maintenance needs, thus reducing unexpected downtimes and prolonging equipment's lifespan. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for optimizing fuel and energy consumption by an automobile paint shop electro-deposition oven.				[]	[]	-1	-1	-1
J	Sommer Obando, Hermann	Reinforcement learning framework for the self-learning suppression of clutch judder in automotive drive trains	2016						Dissertation/Thesis	Jan 01 2016		[]	[]	-1	-1	-1
C	Heimrath, Andreas; Froeschl, Joachim; Rezaei, Razieh; Lamprecht, Martin; Baumgarten, Uwe	Reflex-Augmented Reinforcement Learning for Operating Strategies in Automotive Electrical Energy Management	2019	2019 INTERNATIONAL CONFERENCE ON COMPUTING, ELECTRONICS & COMMUNICATIONS ENGINEERING (ICCECE)		62	67	This paper introduces reflex-augmented reinforcement learning (RARL) for operating strategies in automotive electrical energy management. RARL makes it possible to overcome the limitations of rule-based decision systems (RBDS) and to face the increasing complexity in a vehicle's electrical energy system. We suggest a deep Q-learning-based RARL approach for operating strategies determining the behavior of the electrical energy system. This also provides a general approach to realize reinforcement learning in cybernetic management systems for safety-critical applications. In a simulation-based study of more than 50 hours of driving with an extensive model of a vehicular electrical energy system, we show that RARL-based operating strategies fulfill the major requirements of a real vehicle. Compared to an RBDS, RARL requires less effort to design an operating strategy of this level of performance. Furthermore, we evaluate different variants of the biologically-inspired reflex of RARL enabling the application in safety-critical systems. Finally, we do not only provide an approach to replace the RBDS, but we also suggest that RARL is a key to integrate further sources of information into decision-making to enhance electrical energy management.	Proceedings Paper	2019	10.1109/iccece46942.2019.8941819	[]	[]	-1	-1	-1
B	Velamati, S.; Padmaja, V.	Policy Space Exploration for Linear Quadratic Regulator (LQR) Using Augmented Random Search (ARS) Algorithm	2020	Innovations in Electronics and Communication Engineering. Proceedings of the 8th ICIECE 2019. Lecture Notes in Networks and Systems (LNNS 107)		791	7	Considering the recent developments in embedded systems and automotive industry, it is quite evident that in very near future many application-based electronic devices will adapt the automation in its daily based activities. This automation will make the devices more powerful and will enhance its services. Currently, automation is the result of the algorithms which are pre-coded into the devices, but its future is the result of algorithms which enable devices to learn from environment in which it needs to work. It can be achieved utilizing the resources developed for a particular domain popularly known as reinforcement learning (RL). Main objective of this paper is to enable an agent to explore a policy for achieving a control of dynamic system such that it will be capable to find an optimal solution to solve the environment. It can be achieved using an algorithm known as augmented random search algorithm. To improve the training speed, we will use concept of multiprocessing and environment-specific customizations along with ARS algorithm.	Conference Paper	2020	10.1007/978-981-15-3172-9_74	[]	[]	-1	-1	-1
C	Xu, Risheng; Kuehl, Marvin; von Hasseln, Hermann; Nowotka, Dirk	Reducing Overall Path Latency in Automotive Logical Execution Time Scheduling via Reinforcement Learning	2023	PROCEEDINGS OF 31ST INTERNATIONAL CONFERENCE ON REAL-TIME NETWORKS AND SYSTEMS, RTNS 2023		212	223	The Logical Execution Time paradigm is a promising approach for achieving time-deterministic communication on multi-core CPUs. Task scheduling under this paradigm is a variant of the Multi-Row Facility Layout Problem, which is known to be NP-hard. In this paper, we propose using reinforcement learning to reduce the overall path latency among all scheduled runnables while adhering to other constraints, such as schedulability, load balance, and data contention control. The neural networks, also known as agents, are trained on a real-world automotive powertrain project. We compare two schedules generated by the agents to the current one and one produced by a genetic algorithm. The agent trained with the Proximal Policy Optimization algorithm demonstrated the best performance. Additionally, we investigate the generalization ability of the agents against software updates, and the results show that our agents are well-generalized.	Proceedings Paper	2023	10.1145/3575757.3593658	[]	[]	-1	-1	-1
B	Ferens, M.; Hortelano, D.; De miguel, I.; Duran barroso, R.J.; Aguado, J.C.; Ruiz, L.; Merayo, N.; Fernandez, P.; Lorenzo, R.M.; Abril, E.J.	Deep Reinforcement Learning Applied to Computation Offloading of Vehicular Applications: A Comparison	2022	2022 International Balkan Conference on Communications and Networking (BalkanCom)		31	5	An observable trend in recent years is the increasing demand for more complex services designed to be used with portable or automotive embedded devices. The problem is that these devices may lack the computational resources necessary to comply with service requirements. To solve it, cloud and edge computing, and in particular, the recent multi-access edge computing (MEC) paradigm, have been proposed. By offloading the processing of computational tasks from devices or vehicles to an external network, a larger amount of computational resources, placed in different locations, becomes accessible. However, this in turn creates the issue of deciding where each task should be executed. In this paper, we model the problem of computation offloading of vehicular applications to solve it using deep reinforcement learning (DRL) and evaluate the performance of different DRL algorithms and heuristics, showing the advantages of the former methods. Moreover, the impact of two scheduling techniques in computing nodes and two reward strategies in the DRL methods are also analyzed and discussed.	Conference Paper	2022	10.1109/BalkanCom55633.2022.9900545	[]	[]	-1	-1	-1
P	CHEN J; XU Y; GUO Y; LI Z; SUN J	Multi-vehicle cooperative detection system track            planning method based on multi-intelligent body            strengthening learning for robot intelligent and            control field, involves establishing multiple unmanned            vehicle detection track optimization problem according            to Markov process model						NOVELTY - The method involves constructing a Markov decision process model of multiple unmanned vehicle track planning facing a space-time signal field detection task. Multiple unmanned vehicle detection track optimization problem is established according to the Markov process model. Multiple unmanned vehicle detection track are obtained based on multi-intelligent body reinforcement learning planning strategy. USE - Multi-vehicle cooperative detection system track planning method based on multi-intelligent body strengthening learning for robot intelligent and control field. ADVANTAGE - The method solves the track planning problem in the space-time signal field task cooperatively detected by multiple unmanned vehicles under the condition that the unmanned vehicle dynamic model and the space time signal field distribution to be detected are completely unknown, so that the system obtains enough information in the shortest time and reaches the predetermined target. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of a multi-vehicle cooperative detection system track planning method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
S	Klarmann, Noah; Malmir, Mohammadhossein; Josifovski, Josip; Plorin, Daniel; Wagner, Matthias; Knoll, Alois C.	Optimizing Trajectories in Simulations with Deep Reinforcement Learning for Industrial Robots in Automotive Manufacturing	2021	ARTIFICIAL INTELLIGENCE FOR DIGITISING INDUSTRY: Applications	River Publishers Series in Communications	35	45	This paper outlines the concept of optimizing trajectories for industrial robots by applying deep reinforcement learning in simulations. An application of high technical relevance is considered in a production line of an autmotive manufacturer (AUDI AG), where industrial manipulators apply sealant on a car body to prevent water intrusion and hence corrosion. A methodology is proposed that supports the human expert in the tedious task of programming the robot trajectories. A deep reinforcement learning agent generates trajectories in virtual instances where the use case is simulated. By making use of the automatically generated trajectories, the expert's task is reduced to minor changes instead of developing the trajectory from scratch. This paper describes an appropriate way to model the agent in the context of Markov decision processes and gives an overview of the employed technologies. The use case outlined in this paper is a proof of concept to demonstrate the applicability of reinforcement learning for industrial robotics.	Article; Book Chapter	2021		[]	[]	-1	-1	-1
P	NOTZ D; WINDELEN J; SUNG G; WERLING M; PONNAIYAN V; MIRCHEVSKA B	Method for generating reinforcement learning-based            machine-learning model, involves providing reward            function for machine-learning model, and performing            reinforcement learning on machine-learning model using            reward function						NOVELTY - The method involves providing (110) a reward function for the machine-learning model. The reward function is based on an operating environment the vehicle is to be operated in. The operating environment is based on a country the vehicle is to be operated in, a legal framework under which the vehicle is to be operated and a vehicle model of the vehicle. The reinforcement learning is performed (120) on the machine-learning model using the reward function. The reinforcement learning is based on a simulation of the vehicle in a vehicular environment. The reward function comprises multiple reward function components categorized in multiple categories comprising categories of the group of legal constraints, safety preferences, speed preferences, comfort preferences and energy use preferences. USE - Method for generating reinforcement learning-based machine-learning model used in automotive development. ADVANTAGE - The more natural behavior of the vehicle increases comfort, safety of the vehicle, and satisfaction of customers. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for generating reinforcement learning-based machine-learning model;(2) a computer program product for generating reinforcement learning-based machine-learning model; and(3) an apparatus for generating reinforcement learning-based machine-learning model. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating a method for generating reinforcement learning-based machine-learning model.Step for providing reward function for the machine-learning model (110)Step for performing reinforcement learning on the machine-learning model using the reward function (120)				[]	[]	-1	-1	-1
C	Kerbel, Lindsey; Ayalew, Beshah; Ivanco, Andrej; Loiselle, Keith	Driver Assistance Eco-driving and Transmission Control with Deep Reinforcement Learning	2022	2022 AMERICAN CONTROL CONFERENCE (ACC)		2409	2415	With the growing need to reduce energy consumption and greenhouse gas emissions, Eco-driving strategies provide a significant opportunity for additional fuel savings on top of other technological solutions being pursued in the transportation sector. In this paper, a model-free deep reinforcement learning (RL) control agent is proposed for active Eco-driving assistance that trades-off fuel consumption against other driver-accommodation objectives, and learns optimal traction torque and transmission shifting policies from experience. The training scheme for the proposed RL agent uses an off-policy actor-critic architecture that iteratively does policy evaluation with a multi-step return and policy improvement with the maximum posteriori policy optimization algorithm for hybrid action spaces. The proposed Eco-driving RL agent is implemented on a commercial vehicle in car following traffic. It shows superior performance in minimizing fuel consumption compared to a baseline controller that has full knowledge of fuel-efficiency tables.	Proceedings Paper	2022		[]	[]	-1	-1	-1
B	Karunakaran, D.; Berrio, J.S.; Worrall, S.; Nebot, E.	Critical Concrete Scenario Generation using Scenario-Based Falsification	2022	2022 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE)		1	8	Autonomous vehicles have the potential to lower the accident rate when compared to human driving. Moreover, it has been the driving force of automated vehicles' rapid development over the last few years. In the higher Society of Automotive Engineers (SAE) automation level, the vehicle's and passengers' safety responsibility is transferred from the driver to the automated system, so thoroughly validating such a system is essential. Recently, academia and industry have embraced scenario-based evaluation as the complementary approach to road testing, reducing the overall testing effort required. It is essential to determine the system's flaws before deploying it on public roads as there is no safety driver to guarantee the reliability of such a system. This paper proposes a Reinforcement Learning (RL) based scenario-based falsification method to search for a high-risk scenario in a pedestrian crossing traffic situation. We define a scenario as risky when a system under test (SUT) does not satisfy the requirement. The reward function for our RL approach is based on Intel's Responsibility Sensitive Safety(RSS), Euclidean distance, and distance to a potential collision. Code and videos are available online at https://github.com/dkarunakaran/scenario_based_falsification.	Conference Paper	2022	10.1109/RASSE54974.2022.9989690	[]	[]	-1	-1	-1
P	QIU T; PU Z; LIU Z; YI J; CHANG H	Enhanced A-star algorithm and depth enhanced            learning based unmanned vehicle path planning method,            involves combining global planning path and enforcing            trained local planning network model to unmanned            vehicle navigation						NOVELTY - The method involves establishing an initializing grid cost map according to environment information. Environment is mapped by using SALM technology. Obstacle information is extracted. An obstacle type is calibrated to evaluate threat of a surrounding grid of the obstacle by a cost model. A global path is planned by using an enhanced A-start algorithm. A sliding window is designed based on a global path and a laser radar sensor performance. A local planning network is designed based on deep reinforcement learning process by using an Actor-Critic architecture. A global planning path is combined. A trained local planning network model is enforced to unmanned vehicle navigation. USE - Enhanced A-star algorithm and depth enhanced learning based unmanned vehicle path planning method. ADVANTAGE - The method enables combining knowledge and data process to quickly plan and obtain the optimal path, so that higher autonomy of the unmanned vehicles can be achieved. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an enhanced A-star algorithm and depth enhanced learning based unmanned vehicle path planning method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Bin Al Islam, S. M. A.; Aziz, H. M. Abdul; Wang, Hong; Young, Stanley E.	Minimizing energy consumption from connected signalized intersections by reinforcement learning	2018	2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC)	IEEE International Conference on Intelligent Transportation Systems-ITSC	1870	1875	Explicit energy minimization objectives are often discouraged in signal optimization algorithms due to its negative impact on mobility performance. One potential direction to solve this problem is to provide a balanced objective function to achieve desired mobility with minimized energy consumption. This research developed a reinforcement learning (RL) based control with reward functions considering energy and mobility in a joint manner-a penalty function is introduced for number of stops. Further, we proposed a clustering-based technique to make the state-space finite which is critical for a tractable implementation of the RL algorithm. We implemented the algorithm in a calibrated NG-SIM network within a traffic micro-simulator-PTV VISSIM. With sole focus on energy, we report 47% reduction in energy consumption when compared with existing signal control schemes, however causing a 65.6% increase in system travel time. In contrast, the control strategy focusing on energy minimization with penalty for stops yields 6.7% reduction in energy consumption with 27% increase in system travel time. The developed RL algorithm with a flexible penalty function in the reward will achieve desired energy goals for a network of signalized intersections without compromising on the mobility performance.Disclaimer: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan(http://energy.gov/downloads/doe-public-access-plan	Proceedings Paper	2018		[]	[]	-1	-1	-1
C	Ikai, Yusaku; Yamamoto, Hidehiko; Yamada, Takayoshi	Unit Layout Design Supporting System of Cell Assembly Machine Using Two Robots by Reinforcement Learning	2016	PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON ARTIFICIAL LIFE AND ROBOTICS (ICAROB 2016)		450	453	In this study, we explain the development of Design Supporting System for Cell Assembly Machine System (CAMS) which systemizes the decision of the unit layout that composes the assembly machine using two robots. CAMS uses Profit Sharing which is one of the Reinforcement Learning methods, determining each units layout. We apply CAMS to the assembly of the differential gear box in automotive parts to verify its validity.	Proceedings Paper	2016		[]	[]	-1	-1	-1
P	WANG H; CHANG P; HUANG J; HSU C	Real-time obstacle avoidance system has            contrastive learning unit that maximizes relationship            between environmental sensing signal and environmental            data through cross-modal representation contrast            learning mechanism agreement						NOVELTY - A real-time obstacle avoidance system, a real-time obstacle avoidance method and an unmanned vehicle with real-time obstacle avoidance function are disclosed. In the real-time obstacle avoidance system, the unmanned vehicle is in a specific environment and is set with an environmental sensing module, a data collection module and an operating module. The environmental sensing module is used to sense the specific environment to provide an environmental sensing signal. The data collection module is used to collect an environmental data related to the specific environment. The operating module is coupled to the environmental sensing module and the data collection module and used to receive the environmental sensing signal and the environmental data respectively and generate an autonomous navigation signal through a sim-to-real deep reinforcement learning mechanism. When the unmanned vehicle is driving in the specific environment, the unmanned vehicle avoids obstacles in real time according to the autonomous navigation signal.				[]	[]	-1	-1	-1
J	Shen, L.-H.; Feng, K.-T.; Lee, T.-S.; Lin, Y.-C.; Lin, S.-C.; Chang, C.-C.; Chang, S.-F.	AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities [arXiv]	2024	AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities [arXiv]				The requirement of wireless data demands is increasingly high as the sixth-generation (6G) technology evolves. Reconfigurable intelligent surface (RIS) is promisingly deemed to be one of 6G techniques for extending service coverage, reducing power consumption, and enhancing spectral efficiency. In this article, we have provided some fundamentals of RIS deployment in theory and hardware perspectives as well as utilization of artificial intelligence (AI) and machine learning. We conducted an intelligent deployment of RIS (i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs associated with an mmWave base station (BS) and a receiver. The RISs are deployed on the AGV with configured incident/reflection angles. While, both the mmWave BS and receiver are associated with an edge server monitoring downlink packets for obtaining system throughput. We have designed a federated multi-agent reinforcement learning scheme associated with several AGV-RIS agents and sub-agents per AGV-RIS consisting of the deployment of position, height, orientation and elevation angles. The experimental results presented the stationary measurement in different aspects and scenarios. The i-Dris can reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with comparably low complexity as well as rapid deployment, which outperforms the other existing works. At last, we highlight some opportunities and future issues in leveraging RIS-empowered wireless communication networks.	Preprint	2024		[]	[]	-1	-1	-1
B	[Anonymous]	2010 49th IEEE Conference on Decision and Control (CDC 2010)	2010	2010 49th IEEE Conference on Decision and Control (CDC 2010)				The following topics are dealt with: tall transfer functions; singular spectra and econometric modelling; distributed parameter systems; adaptive control; sliding mode control; decentralized control; delays systems; control theory; electric smart grids; robotics; identification; aerospace; sensor networks; switched systems; filtering and estimation; optimisation; autonomous systems; stochastic control; robust control; network analysis and control; hybrid systems; stability; fault diagnosis; computational methods; biological and biomedical systems; model-based systems engineering; Markov processes; dynamic resource allocation and optimization in networks; energy efficient infrastructures; game theory; air traffic control systems theory; Kalman filtering; agents and autonomous systems; visual servo control; variational methods; energy systems; output feedback and observers; communication networks; plug and play control; image analysis; biology; networked control systems; randomization in systems and control; stochastic systems; decentralised network analysis; nonlinear systems; Petri nets; queueing systems; subspace methods; system identification and estimation; integrated vehicle dynamics and control; sparsity and compressive sensing in system identification; fluid flow systems; quantum information and control; adaptive dynamic programming; reinforcement learning; feedback control; cooperative control; mean field stochastic systems and control; H-infinity control; networked nonlinear dynamical systems; event-based control; Lyapunov methods; process control; LMI; medicine; optimal control; stochastic hybrid systems; switched systems; positivity constraints; electrical and power systems; networked control systems; automotive and aerospace systems; linear systems; constrained control; predictive control; model-controller reduction; geometric control on nonlinear manifolds; robot-assisted exploration; scalar potential fields; automotive control; mobile sensor networks; cooperative control; large-scale systems; formal methods in systems and control; secure control systems; modelling and control of bio mechanical systems; stochastic model predictive control; nonlinear predictive control; stochastic systems; algebraic methods; geometric methods; autonomous robots; nonlinear system identification; control of communication systems; robust stability; large-scale interconnections; information dynamics in social and economic networks; discrete event systems; analysis and design of alarm systems; systems with uncertainty; iterative learning control; distributed control; switched systems stability; joint spectral radius; dynamic networked multi-agent systems; sampled-data control; randomized algorithms; and behavioral systems.	Conference Proceedings	2010		[]	[]	-1	-1	-1
C	Wang, Guohua; Siddhartha; Mishra, Kumar Vijay	STAP in Automotive MIMO Radar with Transmitter Scheduling	2020	2020 IEEE RADAR CONFERENCE (RADARCONF20)	IEEE Radar Conference			Automotive radars often employ multiple-input multiple-output (MIMO) array to attain high angular resolution with few antenna elements. The diversity gain is generally achieved by time-division multiplexing (TDM) during the transmission of frequency-modulated continuous-wave (FMCW) signals. However, TDM mode leads to longer pulse repetition intervals and, therefore, inherently and severely limits the maximum unambiguous Doppler velocity that a radar is able to detect. In this paper, we address the Doppler ambiguity problem in TDM MIMO automotive radars through a space-time adaptive processing (STAP) approach. A direct application of STAP may lead to a high antenna sidelobe level that hampers the detection performance. We mitigate this through optimal transmitter scheduling. We formulate the problem as combinatorial optimization and solve it via reinforcement learning. Numerical and experimental results demonstrate the efficacy of our method when compared with conventional techniques.	Proceedings Paper	2020		[]	[]	-1	-1	-1
J	Leyendecker, Lars; Schmitz, Markus; Zhou, Hans Aoyang; Samsonov, Vladimir; Rittstieg, Marius; Lutticke, Daniel	Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach	2022	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING		381	402	For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing. Yet it still struggles to find widespread implementation in industrial environments. Traditional programming has so far proven to be insufficient in providing the required flexibility and dexterity to solve complex assembly tasks. Research in robotic control using deep reinforcement learning (DRL) advances quickly, however, the transfer to real-world applications in industrial settings is lagging behind. In this study, we apply DRL for robotic motion control to a multi-body contact automotive assembly task. Our focus lies on optimizing the final performance on the real-world setup. We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability of the controller's performance. We train the agent exclusively in simulation and successfully perform the Sim-to-Real transfer. Finally, we evaluate the controller's performance and robustness on an industrial setup and reflect its adherence to the high standards of automotive production.	Article; Proceedings Paper	SEP 2022	10.1142/S1793351X22430024	[]	[]	-1	-1	-1
B	Thornton, C.E.; Howard, W.W.; Buehrer, R.M.	Online Learning-Based Waveform Selection for Improved Vehicle Recognition in Automotive Radar	2023	ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)		1	5	This paper describes important considerations and challenges associated with online reinforcement-learning based waveform selection for target identification in frequency modulated continuous wave (FMCW) automotive radar systems. We present a novel learning approach based on satisficing Thompson sampling, which quickly identifies a waveform expected to yield satisfactory classification performance. We demonstrate through measurement-level simulations that effective waveform selection strategies can be quickly learned, even in cases where the radar must select from a large catalog of candidate waveforms. The radar learns to adaptively select a bandwidth for appropriate resolution and a slow-time unimodular code for interference mitigation in the scene of interest by optimizing an expected classification metric.	Conference Paper	2023	10.1109/ICASSP49357.2023.10096969	[]	[]	-1	-1	-1
C	Pan, Xinlei; Seita, Daniel; Gao, Yang; Canny, John	Risk Averse Robust Adversarial Reinforcement Learning	2019	2019 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)	IEEE International Conference on Robotics and Automation ICRA	8522	8528	Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary. Supplementary materials are available at https://sites.google.com/view/rararl.	Proceedings Paper	2019	10.1109/icra.2019.8794293	[]	[]	-1	-1	-1
C	Leyendecker, Lars; Schmitz, Markus; Zhou, Hans Aoyang; Samsonov, Vladimir; Rittstieg, Marius; Lutticke, Daniel	Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach	2021	2021 FIFTH IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC 2021)		35	42	For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing. Yet it still struggles to find widespread implementation in industrial environments. Traditional programming has so far proven to be insufficient to provide the required flexibility and dexterity to solve complex assembly tasks. Although research in robotic control using deep reinforcement learning (DRL) advances quickly, the transfer to real-world applications in industrial settings is lagging behind. In this study, we apply DRL for robotic motion control at the use-case of a multi-body contact automotive assembly task and focus on optimizing the final performance on the real-world setup. We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability. We train an agent exclusively in simulation and successfully perform the Sim-to-Real transfer. Finally, we evaluate the controller's performance and robustness on an industrial setup and reflect its adherence to the high automotive production standards.	Proceedings Paper	2021	10.1109/IRC52146.2021.00012	[]	[]	-1	-1	-1
P	ZHANG R; GAO H; MA Q; FANG Q; ZHANG X; XU X; DU K; ZHOU X	Intelligent tracking control method for unmanned            vehicle, involves using apprentice learning method to            carry out lifting learning to obtain optimal strategy            according to initial strategy, and tracking and            controlling unmanned vehicle by using optimal            strategy						NOVELTY - The method involves collecting track data tracked by expert teaching. The track data is input into a deep neural network for deep learning to obtain initial strategy of expert teaching tracking. Apprentice learning method is used to carry out lifting learning to obtain optimal strategy according to the initial strategy. An unmanned vehicle is tracked and controlled by using the optimal strategy. An expert data set for expert teaching tracking is collected and stored. Data of the expert data set is generated by utilizing a built vehicle running high-simulation system to carry out multiple times of expert tracking driving, where the data of the expert data set comprises position data of the unmanned vehicle, attitude data, vehicle speed data, yaw angle data, direction angle data and vehicle distance lane line data. USE - Intelligent tracking control method for an unmanned vehicle. ADVANTAGE - The method enables improving driving stability of unmanned vehicle tracking driving, vehicle distance control and obstacle crossing to improve tracking driving performance. The method enables combining the artificial intelligence machine learning method and the apprentice learning method to obtain the initial strategy of the class person from the expert data by deep learning to finish reconstruction process of the reward function, thus obtaining the optimal strategy closer to the expert teaching tracking by reinforcement learning. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) an intelligent unmanned vehicle tracking control system;(2) a computer device comprising a memory stored with computer program for executing an intelligent tracking control method for an unmanned vehicle;(3) a computer readable storage medium storing computer program for executing an intelligent tracking control method for an unmanned vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an unmanned vehicle intelligent tracking control method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Orlowski, Mateusz; Skruch, Pawel	Multiagent Manuvering with the Use of Reinforcement Learning	2023	ELECTRONICS				This paper presents an approach for defining, solving, and implementing dynamic cooperative maneuver problems in autonomous driving applications. The formulation of these problems considers a set of cooperating cars as part of a multiagent system. A reinforcement learning technique is applied to find a suboptimal policy. The key role in the presented approach is a multiagent maneuvering environment that allows for the simulation of car-like agents within an obstacle-constrained space. Each of the agents is tasked with reaching an individual goal, defined as a specific location in space. The policy is determined during the reinforcement learning process to reach a predetermined goal position for each of the simulated cars. In the experiments, three road scenarios-zipper, bottleneck, and crossroads-were used. The trained policy has been successful in solving the cooperation problem in all scenarios and the positive effects of applying shared rewards between agents have been presented and studied. The results obtained in this work provide a window of opportunity for various automotive applications.	Article	APR 2023	10.3390/electronics12081894	[]	[]	-1	-1	-1
P	DI K; CHEN W; JIANG Y	Layered reinforcement learning method for            uncertainty auxiliary task under confrontational scene,            involves performing task execution stage of lower layer            strengthening learning for solving task execution            problem caused by influence of undetermined auxiliary            task to single agent						NOVELTY - The method involves performing task distribution stage of an upper layer reinforcement learning. Global environment information is obtained by an intelligent agent. Auxiliary task information is extracted. Important main task information based on the auxiliary task information. Task distribution strategy is learned by other intelligent bodies. Task execution stage of a lower layer strengthening learning is performed. A sub-environment is constructed by the intelligent agent according to a distribution result. The task execution sequence is learnt in the sub-environment. The specific action is finally executed, where the upper layer strengthening learning is used for solving the task distribution problem caused by the influence of the undetermined auxiliary task to the multi-agents system, and the lower layer strengthening learning is used for solving the task execution problem caused by the influence of the undetermined auxiliary task to the single agent. USE - Layered reinforcement learning method for uncertainty auxiliary task under confrontational scene for autonomous unmanned system such as unmanned aerial vehicle, unmanned vehicle and bionic robot used in military, anti-terrorism and commerce. ADVANTAGE - The method enables efficiently helping the multi-agents system to learn hierarchical reinforcement learning algorithm how to execute the undetermined auxiliary task by firstly learning the multi-axial undefined auxiliary task distribution strategy of the upper layer, and learning the single-agent un-danced auxiliary task execution strategy of a lower layer. The influence of the determined auxiliary task to the group and the individual are solved, which can effectively reduce the complexity of the problem. The layered reinforcement learning process aims at uncertainty auxiliary task under confrontational scene. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a task allocation result calculated by an upper layer reinforcement learning for full intelligent agent cluster (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Huang, Zhengrui; Wang, Shujie	Multilink and AUV-Assisted Energy-Efficient Underwater Emergency Communications	2023	IEEE INTERNET OF THINGS JOURNAL		8068	8082	Recent development in wireless communications has provided many reliable solutions to emergency response issues, especially in scenarios with dysfunctional or congested base stations. Prior studies on underwater emergency communications, however, remain understudied, which poses a need for combining the merits of different underwater communication links (UCLs) and the manipulability of unmanned vehicles. To realize energy-efficient underwater emergency communications, we develop a novel underwater emergency communication network (UECN) assisted by multiple links, including underwater light, acoustic, and radio-frequency links, and autonomous underwater vehicles (AUVs) for collecting and transmitting underwater emergency data. First, we determine the optimal emergency response mode for an underwater sensor node (USN) using greedy search and reinforcement learning (RL), so that isolated USNs (I-USNs) can be identified. Second, according to the distribution of I-USNs, we dispatch AUVs to assist I-USNs in data transmission, i.e., jointly optimizing the locations and controls of AUVs to minimize the time for data collection and underwater movement. Finally, an adaptive clustering-based multiobjective evolutionary algorithm is proposed to jointly optimize the number of AUVs and the transmit power of I-USNs, subject to a given set of constraints on transmit power, signal-to-interference-plus-noise ratios (SINRs), outage probabilities, and energy, achieving the best tradeoff between the maximum emergency response time (ERT) and the total energy consumption (EC). Simulation results indicate that our proposed approach outperforms benchmark schemes in terms of energy efficiency (EE), contributing to underwater emergency communications.	Article	MAY 1 2023	10.1109/JIOT.2022.3230322	[]	[]	-1	-1	-1
B	Chu, A.; Xie, X.; Hermann, C.M.; Stork, W.; Roth-Stielow, J.	Towards Predictive Lifetime-Oriented Temperature Control of Power Electronics in E-vehicles via Reinforcement Learning	2023	2023 IEEE International Conference on Big Data (BigData)		1667	76	As the electric vehicle (EV) industry rapidly grows, the reliability of EVs is an ongoing challenge to the automotive industry. Among them, due to the introduction of electric motors, the aging and degradation of power electronics in EVs have a direct influence on the overall system safety and may lead to total failure. Therefore, extending the lifetime of power electronics has been a focus in the last decades, where reducing the temperature swings is the key to achieving the goal. However, temperature optimization usually requires future information on power loads, which is not available in classic approaches. Therefore, in this paper, we propose a baseline framework for lifetime-oriented temperature control with reinforcement learning (RL). Focusing on long-term prediction, the framework integrates various physical modelings of EV modules (sensors, actuators, vehicle dynamics and temperature management) and utilizes real-time route information to train an agent for driving behavior prediction. Based on further interaction with the EV model, future temperature development can be estimated in advance, thus enabling better swing optimization. Experiments demonstrate the effectiveness of our approach and the lifetime of power electronics can be extended by up to 63% on a representative test route. Compared to the classic approach, our predictive temperature control shows impressive energy efficiency by achieving up to 2.8x power loss reduction with better lifetime optimization.	Conference Paper	2023	10.1109/BigData59044.2023.10386292	[]	[]	-1	-1	-1
P	ZHANG K; LI Z; HU J; DONG Y	Unmanned aerial vehicle automatic trajectory            planning method based on wireless optical            communication, involves using depth to reinforcement            learning plan flight path of unmanned aerial vehicle to            realize optimization target						NOVELTY - The method involves establishing an unmanned aerial vehicle auxiliary wireless optical communication channel model according to a visible light communication link channel model. Channel gain and channel capacity are calculated. Mobile space of an unmanned vehicle and a ground user is limited according to an actual application environment and the channel capacity. A minimum capacity threshold value of a communication link is determined. An optimization target is determined according to the mobile space of the unmanned vehicle. A depth to reinforcement learning plan flight path of unmanned aerial vehicle is used to realize optimization target. USE - Unmanned aerial vehicle automatic trajectory planning method based on wireless optical communication. ADVANTAGE - The method can dynamically adjust the degree of fairness of the unmanned aerial vehicle route planning, while improving the ground user communication capacity, at the same time, fully considering the communication fairness problem, and uses machine learning technology to optimize different target function to output the flight path planning route of the optimum. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer readable storage medium, has a set of instruction for unmanned aerial vehicle automatic trajectory planning method based on wireless optical communication. DESCRIPTION Of DRAWING(S) - The drawing shows an architecture diagram of the unmanned aerial vehicle automatic trajectory planning method based on wireless optical communication (Drawing includes non-English language text).				[]	[]	-1	-1	-1
B	Venayagamoorthy, G.K.	Tutorial CICA-T. Computing with intelligence for identification and control of nonlinear systems	2009	2009 IEEE Symposium on Computational Intelligence in Control and Automation. CICA 2009		1 pp.	1 pp.	System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.	Conference Paper	2009	10.1109/CICA.2009.4982774	[]	[]	-1	-1	-1
B	Biswas, A.; Anselma, P.G.; Emadi, A.	Real-Time Optimal Energy Management of Electrified Powertrains with Reinforcement Learning	2019	2019 IEEE Transportation Electrification Conference and Expo (ITEC)		6 pp.	6 pp.	Reinforcement learning (RL)algorithm is employed in solving energy management problem for electrified powertrain in real-world driving scenarios and the application process is streamlined. A near-global optimal control policy is articulated for the energy management system (EMS) using Q-learning algorithm which is real-time implementable. The core of the EMS is an updating optimal control policy in the form of a changing look-up table comprising near-global optimal action value function (Q-values) corresponding to all feasible state-action combinations. Using the updating control policy, the EMS can optimally decide power-split between electric machines (EMs) and internal combustion engine (ICE) in real-world driving situations.	Conference Paper	2019	10.1109/ITEC.2019.8790482	[]	[]	-1	-1	-1
J	Gaiselmann, Gerd; Altenburg, Stefan; Studer, Stefan; Peters, Steven	Deep reinforcement learning for gearshift controllers in automatic transmissions	2022	ARRAY				Control design for gearshifts in modern automotive automatic transmissions constitutes a challenging, time consuming task performed by highly trained experts. This is due to the fact that a variety of non-linear and partially observable systems need to be actuated, such that a comfortable shifting behavior is achieved within an sufficiently low shifting time. The presented approach leverages deep reinforcement learning (DRL) to control gear shifts, outperforming current state of the art controller performance. This requires formulating the shifting task as a Markov decision process by designing suitable action and observation spaces as well as a meaningful reward function. Due to the sample complexity of DRL methods, the control agents are trained in simulation and are subsequently transferred to a real transmission on a test bench. To successfully transfer DRL agents from simulation to reality, methods such as domain randomization and domain adaption leveraging evolutionary optimization are applied. To the best of the authors' knowledge, this work is the first to successfully apply DRL for the closed loop control of a real world automotive automatic transmission of realistic complexity.	Article	SEP 2022	10.1016/j.array.2022.100235	[]	[]	-1	-1	-1
P	GONG H; ZHEN Z; CAO H; YIN H; ZHAO Q	Method for establishing and controlling unmanned            aerial vehicle formation environment based on deep            reinforcement learning, involves performing instruction            conversion and reward function, training designed            environment, converting action output by controller            into specific instruction						NOVELTY - The method involves configuring a long aircraft to use a first-order speed holder and a second-order course holder automatic driving instrument model. The unmanned vehicle formation relative kinematics model is obtained according to small disturbance principle. A state space of a formation environment is designed. A staff maneuvering is configured as a library. An instruction conversion and a reward function are performed. A designed environment is trained. An action output by a controller is converted into a specific instruction and then input to a wing plane. USE - Method for establishing and controlling unmanned aerial vehicle formation environment based on deep reinforcement learning used in civil and military fields. ADVANTAGE - The unmanned aerial vehicle formation environment building and formation controller design based on deep reinforcement learning, the following aircraft can learn speed following the long machine and maintain the desired formation distance. The controller can make the wing plane strategy for self-learning learning, outputting the optimal action by the controller, at last, making the controller to control the speed and the course at the same time, wing plane the following long machine, and maintaining desired formation. In the actual application, the controller can form the corresponding aircraft instruction according to the characteristic of the unmanned aerial vehicles, so as to meet the requirement of unmanned aerial Vehicles precise formation control. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the unmanned aerial vehicle formation environment establishment and control based on deep reinforcement learning. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
B	Hoffmann, P.; Gorelik, K.; Ivanov, V.	Applicability Study of Model-Free Reinforcement Learning Towards an Automated Design Space Exploration Framework	2023	2023 IEEE Symposium Series on Computational Intelligence (SSCI)		525	32	Design space exploration is a crucial aspect of engineering and optimization, focused on identifying optimal design configurations for complex systems with a high degree of freedom in the actor set. It involves systematic exploration while considering various constraints and requirements. One of the key challenges in design space exploration is the need for a control strategy tailored to the particular design. In this context, reinforcement learning has emerged as a promising solution approach for automatically inferring control strategies, thereby enabling efficient comparison of different designs. However, learning the optimal policy is computationally intensive, as the agent determines the optimal policy through trial and error. The focus of this study is on learning a single strategy for a given design and scenario, enabling the evaluation of numerous architectures within a limited time frame. The study also highlights the importance of plant modeling considering different modeling approaches to effectively capture the system complexity on the example of vehicle dynamics. In addition, a careful selection of an appropriate hyperparameter set for the reinforcement learning algorithm is emphasized to improve the overall performance and optimization process.	Conference Paper	2023	10.1109/SSCI52147.2023.10371864	[]	[]	-1	-1	-1
B	Sharifi, Pouya	A Novel Reinforcement Learning-Optimization Approach for Integrating Wind Energy to Power System with Vehicle-To-Grid Technology	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Sharif, M. Z.; Azmi, W. H.; Ghazali, M. F.; Zawawi, N. N. M.; Ali, H. M.	Numerical and thermo-energy analysis of cycling in automotive air-conditioning operating with hybrid nanolubricants and R1234yf	2023	NUMERICAL HEAT TRANSFER PART A-APPLICATIONS		935	957	Studies on automotive air-conditioning (AAC) systems involving compressor on-off cycling are still limited. This study focuses on improving the cycling of the AAC system using hybrid nanolubricants and hydrofluoroolefin-1234yf refrigerant. A dynamic model for an AAC system with a thermostatic switch that controls the on-off compressor was developed. The model was built in MATLAB Simulink and based on the state-space model using the fundamental conservation principles at the condenser, evaporator, and expansion valve. The experimental data were used to calculate the AAC system pressure, compressor, heat transfer coefficient of the condenser-evaporator, and expansion valve setting. The validation of the experimental data and the predicted data by the simulation suggested that the dynamic model could predict the AAC system's performance within +/- 5% deviation. The AAC system operating with Al2O3-SiO2/PAG nanolubricants has a lower temperature cycling frequency than the AAC system with the original PAG lubricant, representing less energy consumed. In addition, the AAC system with hybrid nanolubricants was performed with lower power consumption and significantly higher cooling capacity than the original system. The present simulation confirmed the feasibility of hybrid nanolubricants for application in an AAC system with a thermostatic switch.	Article; Early Access		10.1080/10407782.2022.2155277	[]	[]	-1	-1	-1
P	WANG H; MENG Y; ZHANG X; MA Y; HAN L; ZHANG J; ZHANG W; XU S; JIN B	Smart training method for unmanned vehicles based            on simulation learning in virtual environment, involves            inputting optimal population individual and            low-dimensional simulation data set into neural network            to obtain driving behavior						NOVELTY - The method involves building a virtual scene. An unmanned vehicle motion model is established. The simulation is performed in the virtual scene built according to the unmanned vehicle motion model. The motion information of the unmanned vehicle is obtained as a simulation data set. The simulation data set is compressed, encoded and calculated to obtain a low-dimensional simulation data set. The low-dimensional simulation data set is fit to obtain the fitted value. The fitted value is input into the covariance matrix adaptation evolution strategy (CMA-ES) algorithm for reinforcement learning training to obtain the optimal population individual. The optimal population individual and low-dimensional simulation data set are input into the neural network to obtain driving behavior. USE - Smart training method for unmanned vehicles based on simulation learning in virtual environment. ADVANTAGE - The basis of the fitting value is optimized. The obtained optimal solution is added to the smart training process of the unmanned vehicle to guide the vehicle to learn good behavior. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a smart training method for unmanned vehicles based on simulation learning in virtual environment. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
C	Hamilton, Oliver K.; Breckon, Toby P.; Bai, Xuejiao; Kamata, Sei-ichiro	A FOREGROUND OBJECT BASED QUANTITATIVE ASSESSMENT OF DENSE STEREO APPROACHES FOR USE IN AUTOMOTIVE ENVIRONMENTS	2013	2013 20TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP 2013)	IEEE International Conference on Image Processing ICIP	418	422	There has been significant recent interest in stereo correspondence algorithms for use in the urban automotive environment [1, 2, 3]. In this paper we evaluate a range of dense stereo algorithms, using a unique evaluation criterion which provides quantitative analysis of accuracy against range, based on ground truth 3D annotated object information. The results show that while some algorithms provide greater scene coverage, we see little differentiation in accuracy over short ranges, while the converse is shown over longer ranges. Within our long range accuracy analysis we see a distinct separation of relative algorithm performance. This study extends prior work on dense stereo evaluation of Block Matching (BM)[4], Semi-Global Block Matching (SGBM)[5], No Maximal Disparity (NoMD)[6], Cross[7], Adaptive Dynamic Programming (AdptDP)[8], Efficient Large Scale (ELAS)[9], Minimum Spanning Forest (MSF)[10] and Non-Local Aggregation (NLA)[11] using a novel quantitative metric relative to object range.	Proceedings Paper	2013		[]	[]	-1	-1	-1
P	XU R; ZHAO Q	Unmanned vehicle trajectory planning method based            on improved pruning neural network, involves inputting            vehicle surrounding state information collected by            unmanned vehicle into small parameter quantity depth            neural network model						NOVELTY - The method involves designing a super-large parameter depth neural network model based on a CNN model. A deep neural network is trained by using a server cluster carrying multiple graphics processors. The deep neural networks are trained using a robust distillation method to perform robust compression on the trained deep network model to obtain a million-order parameter scale model. The vehicle surrounding state information collected by an unmanned vehicle is inputted into a small parameter quantity depth network model. The track planning process is performed by using the large parameter quantity network model according to the current vehicle surrounding status information. USE - Unmanned vehicle trajectory planning method based on improved pruning neural network. ADVANTAGE - The unmanned vehicle on the autonomous track planning stability is improved, reducing the possible presence of countermeasure attack, cannot avoid the sensor error, light change and other factors caused by the traditional vehicle-mounted neural network model cannot accurately predict the unstable condition of the path. The method uses a new neural network pruning method, by calculating the importance of the convolution filter in the hierarchical correlation propagation (LRP) algorithm neural network cutting the small convocation filter neural network function. The performance is better than the existing method. The robust distillation technology, and introduces confrontation learning, firstly recovering the robustness of the depth reinforcement learning the pruning strategy model after pruning. DESCRIPTION Of DRAWING(S) - The drawing shows a main flow chart of an unmanned vehicle trajectory planning method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Lu Chao; Lu Hongliang; Yu Yang; Wang Haoyang; Wu Shaobin	Autonomous Overtaking Decision Making System Based on Hierarchical Reinforcement Learning and Social Preferences	2022	China Journal of Highway and Transport		115	126	To describe the interaction between the host vehicle (HV) and the overtaken vehicle (OV) in overtaking scenarios, the psychological term 'social preference' was introduced to describe the longitudinal behavioral pattern of OV, and a data-driven classification method was adopted to extract the social preference and incorporate it into the design of reinforcement learning based autonomous overtaking decision-making system (RL-based AODMS). By analyzing social preferences of overtaken vehicles based on the realistic overtaking data, this method was able to generate proper overtaking decisions in response to different preferences. First, the state transition probability of an overtaken vehicle during the overtaking interaction was calculated from a large number of realistic overtaking data and divided into three types: altruistic, egoistic, and prosocial. Then, a semi-model-based advanced Q-learning algorithm was proposed to integrate three preferences into decision model training. Meanwhile, an online classifier of social preference was built to determine the real-time preference of the overtaken vehicle. Combined with our previous study on lane-changing controllers, a hierarchical reinforcement learning based autonomous overtaking system (HRL-based AOS) was constructed. Finally, the joint validation on autonomous overtaking was done by collected realistic data and simulation. The results showed that the AODMS considering social preferences can predict the social preference of OV in real time and make reasonable decisions in complicated overtaking scenarios. Meanwhile, compared to the traditional AOS without considering social preference, the complete AOS constructed in this study showed better comfort and stability. To conclude, this study innovatively operationalizes data-driven social preference in overtaking decision making and improving the adaptability and rationality of decisions, which will contribute to the development of safe and reliable AOS.	Article	2022		[]	[]	-1	-1	-1
P	ZHANG C; WANG S; HUANG Z	Unmanned lane change decision control method based            on near-end policy optimization algorithm involves            using trained unmanned lane-change decision control            model based on near-end policy optimization algorithm            to perform lane change to verify reliability of            unmanned lane-change decision control model						NOVELTY - The method involves constructing an unmanned lane-change decision control model based on a near-end policy optimization algorithm. An input sensor obtains a current environment information of an unmanned vehicle. A current environment information is inputted. A data buffer area of the unmanned lane-change decision control model is established. The unmanned lane-change decision control model is trained based on the near-end policy optimization algorithm. A speed of the unmanned vehicle and a target driving state are comprehensively considered. The unmanned lane-change decision control model is tested based on the near-end policy optimization algorithm. The trained unmanned lane-change decision control model is used based on the near-end policy optimization algorithm to perform a lane change in an environment to verify a reliability of the unmanned lane-change decision control model. USE - Unmanned lane change decision control method based on a near-end policy optimization algorithm. ADVANTAGE - The learning rate in the deep reinforcement learning difficult to set, at the same time, it improves the convergence speed of the model training. The unmanned vehicle interacts with the environment to obtain the current environment information, action space, the logarithm probability of the action space, the reward value obtained by executing the action at environment at the next time, and the back-closing mark is a terminal. The method uses PPO algorithm, which can better solve the problem that the learning rate of the deep reinforcement learning is difficult to be set, and it is possible to reduce the convergence time of the unmanned lane-free decision control model. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the unmanned lane change decision control method based on a near-end policy optimization algorithm (Drawing includes non-English language text).				[]	[]	-1	-1	-1
R	Kaichen, Ying	DATA of Fuzzy_Q-learning_Interaction_Controller_Design_for_Collaborative_Robot	2022	Figshare				DATA of Fuzzy_Q-learning_Interaction_Controller_Design_for_Collaborative_Robot Copyright: CC BY 4.0	Data set	2022-09-08	https://doi.org/10.6084/m9.figshare.20747437.v2	[]	[]	-1	-1	-1
J	Drechsler, M. Funk; Fiorentin, T. A.; Goellinger, H.	Actor-Critic Traction Control Based on Reinforcement Learning with Open-Loop Training	2021	MODELLING AND SIMULATION IN ENGINEERING				The use of actor-critic algorithms can improve the controllers currently implemented in automotive applications. This method combines reinforcement learning (RL) and neural networks to achieve the possibility of controlling nonlinear systems with real-time capabilities. Actor-critic algorithms were already applied with success in different controllers including autonomous driving, antilock braking system (ABS), and electronic stability control (ESC). However, in the current researches, virtual environments are implemented for the training process instead of using real plants to obtain the datasets. This limitation is given by trial and error methods implemented for the training process, which generates considerable risks in case the controller directly acts on the real plant. In this way, the present research proposes and evaluates an open-loop training process, which permits the data acquisition without the control interaction and an open-loop training of the neural networks. The performance of the trained controllers is evaluated by a design of experiments (DOE) to understand how it is affected by the generated dataset. The results present a successful application of open-loop training architecture. The controller can maintain the slip ratio under adequate levels during maneuvers on different floors, including grounds that are not applied during the training process. The actor neural network is also able to identify the different floors and change the acceleration profile according to the characteristics of each ground.	Article	DEC 7 2021	10.1155/2021/4641450	[]	[]	-1	-1	-1
P	ZHANG B; ZHANG P	Multi-target cooperative tracking control method            of multi-unmanned aerial vehicle based on Multi-Agent            PPO (MAPPO) algorithm used in automatic driving,            involves controlling unmanned aerial vehicle to work            according to each action control quantity						NOVELTY - The method involves modeling a multi-unmanned aerial vehicle target tracking process. An unmanned aerial vehicle six-degree freedom kinematics model, a moving target constant turning rate and a speed model are established. A state value function, an action value function and a reward return function are constructed. A depth neural network structure is constructed, where the strategy network structure and a value network structure are provided with strategy network structures and value network structures. A multi-target cooperative tracking controller of multiple unmanned aerial vehicles is obtained based on a MAPPO algorithm. A local observation state of each unmanned aerialvehicle is inputted into the multi-objective tracking controller. An action control quantity of the unmanned vehicle is obtained. The unmanned vehicle to work is controlled according to the action control amount. USE - Multi-unmanned aerial vehicle multi-target cooperative tracking control method based on depth-reinforcement learning MAPPO algorithm used in automatic driving, computer vision, medical diagnosis and robot control. ADVANTAGE - The method enables utilizing a distributed frame to reduce requirement of the unmanned aerial vehicle to communication and calculation ability, thus effectively solving problem that the traditional multi-target tracking method of multiple unmanned aerial vehicles is large in calculation amount, and avoiding single intelligent body depth reinforcement learning algorithm processing multiple intelligent body problem caused by centralized control dimension. DESCRIPTION Of DRAWING(S) - The drawing shows a training flowchart of the multi-unmanned aerial vehicle multi-target cooperative tracking control method based on the Multi-Agent PPO (MAPPO) algorithm. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
J	Luthra, S.; Mangla, S.K.; Kumar, S.; Garg, D.; Haleem, A.	Identify and prioritise the critical factors in implementing the reverse logistics practices: a case of Indian auto component manufacturer	2017	International Journal of Business and Systems Research		42	61	In recent years, reverse logistics (RL) practices have been perceived a great recognition among researchers/practitioners. In this paper, we intend to identify and prioritise the critical factors (CFs) in implementing the RL practices, from the industrial viewpoint. There are 13 CFs crucial in accomplishing the RL practices were recognised on the basis of critical review of literature and experts opinion. These finalised 13 CFs were then analysed to determine their priority by means of analytical hierarchy process (AHP) technique. The AHP technique assists in determining the relative importance of the identified RL implementation critical factors. The findings of the work may help managers to address the related issues in RL implementation. Inputs needed to carry out this research work are taken from an Indian automotive components manufacturing company. The results of the study may help researchers/practitioners to prioritise their efforts to implement RL practices in effective manner. In the end, sensitivity analysis is carried out to examine the proposed RL implementation CFs stability.	Journal Paper	2017		[]	[]	-1	-1	-1
B	El Mazgualdi, C.; Masrour, T.; El Hassani, I.; Khdoudi, A.	A deep reinforcement learning (DRL) decision model for heating process parameters identification in automotive glass manufacturing	2021	Artificial Intelligence and Industrial Applications. Smart Operation Management. Advances in Intelligent Systems and Computing (AISC 1193)		77	87	This research investigates the applicability of Deep Reinforcement Learning (DRL) to control the heating process parameters of tempered glass in industrial electric furnace. In most cases, these heating process parameters, also called recipe, are given by a trial and error procedure according to the expert process experience. In order to optimize the time and the cost associated to this recipe choice, we developed an offline decision system which consists of a deep reinforcement learning framework, using Deep Q-Network (DQN) algorithm, and a self-prediction artificial neural network model. This decision system is used to define the main heating parameters (the glass transfer speed and the zone temperature) based on the desired outlet temperature of the glass, and it has the capacity to improve its performance without further human assistance. The results show that our DQN algorithm converges to the optimal policy, and our decision system provides good recipe for the heating process with deviation not exceeding process limits. To our knowledge, it is the first demonstrated usage of deep reinforcement learning for heating process of tempered glass specifically and tempering process in general. This work also provides the basis for dealing with the problem of energy consumption during the tempering process in electric furnace.	Conference Paper	2021	10.1007/978-3-030-51186-9_6	[]	[]	-1	-1	-1
P	LIU Y; ZOU Y; FAN J; ZHANG X	Method for planning unknown environment search            path, involves planning collision-free path from            current position of unmanned vehicle at current moment            to target edge point according to grid map at current            moment and target edge point						NOVELTY - The method involves obtaining (S1) a vehicle pose data and laser radar data at a current moment, where the vehicle pose data is the pose data of an unmanned vehicle used to explore an unknown environment. The grid map at the previous moment is updated (S2) by using the vehicle pose data and laser radar data at the current moment. The grid map at the current moment is obtained. The edge point is detected (S3) on the grid map at the current moment to obtain multiple edge points by using an edge detection algorithm. The grid map at the current moment is taken (S4) as input. A potential target point is selected as a target edge point based on a reinforcement learning algorithm. A collision-free path from the current position of unmanned vehicle at the current moment to the target edge point is planned (S5) according to the grid map at the current moment and the target edge point. Determination is made to check whether the exploration of the unknown environment is completed. USE - Method for planning unknown environment search path. ADVANTAGE - The method selects target edge points through a reinforcement learning algorithm, can solve the situation of long planning path and early termination of exploration, improves the exploration efficiency of unknown environment, and can realize full exploration of unknown environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:a system for planning unknown environment search path;a device for planning unknown environment search path; anda computer readable storage medium has a set of instructions for planning unknown environment search path. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for planning unknown environment search path (Drawing includes non-English language text).S1Step for obtaining vehicle pose data and laser radar data at current momentS2Step for updating grid map at previous moment by using vehicle pose data and laser radar data at current momentS3Step for detecting edge point on grid map at current moment to obtain multiple edge points by using edge detection algorithmS4Step for taking grid map at current moment as inputS5Step for planning collision-free path from current position of unmanned vehicle at current moment to target edge point according to grid map at current moment and target edge point				[]	[]	-1	-1	-1
P	SU B; HE X	Wing parachute motion            proportional-integral-derivative self-adaptive            controlling method, involves selecting certain state            sub-space similar to current state of wing parachute to            synthesize proportional-integral-derivative control            parameter for finishing motion control of wing            parachute system						NOVELTY - The method involves selecting key characteristic factors that affect dynamic performance of wing parachute to generate state space and dividing the state space into multiple state sub-spaces. Effective proportional-integral-derivative (PID) parameter of each state sub-space is obtained by performing deep reinforcement learning process. Certain state sub-space similar to a current state of the wing parachute is selected to synthesize PID control parameter under a current flying state of the wing parachute by utilizing effective PID parameter for finishing motion control of a wing parachute system. USE - Wing parachute motion PID self-adaptive controlling method for training control strategies of various unmanned equipment e.g. unmanned aerial vehicle, unmanned vehicle and robot. ADVANTAGE - The method enables synthesizing the effective PID control parameter under the current flying state of the wing parachute according to deep reinforcement learning PID control strategy, so that the motion control of the wing parachute system can be completed in an efficient manner, thus realizing complementary advantages of the deep reinforcement learning process and a PID controller. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating a wing parachute motion proportional-integral-derivative self-adaptive controlling method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Turlej, Wojciech	High-Level Sensor Models for the Reinforcement Learning Driving Policy Training	2023	ELECTRONICS				Performance limitations of automotive sensors and the resulting perception errors are one of the most critical limitations in the design of Advanced Driver Assistance Systems and Autonomous Driving Systems. Ability to efficiently recreate realistic error patterns in a traffic simulation setup not only helps to ensure that such systems operate correctly in presence of perception errors, but also fulfills a key role in the training of Machine-Learning-based algorithms often utilized in them. This paper proposes a set of efficient sensor models for detecting road users and static road features. Applicability of the models is presented on an example of Reinforcement-Learning-based driving policy training. Experimental results demonstrate a significant increase in the policy's robustness to perception errors, alleviating issues caused by the differences between the virtual traffic environment used in the policy's training and the realistic conditions.	Article	JAN 2023	10.3390/electronics12010071	[]	[]	-1	-1	-1
J	Pang Ke; Zhang Yanxin; Yin Chenkun	A Decision-making Method for Self-driving Based on Deep Reinforcement Learning	2020	Journal of Physics: Conference Series		012025 (8 pp.)	012025 (8 pp.)	L5-level autonomous driving is the development trend of the future in the automotive industry, and the realization of autonomous driving through deep reinforcement learning algorithms are one of the research directions. Soft Actor-Critic the algorithm adds the maximum entropy term to the original deep reinforcement learning the objective function, and it shows great advantages in continuous control problems. Here, based on the open-source platform TORCS, this algorithm will be used to conduct automatic driving simulation experiments, design a reasonable reward function, add relevant constraints, use vehicle radar sensor information to make automatic driving decisions, and compare experiments with the Deep Deterministic Policy Gradient Algorithm. SAC can effectively extend training time, improve stability, and improve generalization ability.	Conference Paper; Journal Paper	2020	10.1088/1742-6596/1576/1/012025	[]	[]	-1	-1	-1
P	ZHANG Y; NING N; WU J; SHI H; JIN Z; ZHOU Y	Air-ground mobile network carrying fair            communication method, involves modeling unmanned aerial            vehicle energy carrying communication problem as            multi-target integer non-convex optimization problem of            fair throughput						NOVELTY - The method involves establishing (S1) an air-ground mobile network architecture based on multiple unmanned aerial vehicle and an intelligent reflecting surface. A wireless communication model is established (S2) by using the intelligent reflective surface to reconstruct a channel state between the unmanned aircraft and a ground user. A fair communication model for considering the communication efficiency and fairness of the user is established (S5). A judging matrix of fair throughput and energy consumption is constructed (S6) according to the user service quality level. The unmanned vehicle energy carrying communication problem is modeled (S7) as a multi-target integer non-convex optimization problem of fair bandwidth and unmanned machine residual energy maximization. A position of unmanned aircraft is updated (S8) by multi-intelligent housing depth reinforcement learning. USE - Air-ground mobile network carrying fair communication method for performing fair carrying communication based on unmanned aerial vehicle and intelligent reflecting surface under limited communication resource. ADVANTAGE - The network optimizes the phase of unmanned aerial vehicle position and intelligent reflecting surface based on multi-intelligent depth reinforcement learning provides fair communication for the ground user and wirelessly charges the unmanned aerial vehicles. The communication efficiency and fairness of the user establishing fair communication model, maximizing the system throughput under the premise of ensuring the user fairness. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating an unmanned aerial vehicle auxiliary communication. (Drawing includes non-English language text).S1Step for establishing an air-ground mobile network architecture based on multiple unmanned aerial vehicle and an intelligent reflecting surfaceS2Step for establishing a wireless communication model by using the intelligent reflective surface to reconstruct a channel state between the unmanned aircraft and a ground userS5Step for establishing a fair communication model for considering the communication efficiency and fairness of the userS6Step for constructing a judging matrix of fair throughput and energy consumption according to the user service quality levelS7Step for modeling the unmanned vehicle energy carrying communication problem as a multi-target integer non-convex optimization problem of fair bandwidth and unmanned machine residual energy maximizationS8Step for updating a position of unmanned aircraft by multi-intelligent body depth reinforcement learning				[]	[]	-1	-1	-1
J	Lian, Renzong; Tan, Huachun; Peng, Jiankun; Li, Qin; Wu, Yuankai	Cross-Type Transfer for Deep Reinforcement Learning Based Hybrid Electric Vehicle Energy Management	2020	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		8367	8380	Developing energy management strategies (EMSs) for different types of hybrid electric vehicles (HEVs) is a time-consuming and laborious task for automotive engineers. Experienced engineers can reduce the developing cycle by exploiting the commonalities between different types of HEV EMSs. Aiming at improving the efficiency of HEV EMSs development automatically, this paper proposes a transfer learning based method to achieve the cross-type knowledge transfer between deep reinforcement learning (DRL) based EMSs. Specifically, knowledge transfer among four significantly different types of HEVs is studied. We first use massive driving cycles to train a DRL-based EMS for Prius. Then the parameters of its deep neural networks, wherein the common knowledge of energy management is captured, are transferred into EMSs of a power-split bus, a series vehicle and a series-parallel bus. Finally, the parameters of 3 different HEV EMSs are fine-tuned in a small dataset. Simulation results indicate that, by incorporating transfer learning (TL) into DRL-based EMS for HEVs, an average 70% gap from the baseline in respect of convergence efficiency has been achieved. Our study also shows that TL can transfer knowledge between two HEVs that have significantly different structures. Overall, TL is conducive to boost the development process for HEV EMS.	Article	AUG 2020	10.1109/TVT.2020.2999263	[]	[]	-1	-1	-1
C	Schoeman, Marlize; Marchand, Renier; van Tonder, Johann; Jakobus, Ulrich; Aguilar, Andres; Longtin, Kitty; Vogel, Martin; Alwajeeh, Taha	New Features in Feko/WinProp 2019	2020	2020 INTERNATIONAL APPLIED COMPUTATIONAL ELECTROMAGNETICS SOCIETY SYMPOSIUM (2020 ACES-MONTEREY)				This paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp). These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.	Proceedings Paper	2020		[]	[]	-1	-1	-1
J	Wang, Yong; Tan, Huachun; Wu, Yuankai; Peng, Jiankun	Hybrid Electric Vehicle Energy Management With Computer Vision and Deep Reinforcement Learning	2021	IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS		3857	3868	Modern automotive systems have been equipped with a highly increasing number of onboard computer vision hardware and software, which are considered to be beneficial for achieving eco-driving. This article combines computer vision and deep reinforcement learning (DRL) to improve the fuel economy of hybrid electric vehicles. The proposed method is capable of autonomously learning the optimal control policy from visual inputs. The state-of-the-art convolutional neural networks-based object detection method is utilized to extract available visual information from onboard cameras. The detected visual information is used as a state input for a continuous DRL model to output energy management strategies. To evaluate the proposed method, we construct 100 km real city and highway driving cycles, in which visual information is incorporated. The results show that the DRL-based system with visual information consumes 4.3-8.8% less fuel compared with the one without visual information, and the proposed method achieves 96.5% fuel economy of the global optimum-dynamic programming.	Article	JUN 2021	10.1109/TII.2020.3015748	[]	[]	-1	-1	-1
J	Schoeman, Marlize; Marchand, Renier; van Tonder, Johann; Jakobus, Ulrich; Aguilar, Andres; Longtin, Kitty; Vogel, Martin; Alwajeeh, Taha	New Features in Feko and WinProp 2019	2020	APPLIED COMPUTATIONAL ELECTROMAGNETICS SOCIETY JOURNAL		1354	1355	paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp). These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.	Article	NOV 2020	10.47037/2020.ACES.J.351146	[]	[]	-1	-1	-1
J	Leng, Jinling; Wang, Xingyuan; Wu, Shiping; Jin, Chun; Tang, Meng; Liu, Rui; Vogl, Alexander; Liu, Huiyu	A multi-objective reinforcement learning approach for resequencing scheduling problems in automotive manufacturing systems	2023	INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH		5156	5175	This study investigated a multi-objective resequencing scheduling problem in the automotive manufacturing systems due to operational requirements on the color-batching of the paint shop and sequential requirements on the sequence adherence of an assembly shop. Resequencing cars as color-oriented batches reduced the costs of color changes and operational costs for paint shops. Also, assembly shops required paint shops to complete cars with fewer delays so that high sequence adherence with its demand was assured. Based on real-world applications, we investigated two contradictory objectives-color change costs and sequence tardiness-in a single-machine flowshop scheduling environment. A multi-objective-deep-Q-network algorithm was developed to determine the Pareto frontier. Reward shaping was designed to improve the convergence of the neural network. The 2D-folded-normal distribution was designed to sample the preference, which made the exploration and exploitation of the neural network more comprehensive and improved the training efficiency. Two experiments were conducted and showed that the proposed approach outperformed the meta-heuristic algorithm and the envelope Q-learning algorithm in solving time, performance, the convergence of the neural network, and the diversity of the Pareto frontier. Therefore, the proposed approach can be used in automotive paint shops to improve scheduling efficiency and reduce operational costs.	Article; Early Access		10.1080/00207543.2022.2098871	[]	[]	-1	-1	-1
P	COENEN O; SINYAVSKIY O; POLONICHKO V	Non-transitory computer-readable storage medium            for performing robotic control of exploration using            e.g. automotive robots for internet traffic routing            application, has instructions for adjusting            stochasticity level to effectuate control						NOVELTY - The medium has a set of instructions for operating spiking neuron in accordance with a reinforcement learning process. A performance metric of a reinforcement learning process is evaluated (1042) by comparing the performance metric to a performance value. A stochasticity level is adjusted to effectuate control of exploration based on the performance metric by decreasing in the stochasticity level when the performance metric is above the performance value by processors, where the performance metric comprises a distance measure between the present process outcome and the target outcome. USE - Non-transitory computer-readable storage medium for performing robotic control of an exploration by a spiking neuron using robotic devices e.g. automotive robots, military devices and surgical robots, for learning applications e.g. internet traffic routing application, visual, auditory or tactile recognition application, assisted air-traffic controller application, robust airplane controller application, adaptive electronic assistant application for mobile and adaptive toys for humans or animals. Can also be used for signal processing systems e.g. machine vision systems, pattern detection and pattern recognition systems, object classification systems, signal filtering systems, data segmentation systems, data compression systems, data mining systems, optimization and scheduling systems and complex mapping systems. ADVANTAGE - The medium enables providing subsequent lowering of stochasticity to reduce energy use and spiking noise when target performance is attained and adaptive stochasticity control increases learning speed of a network. The medium allows a robotic controller to employ a lower-performance version of controller hardware to reduce controller cost, size and energy use. The medium enables combining sensory and state encoding in high dimension by a multitude of linear and nonlinear kernel functions and operators with a reinforcement learning controller, which can efficiently learn to make use of distributed and parallel input signals to construct a controller realizing an optimization of a performance function. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a computer-implemented method for reducing energy use by a computerized spiking neuron network apparatus(2) a reconfigurable robotic controller apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating reinforcement signal generation.Method for performing control of exploration by spiking neuron (1040)Step for evaluating performance metric of reinforcement learning process (1042)Step for comparing performance measure to criterion (1044)Step for generating reinforcement signal mode (1046)Step for generating another reinforcement signal mode (1048)				[]	[]	-1	-1	-1
J	Loffredo, Alberto; May, Marvin Carl; Schaefer, Louis; Matta, Andrea; Lanza, Gisela	Reinforcement learning for energy-efficient control of parallel and identical machines	2023	CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY		91	103	Nowadays, the growing interest in industry for enhancing the sustainability of manufacturing processes is becoming a major trend. Energy consumption can be lowered by controlling machine states with energy -efficient control policies that switch off/on the device. Recent studies have shown that Reinforcement Learning algorithms can effectively control manufacturing systems without the requirement of prior knowledge about system parameters. This is a significant factor since full information on system dynamics is difficult to obtain in real-world applications. This work proposes a new Reinforcement Learning-based algorithm to apply energy-efficient control strategies to a single workstation consisting of identical parallel machines. The model goal is to achieve the optimum trade-off between system productivity and energy demand without relying on full knowledge of the system dynamics. Numerical experiments confirm ef-fectiveness, applicability, and generality of the proposed approach, even when applied to a real-world in-dustrial system from the automotive sector.& COPY; 2023 CIRP.	Article	SEP 2023	10.1016/j.cirpj.2023.05.007	[]	[]	-1	-1	-1
B	Zhu, Zhaoxuan	Reinforcement Learning in Eco-Driving for Connected and Automated Vehicles	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J	XUE Jinlin; ZHANG Weigong; GONG Zongyang	Velocity Tracking Control Based on Reinforcement Learning Neural Network	2007	Measurement & Control Technology		36	38	A new approach for tracking vehicle speeds by robotic driver during emission testing is presented. Based on neural network and combined with adaptive capability of reinforcement learning, it can execute velocity tracking control through on-line learning of neural network. Using the data obtained from a teat vehicle, a neural network model of automotive for velocity tracking is developed at first. A neural network controller is designed based on reinforcement learning neural network framework to achieve adaptive control of velocity tracking. During simulation study, the velocity control neural network model is used to train primary controller rather than the actual test vehicle, and the developed and well-trained self-learning neural network controller is applied to velocity tracking control. Results show that the developed neural network controller has good performance of velocity tracking, and control efficacy is obvious.	Article	2007		[]	[]	-1	-1	-1
C	Crincoli, Giuseppe; Fierro, Fabiana; Iadarola, Giacomo; La Rocca, Piera Elena; Martinelli, Fabio; Mercaldo, Francesco; Santone, Antonella	A Method for Road Accident Prevention in Smart Cities based on Deep Reinforcement Learning	2022	SECRYPT : PROCEEDINGS OF THE 19TH INTERNATIONAL CONFERENCE ON SECURITY AND CRYPTOGRAPHY		513	518	Autonomous vehicles play a key role in the smart cities vision: they bring benefits and innovation, but also safety threats, especially if they suffer from vulnerabilities that can be easily exploited. In this paper, we propose a method that exploits Deep Reinforcement Learning to train autonomous vehicles with the purpose of preventing road accidents. The experimental results demonstrated that a single self-driving vehicle can help to optimise traffic flows and mitigate the number of collisions that would occur if there were no self-driving vehicles in the road network. Our results proved that the training progress is able to reduce the collision frequency from 1 collision every 32.40 hours to 1 collision every 53.55 hours, demonstrating the effectiveness of deep reinforcement learning in road accident prevention in smart cities.	Proceedings Paper	2022	10.5220/0011146500003283	[]	[]	-1	-1	-1
P	ZHOU Y; WANG Z; LU P; GENG S; HE B	Algorithm based on a digital twin-based robot            large-scale long-term inspection task coverage            strategy, includes constructing virtual scene that            corresponds to real scene through digital twin            technology, and completing navigation task of robot            from current position to selected local target            point						NOVELTY - The algorithm includes constructing a virtual scene that corresponds to the real scene one-to-one through digital twin technology. The surrounding environment information is obtained through the robot sensor in the virtual scene and a local map is built. A global target point in an unexplored area is regularly selected according to the local map, and a local target point is selected on the path of the global target point. The deep reinforcement learning algorithm is used to complete the navigation task of the robot from the current position to the selected local target point. The robot completes the inspection and coverage task in the virtual scene. The algorithm is applied to the robot in the real scene, and the data synchronization is completed in the virtual scene and the real scene, so that the robot completes the inspection and coverage task in a certain area, and specifies the target point of the inspection in the virtual scene and guides the robot to go directly. USE - Algorithm based on a digital twin-based robot's large-scale long-term inspection task coverage strategy. ADVANTAGE - The algorithm combines the digital twinning and depth reinforcement learning training the unmanned vehicle to realize autonomous navigation algorithm, at the same time, the information obtained by unmanned vehicle assembling sensor is used for updating the virtual scene, and the position and speed of unmanned vehicle in real scene can be monitored online in real time through virtual scene. The robot finishes the high-efficiency exploration of certain area without prior map. The local navigation based on depth reinforcement training algorithm avoids dynamic pedestrian and vehicle, based on the working mode of virtual-real fusion of digital twinning, thus ensures that the exploration is more efficient. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of an algorithm based on a digital twin-based robot's large-scale long-term inspection task coverage strategy. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Cao, Yushi; Zheng, Yan; Lin, Shang-Wei; Liu, Yang; Teo, Yon Shin; Toh, Yuxuan; Adiga, Vinay Vishnumurthy	Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning	2021	2021 36TH IEEE/ACM INTERNATIONAL CONFERENCE ON AUTOMATED SOFTWARE ENGINEERING ASE 2021		1151	1155	Discovering the underlying structure of HMI software efficiently and sufficiently for the purpose of testing without any prior knowledge on the software logic remains a difficult problem. The key challenge lies in the complexity of the HMI software and the high variance in the coverage of current methods. In this paper, we introduce the PathFinder, an effective and automatic HMI software exploration framework. PathFinder adopts a curiosity-based reinforcement learning framework to choose actions that lead to the discovery of more unknown states. Additionally, PathFinder progressively builds a navigation model during the exploration to further improve state coverage. We have conducted experiments on both simulations and real-world HMI software testing environment, which comprise a full tool chain of automobile dashboard instrument cluster. The exploration coverage outperforms manual and fuzzing methods which are the current industrial standards.	Proceedings Paper	2021	10.1109/ASE51524.2021.9678703	[]	[]	-1	-1	-1
B	Sanghun Yun; Jahyun Kim; Hyogon Kim	Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks	2019	2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall). Proceedings		6 pp.	6 pp.	In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications. For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs). However, the in- vehicle network environment can be starkly different from the Internet where TCP has been optimized. The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments. In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.	Conference Paper	2019	10.1109/VTCFall.2019.8891225	[]	[]	-1	-1	-1
J	Yang, Bin; Wu, Bin; You, Yuwen; Guo, Chunmei; Qiao, Liang; Lv, Zhihan	Edge intelligence based digital twins for internet of autonomous unmanned vehicles	2022	SOFTWARE-PRACTICE & EXPERIENCE				It aims to explore the efficient and reliable wireless transmission and cooperative communication mechanism of Internet of Vehicles (IoV) based on edge intelligence technology. It first proposes an intelligent network architecture for IoV services by combining network slicing and deep learning (DL) technology, and then began to study the key technologies needed to achieve the architecture. It designs the cooperative control mechanism of unmanned vehicle network based on the full study of wireless resource allocation algorithm from the micro level. Second, in order to improve the safety of vehicle driving, deep reinforcement learning is used to configure the wireless resources of IoV network to meet the needs of various IoV services. The research results show that the accuracy rate of the improved AlexNet algorithm model can reach 99.64%, the accuracy rate is more than 80%, the data transmission delay is less than 0.02 ms, and the data transmission packet loss rate is less than 0.05. The algorithm model has practical application value for solving the data transmission related problems of vehicular internet communication, providing an important reference value for the intelligent development of unmanned vehicle internet.	Article; Early Access		10.1002/spe.3080	[]	[]	-1	-1	-1
B	Durr, S.; Lamprecht, R.; Kauffmann, M.; Huber, M.F.	Reinforcement Learning based Optimization of Bayesian Networks for Generating Feasible Vehicle Configuration Suggestions	2021	2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)		16	22	A promising method in the automotive industry to anticipate future customer demands is the concept of planned orders. Due to multi-variant products, changing customer demands, and dynamic environments the process of generating planned orders is challenging. This paper introduces an approach using graphical models to generate planned order suggestions in a multi-variant order management process. Bayesian networks are modelled by learning the structure from different data sources, which enable the possibility to directly sample configuration suggestions. To find an optimized graph structure, a method using hierarchical correlation clustering and reinforcement learning is applied, taking into account technical and sales-operated feasibility constraints. The method has high potential in practical usage and is evaluated by a realworld use case of the Dr. Ing. h.c. F. Porsche AG.	Conference Paper	2021	10.1109/CASE49439.2021.9551428	[]	[]	-1	-1	-1
B	Johansson, Tobias; Morteza; Devdatt	Machine Learning Based Methods for Virtual Validation of Autonomous Driving	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
C	Starace, Luigi Libero Lucio; Romdhana, Andrea; Di Martino, Sergio	GenRL at the SBST 2022 Tool Competition	2022	15TH SEARCH-BASED SOFTWARE TESTING WORKSHOP (SBST 2022)		49	50	GenRL is a Deep Reinforcement Learning-based tool designed to generate test cases for Lane-Keeping Assist Systems. In this paper, we briefly presents GenRL, and summarize the results of its participation in the Cyber-Physical Systems (CPS) tool competition at SBST 2022.	Proceedings Paper	2022	10.1145/3526072.3527533	[]	[]	-1	-1	-1
J	Xinlei Pan; Seita, D.; Yang Gao; Canny, J.	Risk Averse Robust Adversarial Reinforcement Learning [arXiv]	2019	arXiv		7 pp.	7 pp.	Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary.	Journal Paper	31 March 2019		[]	[]	-1	-1	-1
C	Rahman, S. M. Mizanoor	Performance Metrics for Human-Robot Collaboration: An Automotive Manufacturing Case	2021	2021 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR AUTOMOTIVE (METROAUTOMOTIVE)		260	265	A human-robot collaborative system in the form of a power and skill assist robotic system was developed where a human and a robot could collaborate to perform object manipulation for targeted assembly tasks in automotive manufacturing. We assumed such assembly tasks as the representative assembly tasks in automotive manufacturing. We reflected human's weight perception in the dynamics and control of the power and skill assist system following a psychophysical method using a reinforcement learning scheme. We recruited 20 human subjects who separately performed assembly tasks with the system in human-robot collaboration (HRC). We then observed the collaborative assembly tasks, conducted extensive literature reviews, reviewed our previous and ongoing related works and brainstormed with the subjects and other relevant researchers, and then proposed HRC performance assessment metrics and methods for collaborative automotive manufacturing. The proposed metrics comprised of assessment criteria and methods related to both human-robot interaction (HRI) and manufacturing performance. We then verified the proposed performance metrics in pilot studies in the laboratory environment using the same collaborative system and subjects. The verification results proved the effectiveness of the assessment metrics and methods in terms of usability, practicability and reliability. We then proposed to apply classification and regression type machine learning approaches under supervised and reinforcement learning setups to learn different classes and decision-making rules respectively regarding HRC performance. The proposed performance metrics and methods can serve as the preliminary efforts towards developing comprehensive assessment metrics for HRC in general and for human-robot collaborative automotive manufacturing in particular.	Proceedings Paper	2021	10.1109/MetroAutomotive50197.2021.9502881	[]	[]	-1	-1	-1
P	YANG K; YU Q; LI Y; HU J	Multi-unmanned autonomous navigation and task            allocation algorithm of wireless self-powered            communication network, involves solving optimization            problem according to asynchronous multi-intelligent            depth reinforcement learning algorithm						NOVELTY - Multi-unmanned autonomous navigation and task allocation algorithm of wireless self-powered communication network comprises (i) determining network model, communication mode and channel model, (ii) modeling the downlink wireless power transmission and uplink wireless information transmission, and determining the optimized target expression and the constraint condition thereof, (iii) analyzing and optimizing the problem, where the optimization problem modeling is Markov process, (iv) determining the network communication protocol and the unmanned aerial vehicle flight decision model, and (v) defining the neural network input state of each unmanned aerial Vehicle, output action, reward function and input and output of the public neural network, and solving optimization problem according to the asynchronous multi-intelligent deep reinforcement learning algorithm. USE - Multi-unmanned autonomous navigation and task allocation algorithm of wireless self-powered communication network of an unmanned aerial vehicle i.e. aerial base station. ADVANTAGE - The multi-unmanned aerial vehicle autonomous navigation and task allocation algorithm combines the flight paths of multiple unmanned aerial vehicles in the wireless self-powered communication network, and jointly designs user scheduling, flight path of each unmanned aerial vehicle, flight speed, communication mode, and task distribution and track optimization between the unmanned vehicle, finishing the navigation task without collision of unmanned vehicle in the predetermined flight time, and furthest improving the system user average uplink transmission data quantity. The optimization problem modeling is Markov process, and claims an asynchronous multi-intelligent body depth enhanced learning algorithm based on shared neural network, solving the optimization problem, gradually training neural network and finally realizing the target of maximum system uplink total data quantity, thus solving the problem. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the multi-unmanned autonomous navigation and task allocation algorithm of wireless self-powered communication network (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Kaviani, Mohamad Amin; Tavana, Madjid; Kumar, Anil; Michnik, Jerzy; Niknam, Raziyeh; Campos, Elaine Aparecida Regiani de	An integrated framework for evaluating the barriers to successful implementation of reverse logistics in the automotive industry	2020	JOURNAL OF CLEANER PRODUCTION				Reverse logistics (RL) strategy can have a positive impact on productivity, and the diminishing resources, along with the strict environmental regulations, have strengthened the need for this strategy. The purpose of this study is to develop an integrated framework for identifying: (1) the critical barriers to the successful implementation of RL in the automotive industry; (2) the importance and implementation priorities of these barriers; and (3) the causal relations among them. The proposed framework is composed of the Delphi method to identify the most relevant barriers, the best-worst method (BWM) to determine their importance, and the weighted influence non-linear gauge system (WINGS) to analyze their causal relationships. The proposed framework is applied to a case study in the automotive industry. The results indicate the economic barriers are the most important, and the knowledge barriers are the least important barriers to the successful implementation of RL in the automotive industry. (c) 2020 Elsevier Ltd. All rights reserved.	Article	NOV 1 2020	10.1016/j.jclepro.2020.122714	[]	[]	-1	-1	-1
C	Hao, Shengang; Zheng, Jun; Yang, Jie; Ni, Ziwei; Zhang, Quanxin; Zhang, Li	A Multi-agent Deep Reinforcement Learning-Based Collaborative Willingness Network for Automobile Maintenance Service	2022	APPLIED CRYPTOGRAPHY AND NETWORK SECURITY WORKSHOPS, ACNS 2022	Lecture Notes in Computer Science	84	103	With the growth of maintenance market scale of automobile manufacturing enterprises, simple information technology is not enough to solve the problem of uneven resource allocation and low customer satisfaction in maintenance chain services. To solve this problem, this paper abstracts the automotive maintenance collaborative service into a multi-agent collaborative model based on the decentralized partially observable Markov decision progress (Dec-POMDP). Based on this model, a multi-agent deep reinforcement learning algorithm based on collaborative willingness network (CWN-MADRL) is presented. The algorithm uses a value decomposition based MADRL framework, adds a collaborative willingness network based on the original action value network of the agent, and uses the attention mechanism to improve the impact of the collaboration between agents on the action decision-making, while saving computing resources. The evaluation results show that, our CWN-MADRL algorithm can converge quickly, learn effective task recommendation strategies, and achieve better system performance compared with other benchmark algorithms.	Proceedings Paper	2022	10.1007/978-3-031-16815-4_6	[]	[]	-1	-1	-1
P	ZHENG L; ZHANG P	Method for facilitating multi-machine            collaboration simultaneous localization and mapping            (SLAM) for robot, involves transmitting pose            information and actual distance information by robot,            and performing rear-end optimization of SLAM tracks by            using deep deterministic policy gradient (TD3)            algorithm						NOVELTY - The method involves running an ORB-SLAM2 program on a robot. The images are obtained through a camera to perform pose estimation to obtain a multi-machine initial motion track pose diagram. Training process is performed by using a depth reinforcement learning algorithm to optimize the track to obtain a more accurate pose based on the obtained robot motion track pose diagram. An active sensing strategy introduced and the poses of multiple machines are optimized on the basis of a reinforcement learning algorithm. A corresponding robot is selected to optimize pose information by a TD3 algorithm according to a real-time SLAM estimated probability value. The pose information and the actual distance information are mutually transmitted by the robots. The TD3 algorithm is used for performing rear-end optimization of SLAM tracks, so that the effect of eliminating accumulated errors is achieved. USE - Method for facilitating multi-machine collaboration SLAM i.e. group mobile robot based on active deep reinforcement learning for use in the fields of unmanned vehicle and robot. ADVANTAGE - The TD3 algorithm is used for carrying out the rear-end optimization of the SLAM track so as to achieve the effect of eliminating accumulated errors. The method effectively eliminates the error accumulation condition in the SLAM system, improves the positioning and mapping precision of the SLAM, has no restriction of loop, and increases the robustness of the SLAM system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of the multi-machine collaborative visual SLAM system. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	YANG J; ZHU K	Scheduling method for charging drones to charge            task drones in the air, involves charging UAV to obtain            current state by training obtained network decision,            and charging task UAV according to charging scheduling            instruction output by actor network						NOVELTY - The method involves determining a charging mode according to charging requirement information of a task unmanned aerial vehicle. Charging scheduling problem model is established according to the charging requirement and a task type of the task unmanned air vehicle. A charging scheduling strategy of multiple charging unmanned air vehicles is optimized based on the charging scheduling task. A task of the shortest time is used as a target. The charging scheduling strategies of the charging unmanned aerial vehicles are learned based on deep reinforcement learning. The unmanned air vehicle is charged according to a charging scheduling instruction output by an actor network. USE - Scheduling method for charging drones to charge task drones in the air ADVANTAGE - The method enables realizing shortest task completion time of the task unmanned aerial vehicle, and optimizing the charging scheduling strategy of the vehicle charging and charging unmanned aerial vehicles. The method allows a training actor network to realize charging unmanned vehicle scheduling strategy optimization, based on depth reinforcement learning multi-intelligent body reinforcement learning charging the vehicle according to the optimized strategy and the current environment to make decision, thus realizing minimizing the task time, minimizing the energy consumption of the target, with charging time and charging place can be flexibly deployed, and minimizing the effect of distribution. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a scheduling method for charging drones to charge task drones in the air. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Lin, M; Malec, J	Timing analysis of RL programs	1998	REAL TIME PROGRAMMING 1997: (WRTP 97)		99	104	Predictability is a very important feature of complex real-time systems. For a complex system involving a number of subsystems, the timing properties of each embedded system play an important role since the total system's response depends on temporal behaviour of all the subsystems.In this paper, we analyze the timing properties of reactive programs written in a rule-based language RL. RL is a relatively simple rule language for programming discrete-response part of embedded applications in a layered architecture. It has been already used in a complex automotive application.We provide the upper timing bounds for RL programs executed using either of the two evaluation strategies developed earlier. In conclusion we provide some comments on the temporal behaviour of a layered system consisting of a reactive RL program combined with a set of periodic tasks.	Proceedings Paper	1998		[]	[]	-1	-1	-1
J	Zhang, Xumei; Yu, Jiaxuan; Yan, Wei; Wang, Yan; Subramanian, Nachiappan	A Comprehensive Review of Reverse Logistics in the Automotive Industry	2023	IEEE ACCESS		47112	47128	Reverse logistics (RL) of automobiles has received wide attention in recent years with the innovation of resource utilization and the increase of environmental awareness. Many research papers have been published in the RL discipline focusing on the automotive industry. However, no review article is available on product-specific issues. To bridge this gap, 91 papers published in the Web of Science (WOS) database between January 2013 and March 2023 were selected and analyzed using content analysis to classify the articles into survey, evaluation, decision making, framework, modeling and review. The main findings of this paper are as follows: (1) Research on RL activities has mainly focused on recycling and remanufacturing, and insufficient research has been conducted on activities such as dismantling and waste management. (2) In terms of research objects, End-of-Life Vehicles (ELVs) and automotive batteries have received a lot of attention, while less research has been done on automotive tires, engines, waste oil, etc., which need further attention. (3) Integrated economic, environmental and social considerations are research opportunities for future evaluation and decision making. (4) The establishment of multi-objective problems and the innovation of solution methods may be the future research direction. (5) Green and sustainability themes are the main trends in the development of RL in the automotive industry in the future.	Review	2023	10.1109/ACCESS.2023.3273591	[]	[]	-1	-1	-1
C	Yun, Sanghun; Kim, Jahyun; Kim, Hyogon	Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks	2019	2019 IEEE 90TH VEHICULAR TECHNOLOGY CONFERENCE (VTC2019-FALL)	IEEE Vehicular Technology Conference Proceedings			In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications. For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs). However, the in-vehicle network environment can be starkly different from the Internet where TCP has been optimized. The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments. In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.	Proceedings Paper	2019	10.1109/vtcfall.2019.8891225	[]	[]	-1	-1	-1
P	ZHANG H	Intelligent planning device for urban low-altitude            logistics unmanned aerial vehicle route, has task path            planning device that is provided for planning task path            for preventing danger and avoiding obstacles						NOVELTY - The device has a storage device for storing task data, communication data, terrain data, task path planning data and navigation capacity planning data. A pre-processing device pre-processes operation data of a relay station in a range. A processor obtains a target address and delivery aging data and matching according to a received delivery order. A communication device realizes information transmission between a processor and an unmanned aerial vehicle. A task management device classifies unmanned vehicle flight task and transmits the unmanned vehicle to the unmanned aircraft. A transmission distance determining device determines distance constraint condition two adjacent path points on an unmanned vehicle transportation path. A transportation condition determining unit determines flight restriction condition of logistics unmanned vehicle single cargo transportation. USE - Intelligent planning device for urban low-altitude logistics unmanned aerial vehicle (UAV) route used in logistics enterprises. Can also be used in aircraft. ADVANTAGE - The device combines the unmanned aerial vehicle flight path planning process based on depth reinforcement learning the route planning method of the UAV logistics distribution at the same time, thus optimizing path planning problem of two dimensions in logistics of UAV, and hence effectively ensuring the safety and high efficiency of the UPSU logistics path obtained by optimization. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the whole frame of an intelligent planning device for urban low-altitude logistics unmanned aerial vehicle route. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
B	Zheng, H.; Wu, F.; Yang, S.; Ju, X.; Liu, Y.; Zhu, K.	Design and Research of Vehicle Platoon Formation Algorithm Based on Multi-Agent Reinforcement Learning	2023	2023 8th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)		451	5	The field of artificial intelligence is advancing rapidly, with reinforcement learning making significant strides in solving various sequential decision problems in machine learning. As research progresses, multi-agent reinforcement learning has emerged in the field of reinforcement learning and has been applied to numerous domains. Vehicle formation is an important means of transportation for reducing vehicle energy consumption and improving air quality. Controlling sparse vehicles on the road to form formations is a fascinating research topic. In this paper, we propose a planning framework for formation control based on federated learning and multi-agent reinforcement learning to address this problem. We model vehicle energy consumption to accurately assess energy usage during vehicle formation. Additionally, we incorporate reinforcement learning algorithms into the vehicle formation process to enable multi-vehicle asynchronous decision-making and save formation time. We also introduce federated learning into the training process to significantly reduce overall system communication.	Conference Paper	2023	10.1109/IC-NIDC59918.2023.10390616	[]	[]	-1	-1	-1
P	ZHANG W	Humidity sensor for humidity and air quality            measurement in e.g. automotive and truck, has            resistor-inductor (RL) network with inductor and            resistor that is coupled to capacitor with high            relative temperature coefficient of resistance            (TCR)						NOVELTY - The humidity sensor (100) has a resistor-inductor-capacitor (RLC) resonant circuit formed in or on a substrate (101). The resonant circuit has a capacitor (105) with electrically conductive plate (106,107) and a moisture sensitive dielectric interposed between the plates. An RL network has an inductor (115) and a resistor (119) coupled to capacitor having a high relative TCR portion formed from a first material coupled to a low relative TCR portion formed from a second material different from the first material. USE - Humidity sensor for humidity and air quality measurement in automotive and truck for comfort and safety. Can also be used in powertrain, home appliance for moisture and temperature control, energy efficiency, humidity switches, heating, ventilating, and air conditioning, reprography for inkjet and laser, for weather stations, humidity displays and air quality measurement. ADVANTAGE - Enables to adjust and correct the measured relative humidity value based on the temperature value determined from the measured quality factor value of the resonant circuit. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of temperature compensated humidity sensing. DESCRIPTION Of DRAWING(S) - The drawing shows a cross sectional view of an RLC-based humidity sensor having temperature sensing.Humidity sensor (100)Substrate (101)Capacitor (105)Electrically conductive plate (106,107)Inductor (115)Resistor (119)				[]	[]	-1	-1	-1
P	LI B; YUAN X; WANG B; HUANG M	Cooperative competition method for intelligent            body, involves training initial reinforcement learning            model according to virtual air war scene, action space            information, state space information and reward            value						NOVELTY - The method involves determining a virtual air war scene of an intelligent body. Action space information and state space information of the intelligent body are determined according to the virtual air War scenario. The action space information is provided with an action value. The state Space information is contained with a state value. A reward value of the action is determined corresponding to the state value according to a state of the value. An initial reinforcement learning model is trained based on the virtual Air War scenario, the action spatial information, the state spatial information and the reward value. Target reinforcement learning models are obtained when the initial reinforcement training model is in a convergent state. The intelligent body is used to fight. USE - Method for cooperative competition of intelligent body such as unmanned aerial vehicle (UAV) and unmanned ground vehicle (UGV) used in air war autonomous maneuvering decision. Can also be used in intelligent robot, game game and unmanned vehicle. ADVANTAGE - The problem of difficult and unstable target reward is overcome, when the strategy of the multi-intelligent body is changed, it will not influence the strengthening learning of the multidimensional intelligent body. The cooperative competition method of intelligent body provides cooperative competition of the intelligent body, device, terminal device and storage medium, through determining the intelligent structure to the virtual empty warfare scene of the war, according to virtual air war scene, and determining the action space information and state space information of one or more intelligent structures according to the state value, determining the reward value of the action corresponding to state value and training the initial reinforcement learning model, and using the target strengthening learning model and the rule intelligent body to fight, overcoming the problem that the target reward. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(a) a cooperative competition device of intelligent body;(b) a computer-readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Mayer, Sebastian; Classen, Tobias; Endisch, Christian	Modular production control using deep reinforcement learning: proximal policy optimization	2021	JOURNAL OF INTELLIGENT MANUFACTURING		2335	2351	EU regulations on CO2 limits and the trend of individualization are pushing the automotive industry towards greater flexibility and robustness in production. One approach to address these challenges is modular production, where workstations are decoupled by automated guided vehicles, requiring new control concepts. Modular production control aims at throughput-optimal coordination of products, workstations, and vehicles. For this np-hard problem, conventional control approaches lack in computing efficiency, do not find optimal solutions, or are not generalizable. In contrast, Deep Reinforcement Learning offers powerful and generalizable algorithms, able to deal with varying environments and high complexity. One of these algorithms is Proximal Policy Optimization, which is used in this article to address modular production control. Experiments in several modular production control settings demonstrate stable, reliable, optimal, and generalizable learning behavior. The agent successfully adapts its strategies with respect to the given problem configuration. We explain how to get to this learning behavior, especially focusing on the agent's action, state, and reward design.	Article; Early Access		10.1007/s10845-021-01778-z	[]	[]	-1	-1	-1
P	LIU Y; WEN Z; LI J; JIN X; NIU Y	Method for performing small unmanned aerial            vehicle anti-control mixed decision based on deep            reinforcement learning and rule drive, involves            training and optimizing decision model, and updating            control rule model according to decision model						NOVELTY - The method involves obtaining position motion information of a small unmanned aerial vehicle. A three-degree-of-freedom particle motion model of the small unmanned air vehicle is constructed. A small unmanned vehicle control rule model is constructed to describe small unmanned aircraft control steps. A state space, an action space and a reward function are constructed according to a Markov decision process. A D3QN network is established based on a dueling structure. An anti-control decision model is trained and optimized. The small unmanned helicopter control rules model is updated according to the control decision model. USE - Small unmanned aerial vehicle anti-control mixed decision method based on deep reinforcement learning and rule drive for use in civil field, military field and local conflict. ADVANTAGE - The automation level of the small unmanned aerial vehicle control system for preventing and controlling task is effectively improved. The decision speed in the existing small aerial vehicle anti-control command decision is slow, difficult to process complex scene and so on, meeting the command decision requirement of the control small airborne vehicle.				[]	[]	-1	-1	-1
J	Sowmya, Karri; Dhabu, Meera M.	Model free Reinforcement Learning to determine pricing policy for car parking lots	2023	EXPERT SYSTEMS WITH APPLICATIONS				Finding a parking space has not only become painful but also costs a lot in most of the metropolitan cities. With the increase in number of vehicles and limited resources such as manpower and space, the need for effective management of parking lots has increased. Improper management of parking lots can have negative consequences such as traffic congestion, wastage of time in search of parking spaces, air pollution and even loss of revenue for the parking lot managers. Dynamic pricing is a powerful tool to control the behavior of drivers by diverting them towards the unoccupied and cheaper parking lots. Though there are several existing dynamic pricing strategies, determining the right prices is quite challenging due to lack of knowledge of drivers' behavior and several uncertainties like harsh weather and special days. In this paper Reinforcement Learning(RL) technique called Q-learning is used to calculate the dynamic prices for parking lots on hourly basis without the need of prior information about the system. Crucial factors like distance of the parking lots from the city centers, weather and holidays are considered in the proposed algorithm to achieve better accuracy. Price Elasticity of Demand (PED) is used in the proposed work to calculate the new state(occupancy) when an action(dynamic price charged by the parking lot owner) is taken place. Hourly prices are estimated using the proposed algorithm and simulation results show that the calculated prices can efficiently manage parking occupancy during peak and off peak hours. The simulation output also shows that the proposed algorithm can successfully increase the revenue of the parking lot owners.	Article	NOV 15 2023	10.1016/j.eswa.2023.120532	[]	[]	-1	-1	-1
J	Koch, Lucas; Picerno, Mario; Badalian, Kevin; Lee, Sung-Yong; Andert, Jakob	Automated function development for emission control with deep reinforcement learning	2023	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE				The conventional automotive development process for embedded systems today is still time-and data -inefficient, and requires highly experienced software developers and calibration engineers. Consequently, it is cost-intensive and at the same time prone to sub-optimal solutions. Reinforcement Learning offers a promising approach to address these challenges. The evolved agents have proven their ability to master complex control tasks in a close-to-optimal manner without any human intervention, but the training procedures are hardly compatible with current development processes. As a result, Reinforcement Learning has rarely been used in powertrain development until now. This work describes an integration of Reinforcement Learning in the embedded system development process to automatically train and deploy agents in transient driving cycles. Using the example of exhaust gas re-circulation control for a Diesel engine, an agent is successfully trained in a fully virtualized environment, achieving emission reductions of up to 10% in comparison to a state-of-the-art controller. Further investigations are carried out to quantify the impact of the driving cycle and ambient conditions on the agent's performance. To demonstrate the transferability between different levels of virtualization, the experienced agent is then tested in closed-loop with a real hardware controller to operate the physical actuator. By confirming the reproducibility of the learned strategy on real hardware, this article serves as proof-of-concept for a sustainable, Reinforcement Learning based path to automatically develop embedded controllers for complex control problems.	Article; Early Access		10.1016/j.engappai.2022.105477	[]	[]	-1	-1	-1
C	Liu, Qiang; Zhang, Yuru; Wang, Haoxin	<i>EdgeMap</i>: CrowdSourcing High Definition Map in Automotive Edge Computing	2022	IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2022)	IEEE International Conference on Communications	4300	4305	High definition (HD) map needs to be updated frequently to capture road changes, which is constrained by limited specialized collection vehicles. To maintain an up-to-date map, we explore crowdsourcing data from connected vehicles. Updating the map collaboratively is, however, challenging under constrained transmission and computation resources in dynamic networks. In this paper, we propose EdgeMap, a crowdsourcing HD map to minimize the usage of network resources while maintaining the latency requirements. We design a DATE algorithm to adaptively offload vehicular data on a small time scale and reserve network resources on a large time scale, by leveraging the multi-agent deep reinforcement learning and Gaussian process regression. We evaluate the performance of EdgeMap with extensive network simulations in a time-driven end-to-end simulator. The results show that EdgeMap reduces more than 30% resource usage as compared to state-of-the-art solutions.	Proceedings Paper	2022	10.1109/ICC45855.2022.9838617	[]	[]	-1	-1	-1
J	Gan, Jiongpeng; Li, Shen; Wei, Chongfeng; Deng, Lei; Tang, Xiaolin	Intelligent Learning Algorithm and Intelligent Transportation-Based Energy Management Strategies for Hybrid Electric Vehicles: A Review	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		10345	10361	As one of the alternatives to conventional fuel vehicles, hybrid electric vehicles (HEV) offer lower fuel consumption and fewer exhaust emissions. To improve the performance of the HEV, the energy management strategy (EMS) is one of the most critical technologies. Classic EMS can be broadly classified into rule-based and optimization-based. With the development of machine learning technology, the deep reinforcement learning (DRL) algorithm of intelligent learning algorithms has been applied to the EMS. This paper mainly reviews the research progress of the EMS based on DRL from two aspects of the algorithm and training environment, and the EMS research involving combining the intelligent transportation system (ITS) is reviewed. In addition, the experimental test progress situations of DRL-based EMS research are discussed. Finally, the challenge of DRL-based EMSs is analyzed and some solutions are provided. In particular, it also involves some discussion about automotive cyber security in the intelligent transportation environment.	Review; Early Access		10.1109/TITS.2023.3283010	[]	[]	-1	-1	-1
J	Li, Zhaojian; Chu, Tianshu; Kolmanovsky, Ilya V.; Yin, Xiang; Yin, Xunyuan	Cloud resource allocation for cloud-based automotive applications	2018	MECHATRONICS		356	365	"There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive tasks. Efficient utilization of on-demand cloud resources holds a significant potential to improve future vehicle safety, comfort, and fuel economy. In the meanwhile, issues like cyber security and resource allocation pose great challenges. In this paper, we treat the resource allocation problem for cloud-based automotive systems. Both private and public cloud paradigms are considered where a private cloud provides an internal, company-owned Internet service dedicated to its own vehicles while a public cloud serves all subscribed vehicles. This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-based automotive systems. Complications such as stochastic communication delays and task deadlines are explicitly considered. In particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited to utilize the cloud resources for best Quality of Services. On the other hand, a decentralized auction-based model is developed for public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a ""selfish"" agent. Numerical examples are presented to illustrate the effectiveness of the developed techniques."	Article	APR 2018	10.1016/j.mechatronics.2017.10.010	[]	[]	-1	-1	-1
P	SHAO C; ZHANG C; LI T; FAN W; ZHENG X	Unmanned vehicle path tracking and anti-collision            control strategy, involves calculating discount factor            according to time sequence difference method error to            obtain control quantity of improved control            strategy						NOVELTY - The strategy involves analyzing vehicle kinematics and characteristics to establish a dynamic model of a vehicle. A controller input output quantity is determined. A vehicle running track is smoothed and improved to increase an environment constraint condition. A yaw rate stability and lateral speed stability calculation method is introduced to determine a vehicle stable operation range. A neural network controller model is constructed based on a control logic of a DDPG algorithm. A model input data matrix, an experience pool data storage mode and model output control quantity are determined. Reinforcement learning sparsity problem is determined based on vehicle running environment characteristics. A discount factor is calculated according to time sequence difference process error to obtain control quantity of an improved control strategy. USE - Unmanned vehicle path tracking and anti-collision control strategy based on DDPG algorithm. ADVANTAGE - The control policy can autonomously select path tracking and execution priority of obstacle avoidance task, improve the path tracking, anti-collision capability in the vehicle driving process, and can effectively improve the vehicle control precision. The DDPG algorithm has stronger learning ability and can continuously optimize the control effect. DESCRIPTION Of DRAWING(S) - The drawing shows a structural diagram of the neural network controller for unmanned vehicle path tracking and anti-collision based on DDPG algorithm. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	PENG H; ZHU J; LI S; CAI Y; CHEN T; CHEN Y; GENG H; CHAI X	Unmanned cooperative control method based on depth            reinforcement learning between human and machine for            auxiliary robot, involves establishing machine            intelligent body action strategy model by deep            Q-learning algorithm of the loop and constructing            function description of human action intention            model						NOVELTY - The method involves constructing a function description of a human action intention model. A machine intelligent body action strategy model of function description is constructed. A random optimal strategy function is used to describe the machine intelligent body action strategy model. The input of the model is the environment state s. The output of the human action and human action intent is the optimal action. The machine intelligent body action strategy model is established using the deep Q-learning algorithm of the loop and is a multi-layer neural network to achieve the end-to-end mapping from the state to the action. The action of the human operator is selected when the action of human operators is close to the optimal action output by the machine intelligent body action strategy model. The optimal action output is chosen by the machine intelligent body action strategy model. The optimal action is output by the machine intelligent body action strategy model. USE - Unmanned cooperative control method e.g. unmanned aerial vehicle and unmanned vehicle based on depth reinforcement learning for auxiliary robot and remote operation robot. ADVANTAGE - The method avoids the control authority distribution problem in the human-machine cooperative control by depth reinforcement learning can achieve the better without human cooperative control. The safety of the whole human- machine cooperative system, stability and task execution performance are improved. The cooperative policy has strong target performance, and the synergistic efficiency is higher. The method uses the deep Q-learning algorithm of the human in-the-loop based on the deep reinforcement learning based algorithm frame, increases the human machine control strategy design, and achieves the effective human machine-machine control authority adjustment. The large amount of interactive data needed by training is avoided. The real data lack problem is solved in practical application. The control authority is dynamically adjusted and distributed to the human or machine intelligent body in the task execution process, to improve the safety. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram for unmanned cooperative control method based on depth reinforcement learning (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Gankin, Dennis; Mayer, Sebastian; Zinn, Jonas; Vogel-Heuser, Birgit; Endisch, Christian	Modular Production Control with Multi-Agent Deep Q-Learning	2021	2021 26TH IEEE INTERNATIONAL CONFERENCE ON EMERGING TECHNOLOGIES AND FACTORY AUTOMATION (ETFA)	IEEE International Conference on Emerging Technologies and Factory Automation-ETFA			The automotive industry is increasingly focusing on product customization. The concept of Modular Production addresses this issue by providing more flexibility in production with Automated Guided Vehicles transporting products between modular workstations. The added complexity of Modular Production Control calls for approaches that can handle the scheduling complexity while also minimizing production costs. As a result, literature has focused on two promising approaches: Deep Reinforcement Learning and Multi-Agent Systems. Both approaches have their advantages. Especially in complex, large-scale production environments with random breakdowns, those two fields have been seldomly combined, though. As a result, this article aims to fill that research gap by introducing a Deep Reinforcement Learning Multi-Agent System approach for Modular Production Control. We introduce a reward design incentivizing agents to achieve maximal throughput. In addition, we show that the method learns optimal behavior even in a large-scale production environment with random machine breakdowns. Lastly, we compare the Multi-Agent System to a single-agent implementation of the Deep Reinforcement Learning approach and conclude that the Multi-Agent Deep Reinforcement Learning method learns and solves the Modular Production Control problem with the same solution quality as the single agent. Hence, the approach allows to foster MAS benefits such as robustness without losses in the solution quality.	Proceedings Paper	2021	10.1109/ETFA45728.2021.9613177	[]	[]	-1	-1	-1
J	Anzalone, Luca; Barra, Paola; Barra, Silvio; Castiglione, Aniello; Nappi, Michele	An End-to-End Curriculum Learning Approach for Autonomous Driving Scenarios	2022	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		19817	19826	In this work, we combine Curriculum Learning with Deep Reinforcement Learning to learn without any prior domain knowledge, an end-to-end competitive driving policy for the CARLA autonomous driving simulator. To our knowledge, we are the first to provide consistent results of our driving policy on all towns available in CARLA. Our approach divides the reinforcement learning phase into multiple stages of increasing difficulty, such that our agent is guided towards learning an increasingly better driving policy. The agent architecture comprises various neural networks that complements the main convolutional backbone, represented by a ShuffleNet V2. Further contributions are given by (i) the proposal of a novel value decomposition scheme for learning the value function in a stable way and (ii) an ad-hoc function for normalizing the growth in size of the gradients. We show both quantitative and qualitative results of the learned driving policy.	Article; Early Access		10.1109/TITS.2022.3160673	[]	[]	-1	-1	-1
C	Victor Miguel, Velazquez Espitia; Jose Angel, Gonzalez Gonzalez; Juarez Omar, Mata; Pedro, Ponce; Arturo, Molina	Deep Q-learning for Control: Technique and Implementation Considerations on a Physical System: Active Automotive Rear Spoiler Case	2021	2021 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2021)		818	824	"Deep Q-learning is the combination of artificial neural networks advantages (ANNs) with Q-learning. ANNs have expanded the possibilities on a variety of algorithms by enhancing their capabilities and surpassing their limitations. This is the case of reinforcement learning. Nowadays, Deep Q-learning is used in a variety of applications in different fields, including the development of intelligent algorithms to control physical systems. Deep Q-learning has demonstrated the possibility of achieving effective results by solving specific tasks that are highly complex to model through classical approaches. An important drawback is that these models require an elaborated implementation process, and several design decisions must be taken in order to achieve reliable results. Often, developers might find the design process mostly experimental rather than ruled-based. Addressing this problem, the present work describes in detail the implementation process of Deep Q-learning to control a physical system, proposes considerations and analysis parameters for each of the main steps. Demonstrated in the development of the ""Active automotive rear spoiler"", the results present a methodology that successfully guides towards a proper implementation of Deep Q-learning. The knowledge of this paper should not be taken as a recipe, but rather as an evaluation reference to equip the reinforcement learning developers with tools for the development of projects."	Proceedings Paper	2021	10.1109/ICMA52036.2021.9512669	[]	[]	-1	-1	-1
P	WANG C; TIAN W; QIAN X; WANG S; HU H; LIU Q	Processing self-adaptive assembling of radio            frequency connector used as automotive industry,            comprises e.g. obtaining initial position information            of electric connector socket by photographing, driving            high precision three-axis module platform to move, and            assembling electric connector						NOVELTY - Processing self-adaptive assembling of radio frequency connector comprises obtaining initial position information of an electric connector socket by photographing using a binocular camera, driving high precision three-axis module platform to move, completing initial pose alignment of the electric connector and the electrical connector socket, executing electric connector-socket assembly strategy according to a relative pose error between sensor information and off-line database, controlling contact pose error by a variable impedance controller based on reinforcement learning, and assembling electric connector. USE - The method is useful for processing self-adaptive assembling of radio frequency connector, which is used as key component of signal transmission in array antenna of robot flexible control field, and used as automotive industry. ADVANTAGE - The method realizes precise force control of an electric connector assembly, improves assembling effect of the electric connector, effectively reduces attitude error between the connector and socket, allows variable impedance control method based on reinforcement learning, realizes accurate control of contact, reduces tracking error of the contact, and optimizes contact connector in inserting process of the control. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:elf-adaptive assembling system of the radio frequency connector; andA readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for processing self-adaptive assembling of radio frequency connector (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Ye, Yiming; Xu, Bin; Zhang, Jiangfeng; Lawler, Benjamin; Ayalew, Beshah	Reinforcement Learning-Based Energy Management System Enhancement Using Digital Twin for Electric Vehicles	2022	2022 IEEE VEHICLE POWER AND PROPULSION CONFERENCE (VPPC)	IEEE Vehicle Power and Propulsion Conference			Compared to conventional engine-based powertrains, electrified powertrain exhibit increased energy efficiency and reduced emissions, making electrification a key goal for the automotive industry. For a vehicle with hybrid energy storage system, its performance and lifespan are substantially affected by the energy management system. Reinforcement learning-based methods are gaining popularity in vehicle energy management, but most of the literature in this area focus on pure simulation while hardware implementation is still limited. This paper introduces the digital twin methodology to enhance the Q-learning-based energy management system for battery and ultracapacitor electric vehicles. The digital twin model can exploit the bilateral interdependency between the virtual model and the actual system, which improves the control performance of the energy management system. The physical model is established based on a hardware-in-the-loop simulation platform. In addition, battery degradation is also considered for prolonging the battery lifespan to reduce the operating cost. The validation results of the trained reinforcement learning agent illustrate that the digital twin-enhanced Q-learning energy management system improves the energy efficiency by 4.36% and the battery degradation is reduced by 25.28%.	Proceedings Paper	2022	10.1109/VPPC55846.2022.10003411	[]	[]	-1	-1	-1
B	Hua, R.; Tao, F.; Fu, Z.; Zhu, L.	A Deep Reinforcement Learning-Based Energy Management Optimization for Fuel Cell Hybrid Electric Vehicle Considering Recent Experience	2024	Cognitive Systems and Information Processing: 8th International Conference, ICCSIP 2023, Revised Selected Papers. Communications in Computer and Information Science (1918)		374	86	This study emphasizes a recent experience sampling method in conjunction with Deep Deterministic Policy Gradient (DDPG) to enhance the training speed and improve the training outcomes. Firstly, to ensure the safe operation of the battery and energy storage system under peak power, a power demand decoupling method based on frequency domain is proposed to achieve power stratification. Subsequently, a multi-objective equivalent consumption minimization strategy model is established based on the data types of the experimental platform, and the improved DDPG algorithm is employed to solve it. Finally, simulation results demonstrate that compared to conventional DDPG algorithms, the improved DDPG algorithm can enhance efficiency by an average of 2.02%.	Conference Paper	2024	10.1007/978-981-99-8018-5_28	[]	[]	-1	-1	-1
J	Lee, Joash; Niyato, Dusit; Guan, Yong Liang; Kim, Dong In	Learning to Schedule Joint Radar-Communication With Deep Multi-Agent Reinforcement Learning	2022	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		406	422	Radar detection and communication are two essential sub-tasks for the operation of next-generation autonomous vehicles (AVs). The forthcoming proliferation of faster 5G networks utilizing mmWave has raised concerns on interference with automotive radar sensors, which has led to a body of research on Joint Radar-Communication (JRC). This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV. We first formulate the problem as a Markov Decision Process (MDP). We then propose a more general multi-agent system, with an appropriate medium access control (MAC) protocol, which is formulated as a partially observed Markov game (POMG). To solve the POMG, we propose a multi-agent extension of the Proximal Policy Optimization (PPO) algorithm, along with algorithmic features to enhance learning from raw observations. Simulations are run with a range of environmental parameters to mimic variations in real-world operation. The results show that the chosen deep reinforcement learning methods allow the agents to obtain strong performance with minimal a priori knowledge about the environment.	Article	JAN 2022	10.1109/TVT.2021.3124810	[]	[]	-1	-1	-1
B	Hu, Anran	Learning in Mean-Field Games and Continuous-Time Stochastic Control Problems	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
P	WANG Y; ZHAO J; WEI J; CHI P	Unmanned aerial vehicle cluster cooperative attack            and defense decision making method, involves            constructing capture strategy by action space of            vehicle cluster, and using training to obtain action            strategy to attack and defend against vehicle						NOVELTY - The method involves establishing an unmanned aerial vehicle cluster attack resistance task model. An unmanned aerial vehicle cluster is sent to perform intercepting and intercepting of an enemy unmanned aerial vehicle task. A movement model of an unmanned vehicle and an enemy unmanned aerial vehicle is established. A motion model of the unmanned aerial vehicle group and the enemy unmanned aerial vehicle are established. An action mode is selected according to a state of the enemy unmanned aerial vehicle. Multi-intelligent body reinforcement learning training process is performed to train the unmanned aerial vehicle group strategy network to obtain an action strategy. The Multi-intelligent body reinforcement learning training process is used to obtain the action strategy to attack and defend the enemy unmanned aerial vehicle. USE - Unmanned aerial vehicle cluster cooperative attack and defense decision making method. ADVANTAGE - The method enables avoiding dimensional disaster problem caused by directly training large-scale unmanned aerial vehicle cluster so as to improve expansibility of a cluster number and success rate of an attacking and defending against task. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a unmanned aerial vehicle cluster cooperative attack and defense decision making method. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Feher, Arpad; Aradi, Szilard; Becsi, Tamas	Q-learning based Reinforcement Learning Approach for Lane Keeping	2018	2018 18TH IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND INFORMATICS (CINTI)	International Symposium on Computational Intelligence and Informatics	31	35	The paper presents the application of a Q-learning reinforcement learning method in the area of vehicle control. The purpose of the research presented is to design an end-to-end behavior control of a kinematic vehicle model placed in a simulated race track environment, by using reinforcement learning approach. The varied trajectory of the track to provide different situations for the agent. The environment sensing model is based on high-level sensor information. Track curvature, lateral position, and relative yaw angle can be reached from the current automotive sensors, such as camera or IMU based systems. The objectives were reached through the definition of a rewarding system, with subrewards and penalties enforcing lane keeping. After the description of the theoretical basis the environment model with the reward function is detailed. Finally the experiments with the learning process are presented and the results of the evaluation are given from some aspects of control quality and safety.	Proceedings Paper	2018		[]	[]	-1	-1	-1
P	DAI Z; LIU C; YE Y	Space-ground cooperative sensing method for            exploring personality and cooperative characteristic of            unmanned population, involves using self-supervised            classifier to define internal reward based on            characteristic characteristic						NOVELTY - The method involves collecting data from each interest point under a space cooperation group sensing scene by an unmanned group. The total available spectrum of all interest points is divided into Z sub-channels based on space-based non-orthogonal multiple access technology. The data collecting event of each sub-channel z is represented by a tuple (u, g, i, i, i, and i, z, t, where i and i' represent the interest point accessed by the unmanned aerial vehicle and the unmanned vehicle. The local observation is obtained from an environment outside the reward and the next time slot from the environment according to the movement condition of the current time slot and the data collection condition. The simulation platform calculates the value function in the multi-agent actor-evaluation depth reinforcement learning method. USE - Space-ground cooperative sensing method for exploring personality and cooperative characteristic of unmanned population e.g. unmanned aerial vehicle and ground unmanned population e.g. unmanned vehicle. ADVANTAGE - The method uses the self-supervised classifier to define the internal reward, based on the characteristic characteristic generation of the reward, thus effectively making the unmanned aerial vehicle and the unmanned vehicle to reach better space division, and hence greatly improving the task efficiency. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a space-ground cooperative sensing method (Drawing includes Non-English language text).				[]	[]	-1	-1	-1
C	Elmalaki, Salma	MAConAuto: Framework for Mobile-Assisted Human-in-the-Loop Automotive System	2022	2022 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	740	749	Automotive is becoming more and more sensor-equipped. Collision avoidance, lane departure warning, and self-parking are examples of applications becoming possible with the adoption of more sensors in the automotive industry. Moreover, the driver is now equipped with sensory systems like wearables and mobile phones. This rich sensory environment and the real-time streaming of contextual data from the vehicle make the human factor integral in the loop of computation. By integrating the human's behavior and reaction into the advanced driver-assistance systems (ADAS), the vehicles become a more contextaware entity. Hence, we propose MAConAuto, a framework that helps design human-in-the-loop automotive systems by providing a common platform to engage the rich sensory systems in wearables and mobile to have context-aware applications. By personalizing the context adaptation in automotive applications, MAConAuto learns the behavior and reactions of the human to adapt to the personalized preference where interventions are continuously tuned using Reinforcement Learning. Our general framework satisfies three main design properties, adaptability, generalizability, and conflict resolution. We show how MAConAuto can be used as a framework to design two applications as human-centric applications, forward collision warning, and vehicle HVAC system with negligible time overhead to the average human response time.	Proceedings Paper	2022	10.1109/IV51971.2022.9827415	[]	[]	-1	-1	-1
P	LEE S	Vehicle manufacturing process monitoring system, has artificial neural network model for configuring component and assembly unit based on big data, and failure analysis unit for updating threshold value based on reinforcement learning						NOVELTY - The system (100) has a defect analysis unit for detecting defect type based on process data. A display unit outputs status of a production line in which defect type is detected, where the defect type includes injection defect, assembly defect, stain defect, crack defect, paint defect and foreign substance defect. A management server (200) builds big data based on the process data. An artificial neural network model configures component and assembly unit based on the big data. A failure analysis unit updates threshold value based on reinforcement learning. USE - Vehicle manufacturing process monitoring system. ADVANTAGE - The system detects defective parts by monitoring production status of automotive components. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a vehicle manufacturing process monitoring system. (Drawing includes non-English language text).System (100)Management server (200)				[]	[]	-1	-1	-1
J	Leng, Jinling; Jin, Chun; Vogl, Alexander; Liu, Huiyu	Deep reinforcement learning for a color-batching resequencing problem	2020	JOURNAL OF MANUFACTURING SYSTEMS		175	187	In automotive paint shops, changes of colors between consecutive production orders cause costs for cleaning the painting robots. It is a significant task to re-sequence orders and group orders with identical color as a color batch to minimize the color changeover costs. In this paper, a Color-batching Resequencing Problem (CRP) with mix bank buffer systems is considered. We propose a Color-Histogram (CH) model to describe the CRP as a Markov decision process and a Deep Q-Network (DQN) algorithm to solve the CRP integrated with the virtual car resequencing technique. The CH model significantly reduces the number of possible actions of the DQN agent, so that the DQN algorithm can be applied to the CRP at a practical scale. A DQN agent is trained in a deep reinforcement learning environment to minimize the costs of color changeovers for the CRP. Two experiments with different assumptions on the order attribute distributions and cost metrics were conducted and evaluated. Experimental results show that the proposed approach outperformed conventional algorithms under both conditions. The proposed agent can run in real time on a regular personal computer with a GPU. Hence, the proposed approach can be readily applied in the production control of automotive paint shops to resolve order-resequencing problems.	Article	JUL 2020	10.1016/j.jmsy.2020.06.001	[]	[]	-1	-1	-1
J	Zhou, Juanying; Zhao, Jianyou; Wang, Lufeng	An Energy Management Strategy of Power-Split Hybrid Electric Vehicles Using Reinforcement Learning	2022	MOBILE INFORMATION SYSTEMS				With the rapid development of science and technology, the automobile industry is also gradually expanding due to which energy security and ecological security are seriously threatened. This paper was aimed at studying the energy organization strategy of power-split hybrid electric vehicles based on a reinforcement learning algorithm. A power-split hybrid electric vehicle (HEV) combines the advantages of both series and parallel hybrid vehicle architectures by using a planetary gear set to split and combine the power generated by electric machines and a combustion engine. This improves the fuel economy to some extent. However, to increase the fuel economy to a greater extent. This study primarily introduces the hybrid electric vehicle's structure and presents a reinforcement learning-based management of energy approach for hybrid electric vehicles. It constructs the vehicle power model of HEV and Markov probability transfer model, then designs the energy control strategy based on reinforcement learning, and finally compares it with the energy control strategy based on PID (Proportional-IntegralDerivative). Using MATLAB/Simulink, the cycle conditions of NEDC (New European Driving Cycle) and FTP-75 (Federal Test Procedure) are selected to carry out simulation experiments. The energy management technique suggested in this study, which is based on reinforcement learning, may efficiently enhance the usage rate of automotive gasoline. The replication results show that the fuel consumption per 100 km (kilometers) based on reinforcement learning management strategy is 4.6% and 2.7% lower than the PID management strategy under two working conditions. The economy of fuel of the hybrid electric vehicle is also effectively improved.	Article	APR 21 2022	10.1155/2022/9731828	[]	[]	-1	-1	-1
J	Leal-Millan, Antonio; Luis Roldan, Jose; Leal-Rodriguez, Antonio L.; Ortega-Gutierrez, Jaime	IT and relationship learning in networks as drivers of green innovation and customer capital: evidence from the automobile sector	2016	JOURNAL OF KNOWLEDGE MANAGEMENT		444	464	Purpose - Despite the positive effects of customer capital (CC), questions remain over how managers enable CC growth by applying their skills and capabilities through managerial actions and strategies, such as developing information technology (IT) capability, fostering relationship learning (RL) activities and developing green innovation performance (GIP) with clients. These questions are especially pertinent in small and medium-sized enterprises and automotive industry companies that operate through supply chains, where knowledge about customers is likely to result from personal contact between customers and organisational members. The purpose of this paper is to analyse the extent to which these managerial actions were more likely to lead to the successful creation of CC.Design/methodology/approach - Using the partial least squares technique, this paper studies how these three managerial actions impact on CC. To do so, data from 140 companies in the Spanish automotive components manufacturing sector have been used.Findings - The findings support the influence of RL on both GIP and CC. RL is a key managerial action in exploiting customer information and knowledge advantages, enabling firms to structure and reconfigure resources to produce new ways to compete and to satisfy stakeholders. In addition, results show that GIP is a determinant of CC because of its contribution to achieving sustainable competitive advantage, with GIP performing a mediating role in the relationship between RL and CC. A second contribution shows that IT is not in itself able to yield a competitive advantage, thereby validating the existence of complementary or co-focused strategic assets such as RL and GIP, which enhance IT's influence on CC.Research limitations/implications - The authors were unable to explore the subtleties of the processes over time. Future research should include a longitudinal study.Practical implications - This study considers RL an essential factor in achieving both GIP and CC. Consequently, managers should seek to build strong RL cultures. In addition, this study shows that IT is not in itself able to yield a competitive advantage, thereby validating the existence of complementary or co-focused strategic assets such as RL and GIP.Originality/value - No study has ever examined these three antecedent variables (IT, RL and GIP) together, with the aim to examine their effects on CC.	Article	2016	10.1108/JKM-05-2015-0203	[]	[]	-1	-1	-1
P	HUANG Y; LI K; LIU J; ZHE D; LI C	Rubber composition used for manufacturing inside and outside layer of automotive power steering high-pressure hose, contains acid scavenger, antioxidant, processing aid, plasticizer, precipitated barium sulfate, carbon black and silica						NOVELTY - A rubber composition comprises 100 wt.% chlorinated polyethylene rubber, 10-20 wt.% acid scavenger, 1-3 wt.% organotin stabilizing agent, 0.5-2 wt.% antioxidant, 0.5-3 wt.% processing aid, 15-40 wt.% plasticizer, 5-15 wt.% precipitated barium sulfate, 25-100 wt.% carbon black, 5-30 wt.% silica, 3-5 wt.% adhesive RL, 1.5-3 wt.% crosslinking aid and 1-4 wt.% peroxide-based crosslinking agent. USE - Rubber composition is used for manufacturing inside and outside layer of automotive power steering high-pressure hose. ADVANTAGE - The economical rubber composition has excellent high-temperature resistance, oil resistance, extracting property and adhesiveness with fiber-reinforced layer under high temperature and pressure for long period of time. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for production of rubber composition, which involves mixing chlorinated polyethylene rubber, acid scavenger, organotin stabilizing agent, antioxidant, processing aid, plasticizer, precipitated barium sulfate, carbon black, silica and adhesive RL, adding crosslinking aid and peroxide-based crosslinking agent, heating to 120-135 degrees C for 2-5 minutes, and discharging.				[]	[]	-1	-1	-1
P	MATSUNAMI N; ITO M; KATAOKA Y	Learning device for allowing learned model to-be            installed in computer in e.g. aircrafts, in wireless            communication system, has model selection unit            selecting learned model that satisfies performance            requirement when model evaluation unit determines						NOVELTY - The device has a setting unit for setting a requirement value for a predetermined parameter of a communication device controlled by a computer using a learned model. A reinforcement learning unit allows the learning model to learn such that a reward given in a predetermined environment is maximized. A model selection unit selects the learned model to be installed in the computer, when a model evaluation unit determines that performance of the model updated to another requirement value satisfies a performance requirement as the model to-be-installed in the computer. The updating unit changes the latter requirement value. USE - Learning device for allowing a learned model to-be installed in a computer in an unmanned vehicle such as aircrafts, ships and vehicles, in a wireless communication system (all claimed). ADVANTAGE - The device creates a surplus in resources of the wireless communication line so as to expand functions such as effective communication range and function enhancement, thus improving encryption strength, reducing weight and power consumption of the communication device and preventing interference with other stations. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) an unmanned vehicle; (2) a wireless communication system; (3) a learning method for allowing a learned model to be installed in a computer to learn; (4) a non-transitory computer-readable storage medium storing a set of instructions for allowing a learned model to be installed in a computer to learn. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a learning device for allowing a learned model to-be installed in a computer in an unmanned vehicle.2Agent4Environment				[]	[]	-1	-1	-1
C	Inuzuka, Shota; Xu, Fuguo; Zhang, Bo; Shen, Tielong	Reinforcement Learning based on Energy Management Strategy for HEVs	2019	2019 IEEE VEHICLE POWER AND PROPULSION CONFERENCE (VPPC)	IEEE Vehicle Power and Propulsion Conference			This paper presents a new architecture of real-time HEV's energy management problem under a V2V and V21 environment using policy-based deep reinforcement learning. The ideal energy management controller that minimizes HEV energy costs needs to run engines most efficiently in the whole running considering battery SoC. The controller needs to predict the future vehicle speed and plan the power distribution to achieve it because the thermal efficiency of engines is more efficient when its rotational speed is higher. The future vehicle speed has relationship with connectivity information such as the behavior of the car in front, the traffic light signals, crowd of cars, and so on. This paper assumes the connectivity environment in the future and applies proximal policy optimization (PPO) [5] that is known as policy-based deep reinforcement learning algorithm to achieve the optimal power distribution predicting the future behavior by using connectivity information. In addition, this paper shows that locating the local controller in the reinforcement learning loop enables the AI controller to learn robustly. The local controller corrects against an exploration that is obviously not optimal or doesn't satisfy the constraints.	Proceedings Paper	2019	10.1109/vppc46532.2019.8952511	[]	[]	-1	-1	-1
B	Yan, Z.; Tabassum, H.	Reinforcement Learning for Joint V2I Network Selection and Autonomous Driving Policies	2022	GLOBECOM 2022 - 2022 IEEE Global Communications Conference		1241	6	Vehicle-to-Infrastructure (V2I) communication is becoming critical for the enhanced reliability of autonomous vehicles (AVs). However, the uncertainties in the road-traffic and AVs' wireless connections can severely impair timely decision-making. It is thus critical to simultaneously optimize the AVs' network selection and driving policies in order to minimize road collisions while maximizing the communication data rates. In this paper, we develop a reinforcement learning (RL) framework to characterize efficient network selection and autonomous driving policies in a multi-band vehicular network (VNet) operating on conventional sub-6GHz spectrum and Terahertz (THz) frequencies. The proposed framework is designed to (i) maximize the traffic flow and minimize collisions by controlling the vehicle's motion dynamics (i.e., speed and acceleration) from autonomous driving perspective, and (ii) maximize the data rates and minimize handoffs by jointly controlling the vehicle's motion dynamics and network selection from telecommunication perspective. We cast this problem as a Markov Decision Process (MDP) and develop a deep Q-learning based solution to optimize the actions such as acceleration, deceleration, lane-changes, and AV-base station assignments for a given AV's state. The AV's state is defined based on the velocities and communication channel states of AVs. Numerical results demonstrate interesting insights related to the inter-dependency of vehicle's motion dynamics, handoffs, and the communication data rate. The proposed policies enable AVs to adopt safe driving behaviors with improved connectivity.	Conference Paper	2022	10.1109/GLOBECOM48099.2022.10001396	[]	[]	-1	-1	-1
B	Buddareddygari, Prasanth	Physically Realizable Targeted Adversarial Attacks on Autonomous Driving	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
C	Feher, Arpad; Aradi, Szilard; Hegedus, Ferenc; Becsi, Tamas; Gaspar, Peter	Hybrid DDPG Approach for Vehicle Motion Planning	2019	ICINCO: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS, VOL 1		422	429	The paper presents a motion planning solution which combines classic control techniques with machine learning. For this task, a reinforcement learning environment has been created, where the quality of the fulfilment of the designed path by a classic control loop provides the reward function. System dynamics is described by a nonlinear planar single track vehicle model with dynamic wheel mode model. The goodness of the planned trajectory is evaluated by driving the vehicle along the track. The paper shows that this encapsulated problem and environment provides a one-step reinforcement learning task with continuous actions that can be handled with Deep Deterministic Policy Gradient learning agent. The solution of the problem provides a real-time neural network-based motion planner along with a tracking algorithm, and since the trained network provides a preliminary estimate on the expected reward of the current state-action pair, the system acts as a trajectory feasibility estimator as well.	Proceedings Paper	2019	10.5220/0007955504220429	[]	[]	-1	-1	-1
J	Mirza, Muhammad Ayzed; Yu, Junsheng; Raza, Salman; Krichen, Moez; Ahmed, Manzoor; Khan, Wali Ullah; Rabie, Khaled; Shongwe, Thokozani	DRL-assisted delay optimized task offloading in automotive-industry 5.0 based VECNs	2023	JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES				The rapid growth of Automotive-Industry 5.0 and its emergence with beyond fifth-generation (B5G) communications, is making vehicular edge computing networks (VECNs) increasingly complex. The latency constraints of modern automotive applications make it difficult to run complex applications on vehicle on-board units (OBUs). While multi-access edge computing (MEC) can facilitate task offloading to execute these applications, it is still a challenge to access them promptly and optimally. Traditional algorithms struggle to guarantee accuracy in such dynamic environment, but deep reinforcement learning (DRL) methods offer improved accuracy, robustness, and real-time decision-making capabilities. In this paper, we propose a DRL-based mobility, contact, and load aware cooperative task offloading (DCTO) scheme. DCTO is designed for both cellular and mmWave radio access technologies (RATs), and both binary and partial offloading mechanisms. DCTO targets delay minimization by opportunistically switching RATs and offloading mechanisms. We consider relative efficacy and neutrality factors as key performance indicators and use them to derive the DRL agent's reward function. Extensive evaluations demonstrate that the DCTO scheme exhibits a substantial enhancement in task success rate, with an increase from 2.61% to 21.34%. It also improves the efficacy factor from 1.38 to 3.52 and reduces the neutrality factor from 4.99 to 0.76. Furthermore, the average task processing time is reduced by a range of 3.77% to 24.15%. Additionally, the DCTO scheme outperforms the other evaluated schemes in terms of reward and TFPS ratio.(c) 2023 The Author(s). Published by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	Article; Early Access		10.1016/j.jksuci.2023.02.013	[]	[]	-1	-1	-1
P	DU C; ZHANG H; JIA Z; TIE Y; LIU Y	Unmanned aerial vehicle track planning optimizing            method for use in e.g. intelligent agriculture            application, involves outputting optimal flight            strategy of unmanned vehicle through decision model            after strengthening learning						NOVELTY - The method involves pre-setting an internet-of-things system model, a channel model and an energy consumption model. A pre-deployment track of an unmanned aerial vehicle is obtained through the internet-of-things system model. An information age model is used to measure an information age. A decision model related to the optimal track of the unmanned aerial vehicle is established. Reinforcement learning to the decision model is performed by an improved algorithm. An optimal flight strategy of the unmanned vehicle is output through a decision model after strengthening learning. USE - Unmanned aerial vehicle track planning optimizing method for use in real-time sensing applications. Uses include but are not limited to intelligent agriculture application, intelligent automobile application, outdoor disaster monitoring application, and indoor health detection application. ADVANTAGE - An age-of-information-oriented unmanned aerial vehicle track planning algorithm is designed by a multi-agent deep strengthening learning algorithm to improve a depth Q network. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an unmanned aerial vehicle track planning optimizing system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a unmanned aerial vehicle track planning optimizing system. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
B	Yu, Gening	Road following using neural networks and reinforcement learning	1995						Dissertation/Thesis	Jan 01 1995		[]	[]	-1	-1	-1
J	Zhaojian Li; Tianshu Chu; Kolmanovsky, I.V.; Xiang Yin; Xunyuan Yin	Cloud Resource Allocation for Cloud-Based Automotive Applications [arXiv]	2017	arXiv		10 pp.	10 pp.	"There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive tasks. Efficient utilization of on-demand cloud resources holds a significant potential to improve future vehicle safety, comfort, and fuel economy. In the meanwhile, issues like cyber security and resource allocation pose great challenges. In this paper, we treat the resource allocation problem for cloud-based automotive systems. Both private and public cloud paradigms are considered where a private cloud provides an internal, company-owned internet service dedicated to its own vehicles while a public cloud serves all subscribed vehicles. This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-based automotive systems. Complications such as stochastic communication delays and task deadlines are explicitly considered. In particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited to utilize the cloud resources for best Quality of Services. On the other hand, a decentralized auction-based model is developed for public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a ""selfish"" agent. Numerical examples are presented to illustrate the effectiveness of the developed techniques."	Journal Paper	17 Jan. 2017		[]	[]	-1	-1	-1
P	WANG H; ZHANG Q; WANG X; MENG R; ZHANG Z; WANG J	Method for realizing communication operation of            multi-intelligent system based on edge enhancement by            using computer device, involves deploying training            structure frame on centralized system platform, and            performing intelligent communication operation between            two intelligent bodies						NOVELTY - The method involves constructing a multi-intelligent system and an edge enhancement the neural network. A node of an intelligent body and multi-dimensional edge information are generated. A value function is generated by using a single-modulation network structure. Multi-intelligent body-reinforcement learning training operation is performed. A network model is deployed to a multi-intelligent system platform. A training structure frame is deployed on a centralized system platform. Intelligent communication operation is performed between two intelligent bodies. USE - Method for realizing communication operation of a multi-intelligent system during game multi-role decision operation and unmanned vehicle driving decision operation based on edge enhancement by using a computer device (claimed). ADVANTAGE - The method enables increasing communication efficiency among multiples intelligent bodies. The method enables increasing convergence speed and yield effect. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for realizing communication operation of a multi-intelligent system during game multi-role decision operation and unmanned vehicle driving decision operation based on edge enhancement by using a computer device. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
C	Omatseye, Oritsegbubemi; Urbanic, R. Jill	System reconfiguration for reverse logistics: A case study	2022	IFAC PAPERSONLINE		115	120	Manufacturers need to effectively address returned products in their system, and challenges occur in the remanufacturing processes due to the uncertainty related to the quantities and quality of the returned components. This research focuses on identifying the challenges encountered in a remanufacturing Reverse Logistics (RL) system and are illustrated with an automotive case study. Lean Six-sigma techniques are used to find these issues encountered in the RL process or system, and a linear programming approach is taken to reconfigure the system to improve the throughput. Additional analyses need to be performed to explore the influence of different production strategies and system layouts. Copyright (C) 2022 The Authors.	Proceedings Paper; Early Access		10.1016/j.ifacol.2022.09.377	[]	[]	-1	-1	-1
C	Ultsch, Johannes; Mirwald, Jonas; Brembeck, Jonathan; de Castro, Ricardo	Reinforcement Learning-based Path Following Control for a Vehicle with Variable Delay in the Drivetrain	2020	2020 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV)	IEEE Intelligent Vehicles Symposium	532	539	In this contribution we propose a reinforcement learning-based controller able to solve the path following problem for vehicles with significant delay in the drivetrain. To efficiently train the controller, a control-oriented simulation model for a vehicle with combustion engine, automatic gear box and hydraulic brake system has been developed. In addition, to enhance the reinforcement learning-based controller, we have incorporated preview information in the feedback state to better deal with the delays. We present our approach of designing a reward function which enables the reinforcement learning-based controller to solve the problem. The controller is trained using the Soft Actor-Critic algorithm by incorporating the developed simulation model. Finally, the performance and robustness is evaluated in simulation. Our controller is able to follow an unseen path and is robust against variations in the vehicle parameters, in our case an additional payload.	Proceedings Paper	2020		[]	[]	-1	-1	-1
P	LEE S	Device for monitoring vehicle manufacturing process, has display unit that is provided for outputting status of production line, and failure analysis unit that is configured to update threshold value based on reinforcement learning						NOVELTY - The device (100) has a sensor unit that collects process data in a vehicle manufacturing process including manufacturing units for each unit of a vehicle and assembling through units. A defect analysis unit is provided to detect defects by defect type based on the process data. A display unit is provided for outputting the status of the production line in which the defect type and defect are detected. The threshold value for each of the reflected light amount and weight is updated based on the threshold setting information. An artificial neural network model is provided to perform reinforcement learning to adjust the step-by-step threshold based on process data. A failure analysis unit is configured to update the threshold value based on the reinforcement learning. USE - Device for monitoring vehicle manufacturing process, and for use with management server. ADVANTAGE - The device is provided for detecting defective portions by monitoring the production status of automotive portions that are mass-produced. The unevenness in the surface treatment process of the units and the assembly unit are detected through the three-dimensional image. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for monitoring vehicle manufacturing process. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the device for monitoring vehicle manufacturing process. (Drawing includes non-English language text)Device for monitoring vehicle manufacturing process (100)Management server (200)				[]	[]	-1	-1	-1
P	WU Q; WANG W; DONG C; HE S; JIA Z	Method for recovering network route of unmanned            aerial vehicle based on reinforcement learning,            involves obtaining optimal strategy with minimum            end-to-end time delay value, and recovering            communication between source and target unmanned            vehicle						NOVELTY - The method involves obtaining a network architecture of an unmanned aerial vehicle group. A network architecture is obtained from a source unmanned aerial node to a target unmanned aerial network. A maximum allowable transmission rate of a single hop between each unmanned aerial plane node and a next hop unmanned aerial airplane node in each candidate route path is obtained according to a free space path loss model between unmanned aerial planes. A communication between the source unmanned airplane node and the target unmanned aircraft node is recovered based on a route path selected by an optimal strategy. USE - Method for recovering network route of an unmanned aerial vehicle based on reinforcement learning. Uses include but are not limited to national defence, detection, real-time monitoring, monitoring, sampling, search and rescue, agriculture, manufacturing industry and environment monitoring. ADVANTAGE - The method does not need extra collecting state data, the used data is the data sampled in the routing process, the calculation and energy consumption cost is low, and the routing path can be recovered in a short time. The method is used for solving the recovery problem of the unmanned aerial vehicle network after the route interruption caused by the condition of intentional attack. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of an unmanned aerial vehicle network architecture (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	He Yilin; Song Ruoyang; Ma Jian	Trajectory Tracking Control of Intelligent Vehicle Based on DDPG Method of Reinforcement Learning	2021	China Journal of Highway and Transport		335	348	To address the problem of lateral control of an intelligent vehicle during trajectory tracking,a trajectory tracking control method for an intelligent vehicle based on the deep deterministic policy gradient (DDPG) method of reinforcement learning is proposed.First,the tracking control of an intelligent vehicle was described as a reinforcement learning process based on the Markov decision process (MDP).The main framework of reinforcement learning was the actor-critic composed of actor and critic neural networks.The reinforcement learning environment included vehicle,tracking,and road models as well as a reward function.Then,the learning agent of the proposed method was updated by DDPG,in which the replay buffer was used to solve the problem of sample correlate on,and the actor and critic neural networks were copied to solve the problem of update divergence.Finally,the proposed method was tested under different scenarios and compared with the deep Q-learning (DQN) and model predictive control (MPC) methods.The results show that the reinforcement learning method based on DDPG has the advantages of a short learning time,small lateral deviation,and small angular deviation,and it can meet the requirements of vehicle tracking at different speeds.When DDPG and DQN are used as the two reinforcement learning methods,both methods can achieve the maximum cumulative reward of training under different scenarios.In the two simulation scenarios,the total learning time of DDPG is 9.53% and 44.19% of DQN,respectively,and the learning time of a single round of training is only 20.28%and 22.09%of DQN.When DDPG,DQN,and MPC are used for control,in the first scenario,the maximum lateral deviation based on DDPG is 87.5%and 50%of DQN and MPC,respectively.In the second scenario,the maximum lateral deviation based on the DDPG method is 75% and 21.34% of DQN and MPC,respectively,and the simulation time is 20.64%and 58.60%of DQN and MPC,respectively.	Article	2021		[]	[]	-1	-1	-1
C	Feher, Arpad; Aradi, Szilard; Becsi, Tamas; Gaspar, Peter; Szalay, Zsolt	Proving Ground Test of a DDPG-based Vehicle Trajectory Planner	2020	2020 EUROPEAN CONTROL CONFERENCE (ECC 2020)		332	337	The paper presents real-world test cases of an optimal trajectory design solution that combines modern control techniques with machine learning. The first step of the current research is to train a reinforcement learning agent in a simulated environment, where the conditions and the applied vehicle are modeled. System dynamics is described by a nonlinear single-track vehicle with dynamic wheel model. The designed trajectory is evaluated by driving the vehicle using a control loop. The reward of the method is based on the sum of different measures considering safety and passenger comfort. The proposed method forms a special one-step reinforcement learning task handled by Deep Deterministic Policy Gradient (DDPG) learning agent. As a result, the learning process provides a real-time neural-network-based motion planner and a tracking algorithm. The evaluation of the algorithm under real conditions is made by using an experimental test vehicle. The test setup contains a high precision GPS module, an automotive inertial sensor, an industrial PC, and communication interface devices. The test cases were performed on the ZalaZone automotive proving ground.	Proceedings Paper	2020	10.23919/ecc51009.2020.9143675	[]	[]	-1	-1	-1
J	Guan, Zheng; Wang, Yuyang; He, Min	Deep Reinforcement Learning-Based Spectrum Allocation Algorithm in Internet of Vehicles Discriminating Services	2022	APPLIED SCIENCES-BASEL				With the rapid development of global automotive industry intelligence and networking, the Internet of Vehicles (IoV) service, as a key communication technology, has been faced with an increasing spectrum of resources shortage. In this paper, we consider a spectrum utilization problem, in which a number of co-existing cellular users (CUs) and prioritized device-to-device (D2D) users are equipped in a single antenna vehicle-mounted communication network. To ensure a business-aware spectrum access mechanism with delay granted in a complex dynamic environment, we consider optimizing a metric that maintains a trade off between maximizing the total capacity of vehicle to vehicle (V2V) and vehicle to infrastructure (V2I) links and minimizing the interference of high priority links. A low complexity priority-based spectrum allocation scheme based on the deep reinforcement learning method is developed to solve the proposed formulation. We trained our algorithm using the deep Q-learning network (DQN) over a set of public bandwidths. Simulation results show that the proposed scheme can allocate spectrum resources quickly and effectively in a high dynamic vehicle network environment. Concerning improved channel transmission rate, the V2V link rate in this scheme is 2.54 times that of the traditional random spectrum allocation scheme, and the V2I link rate is 13.5% higher than that of the traditional random spectrum allocation scheme. The average total interference received by priority links decreased by 14.2 dB compared to common links, realized service priority distinction and has good robustness to communication noise.	Article	FEB 2022	10.3390/app12031764	[]	[]	-1	-1	-1
P	BAI M; CHEN H; LEE C; LI Z	Automotive virtual surround audio system for automobile i.e. car, has filter receiving temporary rear-left and rear-right audio sources to perform filtering operation and produce virtual rear-left and rear-right audio sources						NOVELTY - The system has a synthesizer (110) receiving left-and right-channel audio sources (L, R). A weighting device (120) produces temporary front-left and front-right audio sources (FL', FR'). A filter (130) i.e. comb filter, receives the temporary front-left and front- right audio sources to perform the filtering operation, and produces virtual front-left and front-right audio sources (VFL, VFR). Another filter (140) receives temporary rear-left and rear-right audio sources (RL', RR') to perform the filtering operation and produce virtual rear-left and rear-right audio sources (VRL, VRR). USE - Automotive virtual surround audio system for use in an automobile i.e. car, to receive left-and right-channel audio sources to reproduce virtual audio image position. ADVANTAGE - The synthesizer produces surround signal to produce different playback contents at front and rear and to produce the appropriate spatial sense, thus improving positioning uncertainty. The system receives a two-channel audio source i.e. left and right channel audio source, thus reproducing the accurate virtual audio image position. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an automotive virtual surround audio system.Temporary front-left audio source (FL')Temporary front-right audio source (FR')Left-channel audio source (L)Right-channel audio source (R)Temporary rear-left audio source (RL')Temporary rear-right audio source (RR')Virtual front-left audio source (VFL)Virtual front-right audio sources (VFR)Virtual rear-left audio source (VRL)Virtual rear-right audio source (VRR)Synthesizer (110)Weighting device (120)Filters (130, 140)				[]	[]	-1	-1	-1
C	Wang, Xiaodong; Guo, Zhigang; Zhang, Wei; Sun, Rui	Coordinated control in Agent of automotive airbag roll device	2014	MODERN TENDENCIES IN ENGINEERING SCIENCES	Applied Mechanics and Materials	307	+	Airbags tension as an important part of quality assurance which is produced in the airbag roll debice. In this paper, an adaptive reinforcement learning method, which is based on Agent, is used to achieve real-time coordination of two servo motor. The results show that the system has a good dynamic and static characteristics, and which can ensure the constant tension of the airbags and product quality.	Proceedings Paper	2014	10.4028/www.scientific.net/AMM.533.307	[]	[]	-1	-1	-1
J	Mohd Azmin, F.; Mustafa, K.	Smart OTA Scheduling for Connected Vehicles using Prescriptive Analytics and Deep Reinforcement Learning	2022	SAE Technical Papers		2022	01-1045	OTA (over the air) updates help automotive manufacturers to reduce vehicle warranty and recall costs. Vehicle recall is expensive, and many automotive manufacturers have implemented OTA updates. Updating parameters for connected vehicles can be challenging when dealing with thousands of vehicles across different regions. For example, how does the manufacturer prioritise which vehicles need updating? Environmental and geographical factors affect degradation rates and vehicles in hotter regions or congested cities may degrade faster. For EVs, updating the BMS (battery management system) parameters requires careful analysis prior to the update being deployed, to maximise impact and reduce the likelihood of adverse behaviour being introduced. The analysis overhead increases with the number of vehicles. This is because it requires simulation and optimisation of the fleet BMS calibration in a digital twin environment. A targeted approach is the best option to prioritise vehicles for software updates. Smart OTA scheduling makes use of predictive analytics for battery health prediction together with prescriptive analytics in a smart decision engine. The smart scheduling system uses a deep reinforcement learning (DRL) agent in a digital twin environment. The DRL agent can learn and simulate different scenarios and identify the best update sequence depending on monthly temperature profile, traffic congestion, and many other factors to slow down the degradation of fleet health. Whenever there is an update, the DRL agent assesses the situation and recommends the appropriate action to minimise vehicle failures and maintain fleet health. In a large-scale simulation, this approach improved average fleet battery life by 8 to 13% and increased average vehicle range by 2%.	Conference Proceedings	2022	10.4271/2022-01-1045	[]	[]	-1	-1	-1
J	Jones, Christopher Charles	Reluctant Reinforcement Learning	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
B	Ahaggach, H.	Data Analytics and Machine Learning for Smart Decision Making in Automotive Sector	2023	Enterprise Design, Operations, and Computing. EDOC 2022 Workshops: IDAMS, SoEA4EE, TEAR, EDOC Forum, Demonstrations Track and Doctoral Consortium, Revised Selected Papers. Lecture Notes in Business Information Processing (466)		357	63	The objective of this thesis is to conduct scientific research on the use of data science and artificial intelligence techniques in the practices of automotive dealership companies to assist them in their decision-making processes and to use data-driven methods with modeling approaches for computing these enterprises. By proposing algorithms capable of continuously extracting relevant information from a diverse and multi-structured automotive environment. Due to the large amount of data available within these companies, we will develop algorithms to correctly assess the situation, suggest recommendations for decision-making, develop marketing strategies, and automate manual tasks that cost time, effort, and money.	Conference Paper	2023	10.1007/978-3-031-26886-1_24	[]	[]	-1	-1	-1
J	Zhou, Y. F.; Huang, L. J.; Sun, X. X.; Li, L. H.; Lian, J.	A Long-term Energy Management Strategy for Fuel Cell Electric Vehicles Using Reinforcement Learning	2020	FUEL CELLS		753	761	The two power sources of a fuel cell electric vehicle (FCEV) are proton electrolyte membrane fuel cell (PEMFC) and Li-ion battery (LIB). The health status of PEMFC and LIB decreases with the use of FCEV, so the energy management strategy (EMS) needs to give an optimal power distribution based on the health status of power sources throughout the lifetime. However, rule-based control strategies cannot achieve this. To prolong the service lifetime of two power sources by optimizing power distribution, this article proposes a long-term energy management strategy (LTEMS) for FCEV, which contains a reinforcement learning module and an improved thermostat controller. By designing a reward function, the reinforcement learning module outputted various LIB state of charge (SOC) boundary which changes with power source attenuation. Based on SOC boundary, the improved thermostat controller will control the fuel cell current under specific driving conditions. Simulation was carried out based on different LIB state of health (SOH) and external temperature, and the simulation results were compared with the data collected from FCEV under rule-based (RB) strategies. It can be found that the proposed LTEMS can effectively reduce fuel cell and LIB attenuation, and meet the FCEV power demand.	Article; Early Access		10.1002/fuce.202000095	[]	[]	-1	-1	-1
J	Coad, J.; Zhiqian Qiao; Dolan, J.M.	Safe Trajectory Planning Using Reinforcement Learning for Self Driving [arXiv]	2020	arXiv		7 pp.	7 pp.	Self-driving vehicles must be able to act intelligently in diverse and difficult environments, marked by high-dimensional state spaces, a myriad of optimization objectives and complex behaviors. Traditionally, classical optimization and search techniques have been applied to the problem of self-driving; but they do not fully address operations in environments with high-dimensional states and complex behaviors. Recently, imitation learning has been proposed for the task of self-driving; but it is labor-intensive to obtain enough training data. Reinforcement learning has been proposed as a way to directly control the car, but this has safety and comfort concerns. We propose using model-free reinforcement learning for the trajectory planning stage of self-driving and show that this approach allows us to operate the car in a more safe, general and comfortable manner, required for the task of self driving.	Journal Paper	9 Nov. 2020		[]	[]	-1	-1	-1
J	Zhou, Yu; Ong, Ghim Ping; Meng, Qiang; Cui, Haipeng	Electric bus charging facility planning with uncertainties: Model formulation and algorithm design	2023	TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES				This paper investigates the electric bus charging facility planning (EB-CFP) problem for a bus transit company operating a heterogeneous electric bus (EB) fleet to provide public transportation services, taking into account uncertainties in both EB travel time and battery degradation. The goal of the EB-CFP problem is to determine the number and type of EB chargers that should be deployed at bus terminals and depots to meet daily EB charging demand while minimizing total cost. The problem is formulated as a two-stage stochastic programming model, with the first stage determining the EB charger deployment scheme and the second stage estimating the EB daily operational cost with respect to a predetermined EB trip timetable for a given EB charger deployment scheme. To effectively address the second stage problem, a multi -agent EB transit simulation system that mimics the daily EB operation process is developed. We then design two heuristic methods, the reinforcement learning (RL)-based method and the surrogate-based optimization (SBO), that use the developed multi-agent EB transit simulation system to solve the two-stage stochastic programming model for large-scale instances. Lastly, we run a series of experiments on a fictitious EB transit network and a real-world EB transit network in Singapore to evaluate the performance of the models and algorithms. In order to improve the performance of the EB transit system, some managerial insights are also provided to urban bus transit companies.	Article; Early Access		10.1016/j.trc.2023.104108	[]	[]	-1	-1	-1
C	Leal-Millan, Antonio; Leal-Rodriguez, Antonio; Roldan, Jose; Ortega-Gutierrez, Jaime	ICTs and Relational Learning in Networks as Drivers of Green Innovation and Customer Capital: Empirical Evidence From the Spanish Automotive Industry	2015	PROCEEDINGS OF THE 7TH EUROPEAN CONFERENCE ON INTELLECTUAL CAPITAL (ECIC 2015)	Proceedings of the European Conference on Intellectual Capital	208	216	For the purposes of our research, we use the concept of information technology (IT) infrastructure, defined as the shared IT capabilities that enable the flow of knowledge in an organization to be supported. In this category we include a set of technological resources, both hardware and software applications, which support different utilization characteristics of knowledge and relational learning (RL) activities, such as: business intelligence, technologies for collaborating and distributing knowledge, knowledge generation and storage, and support hardware for these technologies. An emerging stream of research on IT and RL seeks to guide the application of technologies that support RL. IT is involved in the various knowledge management processes, which include knowledge creation. A great variety of procedures, tools and activities may act as a support to the knowledge generation and creation process. IT contributes to sustainable competitive advantage through its interaction with other resources. Recent literature suggests that RL is a process that plays an important role in enhancing a firm's capabilities and competitive advantage and which may benefit from the judicious application of IT. It has also been argued that for firms to be successful they must complement IT with RL. This study aims to assess the role played by information technology (IT) in relational learning activities (RL). We also examine how IT and RL influence both green innovations (GI) and the development of the customer capital (CC). These relationships have been tested via an empirical analysis carried out with a sample of industrial companies belonging to the Spanish automotive industry. Our findings allow us to confirm that IT acts as an enabler of the RL process and influences on the development of GI, which allow the achievement of a better customer capital (CC).	Proceedings Paper	2015		[]	[]	-1	-1	-1
P	ZHENG Z; ZHU D; ZHOU B	Risk-aware path planning method based on deep            reinforcement learning, involves repeating preset steps            to continuously update experience in experience            playback cache storage module, and training and            updating IMADQN model						NOVELTY - The method involves collecting map data, classifying land according to an application, and forming node information. A road distance data and a risk value initial data are stored in a matrix form. An IMADQN model is built. The IMADQN model comprises a neural network group module, an experience playback cache storage module, an agent and environment module. The neural network group module comprises multiple neural networks for simulating different types of smart agents, input data of the neural network group module is set as a reagent, the reagent comprises a departure point and a destination, and output data of the neural network group module is set as action. Preset steps are repeated until reaching a destination, and the neural network group module is updated every time multiple actions are performed. The preset steps are repeated to continuously update experience in experience playback cache storage module, and the IMADQN model is trained and updated. USE - Risk-aware path planning method based on deep reinforcement learning for unmanned vehicle and robot. Can also be used for path planning of large map. ADVANTAGE - The behavior of multiple agents with rules existing in the map is simulated through multiple neural networks, the neural network group module is updated continuously, dynamic changes in the simulated map are enabled to be time-efficient, and various road factors are effectively combined together, so that the problem of multi-objective path planning with changeable road information is solved. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of an IMADQN model. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
J	Hemavathi; Akhila, Sreenatha Reddy; Alotaibi, Youseef; Khalaf, Osamah Ibrahim; Alghamdi, Saleh	Authentication and Resource Allocation Strategies during Handoff for 5G IoVs Using Deep Learning	2022	ENERGIES				One of the most sought-after applications of cellular technology is transforming a vehicle into a device that can connect with the outside world, similar to smartphones. This connectivity is changing the automotive world. With the speedy growth and densification of vehicles in Internet of Vehicles (IoV) technology, the need for consistency in communication amongst vehicles becomes more significant. This technology needs to be scalable, secure, and flexible when connecting products and services. 5G technology, with its incredible speed, is expected to power the future of vehicular networks. Owing to high mobility and constant change in the topology, cooperative intelligent transport systems ensure real time connectivity between vehicles. For ensuring a seamless connectivity amongst the entities in vehicular networks, a significant alternative to design is support of handoff. This paper proposes a scheme for the best Road Side Unit (RSU) selection during handoff. Authentication and security of the vehicles are ensured using the Deep Sparse Stacked Autoencoder Network (DS2AN) algorithm, developed using a deep learning model. Once authenticated, resource allocation by RSU to the vehicle is accomplished through Deep-Q learning (DQL) techniques. Compared with the existing handoff schemes, Reinforcement Learning based on the MDP (RL-MDP) has been found to have a 13% lesser decision delay for selecting the best RSU. A higher level of security and minimum time requirement for authentication is achieved using DS2AN. The proposed system simulation results demonstrate that it ensures reliable packet delivery, significantly improving system throughput, upholding tolerable delay levels during a change of RSUs.	Article	MAR 2022	10.3390/en15062006	[]	[]	-1	-1	-1
J	Abdulrahman, Muhammad D.; Gunasekaran, Angappa; Subramanian, Nachiappan	Critical barriers in implementing reverse logistics in the Chinese manufacturing sectors	2014	INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS		460	471	Reverse logistics (RL) is gaining momentum worldwide due to global awareness and as a consequence of resource depletion and environmental degradation. Firms encounter RI implementation challenges from different stakeholders, both internally and externally. On the one hand, various governmental agencies are coming out with different environmental regulations while on the other hand academics and researchers are contributing solutions and suggestions in different country contexts. In a real sense however, the benefits of RL implementation is not yet fully realized in the emerging economies. This paper proposes a theoretical RL implementation model and empirically identifies significant RL barriers with respect to management, financial, policy and infrastructure in the Chinese manufacturing industries such as automotive, electrical and electronic, plastics, steel/construction, textiles and paper and paper based products. Key barriers from our study, with respect to these four categories, are: within management category a lack of reverse logistics experts and low commitment, within financial category a lack of initial capital and funds for return monitoring systems, within policy category a lack of enforceable laws and government supportive economic policies and, finally, within infrastructure category a the lack of systems for return monitoring. Contingency effect of ownership was carried out to understand the similarities and differences in RL barriers among the multinational firms and domestic firms investigated. (C) 2012 Elsevier B.V. All rights reserved.	Article	JAN 2014	10.1016/j.ijpe.2012.08.003	[]	[]	-1	-1	-1
B	Luan, J.; Guo, L.; Ma, B.; Chen, H.	Time Series Prediction Integrated RNN-DRL for Energy-Emission Management in HEVs	2023	2023 7th CAA International Conference on Vehicular Control and Intelligence (CVCI)		1	6	Learning-based strategies have gained widespread adoption in the field of energy management strategy (EMS) for hybrid electric vehicles (HEVs). However, existing algorithms often struggle to maintain the effectiveness in different driving scenarios. Additionally, achieving a fully observable Markov decision process in real-world situations is impractical due to the limitation in sensor sensitivity and noise. Therefore, it is crucial to design a stable EMS for HEVs that can adapt to various driving scenarios. This paper proposes a novel approach called the twin-delayed deep deterministic policy gradient with a long-short-term combined algorithm. It aims to enhance the generalization performance and EMS capability of HEVs' learning-based EMS by integrating temporal predicted information using a recurrent neural network. Comparing this algorithm with other established deep reinforcement learning algorithms, the results demonstrate significant improvements in EMS performance for HEVs, including state of charge maintenance, energy conservation, and emission reduction. Notably, the algorithm achieves these improvements while ensuring robust generalization across diverse driving cycles and scenarios.	Conference Paper	2023	10.1109/CVCI59596.2023.10397382	[]	[]	-1	-1	-1
C	Baumann, Daniel; Pfeffer, Raphael; Sax, Eric	Automatic Generation of Critical Test Cases for the Development of Highly Automated Driving Functions	2021	2021 IEEE 93RD VEHICULAR TECHNOLOGY CONFERENCE (VTC2021-SPRING)	IEEE Vehicular Technology Conference VTC			The development of highly automated driving functions is currently one of the key drivers for the automotive industry and research. In addition to the technical constraints in the implementation of these functions, a major challenge is the verification of functional safety. Conventional approaches aiming at statistical validation in the sense of real test drives are reaching their economic limits. On the other hand, there are simulation methods that allow a lot of freedom in test case design, but whose representativeness and relevance must be proven separately. In this paper an approach is presented that allows to generate critical concrete scenarios and test cases for automated driving functions by means of a reinforcement learning based optimization using here the example of an overtaking assistant. For this purpose, a Q-Learning approach is used that automates the parameter generation for the test cases. While pure combinatorics of the variable parameters leads to an unmanageable amount of test cases, the percentage of actually relevant critical test cases is very low. In this work we show how the share of critical and thus relevant test cases can be increased significantly by using the presented method compared to a purely combinatorial parameter variation.	Proceedings Paper	2021	10.1109/VTC2021-Spring51267.2021.9448686	[]	[]	-1	-1	-1
C	Aradi, Szilard; Becsi, Tamas; Gaspar, Peter	Policy Gradient based Reinforcement Learning Approach for Autonomous Highway Driving	2018	2018 IEEE CONFERENCE ON CONTROL TECHNOLOGY AND APPLICATIONS (CCTA)		670	675	The paper presents the application of the Policy Gradient reinforcement learning method in the area of vehicle control. The purpose of the research presented is to design an end-to-end behavior control of a kinematic vehicle model placed in a simulated highway environment, by using reinforcement learning approach. The environment model for the surrounding traffic uses microscopic simulation to provide different situations for the agent. The environment sensing model is based on high-level sensor information, e.g. the odometry, lane position and surrounding vehicle states that can be reached from the current automotive sensors, such as camera and radar based systems. The objectives were reached through the definition of a rewarding system, with subrewards and penalties enforcing desired speed, lane keeping, keeping right and avoiding collision. After the description of the theoretical basis the environment model with the reward function is detailed. Finally the experiments with the learning process are presented and the results of the evaluation are given from some aspects of control quality and safety.	Proceedings Paper	2018		[]	[]	-1	-1	-1
J	Zhou, Yafu; Wang, Hantao; Li, Linhui; Lian, Jing	Bench calibration method for automotive electric motors based on deep reinforcement learning	2020	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS		607	626	The efficiency and control accuracy of Interior Permanent Magnet Synchronous Motor (IPMSM) are the main factors affecting performance. Manual calibration has the disadvantage of high work intensity, long calibration period and high technical requirement, which leads to low calibration accuracy and motor efficiency. Thus, a novel calibration method based on Deep Deterministic Policy Gradient (DDPG) and Long Short-Term Memory (LSTM) is proposed. By constructing a deep reinforcement learning network, the self-optimization of the optimal working point under any working condition is realized, and the MAP for IPMSM in full speed-torque range is obtained. The method can be used to quickly realize the optimal matching of d-q axis current with arbitrary stator current. It focuses on solving the problem of motor overheating caused by long adjustment time of manually calibrated MAP when the motor is overloaded, to realize fast calibration in overload area. Moreover, the method reduces the dependence on the motor parameters and increases the adaptability of the calibration MAP data to the operating conditions. The simulation and bench test indicate that the method can meet the response requirements of motor torque, and results reveal that the motor efficiency is greatly improved.	Article	2020	10.3233/JIFS-191567	[]	[]	-1	-1	-1
J	Manjunatha, H.; Pak, A.; Filev, D.; Tsiotras, P.	KARNet: Kalman Filter Augmented Recurrent Neural Network for Learning World Models in Autonomous Driving Tasks [arXiv]	2023	arXiv				Autonomous driving has received a great deal of attention in the automotive industry and is often seen as the future of transportation. The development of autonomous driving technology has been greatly accelerated by the growth of end-to-end machine learning techniques that have been successfully used for perception, planning, and control tasks. An important aspect of autonomous driving planning is knowing how the environment evolves in the immediate future and taking appropriate actions. An autonomous driving system should effectively use the information collected from the various sensors to form an abstract representation of the world to maintain situational awareness. For this purpose, deep learning models can be used to learn compact latent representations from a stream of incoming data. However, most deep learning models are trained end-to-end and do not incorporate any prior knowledge (e.g., from physics) of the vehicle in the architecture. In this direction, many works have explored physics-infused neural network (PINN) architectures to infuse physics models during training. Inspired by this observation, we present a Kalman filter augmented recurrent neural network architecture to learn the latent representation of the traffic flow using front camera images only. We demonstrate the efficacy of the proposed model in both imitation and reinforcement learning settings using both simulated and real-world datasets. The results show that incorporating an explicit model of the vehicle (states estimated using Kalman filtering) in the end-to-end learning significantly increases performance.	Journal Paper	23 May 2023		[]	[]	-1	-1	-1
J	Velmurugan, P.; Ashok, B.	Improving the quality of service by continuous traffic monitoring using reinforcement learning model in VANET	2022	INTERNATIONAL JOURNAL OF MODELING SIMULATION AND SCIENTIFIC COMPUTING				The growing number of automobiles on the road has now become a significant source of traffic, accidents, and pollution. Intelligent Transportation Systems (ITSs) could be the key to finding solutions that drastically reduce these issues. The linked vehicular networks channel is a fast-expanding topic for workflow management system and development. Traffic detection is a big issue on city streets. To make informed decisions in order to prevent traffic jams, one of the solutions is the Vehicular Ad-Hoc Network (VANET). We describe an approach for detecting traffic jams in both vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, which provides vehicle drivers with multiple alternatives for determining the location of congestion, its size, and how to prevent becoming stuck in a traffic jam. The data are sent to the driver to assist him in making the appropriate selection. Delivering Quality of Service (QoS) for automotive networks is a difficult challenge due to the characteristics such as terms of transmission or high mobility, congested and fragmented channels, hardware defects, and a large number of vehicular devices. As a result, it is extremely desired to get and distribute resources efficiently. This research uses Reinforcement Learning with Decision-Making Model (RLDMM) method to enhance channel allocation. By using latency, Signal-to-Interference Ratio (SIR) and QoS, the available channel is initially determined. This method is used to determine suitable channel for platoon members. As a result, the proposed RLDMM achieves an SINR reduction of 29 Db, 69 kbps of throughput, 21.4% of collision probability with 4ms of latency.	Article; Early Access		10.1142/S1793962323500344	[]	[]	-1	-1	-1
P	SAKA Y; ONIZUKA T	Electric current distribution system for            automotive vehicles has fusible link connected between            input terminal of junction box and battery, with device            for earthing input terminal in response to crash            sensor										[]	[]	-1	-1	-1
B	Nie, J.; Hu, L.; Liu, Y.; Fan, Y.; Preindl, M.; Jiang, X.	Human-centric data-driven optimization and recommendation in EV-interfaced grid at city scale: poster abstract	2022	BuildSys '22: Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation		295	6	The fast development of electric vehicles (EV) and EV chargers introduces many factors that affect the grid. EV charging and charge scheduling also bring challenges to EV drivers and grid operators. In this work, we propose a human-centric, data-driven, city-scale, multivariate optimization approach for the EV-interfaced grid. This approach takes into account user historical driving and charging habits, user preferences, EV characteristics, city-scale mobility, EV charger availability and price, and grid capacity. The user preferences include the trade-off between cost and time to charge, as well as incentives to participate in different energy-saving programs. We leverage deep reinforcement learning (DRL) to make recommendations to EV drivers and optimize their welfare while enhancing grid performance.	Conference Paper	2022	10.1145/3563357.3567752	[]	[]	-1	-1	-1
J	Ghose, A.; Maity, S.; Kar, A.; Maloo, K.; Dey, S.	Intelligent Orchestration of ADAS Pipelines on Next Generation Automotive Platforms [arXiv]	2020	arXiv		6 pp.	6 pp.	Advanced Driver-Assistance Systems (ADAS) is one of the primary drivers behind increasing levels of autonomy, driving comfort in this age of connected mobility. However, the performance of such systems is a function of execution rate which demands on-board platform-level support. With GPGPU platforms making their way into automobiles, there exists an opportunity to adaptively support high execution rates for ADAS tasks by exploiting architectural heterogeneity, keeping in mind thermal reliability and long-term platform aging. We propose a future-proof, learning-based adaptive scheduling framework that leverages Reinforcement Learning to discover suitable scenario based task-mapping decisions for accommodating increased task-level throughput requirements.	Journal Paper	13 April 2020		[]	[]	-1	-1	-1
J	Lin, M; Malec, J	Timing analysis of reactive rule-based programs	1998	CONTROL ENGINEERING PRACTICE		403	408	Predictability is the crucial feature of real-time systems. It requires that all the tasks in a system meet their timing constraints. For an embedded complex system involving a number of subsystems, the timing properties of the subsystems play an important role, since the total system's response depends on the temporal behaviour of all the subsystems. In this paper, the timing properties of reactive programs written in a rule-based language RL are analyzed. RL is a relatively simple rule language for programming the discrete part of hybrid applications, implemented using a layered architecture. The language has already been used in prototyping a complex automotive application.The upper timing bounds for RL programs, executed using either of the two evaluation strategies, have been studied. Moreover, an analysis tool has been implemented and applied to derive the timing behaviour of a non-trivial application. Some comments on the temporal behaviour of a layered system, consisting of a discrete RL program combined with a set of periodic tasks, are given. (C) 1998 Elsevier Science Ltd. All rights reserved.	Article; Proceedings Paper	MAR 1998	10.1016/S0967-0661(98)00018-5	[]	[]	-1	-1	-1
J	Leal-Rodriguez, Antonio L.; Roldan, Jose L.; Ariza-Montes, Antonio; Leal-Millan, Antonio	From potential absorptive capacity to innovation outcomes in project teams: The conditional mediating role of the realized absorptive capacity in a relational learning context	2014	INTERNATIONAL JOURNAL OF PROJECT MANAGEMENT		894	907	Starting from the construct absorptive capacity, this study separately treats its two dimensions - potential absorptive capacity (PACAP) and realized absorptive capacity (RACAP) - and analyzes their influence on innovation outcomes (IO) in project teams. We also examine potential absorptive capacity as an antecedent of realized absorptive capacity. In addition, we propose that relational learning (RL) will play a moderator role reinforcing the PACAP and RACAP link. Consequently, this paper builds and tests a conditional process model. Data was collected from a sample of 110 project managers of firms belonging to the Spanish automotive components manufacturing sector. Results from variance-based structural equation modeling and PROCESS tool show that RACAP fully mediates the influence of the PACAP on IO, and this indirect effect is positively conditioned by RL. This paper provides evidence that when RL achieves a low value, this indirect influence is not different from zero. (C) 2014 Elsevier Ltd. APM and IPMA. All rights reserved.	Article	AUG 2014	10.1016/j.ijproman.2014.01.005	[]	[]	-1	-1	-1
P	ZENG L; LIU J; SONG W; CHEN S; WANG M	Method for forming multi-intelligent body            reinforced learning unmanned driving based on twin            characterization learning for automatic driving and            robot decision task in e.g. computer science and            control science fields, involves taking joint state and            estimated action value as input of MIX network						NOVELTY - The method involves obtaining an environment information as observation input in a trained decision network. An action decision of an unmanned vehicle is obtained. A formation is realized. A decision-making network comprises a main network and a twin network. The main network is composed of a Q network and a MIX network. The twin network is composed of a twin subnet. A joint state and an estimated action value are taken as an input of the MIX network. The MIX network performs a feature extraction on the joint state under the guidance of a shared network parameter. An estimated action value is combined to obtain an optimal joint action value of the unmanned vehicle under the current observation, the combined action combination of the unmanned vehicle corresponding to the optimal combined action value is used as the action decision. USE - Method for forming multi-intelligent body reinforced learning unmanned driving based on twin characterization learning for automatic driving and robot decision task in computer science, control science, transportation science fields. ADVANTAGE - The method enables realizing flexible formation under the premise of safe and fast driving when the intelligent body has independent decision ability. The barrier is safely avoided, when the traffic flow is large, and the formation is not needed to be maintained. The method uses the twin network to construct the auxiliary task for representing the learning so as to reach the purpose of accurately representing the combined state of the learning, which can effectively solve the problem that the final decision result deviation caused by the unstable combined state is large and the learning cannot be converged or difficult to converge. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the multi-agent reinforcement learning system (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	TAN L; WANG P; XIA J	Edge computing method of cloud-edge-terminal            cooperation involves receiving data amount of            calculation task transmitted by mobile device and            position data of mobile device, and updating unmanned            aerial vehicles in unmanned vehicle group						NOVELTY - The method involves receiving data amount of a calculation task transmitted by a mobile device and position data of the mobile device when generating the calculation task by a base station. The calculation task of device in a cloud side end system is unloaded to the base station until the result is converged or reaches the maximum cycle iteration times. Unloading decision of the unmanned aerial vehicle or task processing device is uninstalled. The unmanned aerial vehicles are deployed in a target area by using genetic method to solve the position deployment of each unmanned aerial vehicle in the target area to realize updating of unmanned aerial vehicles in the unmanned vehicle group. USE - The method is useful for edge computing of cloud-edge-terminal cooperation. ADVANTAGE - The method: comprehensively considers all available computing devices in the network; reduces the system processing energy consumption of computing tasks; improves the service quality of users; has certain flexibility; adopts dueling deep Q-network (DDQN) deep reinforcement learning calculation to obtain the optimal unloading strategy, obtain unloading actions of large number of mobile devices with low complexity, and adapt to dynamic environmental changes; saves computing resources after reducing the complexity and saves computing time; reduces the power consumption of the mobile system; improves the real-time performance of the system; and efficiently utilizes solar energy and clean energy. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of the edge computing method of cloud-edge-terminal cooperation (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	ZHANG G; XUAN S; WANG Y; WANG X; GUAN J; TIAN M; ZHANG X; GAO F; CHEN Y; LIU Z; KE L; GENG H; CHAI X	Simulating learning and strengthening learning            based unmanned aerial vehicle intelligent decision            system, has control execution module for selecting            action instruction with maximum probability by unmanned            vehicle and obtaining environment state of unmanned            aerial vehicle after executing action						NOVELTY - The system has an environment sensing module for extracting and fusing environment information collected by multiple sensors in unmanned aerial vehicle flying process. An expert behavior demonstration module collects expert in various environments and events. A decision learning module obtains a state space vector as an input of a network structure. A control execution module selects an action instruction with maximum probability to be executed by the unmanned vehicle and obtains a new environment state of the unmanned aerial vehicle after executing the action. The decision learning module comprises a simulation learning module and a strengthening learning module. USE - Simulating learning and strengthening learning based unmanned aerial vehicle intelligent decision system. ADVANTAGE - The system can make accurate decision of complex real-time scene, reach target flying from a starting point to a preset ending point to ensure autonomous decision in flight process, according to environment information, real time event and selecting unmanned aerial vehicle control strategy. The system avoids barrier, improves safety efficiency to reach the target point. The system simulates learning to clone the behavior of the expert to obtain the initial value of the decision control network, and realizes deep reinforcement learning. The system sets reward function according to event in the flight task process to ensure accuracy, so that generalization is better decision control Q network for improving unmanned aerial vehicles autonomous control performance. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a simulating learning and strengthening learning based unmanned aerial vehicle intelligent decision system. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
P	LUO L; ZHANG P	Depth deterministic policy gradient (DDPG)            unmanned aerial vehicle landing method based on expert            experience for facing mobile platform, involves            constructing unmanned aerial vehicles landing scene in            simulation simulator using deterministic strategy            gradient method						NOVELTY - The method involves constructing a Markov model of a landing of an unmanned aerial vehicle mobile platform. A neural network is constructed in a DDPG algorithm. Unmanned aerial vehicle and a target state are updated. A landing scene in a simulation simulator is constructed. A deterministic strategy gradient method is used based on expert experience to train the unmanned vehicle to land to the mobile platform is used. The unmanned vehicle and the target state in the simulation simulator are determined. The unmanned aerial vehicle state and the mobile platform state are combined. The observation space of Markov model is defined. USE - Depth deterministic policy gradient (DDPG) unmanned aerial vehicle landing method based on expert experience for facing mobile platform. ADVANTAGE - The convergence speed of the algorithm greatly solves the problem of low sampling efficiency of depth reinforcement learning early stage. The method does not depend on the environment model, by establishing depth, neural network unmanned aerial vehicle and target position, speed sensor information as the input of the neural network. The actor network trained by expert experience is used for later learning, thus increasing the possibility of correct action of strategy network output, indirectly causing the Critic network training acceleration, forming good circulation between the two, improving the training time and convergence time of the model, and greatly reducing the cost. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a DDPG unmanned aerial vehicle landing method based on expert experience of facing mobile platform. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
J	Konstantinidis, Fotios K.; Myrillas, Nikolaos; Tsintotas, Konstantinos A.; Mouroutsos, Spyridon G.; Gasteratos, Antonios	A technology maturity assessment framework for Industry 5.0 machine vision systems based on systematic literature review in automotive manufacturing	2023	INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH				When considering how an intelligent factory can 'see,' the answer lies in machine vision technology. To assess the current technological advancements of machine vision systems and propose a technology maturity assessment framework, a nine-phase Systematic Literature Review (SLR) strategy was implemented. As the automotive industry stands at the forefront of autonomous systems, we analysed 85 works across the entire automotive manufacturing life cycle. The findings revealed that machine vision is utilised in each technological pillar of Industry 4.0, encompassing autonomous robots, augmented reality, predictive maintenance, additive manufacturing, and more. In analysing 22 vision-based applications in 47 automotive components, we clustered machine vision systems' architectural components and processing techniques, ranging from threshold-based methods to advanced reinforcement learning techniques suitable for the I5.0 environment. Leveraging the insights gathered, we propose the I5.0 technology maturity assessment framework for machine vision systems, evaluating nine functional components across five scaling technology levels. This framework serves as a valuable tool to identify weaknesses and opportunities for improvement, guiding machine vision integration into an intelligent factory.	Review; Early Access		10.1080/00207543.2023.2270588	[]	[]	-1	-1	-1
J	Boettinger, M.; Klotz, D.	Mastering Nordschleife -- A comprehensive race simulation for AI strategy decision-making in motorsports [arXiv]	2023	arXiv				"In the realm of circuit motorsports, race strategy plays a pivotal role in determining race outcomes. This strategy focuses on the timing of pit stops, which are necessary due to fuel consumption and tire performance degradation. The objective of race strategy is to balance the advantages of pit stops, such as tire replacement and refueling, with the time loss incurred in the pit lane. Current race simulations, used to estimate the best possible race strategy, vary in granularity, modeling of probabilistic events, and require manual input for in-laps. This paper addresses these limitations by developing a novel simulation model tailored to GT racing and leveraging artificial intelligence to automate strategic decisions. By integrating the simulation with OpenAI's Gym framework, a reinforcement learning environment is created and an agent is trained. The study evaluates various hyperparameter configurations, observation spaces, and reward functions, drawing upon historical timing data from the 2020 N\""urburgring Langstrecken Serie for empirical parameter validation. The results demonstrate the potential of reinforcement learning for improving race strategy decision-making, as the trained agent makes sensible decisions regarding pit stop timing and refueling amounts. Key parameters, such as learning rate, decay rate and the number of episodes, are identified as crucial factors, while the combination of fuel mass and current race position proves most effective for policy development. The paper contributes to the broader application of reinforcement learning in race simulations and unlocks the potential for race strategy optimization beyond FIA Formula~1, specifically in the GT racing domain."	Journal Paper	28 June 2023		[]	[]	-1	-1	-1
B	Yang, B.; Zhou, Z.; Wang, C.; Li, Z.; Li, X.; Chen, F.	Online Scheduling Optimization of User Load Based on Deep Reinforcement Learning and Demand Response	2023	2023 3rd Power System and Green Energy Conference (PSGEC)		609	15	In recent years, the daily electricity consumption of ordinary users has been continuously increasing, which may result in the wastage of power resources and an increase in the burden of power resource scheduling on the grid. Responding to electricity demand and making reasonable use of power resources has become one of the solutions to this problem. In this paper, we propose the use of Deep Reinforcement Learning (DRL) technology to optimize the scheduling of user load Demand Response (DR). Firstly, we establish a comprehensive scheduling model of user load for both the power grid-load and user-load parts. We then add controllable loads, such as commonly used household appliances, user electric vehicles (EVs), and distributed photovoltaic (PV), to construct a load interaction environment. We also proposes that electric vehicles can charge and discharge flexibly based on real-time electricity prices to alleviate the burden of power grid scheduling and prioritize the consumption of distributed photovoltaics. Finally, we solve the Markov decision process and provide experimental evidence for the feasibility and effectiveness of deep reinforcement learning methods in solving the problem of user load demand response.	Conference Paper	2023	10.1109/PSGEC58411.2023.10255917	[]	[]	-1	-1	-1
J	Liu, Jiaqi; Sun, Jian; Qi, Xiao	Optimal Placement of Charging Stations in Road Networks: A Reinforcement Learning Approach with Attention Mechanism	2023	APPLIED SCIENCES-BASEL				With the aim of promoting energy conservation and emission reduction, electric vehicles (EVs) have gained significant attention as a strategic industry in many countries. However, the insufficiency of accessible charging infrastructure remains a challenge, hindering the widespread adoption of EVs. To address this issue, we propose a novel approach to optimize the placement of charging stations within a road network, known as the charging station location problem (CSLP). Our method considers multiple factors, including fairness in charging station distribution, benefits associated with their placement, and drivers' discomfort. Fairness is quantified by the balance in charging station coverage across the network, while driver comfort is measured by the total time spent during the charging process. Then, the CSLP is formulated as a reinforcement learning problem, and we introduce a novel model called PPO-Attention. This model incorporates an attention layer into the Proximal Policy Optimization (PPO) algorithm, enhancing the algorithm's capacity to identify and understand the intricate interdependencies between different nodes in the network. We have conducted extensive tests on urban road networks in Europe, North America, and Asia. The results demonstrate the superior performance of our approach compared to existing baseline algorithms. On average, our method achieves a profit increase of 258.04% and reduces waiting time by 73.40%, travel time by 18.46%, and charging time by 40.10%.	Article	JUL 2023	10.3390/app13148473	[]	[]	-1	-1	-1
J	Wang, Kunyu; Yang, Rong; Huang, Wei; Mo, Jinchuan; Zhang, Song	Deep reinforcement learning-based energy management strategies for energy-efficient driving of hybrid electric buses	2023	PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING		1790	1804	The fuel economy of hybrid electric vehicles is inextricably linked to the energy management strategy (EMS). In this study, a practicality-oriented learning-based EMS for a power-split hybrid electric bus (HEB) is presented, which combines the generative adversarial imitation learning (GAIL) and deep reinforcement learning (DRL). Considering the regular and fixed route of the HEB, optimal control samples that are not affected by the cost function can be obtained by the boundary-line dynamic programing (B-DP) method. On this basis, the samples are dynamically fitted using the GAIL method to inverse derive the reward function that can explain the B-DP control behavior. Concurrently, the proximal policy optimization DRL algorithm will regulate the energy distribution of the vehicle in real time and continuously optimize the energy management capability based on the reward feedback from GAIL. Finally, the feasibility and effectiveness of the proposed EMS is verified by simulation. The results show that the proposed strategy exhibits near-optimal control performance under both the China heavy-duty commercial vehicle cycle-bus and China city bus cycle.	Review; Early Access		10.1177/09544070221103392	[]	[]	-1	-1	-1
P	FEDOR T; STULRAJTER M; MUSAK M	System for determining electrical characteristics            of electric load, has resistance and inductance (RL)            estimation circuit that determines inductance based on            estimated maximum alternating current (AC) current and            phase shift						NOVELTY - The system has a signal modulation circuit (102) including a first integral controller to control alternating current (AC) reference voltage based on a requested maximum AC current and an estimated maximum AC. A second integral controller controls direct current (DC) reference voltages based on requested DC current and estimated DC current. A signal demodulation circuit includes an AC current estimation circuit to generate the estimated maximum current for the modulation circuit. A DC current estimation unit generates the estimated current for modulation circuit, and a resistance and inductance (RL) estimation circuit (108) determines inductance of an electric load (106) based on the estimated AC and phase shift. The estimated AC current is a value greater than a zero-crossing current value. USE - System for determining electrical characteristics of an electric load i.e. electric motor, in an automotive vehicle application. Can also be used for determining resistance and inductance of the electric motor. ADVANTAGE - The apparatus measures the real phase voltage applied to the electric motor, thus enabling precise measurement of the electrical motor parameters such as load inductance and resistance, of the electric load drive system. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for determining electrical characteristics of an electric load. DESCRIPTION Of DRAWING(S) - The drawing shows a simplified block diagram of the electric load drive system.Signal modulation circuit (102)Voltage supply inverter (104)Electric load (106)RL estimation circuit (108)Signal demodulation circuit (110)				[]	[]	-1	-1	-1
J	Tuchnitz, Felix; Ebell, Niklas; Schlund, Jonas; Pruckner, Marco	Development and Evaluation of a Smart Charging Strategy for an Electric Vehicle Fleet Based on Reinforcement Learning	2021	APPLIED ENERGY				Governments are currently subsidizing growth in the electric car market and the associated infrastructure in order to accelerate the transition to more sustainable mobility. To avoid the grid overload that results from simultaneously charging too many electric vehicles, there is a need for smart charging coordination systems. In this paper, we propose a charging coordination system based on Reinforcement Learning using an artificial neural network as a function approximator. Taking into account the baseload present in the power grid, a central agent creates forward-looking, coordinated charging schedules for an electric vehicle fleet of any size. In contrast to optimization-based charging strategies, system dynamics such as future arrivals, departures, and energy consumption do not have to be known beforehand. We implement and compare a range of parameter variants that differ in terms of the reward function and prioritized experience. Subsequently, we use a case study to compare our Reinforcement Learning algorithm with several other charging strategies. The Reinforcement Learning-based charging coordination system is shown to perform very well. All electric vehicles have enough energy for their next trip on departure and charging is carried out almost exclusively during the load valleys at night. Compared with an uncontrolled charging strategy, the Reinforcement Learning algorithm reduces the variance of the total load by 65%. The performance of our Reinforcement Learning concept comes close to that of an optimization-based charging strategy. However, an optimization algorithm needs to know certain information beforehand, such as the vehicle's departure time and its energy requirement on arriving at the charging station. Our novel Reinforcement Learning-based charging coordination system therefore offers a flexible, easily adaptable, and scalable approach for an electric vehicle fleet under realistic operating conditions.	Article; Early Access		10.1016/j.apenergy.2020.116382	[]	[]	-1	-1	-1
B	Guohua Wang; Siddhartha; Mishra, K.V.	STAP in Automotive MIMO Radar with Transmitter Scheduling	2020	2020 IEEE Radar Conference (RadarConf20)		6 pp.	6 pp.	Automotive radars often employ multiple-input multiple-output (MIMO) array to attain high angular resolution with few antenna elements. The diversity gain is generally achieved by time-division multiplexing (TDM) during the transmission of frequency-modulated continuous-wave (FMCW) signals. However, TDM mode leads to longer pulse repetition intervals and, therefore, inherently and severely limits the maximum unambiguous Doppler velocity that a radar is able to detect. In this paper, we address the Doppler ambiguity problem in TDM MIMO automotive radars through a space-time adaptive processing (STAP) approach. A direct application of STAP may lead to a high antenna sidelobe level that hampers the detection performance. We mitigate this through optimal transmitter scheduling. We formulate the problem as combinatorial optimization and solve it via reinforcement learning. Numerical and experimental results demonstrate the efficacy of our method when compared with conventional techniques.	Conference Paper	2020	10.1109/RadarConf2043947.2020.9266601	[]	[]	-1	-1	-1
J	Zhu Bing; Jiang Yuande; Zhao Jian; Chen Hong; Deng Weiwen	A Car-following Control Algorithm Based on Deep Reinforcement Learning	2019	China Journal of Highway and Transport		53	60	Longitudinal acceleration decisions in a car-following control mode are directly determined by the state of the preceding vehicle.A driver's uncertainty makes car-following control difficult because of the complexity in state prediction of the target vehicle.To address the problem in which the performance of adaptive cruise control may deteriorate without consideration of the uncertainty of the preceding vehicle,a car-following control strategy based on deep reinforcement learning was proposed.To study the characteristics of human drivers,a drivingdata- acquisition platform was established,and substantial amounts of human-driving data were collected.Based on the assumption that longitudinal control decisions are mainly affected by the preceding vehicle,a two-predecessor following structure was established.The vehicles in the driving dataset were taken as target vehicles 1#and 2#of the car-following control.Based on the real-world driving dataset,a stochastic process model was established to describe the characteristics of preceding vehicle 1# based on Gaussian process algorithm.Then car-following control was established as a Markov decision process.A car-following control method based on deep reinforcement learning was obtained through iterative learning with the stochastic process model using proximal policy optimization.Finally,the algorithm was verified based on the driving dataset.The results demonstrate that the mapping between longitudinal acceleration decisions and the states of the host and preceding vehicles can be obtained through iterative learning with consideration of the uncertainty of the target vehicle.	Article	2019		[]	[]	-1	-1	-1
J	Ju, Siwei; van Vliet, Peter; Arenz, Oleg; Peters, Jan	Digital Twin of a Driver-in-the-Loop Race Car Simulation With Contextual Reinforcement Learning	2023	IEEE ROBOTICS AND AUTOMATION LETTERS		4107	4114	In order to facilitate rapid prototyping and testing in the advanced motorsport industry, we consider the problem of imitating and outperforming professional race car drivers based on demonstrations collected on a high-fidelity Driver-in-the-Loop (DiL) hardware simulator. We formulate a contextual reinforcement learning problem to learn a human-like and stochastic policy with domain-informed choices for states, actions, and reward functions. To leverage very limited training data and build human-like diverse behavior, we fit a probabilistic model to the expert demonstrations called the reference distribution, draw samples out of it, and use them as context for the reinforcement learning agent with context-specific states and rewards. In contrast to the non-human-like stochasticity introduced by Gaussian noise, our method contributes to a more effective exploration, better performance and a policy with human-like variance in evaluation metrics. Compared to previous work using a behavioral cloning agent, which is unable to complete competitive laps robustly, our agent outperforms the professional driver used to collect the demonstrations by around 0.4 seconds per lap on average, which is the first time known to the authors that an autonomous agent has outperformed a top-class professional race driver in a state-of-the-art, high-fidelity simulation. Being robust and sensitive to vehicle setup changes, our agent is able to predict plausible lap time and other performance metrics. Furthermore, unlike traditional lap time calculation methods, our agent indicates not only the gain in performance but also the driveability when faced with modified car balance, facilitating the digital twin of the DiL simulation.	Article	JUL 2023	10.1109/LRA.2023.3279618	[]	[]	-1	-1	-1
J	Lu Xinyi; Han Xiaolong	Improved NSGA-â…¡ based on reinforcement learning to solve energy-saving scheduling problem of flexible job shop	2023	Modern Manufacturing Engineering		22	35	For the flexible job shop scheduling problem in the context of green manufacturing,a multi-objective integer programming model was established to minimize the completion time,machine load and energy consumption of the workshop,and an improved Non-dominated Sorting Genetic Algorithm-â…¡ (NSGA-â…¡)based on Q-learning was proposed to solve it.Firstly,multiple heuristic algorithms were used to initialize the population to balance the machine load,and an elite pool was introduced to conduct a dual-strategy hybrid crossover for improving the population quality.Secondly,the state space of reinforcement learning based on two metrics of the population was constructed and the hybrid crossover ratio was adjusted by Q-learning training to ensure the uniformity and diversity of the population distribution, as well as avoiding premature maturity of the algorithm. Finally, the performance of the algorithm was analyzed and evaluated by solving Kacem and Brandimarte benchmark cases and a production example of manufacturing parts for automotive engine cooling system.The effectiveness of the model and algorithm in solving the flexible job shop scheduling problem and the superiority in balancing machine load and energy consumption are verified.	Article	2023		[]	[]	-1	-1	-1
C	Elmalaki, Salma; Tsai, Huey-Ru; Srivastava, Mani	Sentio: Driver-in-the-Loop Forward Collision Warning Using Multisample Reinforcement Learning	2018	SENSYS'18: PROCEEDINGS OF THE 16TH CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS		28	40	Thanks to the adoption of more sensors in the automotive industry, context-aware Advanced Driver Assistance Systems (ADAS) become possible. On one side, a common thread in ADAS applications is to focus entirely on the context of the vehicle and its surrounding vehicles leaving the human (driver) context out of consideration. On the other side, and due to the increasing sensing capabilities in mobile phones and wearable technologies, monitoring complex human context becomes feasible which paves the way to develop driver-in-the-loop context-aware ADAS that provide personalized driving experience. In this paper, we propose Sentio(1); a Reinforcement Learning based algorithm to enhance the Forward Collision Warning (FCW) system leading to Driver-in-the-Loop FCW system. Since the human driving preference is unknown a priori, varies between different drivers, and moreover, varies across time for the same driver, the proposed Sentio algorithm needs to take into account all these variabilities which are not handled by the standard reinforcement learning algorithms. We verified the proposed algorithm against several human drivers. Our evaluation, across distracted human drivers, shows a significant enhancement in driver experience-compared to standard FCW systems-reflected by an increase in the driver safety by 94.28%, an improvement in the driving experience by 20.97%, a decrease in the false negatives from 55.90% down to 3.26%, while adding less than 130 ms runtime execution overhead.	Proceedings Paper	2018	10.1145/3274783.3274843	[]	[]	-1	-1	-1
B	Roba, Edoardo	Leap: A Model-Based Reinforcement Learning Framework for Fast Object Detection	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Wang, Cheng; Zhang, Luomeng; Li, Zhong; Jiang, Changjun	SDCoR: Software Defined Cognitive Routing for Internet of Vehicles	2018	IEEE INTERNET OF THINGS JOURNAL		3513	3520	The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field. Large amounts of sensor data require to be transferred in real-time. Most of the routing protocols are specifically targeted to specific situations in IoV. But communication environment of IoV usually changes in the space-time dimension. Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy. Sensing and learning constitute two key steps of the cognition procedure. Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability. To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV. We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm. Results show that our algorithm achieves better performance than several typical protocols in IoV. We also show the feasibility and effectiveness of our proposed SDCIV.	Article	OCT 2018	10.1109/JIOT.2018.2812210	[]	[]	-1	-1	-1
B	Fu, Y.; Versen, D.S.; Plenz, M.; Stiemer, M.; Schulz, D.	Electric Vehicle Charging Management for Avoiding Transformer Congestion Using Policy-based Reinforcement Learning	2023	2023 IEEE PES Innovative Smart Grid Technologies Europe (ISGT EUROPE)		1	5	With the widespread integration of electric vehicles (EV) in distribution networks (DN), the distribution network operator faces new challenges relating to facility overloads such as distribution transformers. Due to similar charging behaviors of EV customers in residential areas, the peak loads caused by simultaneous charging processes will have a significant impact on the operation of DN. In this paper, the focus is on a cooperative approach among EVs with different individual preferences regarding charging demand urgency with the aim to jointly alleviate transformer congestion. An online algorithm using policy-based reinforcement learning is proposed, which takes the advantage of bidirectional charging infrastructures with continuous controllable charging rate for more flexible cooperation among EVs. A particular aim is to overcome the unscalability of neural networks due to the determined network structures before training processes. For this reason, a grouping method concerning individual charging behaviors is used for dimensionality reduction and scalable representation of the state and action space. Simulation results show that the proposed charging management is able to identify the preference of each EV group and to generate an appropriate charging rate with the purpose of minimizing the damage of EV charging satisfactions while avoiding overloading of transformers.	Conference Paper	2023	10.1109/ISGTEUROPE56780.2023.10407357	[]	[]	-1	-1	-1
J	Ahmed, Manzoor; Liu, Jinshi; Mirza, Muhammad Ayzed; Khan, Wali Ullah; Al-Wesabi, Fahd N.	MARL based resource allocation scheme leveraging vehicular cloudlet in automotive-industry 5.0	2023	JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES				Automotive-Industry 5.0 will use Beyond Fifth-Generation (B5G) communications to provide robust, abundant computation resources and energy-efficient data sharing among various Intelligent Transportation System (ITS) entities. Based on the vehicle communication network, the Internet of Vehicles (IoV) is created, where vehicles' resources, including processing, storage, sensing, and communication units, can be leveraged to construct Vehicular Cloudlet (VC) to realize resource sharing. As Connected and Autonomous Vehicles (CAV) onboard computing is becoming more potent, VC resources (comprising stationary and moving vehicles' idle resources) seems a promising solution to tackle the incessant computing requirements of vehicles. Furthermore, such spare computing resources can significantly reduce task requests' delay and transmission costs. In order to maximize the utility of task requests in the system under the maximum time constraint, this paper proposes a Secondary Resource Allocation (SRA) mechanism based on a dual time scale. The request service process is regarded as M/ M/1 queuing model and considers each task request in the same time slot as an agent. A Partially Observable Markov Decision Process (POMDP) is constructed and combined with the Multi-Agent Reinforcement Learning (MARL) algorithm known as QMix, which exploits the overall vehicle state and queue state to reach effective computing resource allocation decisions. There are two main performance metrics: the system's total utility and task completion rate. Simulation results reveal that the task completion rate is increased by 13%. Furthermore, compared with the deep deterministic policy optimization method, our proposed algorithm can improve the overall utility value by 70% and the task completion rate by 6%. & COPY; 2022 The Author(s). Published by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).	Article	JUN 2023	10.1016/j.jksuci.2022.10.011	[]	[]	-1	-1	-1
J	Chang, Chengcheng; Zhao, Wanzhong; Wang, Chunyan; Luan, Zhongkai	An energy management strategy of deep reinforcement learning based on multi-agent architecture under self-generating conditions	2023	ENERGY				To improve the driving efficiency of hybrid power vehicle, an energy management strategy of deep reinforcement learning based on multi-agent architecture under self-generating vehicle driving conditions is proposed. Firstly, the kinematics segments are self-generated based on the Wasserstein generative adversarial network. The generator network G is used to generate kinematics segments. The discriminator network D is used to judge the credibility of the generated kinematics segments with the Wasserstein distance. The speed distribution characteristics of the training conditions and verification conditions established based on the self-generated segments are verified. Afterward, a multi-agent algorithm based on twin delayed deep deterministic policy gradient algorithm for hybrid systems is proposed by introducing centralized training with decentralized execution framework. The engine and a motor are used as two independent agents respectively. Different reward functions are designed based on training objectives to establish a mutually beneficial relationship of cooperation-restraint between the two agents. A driving mode constraint is designed in the environment to improve sample utilization. Finally, the simulation results demonstrate that our method can achieve better performance compared with other existing works.	Article; Early Access		10.1016/j.energy.2023.128536	[]	[]	-1	-1	-1
B	Adjailia, F.; Takac, M.	Exploring the Application of Machine Learning in Computational Fluid Dynamics	2023	2023 IEEE World Conference on Applied Intelligence and Computing (AIC)		51	7	Computational fluid dynamics (CFD) is an important tool for understanding and predicting the behavior of fluids in various systems. Machine learning techniques have the potential to significantly improve the efficiency and accuracy of CFD simulations, and have been applied to a wide range of tasks, including turbulence modeling, boundary layer prediction, flow separation prediction, and optimization of system design. In this review, we provide an overview of the main machine learning methods that have been applied in CFD, including neural networks, support vector machines, evolutionary algorithms, and reinforcement learning. We also discuss the benefits and limitations of these methods and their potential applications in different fields, such as aerospace engineering, automotive engineering, biomedical engineering, and environmental engineering. Finally, we identify future research directions and provide recommendations for the effective use of machine learning techniques in CFD.	Conference Paper	2023	10.1109/AIC57670.2023.10263832	[]	[]	-1	-1	-1
B	[Anonymous]	2011 IEEE International Symposium on Computer-Aided Control System Design	2011	2011 IEEE International Symposium on Computer-Aided Control System Design				The following topics are dealt with: formation shape control; multiagent system; energy systems control; embedded control; fault diagnostics; adaptive control; aerospace application; reinforcement learning; robust model-based control; fusion plasmas control; automotive application; control education; repetitive processe; iterative learning control; optimization; intelligent mechatronics; drill string vibration control; Kalman filtering; optimal control and renewable energy electric grid integration.	Conference Proceedings	2011		[]	[]	-1	-1	-1
B	Sinha, S.; Franciosa, P.; Ceglarek, D.	Object Shape Error Correction using Deep Reinforcement Learning for Multi-Station Assembly Systems	2021	2021 IEEE 19th International Conference on Industrial Informatics (INDIN)		8 pp.	8 pp.	The paper proposes a novel approach, Object Shape Error Correction (OSEC), to determine corrective action in order to mitigate root cause(s) (RCs) of dimensional and geometric product shape errors. It leverages Deep Deterministic Policy Gradient (DDPG) algorithm to learn optimal process parameters update policies based on high dimensional state estimates of multi-station assembly systems (MAS). These policies can be interpreted in engineering terms as sequential corrective adjustments of process parameters that are necessary to mitigate RCs of product shape errors. The approach has the capability to estimate adjustments of process parameters related to fixturing and joining while simultaneously accounting for (i) RC uncertainty estimation, (ii) Key Performance Indicator (KPI) improvement, (iii) MAS design architecture; and, (iv) MAS inherent stochasticity. In addition, the OSEC methodology leverages a reward function parameterized by user interpretable functional coefficients for optimal tradeoff involving various corrections requirements. Benchmarking using an industrial, automotive cross-member assembly system demonstrates a 40% increase in the effectiveness of corrective actions when compared to current approaches.	Conference Paper	2021	10.1109/INDIN45523.2021.9557359	[]	[]	-1	-1	-1
J	Fang, H.; Tu, Y.; He, S.; Wang, H.; Sun, C.; Cheng, S.S.	Self-Learning Takagi-Sugeno Fuzzy Control With Application to Semicar Active Suspension Model	2024	IEEE Transactions on Fuzzy Systems		64	74	In this article, we investigate the optimal control problem for semicar active suspension systems (SCASSs). First, we model the SCASSs by Newtonian dynamics as well as considering the uncertainties and nonlinear dynamics of the actuator. Second, in order to solve the complexity brought by uncertainties, we apply the Takagi-Sugeno (T-S) fuzzy approach to transform the SCASSs as multilinear systems, as well as solving the optimal control problem as a zero-sum problem to find the solution of Nash-equilibrium. Third, we construct a novel self-learning method based on the reinforcement learning framework, and propose two algorithms to solve the fuzzy game algebraic Riccati equation. Especially, in the second algorithm, without using any model information of the SCASSs, we only use the state and input information in control design by a self-learning manner removing the traditional dependence problem, which is more preferable for practical applications. Finally, we give a simulation result of the SCASSs to demonstrate the effectiveness and practicability for the designed self-learning algorithms.	Journal Paper	2024	10.1109/TFUZZ.2023.3290041	[]	[]	-1	-1	-1
B	Lee, J.; Niyato, D.; Yong Liang Guan; Dong In Kim	Learning to Schedule Joint Radar-Communication Requests for Optimal Information Freshness	2021	2021 IEEE Intelligent Vehicles Symposium (IV)		8	15	Radar detection and communication are two of several sub-tasks essential for the operation of next-generation autonomous vehicles (AVs). The former is required for sensing and perception, more frequently so under various unfavorable environmental conditions such as heavy precipitation; the latter is needed to transmit time-critical data. Forthcoming proliferation of faster 5G networks utilizing mmWave is likely to lead to interference with automotive radar sensors, which has led to a body of research on the development of Joint Radar Communication (JRC) systems and solutions. This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV. We formulate the problem as a Markov Decision Process (MDP) where the JRC agent determines in a real-time manner when radar detection is necessary, and how to manage a multiclass data queue where each class represents different urgency levels of data packets. Simulations are run with a range of environmental parameters to mimic variations in real-world operation. The results show that deep reinforcement learning allows the agent to obtain good results with minimal a priori knowledge about the environment.	Conference Paper	2021	10.1109/IV48863.2021.9575131	[]	[]	-1	-1	-1
J	Sorker, Farhana	Investigation on Reverse Logistics of End of Life Cars in the Uk	2019						Dissertation/Thesis	Jan 01 2019		[]	[]	-1	-1	-1
J	Guo Bocang; Wang Yinlin; Xie Xianyi; Jin Lisheng; Han Guangde	Decision Making Method for Control Right Transition of Human- machine Shared Driving Based on Driver-vehicle Risk State	2022	China Journal of Highway and Transport		153	165	Frequent traffic accidents have proved that driving is a high-risk event and risky driving behaviors are one of the main causes. Using an automatic driving system as an agent to assist or replace human drivers is considered an effective way to fundamentally solve the threats caused by human factors. First, to maximize the overall safety of intelligent vehicles, a human-vehicle risk game model was established by utilizing entropy- technique for order preference by similarity to ideal solution (TOPSIS) and complete static game theory. A strategy function to maximize the relative utility was proposed and embedded in the reinforcement learning reward function, then the reward and punishment mechanism guided by maximizing vehicle safety expectation were deduced. Second, taking advantage of reinforcement learning which is good at solving sequence decision-making problems, a human-vehicle driving control transition method based on advantage actor critical (A2C) was proposed. The output effect of the decision model was optimized by iterating the decision weights and reward functions, and the validity of the training process and result was verified by the model performance evaluation indices. Finally, the influence of different transition times on vehicle safety was analyzed through simulation test. A control right decision-making method that can limit risky behaviors and improve vehicle safety timely and effectively was proposed. The results showed that this research innovatively takes the actor and critic modules mapped from the human and vehicle risk monitoring module to A2C as the framework, which fully utilizes the interaction between intelligent vehicle and human-vehicle risk state. Moreover, it achieves the maximum return by obtaining rewards updated iteratively. The decision-making method of human-machine driving control right guided by promoting the maximization of vehicle safety is realized.	Article	2022		[]	[]	-1	-1	-1
B	Kraus, D.; Specht, E.; Merz, T.; Hiller, M.	Optimized Real-Time Control for Modular Multilevel Converters using Adaptive Neural Networks	2019	2019 21st European Conference on Power Electronics and Applications (EPE '19 ECCE Europe)		8 pp.	8 pp.	This paper presents a novel control approach using a Neural Network and reinforcement learning for Modular Multilevel Series Parallel Converters. The Modular Multilevel Series Parallel Converters topology offers great advantages in automotive applications such as higher efficiency compared to conventional converters and active balancing of the State of Charge. Furthermore the internal resistance and the current ripple of the batteries can be reduced by parallel interconnections. For ideal control, all of these parameters have to be weighted according to their significance, which changes during operation of the MMSPC. Due to multiple degrees of freedom which are mostly non-linear, it is challenging to find the optimal switching strategy in conventional ways. We present a novel approach which uses not only a Neural Network, but also reinforcement learning to optimize the control strategy in a simulative setup. Compared to conventional optimization methods a faster computation time can be achieved component tolerances and ageing processes can be taken into account.	Conference Paper	2019	10.23919/EPE.2019.8915464	[]	[]	-1	-1	-1
J	Lin Xinyou; Xia Yutian; Wei Shenshen	Energy management control strategy for plug-in fuel cell electric vehicle based on reinforcement learning algorithm	2019	Chinese Journal of Engineering		1332	1341	To cope with the increasingly stringent emission regulations,major automobile manufacturers have been focusing on the development of new energy vehicles. Fuel-cell vehicles with advantages of zero emission,high efficiency,diversification of fuel sources,and renewable energy have been the focus of international automotive giants and Chinese automotive enterprises. Establishing a reasonable energy management strategy,effectively controlling the vehicle working mode,and reasonably using battery energy for hybrid fuel-cell vehicles are core technologies in domestic and foreign automobile enterprises and research institutes. To improve the equilibrium between fuel-cell hydrogen consumption and battery consumption and realize the optimal energy distribution between fuel-cell systems and batteries for plug-in fuel-cell electric vehicles (PFCEVs),considering vehicles as the environment and vehicle control as an agent,an energy management strategy for the PFCEV based on reinforcement learning algorithm was proposed in this paper. This strategy considered the immediate return and future cumulative discounted returns of a fuel-cell vehicles real-time energy allocation. The vehicle simulation model was built by Matlab /Simulink to carry out the simulation test for the proposed strategy. Compared with the rule-based strategy,the battery can store a certain amount of electricity,and the integrated energy consumption of the vehicle was notably reduced under different mileages. The energy consumption in 100 km was reduced by 8.84%,29.5%,and 38.6% under 100, 200,and 300 km mileages,respectively. The hardware-in-loop-test was performed on the D2P development platform,and the final energy consumption of the vehicle was reduced by 20.8% under urban dynamometer driving schedule driving cycle. The hardware-in loop-test results are consistent with the simulation findings,indicating the effectiveness and feasibility of the proposed energy management strategy.	Article	2019		[]	[]	-1	-1	-1
B	Zhu, Meixin	Behavior Modeling and Motion Planning for Autonomous Driving Using Artificial Intelligence	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
P	GONG J; GAO T; WANG B; WU S	Speed difference model based vehicle driver steering controller, has fuzzy proportional-integral controller for triggering control command, and fuzzy rule table for optimizing output control current based on servo driving actuator action						NOVELTY - The controller has a Bang-Bang controller driver operating rod and an empirical driver control rod cluster model connected with a current vehicle running gear. A desired position of the operating rod outputs positive and negative maximum current trigger signal. A fuzzy proportional-integral (PI) controller triggers a control command and a switching mark based on reinforcement learning Q- learning algorithm. A fuzzy rule table optimizes output control current based on electro-hydraulic servo driving actuator action. The model comprises an operating rod GMM clustering module. USE - Speed difference model based vehicle driver steering controller. ADVANTAGE - The controller ensures rapidity and response speed of the operating rod, and improves system position accuracy, and satisfies unmanned vehicle steering movement. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a speed difference model based vehicle driver steering controlling method. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a speed difference model based vehicle driver steering controller. '(Drawing includes non-English language text)'				[]	[]	-1	-1	-1
C	Kraus, David; Specht, Eduard; Merz, Tobias; Hiller, Marc	Optimized Real-Time Control for Modular Multilevel Converters using Adaptive Neural Networks	2019	2019 21ST EUROPEAN CONFERENCE ON POWER ELECTRONICS AND APPLICATIONS (EPE '19 ECCE EUROPE)	European Conference on Power Electronics and Applications			This paper presents a novel control approach using a Neural Network and reinforcement learning for Modular Multilevel Series Parallel Converters. The Modular Multilevel Series Parallel Converters topology offers great advantages in automotive applications such as higher efficiency compared to conventional converters and active balancing of the State of Charge. Furthermore the internal resistance and the current ripple of the batteries can be reduced by parallel interconnections. For ideal control, all of these parameters have to be weighted according to their significance, which changes during operation of the MMSPC. Due to multiple degrees of freedom which are mostly non-linear, it is challenging to find the optimal switching strategy in conventional ways. We present a novel approach which uses not only a Neural Network, but also reinforcement learning to optimize the control strategy in a simulative setup. Compared to conventional optimization methods a faster computation time can be achieved component tolerances and ageing processes can be taken into account.	Proceedings Paper	2019		[]	[]	-1	-1	-1
B	Chen, G.; Wang, X.; Wang, Z.	Pricing Strategy of Fast Charging Stations in Coupled Power-Transportation System Considering Responsive Traffic Demand	2023	The 37th Annual Conference on Power System and Automation in Chinese Universities (CUS-EPSA). Lecture Notes in Electrical Engineering (1030)		460	9	The price of fast charging stations (FCSs) can dramatically impact the behavior of electric vehicle (EV) users on choosing paths or charging stations, which will further influence the operation of both power distribution network (PDN) and transportation network (TN). This paper proposes a unique framework to get an optimal pricing strategy for FCSs considering the responsive traffic demand of EV users. The responsive traffic demand correlates closely with the current charging price and traffic condition. The optimal pricing problem of FCSs considered coupled operation of PDN and TN is first formulated as a bi-level optimization model to maximize the profit of the distribution power system operator (DSO). Then a deep reinforcement learning method is adopted to approximately solve the hard bi-level problem. Numerical experiments demonstrate that the proposed strategy can redistribute charging loads and enhance the operation of PDN.	Conference Paper	2023	10.1007/978-981-99-1439-5_43	[]	[]	-1	-1	-1
B	Li, Y.; Wang, B.; Zhang, L.; Liu, L.; Fan, H.	A Neural Approach Towards Real-time Management for Integrated Energy System Incorporating Carbon Trading and Electrical Vehicle Scheduling	2023	International Conference on Neural Computing for Advanced Applications: 4th International Conference, NCAA 2023, Proceedings. Communications in Computer and Information Science (1870)		561	75	This paper proposes a real-time integrated energy system (IES) management approach which aims at promoting overall energy efficiency, increasing renewable energy penetration, and smoothing load fluctuation. The electric vehicles (EVs) charging scheduling is incorporated into the IES management, where the uncertain arrivals and departures of multiple EVs are considered as a stochastic but flexible load to the IES. Furthermore, towards the carbon neutralization target, a carbon emissions trading mechanism is introduced into the IES management to incentivize the system to operate in an eco-friendlier manner. To tackle the computational complexity induced by the stochastic and intermittent nature of the renewable energy sources and EVs load, the scheduling of the IES is realized in a neural network based real-time manner, driven by a deep reinforcement learning approach that guarantees safe training and operation. The case study verifies the effectiveness of the proposed approach.	Conference Paper	2023	10.1007/978-981-99-5847-4_40	[]	[]	-1	-1	-1
P	YANG F; LIU Y; ZHANG L; LI M; HUANG K; HUANG J; LIU Z; ZHU S; FENG Y; ZHANG Y	Method for optimizing aerial vehicle knowledge            model combination parameter based on reinforcement            learning, involves configuring trained parameter            optimization network to optimize parameter of unmanned            aerial vehicle model combination to-be-optimized						NOVELTY - The method involves obtaining a parameter sample of an unmanned aerial vehicle knowledge model combination and a historical time unmanned aerial vehicle knowledge model to-be-optimized (S102). The parameter sample is obtained corresponding to an ultra-parameter as a combined action. A combined action loss function is constructed (S104) according to an environment time feedback value and the parameter sample in historical time. A pre-constructed combination action neural network is trained (S106) according to a pre-built atomic motion neural network function and the atomic motion loss function to obtain an initial atomic motion evaluation network. An output of the combined action network and an output of an initial atom motion evaluation network are trained (S108) to train the initial atom action evaluation network according to preset training constraint condition. USE - Method for optimizing aerial vehicle knowledge model combination parameter based on reinforcement learning. ADVANTAGE - The method enables improving accuracy of the combination of the knowledge model of the unmanned aerial vehicle, efficiency and success rate of the task when executing the task, continuously reducing error between the combined action evaluation values and the atomic action evaluation value, realizing better consistency of the atomic actions and the combined actions and consistency between the unmanned vehicle knowledge model and the unmanned aircraft vehicle combination so as to evaluate atomic motion based on the ultra-parameter in an accurate manner, using the trained atom action evaluation network to evaluate multiple one-dimensional atom action sequence and obtaining the optimal evaluation value. DETAILED DESCRIPTION - The trained parameter optimization network is configured (S110) to optimize the parameter of the unmanned aerial vehicle knowledge model combination to-be-optimized. DESCRIPTION Of DRAWING(S) - The drawing flow diagram illustrating a method for optimizing aerial vehicle knowledge model combination parameter based on reinforcement learning. (Drawing includes non-English language text).S102Step for obtaining parameter sample of unmanned aerial vehicle knowledge model combination and historical time unmanned aerial vehicle knowledge model to-be-optimizedS104Step for constructing combined action loss function according to environment time feedback value and parameter sample in historical timeS106Step for training pre-constructed combination action neural network according to pre-built atomic motion neural network function and atomic motion loss function to obtain initial atomic motion evaluation networkS108Step for training output of combined action network and output of initial atom motion evaluation network to train initial atom action evaluation network according to preset training constraint conditionS110Step for configuring trained parameter optimization network to optimize parameter of unmanned aerial vehicle knowledge model combination to-be-optimized				[]	[]	-1	-1	-1
J	Leal-Millan, A.; Guadix-Martin, J.; Garcia-Legaz, F. Criado	From networking orientation to green image: A sequential journey through relationship learning capability and green supply chain management practices. Evidence from the automotive industry	2023	TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE				Drawing on the resource-based view of the firm (RBV) and resources and capabilities theory, this study develops a model that extends our understanding of the mechanisms through which strategic assets, capabilities, and green supply chain management practices (GSCMP) contribute to green image (GI). The model comprises (i) two new antecedents of GSCMP: relationship learning capability (RL) and strategic networking orientation (NO), and (ii) the direct and mediated impacts of GSCMP and their antecedents on firms' GI. To empirically study the proposed relationships, data were collected from 106 Spanish firms in the automotive industry and analyzed using partial least squares structural equation modelling (PLS-SEM). The results indicate that NO, RL capability, and GSCMP positively affect GI through a sequential mediation relationship. An important implication is the identification of a stream of research proposing that GSCMP act similarly to a lower-order capability and that it is the interaction with other ordinary capabilities that can contribute to improving the green image.	Article; Early Access		10.1016/j.techfore.2023.122569	[]	[]	-1	-1	-1
J	Alonso, Monica; Amaris, Hortensia; Martin, David; de la Escalera, Arturo	Proximal Policy Optimization for Energy Management of Electric Vehicles and PV Storage Units	2023	ENERGIES				Connected autonomous electric vehicles (CAEVs) are essential actors in the decarbonization process of the transport sector and a key aspect of home energy management systems (HEMSs) along with PV units, CAEVs and battery energy storage systems. However, there are associated uncertainties which present new challenges to HEMSs, such as aleatory EV arrival and departure times, unknown EV battery states of charge at the connection time, and stochastic PV production due to weather and passing cloud conditions. The proposed HEMS is based on proximal policy optimization (PPO), which is a deep reinforcement learning algorithm suitable for continuous complex environments. The optimal solution for HEMS is a tradeoff between CAEV driver's range anxiety, batteries degradation, and energy consumption, which is solved by means of incentives/penalties in the reinforcement learning formulation. The proposed PPO algorithm was compared to conventional methods such as business-as-usual (BAU) and value iteration (VI) solutions based on dynamic programming. Simulation results indicate that the proposed PPO's performance showed a daily energy cost reduction of 54% and 27% compared to BAU and VI, respectively. Finally, the developed PPO algorithm is suitable for real-time operations due to its fast execution and good convergence to the optimal solution.	Article	AUG 2023	10.3390/en16155689	[]	[]	-1	-1	-1
J	Bo Lin; Ghaddar, B.; Nathwani, J.	Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows [arXiv]	2020	arXiv		10 pp.	10 pp.	The past decade has seen a rapid penetration of electric vehicles (EV) in the market, more and more logistics and transportation companies start to deploy EVs for service provision. In order to model the operations of a commercial EV fleet, we utilize the EV routing problem with time windows (EVRPTW). In this research, we propose an end-to-end deep reinforcement learning framework to solve the EVRPTW. In particular, we develop an attention model incorporating the pointer network and a graph embedding technique to parameterize a stochastic policy for solving the EVRPTW. The model is then trained using policy gradient with rollout baseline. Our numerical studies show that the proposed model is able to efficiently solve EVRPTW instances of large sizes that are not solvable with any existing approaches.	Journal Paper	5 Oct. 2020		[]	[]	-1	-1	-1
P	LI H; WU H; HUANG Z	New cerium-enriched sintered neodymium-iron-boron            permanent magnet material, is prepared by batching,            smelting and pouring alloy raw materials to obtain            alloy flakes, hydrogenating and dehydrogenating, mixing            coarse powder, antioxidant, and lubricant and isostatic            pressing and sintering						NOVELTY - A cerium-enriched sintered neodymium-iron-boron permanent magnet material (I), is new. USE - The cerium-enriched sintered neodymium-iron-boron permanent magnet material is used in permanent magnet for automotive applications. ADVANTAGE - The magnet has high cerium content and excellent performance. DETAILED DESCRIPTION - A cerium-enriched sintered neodymium-iron-boron permanent magnet material having composition formula of R-FeM-B (I), is new. R includes RL and RH, RL includes cerium and other light rare earth elements. The other light rare earth elements are Praseodymium, Neodymium or other light rare earth elements. RH is terbium or other heavy rare earth elements; M is copper, aluminum, cobalt, gallium, zirconium, niobium, bismuth and titanium.An INDEPENDENT CLAIM is also included for a preparing (I).				[]	[]	-1	-1	-1
J	Liu, Qiang; Han, Tao; Xie, Jiang; Kim, BaekGyu	Real-Time Dynamic Map With Crowdsourcing Vehicles in Edge Computing	2023	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		2810	2820	Autonomous driving perceives surroundings with line-of-sight sensors that are compromised under environmental uncertainties. To achieve real time global information in high definition map, we investigate to share perception information among connected and automated vehicles. However, it is challenging to achieve real time perception sharing under varying network dynamics in automotive edge computing. In this paper, we propose a novel real time dynamic map, named LiveMap to detect, match, and track objects on the road. We design the data plane of LiveMap to efficiently process individual vehicle data with multiple sequential computation components, including detection, projection, extraction, matching and combination. We design the control plane of LiveMap to achieve adaptive vehicular offloading with two new algorithms (central and distributed) to balance the latency and coverage performance based on deep reinforcement learning techniques. We conduct extensive evaluation through both realistic experiments on a small-scale physical testbed and network simulations on an edge network simulator. The results suggest that LiveMap significantly outperforms existing solutions in terms of latency, coverage, and accuracy.	Article	APR 2023	10.1109/TIV.2022.3214119	[]	[]	-1	-1	-1
P	MAKI K; KOKUBO K; NAITO M	Automotive hydraulic braking system for e.g.            hybrid powered automobile incorporates regeneration            brake system that linked to all wheels						NOVELTY - An automotive brake system has a hydraulic brake assembly (B) that generates a regulated base-level pressure for the wheel brakes (FL, FR, RL, RR). The brake system further incorporates a regeneration brake system (A) that is linked to all wheels. The brake system further incorporates a brake force exchange control system (60) that gradually exchanges regenerative brake force through the regulated hydraulic brake forces during the brake application. This action results in graduated brake application over a given range by reduction of the regeneration forces. The system further increases the regulated hydraulic brake force as a reaction to the regenerated brake force. USE - Automotive hydraulic braking system for e.g. hybrid powered automobile. ADVANTAGE - The brake system provides better control than prior art. Further claimed is that the brake operation is maintained in the absence of a full charge of hydraulic brake fluid. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic arrangement of the brake system.Internal combustion engine (11)electric motor (12)generator (15)d.c. converter (16)battery (17)electronic control unit (18)brake pedal (21)pedal sensor (21a)pressure sensor (P)regeneration brake system (A)hybrid electronic control unit (19)				[]	[]	-1	-1	-1
P	HOSOMI K; HOSOMI S	Traction control system, for drive wheels of            automotive vehicle, uses sensors to detect wheel slip            when vehicle accelerates, and traction control unit            combining acceleration factor, throttle valve and            throttle valve motor						NOVELTY - Traction control system includes (a) a unit to detect the skidding tendency, (b) an unit to measure the speed of rotation of the engine (1), (c) a traction control unit (100), combining an acceleration factor (220), a throttle valve (7), throttle valve motor (8), and other factors for acting on the brakes (10) of the drive wheels (RR,RL) and on the output force of the engine. USE - For drive wheels of automotive vehicle. ADVANTAGE - Greatly reduces the tendency of the drive wheels to slip during acceleration, thus avoiding the vehicle engine to stall, and enabling engine to function with greater efficiency, as a function of the rotation speed of the engine, and the rate of variation of engine speed. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for the method for controlling traction control system. DESCRIPTION Of DRAWING(S) - The drawing is a schematic representation of the automotive vehicle having two drive wheels, and having a traction control unit.engine (1)throttle valve (7)throttle valve motor (8)brakes (10)traction control unit (100)acceleration factor (220)				[]	[]	-1	-1	-1
R	Chen, Zhong; Wang, Ruisheng; Sun, Kehui; Zhang, Tian; Du, Puliang; Zhao, Qi	DataSheet1_A Modified Long Short-Term Memory-Deep Deterministic Policy Gradient-Based Scheduling Method for Active Distribution Networks.docx	2022	Figshare				To improve the decision-making level of active distribution networks (ADNs), this paper proposes a novel framework for coordinated scheduling based on the long short-term memory network (LSTM) with deep reinforcement learning (DRL). Considering the interaction characteristics of ADNs with distributed energy resources (DERs), the scheduling objective is constructed to reduce the operation cost and optimize the voltage distribution. To tackle this problem, a LSTM module is employed to perform feature extraction on the ADN environment, which can realize the recognition and learning of massive temporal structure data. The concerned ADN real-time scheduling model is duly formulated as a finite Markov decision process (FMDP). Moreover, a modified deep deterministic policy gradient (DDPG) algorithm is proposed to solve the complex decision-making problem. Numerous experimental results within a modified IEEE 33-bus system demonstrate the validity and superiority of the proposed method. Copyright: CC BY 4.0	Data set	2022-07-06	https://doi.org/10.3389/fenrg.2022.913130.s001	[]	[]	-1	-1	-1
J	Lee, Jaehyun; Lee, Eunjung; Kim, Jinho	Electric Vehicle Charging and Discharging Algorithm Based on Reinforcement Learning with Data-Driven Approach in Dynamic Pricing Scheme	2020	ENERGIES				In the smart grid environment, the penetration of electric vehicle (EV) is increasing, and dynamic pricing and vehicle-to-grid technologies are being introduced. Consequently, automatic charging and discharging scheduling responding to electricity prices that change over time is required to reduce the charging cost of EVs, while increasing the grid reliability by moving charging loads from on-peak to off-peak periods. Hence, this study proposes a deep reinforcement learning-based, real-time EV charging and discharging algorithm. The proposed method utilizes kernel density estimation, particularly the nonparametric density function estimation method, to model the usage pattern of a specific charger at a specific location. Subsequently, the estimated density function is used to sample variables related to charger usage pattern so that the variables can be cast in the training process of a reinforcement learning agent. This ensures that the agent optimally learns the characteristics of the target charger. We analyzed the effectiveness of the proposed algorithm from two perspectives, i.e., charging cost and load shifting effect. Simulation results show that the proposed method outperforms the benchmarks that simply model usage pattern through general assumptions in terms of charging cost and load shifting effect. This means that when a reinforcement learning-based charging/discharging algorithm is deployed in a specific location, it is better to use data-driven approach to reflect the characteristics of the location, so that the charging cost reduction and the effect of load flattening are obtained.	Article	APR 2020	10.3390/en13081950	[]	[]	-1	-1	-1
C	Nie, Jingping; Xia, Stephen; Liu, Yanchen; Ding, Shengxuan; Hu, Lanxiang; Zhao, Minghui; Fan, Yuang; Abdel-Aty, Mohamed; Preindl, Matthias; Jiang, Xiaofan	A Data-Driven and Human-Centric EV Charging Recommendation System at City-Scale	2023	PROCEEDINGS OF THE 2023 THE 14TH ACM INTERNATIONAL CONFERENCE ON FUTURE ENERGY SYSTEMS, E-ENERGY 2023		427	438	Electric vehicles (EVs) have gained widespread popularity in recent years, and the scheduling and routing of EV charging have impacted the welfare of both EV drivers and the grid. In this paper, we present a practical, data-driven, and human-centric EV charging recommendation system at the city-scale based on deep reinforcement learning (DRL). The system co-optimizes the welfare of both the EV drivers and the grid. We augmented and aggregated data from various sources, including public data, location-based data companies, and government authorities, with different formats and time granularities. The data includes EV charger information, grid capacity, EV driving behavior information, and city-scale mobility. We created a 30-day per-minute unified EV charger information dataset with charging prices and grid capacity, as well as an EV driving behavior dataset with location and State of Charge (SoC) information. Our evaluation of the recommendation system shows that it is able to provide recommendations that reduce the average driver-to-charger distance and minimize the number of times chargers switch to a different driver. The dataset we prepared for training the DRL agent, including augmented EV driving data and charging station information, will be open-sourced to benefit future research in the community.	Proceedings Paper	2023	10.1145/3575813.3597350	[]	[]	-1	-1	-1
J	El Sallab, A.; Abdou, M.; Perot, E.; Yogamani, S.	Deep Reinforcement Learning framework for Autonomous Driving [arXiv]	2017	arXiv		7 pp.	7 pp.	Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles. [IS&T Electronic Imaging, Autonomous Vehicles and Machines 2017, AVM-023, pg. 70-76 (2017) doi:10.2352/ISSN.2470-1173.2017.19.AVM-023].	Journal Paper	8 April 2017		[]	[]	-1	-1	-1
B	Al-habob, A.; Tabassum, H.; Waqar, O.	Dynamic Unicast-Multicast Scheduling for Age-Optimal Information Dissemination in Vehicular Networks	2022	2022 IEEE Globecom Workshops (GC Wkshps)		1218	23	This paper investigates the problem of minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. Each vehicle is interested in maintaining the freshness of its information status about one or more physical processes. A framework is proposed to optimize the decisions to unicast, multicast, broadcast, or not transmit updates to vehicles as well as power allocations to minimize the AoI and the RSU's power consumption over a time horizon. The formulated problem is a mixed-integer nonlinear programming problem (MINLP), thus a global optimal solution is difficult to achieve. In this context, we first develop an ant colony optimization (ACO) solution which provides near-optimal performance and thus serves as an efficient benchmark. Then, for real-time implementation, we develop a deep reinforcement learning (DRL) framework that captures the vehicles' demands and channel conditions in the state space and assigns processes to vehicles through dynamic unicast-multicast scheduling actions. Complexity analysis of the proposed algorithms is presented. Simulation results depict interesting trade-offs between AoI and power consumption as a function of the network parameters.	Conference Paper	2022	10.1109/GCWkshps56602.2022.10008649	[]	[]	-1	-1	-1
J	Gong, Cihun-Siyong Alex; Su, Chih-Hui Simon; Chen, Yu-Hua; Guu, De-Yu	How to Implement Automotive Fault Diagnosis Using Artificial Intelligence Scheme	2022	MICROMACHINES				The necessity of vehicle fault detection and diagnosis (VFDD) is one of the main goals and demands of the Internet of Vehicles (IoV) in autonomous applications. This paper integrates various machine learning algorithms, which are applied to the failure prediction and warning of various types of vehicles, such as the vehicle transmission system, abnormal engine operation, and tire condition prediction. This paper first discusses the three main AI algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, and compares the advantages and disadvantages of each algorithm in the application of system prediction. In the second part, we summarize which artificial intelligence algorithm architectures are suitable for each system failure condition. According to the fault status of different vehicles, it is necessary to carry out the evaluation of the digital filtering process. At the same time, it is necessary to preconstruct its model analysis and adjust the parameter attributes, types, and number of samples of various vehicle prediction models according to the analysis results, followed by optimization to obtain various vehicle models. Finally, through a cross-comparison and sorting, the artificial intelligence failure prediction models can be obtained, which can correspond to the failure status of a certain car model and a certain system, thereby realizing a most appropriate AI model for a specific application.	Review	SEP 2022	10.3390/mi13091380	[]	[]	-1	-1	-1
J	Schmiedt, Marius; He, Ping; Rinderknecht, Stephan	Target State Optimization: Drivability Improvement for Vehicles with Dual Clutch Transmissions	2022	APPLIED SCIENCES-BASEL				Vehicles with dual clutch transmissions (DCT) are well known for their comfortable drivability since gear shifts can be performed jerklessly. The ability of blending the torque during gear shifts from one clutch to the other, making the type of automated transmission a perfect alternative to torque converters, which also comes with a higher efficiency. Nevertheless, DCT also have some drawbacks. The actuation of two clutches requires an immense control effort, which is handled in the implementation of a wide range of software functions on the transmission control unit (TCU). These usually contain control parameters, which makes the behavior adaptable to different vehicle and engine platforms. The adaption of these parameters is called calibration, which is usually an iterative time-consuming process. The calibration of the embedded software solutions in control units is a widely known problem in the automotive industry. The calibration of any vehicle subsystem (e.g., engine, transmission, suspension, driver assistance systems for autonomous driving, etc.) requires costly test trips in different ambient conditions. To reduce the calibration effort and the accompanying use of professionals, several approaches to automize the calibration process are proposed. Due to the fact that a solution is desired which can optimize different calibration problems, a generic metaheuristic approach is aimed. Regardless, the scope of the current research is the optimization of the launch behavior for vehicles equipped with DCT since, particularly at low speeds, the transmission behavior must meet the intention of the driver (drivers tend to be more perceptive at low speeds). To clarify the characteristics of the launch, several test subject studies are performed. The influence factors, such as engine sound, maximal acceleration, acceleration build-up (mean jerk), and the reaction time, are taken into account. Their influence on the evaluation of launch with relation to the criteria of sportiness, comfort, and jerkiness, are examined based on the evaluation of the test subject studies. According to the results of the study, reference values for the optimization of the launch behavior are derived. The research contains a study of existing approaches for optimizing driving behavior with metaheuristics (e.g., genetic algorithms, reinforcement learning, etc.). Since the existing approaches have different drawbacks (in scope of the optimization problem) a new approach is proposed, which outperforms existing ones. The approach itself is a hybrid solution of reinforcement learning (RL) and supervised learning (SL) and is applied in a software in the loop environment, and in a test vehicle.	Article	OCT 2022	10.3390/app122010283	[]	[]	-1	-1	-1
J	Ng, L.; Clark, C.M.; Huissoon, J.P.	Reinforcement learning of dynamic collaborative driving: Part I. Longitudinal adaptive control	2008	International Journal of Vehicle Information and Communication Systems		208	28	Dynamic collaborative driving involves the motion coordination of multiple vehicles using shared information from vehicles instrumented to perceive their surroundings in order to improve road usage and safety. A basic requirement of any vehicle participating in dynamic collaborative driving is longitudinal control. Without this capability, higher-level coordination is not possible. Each vehicle involved is a composite non-linear system powered by an internal combustion engine, equipped with automatic transmission, rolling on rubber tyres with a hydraulic braking system. This paper focuses on the problem of longitudinal motion control. A longitudinal vehicle model is introduced which serves as the control system design platform. A longitudinal adaptive control system that uses Monte Carlo reinforcement learning (RL) is introduced. The results of the RL phase and the performance of the adaptive control system for a single automobile, as well as the performance in a multi-vehicle platoon, are presented.	Journal Paper	2008	10.1504/IJVICS.2008.022355	[]	[]	-1	-1	-1
J	Siebinga, Olger; Zgonnikov, Arkady; Abbink, DA	"Code accompanying the paper ""Validating human driver models for interaction-aware automated vehicle controllers: A human approach"""	2021	Figshare				"This python package contains scripts needed to train IRL Driver models on HighD datasets. This code is accompanying the paper ""Validating human driver models for interaction-aware automated vehicle controllers: A human factors approach - Siebinga, Zgonnikov & Abbink 2021"" and should be used in combination with TraViA, a program for traffic data visualization and annotation. A preprint of this paper can be found on arxiv: https://arxiv.org/abs/2109.13077 Copyright: GPL 3.0+"	Software	2021-10-26	http://dx.doi.org/10.4121/16847203.v1	[]	[]	-1	-1	-1
J		Adaptive Dynamic Programming-based Control of Unknown Networked Control Systems	2011					Abstract<br/>The overall objective of this study is to provide online robust adaptive dynamic programming (ADP) based optimal controllers with guaranteed performance, supported by a rigorous design and mathematical framework, and without utilizing policy and value iterations, for unknown linear and nonlinear networked control systems (NCS). The approach taken here employs adaptive network learning as a fundamental block and utilizes past history of cost-to-go information, and updates the control input once a sampling interval in a forward-in-time manner without using a system model and offline learning phase for the NNs.<br/><br/>Intellectual Merit<br/>The proposed research presents an opportunity to deal with a more powerful and unified paradigm of complex learning problems and envisions a brain-like controller. The proposed effort will advance the state of the art in ADP for control and guarantees stability and performance in the presence of not only uncertain system dynamics and disturbances, but also network imperfections such as random delays, packet losses and quantization errors without using iterative approach.  <br/><br/>Broader Impact<br/>This effort would directly impact all real-time practical systems such as the efficient operation and energy security of the smart grid, near zero-emission automotive control systems, and next generation manufacturing system. Such control schemes are required for global competitiveness of the US industry. Technology transfer will occur through the NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems. Within the research community, this work will inspire more theoretical results while providing training opportunities to next generation students, future scientists and engineers including from underrepresented groups.	Awarded Grant	Aug 01 2011		[]	[]	-1	-1	-1
J	Liu, Yonggang; Wu, Yitao; Wang, Xiangyu; Li, Liang; Zhang, Yuanjian; Chen, Zheng	Energy management for hybrid electric vehicles based on imitation reinforcement learning	2023	ENERGY				An effective energy management strategy (EMS) in hybrid electric vehicles (HEVs) is indispensable to promote consumption efficiency due to time-varying load conditions. Currently, learning based algorithms have been widely applied in energy controlling performance of HEVs. However, the enormous computation intensity, massive data training and rigid requirement of prediction of future operation state hinder their substantial exploitation. To mitigate these concerns, an imitation reinforcement learning-based algorithm with optimal guidance is proposed in this paper for energy control of hybrid vehicles to accelerate the solving process and meanwhile achieve preferable control performance. Firstly, offline global optimization is firstly conducted considering various driving conditions to search power allocation trajectories. Then, the battery depletion boundaries with respect to driving distance are introduced to generate a narrowed state space, in which the optimal trajectory is fused into the training process of reinforcement learning to guide the high-efficiency strategy production. The simulation validations reveal that the proposed method provides preferable energy reduction for HEVs in arbitrary driving scenarios, and suggests an efficient solution instruction for similar problems in mechanical and electrical systems with constraints and optimal information.	Article; Early Access		10.1016/j.energy.2022.125890	[]	[]	-1	-1	-1
B	Timilsina, A.; Silvestri, S.	e-Uber: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing	2023	2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS)		359	65	There is growing interest in deploying the sharing-economy-based business model to energy systems, with modalities like peer-to-peer (P2P) energy trading, Electric Vehicles (EV)-based Vehicle-to-Grid (V2G), Vehicle-to-Home (V2H), Vehicle-to-Vehicle (V2V), and Battery Swapping Technology (BST). This paper exploits the increasing diffusion of EVs to realize a crowdsourcing platform called e-Uber that jointly enables ride-sharing and energy-sharing through V2G and BST. We employ theoretical concepts of online spatial crowdsourcing, reinforcement learning, and reverse auction to devise this novel platform. Experimental results using real data from New York City taxi trips and energy consumption show that e-Uber performs close to the optimum and outperforms a state-of-the-art approach.	Conference Paper	2023	10.1109/MASS58611.2023.00051	[]	[]	-1	-1	-1
R	Guan, Qingshu; Zhang, Xiangquan; Xie, Minghui; Nie, Jianglong; Cao, Hui; Chen, Zhao; He, Zhouqiang	DataSheet1_Large-scale power inspection: A deep reinforcement learning approach.PDF	2023	Figshare				Power inspection plays an important role in ensuring the normal operation of the power grid. However, inspection of transmission lines in an unoccupied area is time-consuming and labor-intensive. Recently, unmanned aerial vehicle (UAV) inspection has attracted remarkable attention in the space-ground collaborative smart grid, where UAVs are able to provide full converge of patrol points on transmission lines without the limitation of communication and manpower. Nevertheless, how to schedule UAVs to traverse numerous, dispersed target nodes in a vast area with the least cost (e.g., time consumption and total distance) has rarely been studied. In this paper, we focus on this challenging and practical issue which can be considered as a family of vehicle routing problems (VRPs) with regard to different constraints, and propose a Diverse Trajectory-driven Deep Reinforcement Learning (DT-DRL) approach with encoder-decoder scheme to tackle it. First, we bring in a threshold unit in our encoder for better state representation. Secondly, we realize that the already visited nodes have no impact on future decisions, and then devise a dynamic-aware context embedding which removes irrelevant nodes to trace the current graph. Finally, we introduce multiply decoders with identical structure but unshared parameters, and design a Kullback-Leibler divergence based regular term to enforce decoders to output diverse trajectories, which expands the search space and enhances the routing performance. Comprehensive experiments on five types of routing problems show that our approach consistently outperforms both DRL and heuristic methods by a clear margin. Copyright: CC BY 4.0	Data set	2023-02-02	https://doi.org/10.3389/fenrg.2022.1054859.s001	[]	[]	-1	-1	-1
B	Ng, Luke	Reinforcement learning of Dynamic Collaborative Driving	2008						Dissertation/Thesis	Jan 01 2008		[]	[]	-1	-1	-1
C	Oakes, Bentley James; Moradi, Mehrdad; Van Mierlo, Simon; Vangheluwe, Hans; Denil, Joachim	Machine Learning-Based Fault Injection for Hazard Analysis and Risk Assessment	2021	COMPUTER SAFETY, RELIABILITY, AND SECURITY (SAFECOMP 2021)	Lecture Notes in Computer Science	178	192	Current automotive standards such as ISO 26262 require Hazard Analysis and Risk Assessment (HARA) on possible hazards and consequences of safety-critical components. This work attempts to ease this labour-intensive process by using machine learning-based fault injection to discover representative hazardous situations. Using a Simulation-Aided Hazard Analysis and Risk Assessment (SAHARA) methodology, a visualisation and suggested hazard classification is then presented for the safety engineer. We demonstrate this SAHARA methodology using machine learning-based fault injection on a safety-critical use case of an adaptive cruise control system, to show that our approach can discover, visualise, and classify hazardous situations in a (semi-)automated manner in around twenty minutes.	Proceedings Paper	2021	10.1007/978-3-030-83903-1_12	[]	[]	-1	-1	-1
B	Schoeman, M.; Marchand, R.; van Tonder, J.; Jakobus, U.; Aguilar, A.; Longtin, K.; Vogel, M.; Alwajeeh, T.	New Features in Feko / WinProp 2019	2020	2020 International Applied Computational Electromagnetics Society Symposium (ACES)		2 pp.	2 pp.	This paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp). These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.	Conference Paper	2020	10.23919/ACES49320.2020.9196079	[]	[]	-1	-1	-1
B	He, S.; Wang, Y.; Han, S.; Zou, S.; Miao, F.	A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems	2023	2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)		5637	44	Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g. state transition probability). Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications. However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging. In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems. We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-demand ratio and the charging utilization rate across the city under model uncertainty. Experiments show that the ROCOMA can learn an effective and robust rebalancing policy. It outperforms non-robust MARL methods in the presence of model uncertainties. It increases the system fairness by 19.6% and decreases the rebalancing costs by 75.8%.	Conference Paper	2023	10.1109/IROS55552.2023.10342342	[]	[]	-1	-1	-1
J	Lin, Yu-Chen; Nguyen, Ha Ly Thi; Yang, Ji-Fan; Chiou, Hung-Jui	A reinforcement learning backstepping-based control design for a full vehicle active Macpherson suspension system	2022	IET CONTROL THEORY AND APPLICATIONS		1417	1430	This paper presents a reinforcement learning backstepping-based control scheme for the design of a full vehicle active suspension system such that both the superior ride comfort and stabilization are achieved. It is well known that the traditional backstepping control scheme is suitable to solve higher order nonlinear system based on the strict-feedback form; however, the disadvantage of the procedure requires computing the derivatives of virtual control signals, which is a very complex task. Therefore, a reinforcement learning scheme using the deep deterministic policy gradient (DDPG) control strategy is developed to replace the process of finding virtual control force in backstepping method. It not only avoids the complexity of analytically calculating the derivatives of the virtual control signals, but also retains the systems robustness when there exist random disturbances from road irregularities. To verify the performance of the proposed active suspension control scheme, the random road unevenness profiles according to ISO 8608 is considered as vertical and lateral disturbance input excitation of a full-vehicle suspension system. Compared with conventional passive suspension system and backstepping control scheme, the proposed approach can be demonstrated to not only effectively improve ride comfort under random road excitation, but also improve the transient response and robustness.	Article; Early Access		10.1049/cth2.12317	[]	[]	-1	-1	-1
C	Acquarone, Matteo; Borneo, Angelo; Misul, Daniela Anna	Acceleration control strategy for Battery Electric Vehicle based on Deep Reinforcement Learning in V2V driving	2022	2022 IEEE/AIAA TRANSPORTATION ELECTRIFICATION CONFERENCE AND ELECTRIC AIRCRAFT TECHNOLOGIES SYMPOSIUM (ITEC+EATS 2022)		202	207	The transportation sector is seeing the flourishing of one of the most interesting technologies, autonomous driving (AD). In particular, Cooperative Adaptive Cruise Control (CACC) systems ensure higher levels both of safety and comfort, enhancing at the same time the reduction of energy consumption. In this framework a real-time velocity planner for a Battery Electric Vehicle, based on a Deep Reinforcement Learning algorithm called Deep Deterministic Policy Gradient (DDPG), has been developed, aiming at maximizing energy savings, and improving comfort, thanks to the exchange of information on distance, speed and acceleration through the exploitation of vehicle-to-vehicle technology (V2V). The aforementioned DDPG algorithm relies on a multi-objective reward function that is adaptive to different driving cycles. The simulation results show how the agent can obtain good results on standard cycles, such as WLTP, UDDS and AUDC, and on real-world driving cycles. Moreover, it displays great adaptability to driving cycles different from the training one.	Proceedings Paper	2022	10.1109/ITEC53557.2022.9813785	[]	[]	-1	-1	-1
J	Nidamanuri, Jaswanth; Nibhanupudi, Chinmayi; Assfalg, Rolf; Venkataraman, Hrishikesh	A Progressive Review: Emerging Technologies for ADAS Driven Solutions	2022	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		326	341	Over the last decade, the Advanced Driver Assistance System (ADAS) concept has evolved significantly. ADAS involves several technologies such as automotive electronics, vehicle-to-vehicle (V2V) vehicle-to-infrastructure (V2I) communication, RADAR, LIDAR, computer vision, and machine learning. Of these, computer vision and machine learning based solutions have mainly been effective that have allowed real-time vehicle control, driver aided systems, etc. However, most of the existing works deal with the deployment of ADAS and autonomous driving functionality in countries with well-disciplined lane traffic. Nevertheless, these solutions and frameworks do not work in countries and cities with less-disciplined/chaotic traffic. This paper identifies the research gaps, reviews the state-of-the-art looking at the different functionalities of ADAS and its levels of autonomy. Importantly, it provides a detailed description of vision intelligence and computational intelligence for ADAS. The eye-gaze and head pose estimation in vision intelligence is detailed. Notably, the learning algorithms such as supervised, unsupervised, reinforcement learning and deep learning solutions for ADAS are considered and discussed. Significantly, this would enable developing a real-time recommendation system for system-assisted/autonomous vehicular environments with less-disciplined road traffic.	Review	JUN 2022	10.1109/TIV.2021.3122898	[]	[]	-1	-1	-1
B	Zhao, C.; Wu, G.; Xiong, W.	Decentralized Multiagent Reinforcement Learning-based Cooperative Perception with Dual-functional Radar-Communication V2V Links	2023	2023 IEEE International Conference on Communications Workshops (ICC Workshops)		1100	5	The ever-increasing deployment of connected automated vehicles (CAVs) equipped with multiple radar transceivers increases the demand for both efficient environment sensing and reliable data sharing for safe driving. However, two challenges are observed for cooperative perception used in automotive scenarios, i.e., performance tradeoff between sensing and communication, as well as effective resource scheduling in the dynamic environment. In this paper, a fully decentralized multiagent reinforcement framework is developed to optimize cooperative perception based on integrated sensing and communication (ISAC) technology. We first formulate the beamforming and power allocation problem as a Markov decision process (MDP). Then we design a beacon signal to implement data broadcast and reformulate the problem as a more general multiagent partially observed Markov decision process (POMDP). In this case, the dual-functional radar- communication (DFRC) waveform can implement sensing and vehicle-to-vehicle (V2V) data exchange simultaneously without central nodes, mitigating interference and avoiding an extra communication resource demand. Numerical results show that our method is effective for real-time perception. It fairly provides identical quality of service (QoS) to all CAVs and achieves a low age of information (AoI) with minimal prior knowledge about the environment.	Conference Paper	2023	10.1109/ICCWorkshops57953.2023.10283653	[]	[]	-1	-1	-1
J	Zhao Jian; Song Dongjian; Zhu Bing; Liu Bin; Chen Zhicheng; Zhang Peixing	Intelligent Vehicle-following Control Strategy Based on Self- learning and Supervised-learning Hybrid-driven Framework	2022	China Journal of Highway and Transport		55	65	With the continuous progress of artificial intelligence, an increasing number of data-driven methods have been adopted for car-following control for intelligent vehicles. A car-following control strategy based on self-learning and a supervised-learning hybrid-driven framework was proposed for human-like high-performance car-following control. First, the car-following data were collected through a real vehicle-testing platform. The car-following control problem was modeled as a Markov decision process, and a self-learning car-following control strategy was established by using the deep deterministic policy gradient, which is a deep reinforcement-learning method, long short-term memory was adopted to model the state transition of the Markov decision process and predict future states according to historical data, and a car-following reference model with human-driver characteristics was established based on Gaussian mixture regression combined with a hidden Markov model and introduced into the reinforcement-learning architecture. A self-learning and supervised-learning hybrid-driven architecture was proposed. The introduction of the human-driver car-following reference model can guide the reinforcement learning to learn the characteristics of a human driver correctly, the personification of the strategy is improved, and the Marko-decision-process state-transition model enables the strategy to consider the state randomness of the car-following process when acceleration decisions are made. Finally, we used real car-following data to verify the strategy, compared the car-following control performance of the proposed strategy and that of the supervised learning strategy, and a series of quantitative evaluation indexes were provided. The results demonstrate that the proposed strategy outperforms human drivers in tracking front-vehicle speed and maintaining the ideal following distance, effectively reduces the acceleration shock in the process of car following, achieves satisfactory personification, and ensures safety, tracking, and comfort in car-following.	Article	2022		[]	[]	-1	-1	-1
C	Teng, Jiaxin; Qu, Lizhi; Czarkowski, Dariusz	A Reinforcement Learning-Based Data-Driven Voltage Regulator for Wireless Chargers of Electric Vehicles	2020	2020 IEEE ENERGY CONVERSION CONGRESS AND EXPOSITION (ECCE)	IEEE Energy Conversion Congress and Exposition	3199	3204	As a critical component in the concept of smart cities, autonomous vehicles are under extensive investigation. The need for intelligent fueling without human assistance stimulates the development of wireless charging. There are, however, several issues to be addressed to enhance the efficiency and reliability of charging. The vehicle battery exhibits varying resistance during the charging process. Additionally, the alignment and gap may change with external positioning, which affects the coupling coefficient of the transmitter and receiver coils. Thus, a data-driven control scheme is desired to tackle these environmental uncertainties. This paper adopts the control scheme of embedding a Buck converter at the receiver side. Different from state-of-the-art literature, a reinforcement learning-based data-driven control approach is employed to regulate the charging voltage. Stable charging voltage is attained regardless of the knowledge of the coupling coefficient, load variations, or component values. System simulations in Simulink have proved the effectiveness of the proposed control method.	Proceedings Paper	2020	10.1109/ecce44975.2020.9235919	[]	[]	-1	-1	-1
J		An AI system for automated profiling of driving patterns	2018					In this project PhD project, in collaboration with a dynamic start-up, Evezy, we will develop an AI system for the automated profiling of driving patterns using historical measurements generated by a large fleet of vehicles and the detection of risky behaviour in real-time. This aligns in particular with EPSRC research aims for artificial intelligence technologies, complexity science, and infrastructure and urban systems.Vehicle fleet management businesses rely on safe drivers to support their operations. Safe drivers are preferable as they have a direct economic and financial impact on the fleet operator's business. Aggressive driving increases fuel consumption, cost of vehicle maintenance, risk of accidents, cost of insurance, and risk of poor customer experience (e.g. for taxi related operations). This work aims to assess sensor information for risk identification, in a manner that is not only accurate but has an explanatory process or assessment criteria. While something like a deep neural networks may be necessary to process and classify characteristics from a raw image data-set, the implications of the identified characteristics can be described by a myriad of plausible Reinforcement Learning techniques. Potential impacts or applications include utilising images captured by sensors on automotive vehicles to identify, classify and react to possible threats to driver and pedestrian safety.The system will implement a metric score reflecting the degree of risk that a driver exhibits at any given time based on a combination of historical as well as current driving data. The AI system will leverage a large volume of historical data collected by Evezy customers characterising how the vehicles are being operated and in what driving conditions, at a temporal resolution of approximately 2 seconds. Measurements generated from the on-board telemetry system and other sensors (such as GPS coordinates, acceleration/breaking, front camera video feeds, battery levels, mileage, vehicle's status indicating whether the engine is operating, doors are locked, and others) will be overlaid with publicly available data (such as street maps and places of interest, historical traffic conditions and accidents, speed limits, etc.) and mobile usage data (Evezy customers have to use the Evezy app to unlock and start the car, as well as to swap vehicle). Using this unique historical database, we will implement a robust risk scoring system whereby higher scores indicate riskier driving behaviour, and a tracking system for adaptively assigning risk scores and raising alerts in real-time.Machine learning methodologies will be used to processes the data to identify key characteristics. However, a standard black box algorithm approach may not have sufficient capacity to perform this task, the lack of descriptive power can create an impasse to being implemented for real-world purposes where risk to human safety is concerned. Therefore, while something like a deep neural network may be necessary to process and classify characteristics from a raw image data-set, the implications of the identified characteristics can be described by several possible flavours of Reinforcement Learning (RL); Deep RL, Model-Based RL, or even Hierarchical RL which creates an encoding of a kind of memory into the algorithm so as that sequences of events can be identified as potential problems not just the event at one particular instant of time. It may also be necessary to compare several different methods and optimise the trade off between computational efficiency and descriptive power.	Awarded Grant	Sep 23 2018		[]	[]	-1	-1	-1
B	Elmalaki, S.	FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT	2021	IoTDI '21: Proceedings of the International Conference on Internet-of-Things Design and Implementation		119	32	Thanks to the rapid growth in wearable technologies, monitoring complex human context becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously. Nevertheless, a central challenge in designing such personalized IoT applications arises from human variability. Such variability stems from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change the behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability). To that end, we propose FaiR-IoT, a general reinforcement learning-based framework for adaptive and fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three levels of reinforcement learning agents interact to continuously learn human preferences and maximize the system's performance and fairness while taking into account the intra-, inter-, and multi-human variability. We validate the proposed framework on two applications, namely (i) Human-in-the-Loop Automotive Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House. Results obtained on these two applications validate the generality of FaiR-IoT and its ability to provide a personalized experience while enhancing the system's performance by 40%-60% compared to non-personalized systems and enhancing the fairness of the multi-human systems by 1.5 orders of magnitude.	Conference Paper	18 May 2021	10.1145/3450268.3453525	[]	[]	-1	-1	-1
J	Feher, Arpad; Aradi, Szilard; Becsi, Tamas	Hierarchical Evasive Path Planning Using Reinforcement Learning and Model Predictive Control	2020	IEEE ACCESS		187470	187482	Motion planning plays an essential role in designing self-driving functions for connected and autonomous vehicles. The methods need to provide a feasible trajectory for the vehicle to follow, fulfilling different requirements, such as safety, efficiency, and passenger comfort. In this area, algorithms must also meet strict real-time expectations, since, especially in an emergency, the decision time is limited, which raises a trade-off for the feasibility requirements. This article proposes a hierarchical path planning solution for evasive maneuvering, where a Twin Delayed DDPG reinforcement learning agent generates the parameters of a geometric path consisting of chlotoids and straight sections, and an underlying model predictive control loop fulfills the trajectory following tasks. The method is applied to the automotive double lane-change test, a common emergency situation, comparing its results with human drivers' performance using a dynamic simulation environment. Besides the test's standardized parameters, a broader range of topological layouts is chosen, both for the training and performance evaluation. The results show that the proposed method highly outperforms human drivers, especially in challenging situations, while meeting the computational requirements, as the pre-trained neural network and path generation algorithm can provide a solution in an instant, based on the experience gained during the training process.	Article	2020	10.1109/ACCESS.2020.3031037	[]	[]	-1	-1	-1
P	FIGUEROA R; KLOSIN J	New metal-ligand complex useful in catalyst for            preparing polyolefin such as ethylene homopolymer for            synthetic lubricants, elastic films, sporting goods,            building and construction, automotive, and medical            application						NOVELTY - A metal-ligand complex (I) in aggregate and neutral form is new. USE - In the catalyst for preparing polyolefin (claimed) selected from ethylene homopolymer, ethylene/alpha-olefin interpolymer, and ethylene/alpha-olefin/diene interpolymer useful in synthetic lubricants, elastic films, flexible molded goods, sporting goods, building and construction, automotive, and medical applications; and as materials for use in manufacturing foams, films, coatings, fibers, fabrics, extruded articles, and molded articles. ADVANTAGE - The complex improves reaction yields, reduces manufacturing costs, and improves process safety. DETAILED DESCRIPTION - A metal-ligand complex of formula (I) in aggregate and neutral form is new.L=halo, hydrogen atom, 1-40C hydrocarbylC(O)N(H)-, 1-40C hydrocarbylC(O)N(1-20C)hydrocarbyl), 1-40C hydrocarbylC(O)O-, 1-40C hydrocarbyl, 1-40C heterohydrocarbyl, RkRlN-, RlO-, RlS-, or RkRlP-;Rk and Rl=H, 1-40C hydrocarbyl, ((1-40C)hydrocarbyl)3Si, ((1-10C hydrocarbyl)3-Si(1-10C)hydrocarbyl, or (1-40C)heterohydrocarbyl;Rk+Rl=2-40C hydrocarbylene or 1-40C heterohydrocarbylene;L=monoanionic moiety that is bonded;X=absent or a neutral Lewis base group that is RxNRkRl, RkORl, RkSRl, or RxPRkRl;Rx=H, 1-40C hydrocarbyl, 1-40C hydrocarbyl)3Si,((2-20C) hydrocarbyl)3Si(2-20C)hydrocarbyl, or (1-40C)heterohydrocarbyl;Lq and Lr=absent, L or X;Lq+Lr=(Rd)2C=C(Rd)-C(Rd)=C(Rd)2;Rd=H, unsubstituted (1-6C)alkyl, phenyl, or naphthyl;M=metal of any one of groups III to VI or groups VII to IX, where the metal being in a formal oxidation state of +2, +3, +4, +5, or +6;n=1 or 2;R1=H, (1-40C) hydrocarbyl, or 1-40C heterohydrocarbyl;R2, R3, and R4=H, 1-40C hydrocarbyl, 1-40C hydrocarbylO-, 1-40C hydrocarbylS-, 1-40C hydrocarbyl-S(O)-, 1-40C hydrocarbyl-S(O)2-, (1-40C hydrocarbyl)2N-, (1-40C hydrocarbyl)2P-, or 1-40C heterohydrocarbyl;R5=1-40C hydrocarbyl or 1-40C heterohydrocarbyl;R1+R2, R2+R3, R3+R4, R4+R5, R1 or R5+Rk of X, or R1 or R5+Rl of L=(1-40C)hydrocarbylene or 1-40C heterohydrocarbylene;R1 or R5+L=1-40C hydrocarbylene-C(O)N(H)-, 1-4C hydrocarbylene-C(O)N((1-20C)hydrocarbyl), 1-40C hydrocarbylene-C(O)O-, 1-40C hydrocarbylene, or 1-40C heterohydrocarbylene;j=1 or 2;R1+R1, R5+R5, R1+R5=1-40C hydrocarbylene or 1-40C heterohydrocarbylene;Rs=halo, polyfluoro, perfluoro, unsubstituted (1-18C)hydrocarbyl, F3C-, FCH2O-, F2HCO-, F3CO-, oxo (i.e., =O), R3Si-, RO-, RS-, RS(O)-, RS(O)2-, R2P-, R2N-, R2C=N-, NC-, RC(O)O-, ROC(O)-, RC(O)N(R)-, or R2NC(O)-;andR=unsubstituted 1-18C hydrocarbyl.In above value each of 1-10C hydrocarbyl, 1-20C hydrocarbyl, 1-40C hydrocarbyl, 1-40C heterohydrocarbyl, 1-40C hydrocarbylene, 2-40C hydrocarbylene, and 1-40C heterohydrocarbylene are optionally substituted by Rs. Any three or four of R1 to R5 , Rl of X, and Rl of L optionally are taken together to form a respective trivalent or tetravalent analog of 1-40C hydrocarbylene or 1-40C heterohydrocarbylene; or R1 or R5 and any one or two of the remainder of R1 to R5, Rk of X, and Rl of L optionally are taken together with L to form a respective trivalent or tetravalent analog of 1-4C hydrocarbylene-C(O)N(H)-, 1-40C hydrocarbylene-C(O)N((1-20C)hydrocarbyl), 1-40C hydrocarbylene-C(O)O-, 1-40C hydrocarbylene, or 1-40C heterohydrocarbylene. Provided that:(1) when n is 2, then one L is absent; and(2) when j is 2, then n is 2.INDEPENDENT CLAIMS are included for following:(1) a catalyst (C1) comprising the metal-ligand complex (I) and at least activating co-catalysts, or its reaction product, where the ratio of total number of moles of the metal-ligand complexes to total number of moles of activating co-catalyst is 1:10000 to 100:1;(2) preparing a polyolefin involving contacting at least one polymerizable olefin to the catalyst (C1) under olefin-polymerizing conditions to polymerize at least some of the at least one polymerizable olefin and produce polyolefin from it;(3) preparing catalyst (C1) involving contacting metal-ligand complex (I) to at least activating co-catalysts under conditions to prepare the catalyst, where the ratio of total moles of the metal-ligand complexes to total moles of the activating co-catalyst is 1:10000 to 100:1;(4) preparing the metal-ligand complex (I); and(5) compound of formula (III) is new intermediate.R'1=1-40C alkyl or 6-40C aryl;andR'5=6-40C aryl or 1-40C alkyl.				[]	[]	-1	-1	-1
P	OKAZAKI K; TAKEDA M; KATOU T; SAWADA M	Automotive brake fluid pressure control appts										[]	[]	-1	-1	-1
P	VERONESE L; SHANTIA A; PATHAK S	Method for automatically initiating change of lane            in automated automotive vehicle, involves providing to            sensory inputs sensory data from disparate sources,            data being representative of sensed vehicular driving            environment of ego vehicle						NOVELTY - The method involves providing to sensory inputs sensory data from disparate sources, data being representative of a sensed vehicular driving environment of ego vehicle. The sensory data is combined in the sensory fusion processor to generate a semantic image of sensed vehicular driving environment. The semantic image is simplified static representation in two dimensions of the vehicular driving environment at the time sensory data which is provided to sensory inputs dimensions extending along the roadway both ahead and behind ego vehicle and laterally across at two lanes. The reinforcement learning is used to solve the Markov Decision Process (MDP) for a change of the agent state representing a successful change of lane of the ego vehicle. The solution of the MDP is embodied in the automated lane change system. USE - Method for automatically initiating a change of lane in an automated automotive vehicle. ADVANTAGE - The method approach is readily combined with a general approach for automatic cruise control or in a fully automatically driven vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method of using a vehicular automated driving system to drive automatically an ego vehicle in a vehicular driving environment comprises lanes of traffic flowing along the same roadway; and(2) a vehicular automated driving system for driving automatically an ego vehicle in a vehicular driving environment. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system.Safe Scenarios (30)Simulation (31)Vehicles (32)Lanes (33)Road Signage (34)				[]	[]	-1	-1	-1
B	Degadwala, S.; Upadhyay, R.; Upadhyay, S.; Soni, M.; Parikh, D.J.; Vyas, D.	DeepTread: Exploring Transfer Learning in Tyre Quality Classification	2023	2023 International Conference on Sustainable Communication Networks and Application (ICSCNA)		1448	53	"In recent years, the automotive industry has witnessed a significant shift towards leveraging advanced technologies for quality control and assessment, with a particular focus on tire quality. This research study presents ""DeepTread,"" an innovative approach that explores the untapped potential of transfer learning in the domain of tire quality classification. Transfer learning, a powerful paradigm within deep learning, allows the adaptation of pre-trained neural networks for the purpose of tire quality evaluation. By leveraging the knowledge gained from various related domains, DeepTread aims to improve the accuracy and efficiency of tire quality classification, thereby contributing to safer and more reliable automotive solutions. The methodology's effectiveness is validated through extensive experiments, demonstrating promising results and encouraging future developments in the field of tire quality assessment."	Conference Paper	2023	10.1109/ICSCNA58489.2023.10370168	[]	[]	-1	-1	-1
P	YIN Q; QIN L; ZHANG Q; XU K; ZENG J; FANG Q; XU H; HU Y	Method for realizing temporary machine cooperation            process of unmanned vehicles based on diversity            population training, involves applying joint strategy            in driving vehicle controlled by party, and realizing            adjacent machine collaboration with non-party vehicle            of same road						NOVELTY - The method involves obtaining a cooperative strategy to perform machine collaboration process by using driving behaviors of two intelligent bodies. Action decision is realized based on reinforcement learning algorithm. A reward matrix of meta game is calculated by using experience game analyzing process. An expected base is selected as a population diversity evaluation index for guiding bottom training to generate a current strategy. The current strategy is added to a driving strategy population. Iterative training process is performed continuously. Iteration process is stopped when scale of the population reaches maximum diversity. The driving strategy population is taken as a training set. Cooperation strategies of two intelligent agents are cross verified to obtain a joint strategy of two intelligent agents. The joint strategy is applied in driving vehicle controlled by a party. Adjacent machine collaboration with non-party vehicle of a same road is realized. USE - Method for realizing temporary machine cooperation process of unmanned vehicles based on diversity population training. ADVANTAGE - The method enables utilizing the joint strategy for an unmanned vehicle in a traffic road multi-vehicle cooperative driving scene to realize near-machine cooperation process effectively, enhancing vehicle cooperative capability of the unmanned vehicle in the traffic road multi-vehicle cooperative driving scene so as to reduce collision probability, solving road conflict, and improving driving efficiency of the road vehicle. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a device for realizing temporary machine cooperation process of unmanned vehicles based on diversity population training; and (2) a computer device comprising a memory and a processor for realizing temporary machine cooperation process of unmanned vehicles based on diversity population training. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for realizing temporary machine cooperation process of unmanned vehicles based on diversity population training. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Fu, Fang; Kang, Yunpeng; Zhang, Zhicai; Yu, F. Richard; Wu, Tuan	Soft Actor-Critic DRL for Live Transcoding and Streaming in Vehicular Fog-Computing-Enabled IoV	2021	IEEE INTERNET OF THINGS JOURNAL		1308	1321	With the rapid development of automotive industry and telecommunication technologies, live streaming services in the Internet of Vehicles (IoV) play an even more crucial role in vehicular infotainment systems. However, it is a big challenge to provide a high quality, low latency, and low bitrate variance live streaming service for vehicles due to the dynamic properties of wireless resources and channels of IoV. To solve this challenge, we propose a novel live video transcoding and streaming scheme that maximizes the video bitrate and decreases time-delays and bitrate variations in vehicular fog-computing (VFC)-enabled IoV, by jointly optimizing vehicle scheduling, bitrate selection, and computational/spectrum resource allocation. This joint optimization problem is modeled as a Markov decision process (MDP), considering time-varying characteristics of the available resources and wireless channels of IoV. A soft actor-critic deep reinforcement learning (DRL) algorithm that is based on the maximum entropy framework, is subsequently utilized to solve the above MDP. Extensive simulation results based on the data set of the real world show that compared to other baseline algorithms, the proposed scheme can effectively improve video quality while decreasing latency and bitrate variations, and access excellent performance in terms of learning speed and stability.	Article	FEB 1 2021	10.1109/JIOT.2020.3003398	[]	[]	-1	-1	-1
B	Chen, Yi	Human-Robot Collaboration in Automotive Assembly	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
P	ZHANG D; MA L; NIU Y; MENG J; XU J	Fine gesture recognition based four-rotor unmanned            aerial vehicle man-machine interaction system, has            self-adaptive cascade proportional integral derivation            control module to perform dynamic action selection to            select cascade PID control system parameter						NOVELTY - The system has a user for wearing an IMU sensor glove under hidden environment for observing real-time environment of an unmanned aerial vehicle (UAV). An environment interaction module uses a characteristic fusion network to perform gesture recognition to data sent by the IMU sensor glove based on cavity residual, and sends identified gesture instruction to a self-adaptive cascade proportional integral derivation (PID) control module. The self-adaptive cascade PID control module sets position coordinate to-be tracked according to the received gesture instruction, and minimizes loss function as a target. The self-adaptive cascade PID control module performs dynamic action selection to select cascade PID control system parameter. USE - Fine gesture recognition based four-rotor unmanned aerial vehicle man-machine interaction system. ADVANTAGE - The system improves accuracy of fine gesture recognition, and uses reinforcement learning to dynamically adjust the cascade PID controller parameter, and improves robustness of PID control, and reduces noise information. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for controlling a fine gesture recognition based four-rotor unmanned vehicle man-machine interaction system. (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Zhang Guohui; Wang Xuan; Zhang Yanan; Gao Ang	Research on Cooperative Path Planning Model of Multiple Unmanned Vehicles in Real Environment	2023	Journal of System Simulation		408	422	The cluster combat application of unmanned ground vehicles(UVS)is a hot research issue of the intersection of artificial intelligence and battle command. Aiming at the cooperative path planning multiple unmanned vehicles not meeting the dynamic threat condition requirement, by combining the global path planning algorithm A-STAR with the local path planning algorithm RL, from the perspective of perception to behavioral decision making, the cooperative path planning model of multiple unmanned vehicles is studied. The cooperative combat situation threat algorithm, state and action space, reward function and sphere of influence function are designed, the sub-models of formation configuration strategy generation and dynamic optimization of strike path are carried out, and the cooperative path planning control model of multiple unmanned vehicles based on autonomous learning is constructed and solved. An application example shows that the proposed path planning model can effectively cope with the requirements of multi-unmanned vehicle collaborative path planning task in complex urban environment,and has important theoretical research and practical application value.	Article	2023		[]	[]	-1	-1	-1
J	Fernando, Yudi; Shaharudin, Muhammad Shabir; Abideen, Ahmed Zainul	Circular economy-based reverse logistics: dynamic interplay between sustainable resource commitment and financial performance	2023	EUROPEAN JOURNAL OF MANAGEMENT AND BUSINESS ECONOMICS		91	112	PurposeThe study aims to propose a circular economy-based reverse logistics (CERL) that emphasises the mediation effect of reverse logistics (RL) on sustainable resource commitment and financial performance.Design/methodology/approachThe structural equation modelling (SEM) approach has been applied to analyse the data acquired through the survey method that included 113 vendors of automotive supplies of the 1st and 2nd levels.FindingsThe results confirm that CERL acts as an essential intervening entity between resources and financial performance. The findings of the study have provided research and development (R & D) opportunities for the industries to find alternative revenue streams and generate profit from resource investment whilst upholding environmental standards through reverse logistic practices.Practical implicationsReverse logistic practices are the key components of a circular business model and a sustainable supply chain. The manufacturing companies need to explore critical enablers that can contribute to business productivity and financial growth.Originality/valueThe study has validated a CERL model that portrays the circular economy's resilient relationship with RL practices.	Article	FEB 28 2023	10.1108/EJMBE-08-2020-0254	[]	[]	-1	-1	-1
J	Liu, Rui; Wang, Chun; Tang, Aihua; Zhang, Yongzhi; Yu, Quanqing	A twin delayed deep deterministic policy gradient-based energy management strategy for a battery-ultracapacitor electric vehicle considering driving condition recognition with learning vector quantization neural network	2023	JOURNAL OF ENERGY STORAGE				Deep reinforcement learning algorithms have been widely applied in the energy management of hybrid energy storage systems. However, these deep reinforcement learning algorithms, such as DQN and DDPG, have the problem of discontinuous action space and consistently overestimated Q values. To address this issue, a novel energy management strategy based on a twin delayed deep deterministic policy gradient (TD3) algorithm is proposed for the battery-ultracapacitor electric vehicles in this study. In addition, the driving condition recognition method is integrated into the energy management strategy framework to reduce the training time of the TD3 agent. The detailed implementation steps are as follows. At first, dynamic experiments were performed to establish high-precision models of the battery and ultracapacitor. Secondly, learning vector quantization neural networks are applied to classify driving conditions, namely, urban, suburban and highway conditions. Furthermore, three parallel TD3 agents are trained for urban, suburban and highway conditions, respectively. Finally, the proposed strategy is evaluated under standard driving cycles. The simulation results indicate that compared with the TD3-based strategy, the proposed strategy improves the economy by 1 % and reduces the training time by 34 %, and the economic gap with the dynamic programming-based energy management strategy is narrowed down to 3 %.	Article; Early Access		10.1016/j.est.2023.108147	[]	[]	-1	-1	-1
J	Izzo, D.; Tailor, D.; Vasileiou, T.	On the stability analysis of optimal state feedbacks as represented by deep neural models [arXiv]	2018	arXiv		13 pp.	13 pp.	Research has shown how the optimal feedback control of several non linear systems of interest in aerospace applications can be represented by deep neural architectures and trained using techniques including imitation learning, reinforcement learning and evolutionary algorithms. Such deep architectures are here also referred to as Guidance and Control Networks, or G&CNETs. It is difficult to provide theoretical proofs on the control stability of such neural control architectures in general, and G&CNETs in particular, to perturbations, time delays or model uncertainties or to compute stability margins and trace them back to the network training process or to its architecture. In most cases the analysis of the trained network is performed via Monte Carlo experiments and practitioners renounce to any formal guarantee. This lack of validation naturally leads to scepticism especially in cases where safety and validation are of paramount importance such as is the case, for example, in the automotive or space industry. In an attempt to narrow the gap between deep learning research and control theory, we propose a new methodology based on differential algebra and automated differentiation to obtain formal guarantees on the behaviour of neural based control systems.	Journal Paper	6 Dec. 2018		[]	[]	-1	-1	-1
J	Roveda, Loris; Pallucca, Giacomo; Pedrocchi, Nicola; Braghin, Francesco; Tosatti, Lorenzo Molinari	Iterative Learning Procedure With Reinforcement for High-Accuracy Force Tracking in Robotized Tasks	2018	IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS		1753	1763	The paper focuses on industrial interaction robotics tasks, investigating a control approach involving multiples learning levels for training the manipulator to execute a repetitive (partially) changeable task, accurately controlling the interaction. Based on compliance control, the proposed approach consists of two main control levels: 1) iterative friction learning compensation controller with reinforcement and 2) iterative force-tracking learning controller with reinforcement. The learning algorithms rely on the iterative learning and reinforcement learning procedures to automatize the controllers parameters tuning. The proposed procedure has been applied to an automotive industrial assembly task. A standard industrial UR 10 Universal Robot has been used, equipped by a compliant pneumatic gripper and a force/torque sensor at the robot end-effector.	Article	APR 2018	10.1109/TII.2017.2748236	[]	[]	-1	-1	-1
B	Turlej, W.; Pankiewicz, N.	Adversarial Trajectories Generation for Automotive Applications	2021	2021 25th International Conference on Methods and Models in Automation and Robotics (MMAR)		115	20	The development of Advanced Driver Assistance Systems (ADAS) with a high level of autonomy requires immense testing efforts to ensure the safety and robustness of developed algorithms in critical situations. Unfortunately, exploration of difficult situations through test drives in natural traffic is ineffective due to the rarity of such events. While scenario-based testing in a virtual environment is often proposed as an effective method that helps to evaluate system performance in difficult situations, the manual definition of virtual test scenarios poses a significant challenge itself. Performance drops in tested systems, especially ones containing machine learning components, may be related to situations that are not necessarily considered challenging for a human driver and thus are difficult to predict in a test design. In this paper, we propose a method that allows to generate a variety of virtual test scenarios for ADAS through an adversarial trajectories generation. The method generates scenarios by finding trajectories of the road users in the proximity of the vehicle controlled by the tested algorithm that result in safety-critical events, such as collisions. We demonstrate the effectiveness of the presented method on an example of a critical scenario generation for a vehicle control policy based on Reinforcement Learning methods.	Conference Paper	2021	10.1109/MMAR49549.2021.9528492	[]	[]	-1	-1	-1
J	Magu, Georgiana; Lucaciu, Radu; Isar, Alexandru	Improving the Targets' Trajectories Estimated by an Automotive RADAR Sensor Using Polynomial Fitting	2021	APPLIED SCIENCES-BASEL				A way to reduce the uncertainty at the output of a Kalman filter embedded into a tracker connected to an automotive RADAR sensor consists of the adaptive selection of parameters during the tracking process. Different informed strategies for automatically tuning the tracker's parameters and to jointly learn the parameters and state/output sequence using: expectation maximization; optimization approaches, including the simplex algorithm; coordinate descent; genetic algorithms; nonlinear programming using finite differencing to estimate the gradient; Bayesian optimization and reinforcement learning; automatically tuning hyper-parameters in the least squares, were already proposed. We develop here a different semi-blind post-processing approach, which is faster and more robust. Starting from the conjecture that the trajectory is polynomial in Cartesian coordinates, our method supposes to fit the data obtained at the output of the tracker to a polynomial. We highlight, by simulations, the improvement of the estimated trajectory's accuracy using the polynomial fitting for single and multiple targets. We propose a new polynomial fitting method based on wavelets in two steps: denoising and polynomial part extraction, which compares favorably with the classical polynomial fitting method. The effect of the proposed post-processing methods is visible, the accuracy of targets' trajectories estimations being hardly increased.	Article	JAN 2021	10.3390/app11010361	[]	[]	-1	-1	-1
B	Qiang Liu; Tao Han; Xie, J.L.; BaekGyu Kim	LiveMap: Real-Time Dynamic Map in Automotive Edge Computing	2021	IEEE INFOCOM 2021 - IEEE Conference on Computer Communications		10 pp.	10 pp.	Autonomous driving needs various line-of-sight sensors to perceive surroundings that could be impaired under diverse environment uncertainties such as visual occlusion and extreme weather. To improve driving safety, we explore to wirelessly share perception information among connected vehicles within automotive edge computing networks. Sharing massive perception data in real time, however, is challenging under dynamic networking conditions and varying computation work-loads. In this paper, we propose LiveMap, a real-time dynamic map, that detects, matches, and tracks objects on the road with crowdsourcing data from connected vehicles in sub-second. We develop the data plane of LiveMap that efficiently processes individual vehicle data with object detection, projection, feature extraction, object matching, and effectively integrates objects from multiple vehicles with object combination. We design the control plane of LiveMap that allows adaptive offloading of vehicle computations, and develop an intelligent vehicle scheduling and offloading algorithm to reduce the offloading latency of vehicles based on deep reinforcement learning (DRL) techniques. We implement LiveMap on a small-scale testbed and develop a large-scale network simulator. We evaluate the performance of LiveMap with both experiments and simulations, and the results show LiveMap reduces 34.1% average latency than the baseline solution.	Conference Paper	2021	10.1109/INFOCOM42981.2021.9488872	[]	[]	-1	-1	-1
J	Jin, Jiangliang; Mao, Shuai; Xu, Yunjian	Optimal Priority Rule-Enhanced Deep Reinforcement Learning for Charging Scheduling in an Electric Vehicle Battery Swapping Station	2023	IEEE TRANSACTIONS ON SMART GRID		4581	4593	For a battery swapping station (BSS) with solar generation, N charging bays, and an inventory of M batteries, we study the charging scheduling problem under random EV arrivals, renewable generation, and electricity prices. To minimize the expected weighted sum of charging cost (sum of electricity and battery degradation costs) and EV owners' waiting cost, we formulate the problem as a Markov decision process with unknown state transition probability. Under a mild heavy-traffic assumption, we rigorously establish the optimality of the Less Demand First (LDF) priority rule under arbitrary system dynamics: batteries with less demand shall be charged first. The optimality result enables us to integrate the LDF rule into a state-of-the-art deep reinforcement learning (DRL) method, proximal policy optimization (PPO), reducing the dimensionality of its output from O(M+N) to O(1), without loss of optimality in the heavy-traffic scenario. Numerical results (on real-world data) demonstrate that the proposed LDF enhanced PPO approach significantly outperforms classical DRL methods and FCFS (first come, first served) priority rule based DRL counterparts.	Article	NOV 2023	10.1109/TSG.2023.3250505	[]	[]	-1	-1	-1
B	[Anonymous]	2011 IEEE International Conference on Control Applications (CCA), part of the IEEE Multi-Conference on Systems & Control (MSC)	2011	2011 IEEE International Conference on Control Applications (CCA), part of the IEEE Multi-Conference on Systems & Control (MSC)		NULL pp	NULL pp	The following topics are dealt with: multi-agent control systems; control engineering computing; energy systems control; industrial control; adaptive control; aerospace applications; reinforcement learning; robust model-based control; tokamak control; automotive applications; control education; iterative learning control; drill string vibration control; Kalman filtering; optimal control; and renewable energy electric grid integration.	Conference Proceedings	2011		[]	[]	-1	-1	-1
J	Jindal, A.; Sangwan, K.S.	Evaluation of collection methods in reverse logistics by using fuzzy mathematics	2015	Benchmarking: An International Journal		393	410	Purpose - The efficiency and effectiveness of reverse logistics (RL) is dependent on collection methods as the collection activities are critical in determining the economic viability of the entire recovery chain. The purpose of this paper is to evaluate the various collection methods used in RL under uncertain environment. Design/methodology/approach - An integrated fuzzy multi-criteria decision model has been developed for the evaluation of various collection methods. The evaluation has been done based on the criteria of initial investment, value added recovery, return volume, operating cost, degree of supply chain control, and level of customer satisfaction. The three alternatives used in the study are collection by the manufacturer directly from the customer, collection by the retailer, and collection by the third party. The fuzzy analytical hierarchy process has been used to compute the criteria weights and fuzzy technique for order preference by similarity to ideal solution has been used to rank the alternative collection methods. Fuzzy mathematics has been used to take care of uncertainties in the RL. Findings - Selection and evaluation of alternative collection methods is affected by multiple criteria like initial investment, value added recovery, return volume, operating cost, degree of supply chain control, and level of customer satisfaction. The utility of the proposed evaluation methodology has been validated by solving a case example from automotive company. Originality/value - The proposed methodology will provide a useful tool to the decision maker for the evaluation and selection of the alternative collection methods in RL. This will help companies in strategic decision making to prioritize and develop collection facilities accordingly.	Journal Paper	2015	10.1108/BIJ-05-2013-0062	[]	[]	-1	-1	-1
J	Singh, Nikhilesh; Renganathan, Karthikeyan; Rebeiro, Chester; Jose, Jithin; Mader, Ralph	Kryptonite : Worst-Case Program Interference Estimation on Multi-Core Embedded Systems	2023	ACM TRANSACTIONS ON EMBEDDED COMPUTING SYSTEMS				Due to the low costs and energy needed, cyber-physical systems are adopting multi-core processors for their embedded computing requirements. In order to guarantee safety when the application has real-time constraints, a critical requirement is to estimate the worst-case interference from other executing programs. However, the complexity of multi-core hardware inhibits precisely determining the Worst-Case Program Interference. Existing solutions are either prone to overestimate the interference or are not scalable to different hardware sizes and designs.In this paper we present Kryptonite, an automated framework to synthesize Worst-Case Program Interference (WCPI) environments for multi-core systems. Fundamental to Kryptonite is a set of tiny hardware-specific code gadgets that are crafted to maximize interference locally. The gadgets are arranged using a greedy approach and then molded using a Reinforcement Learning algorithm to create the WCPI environment. We demonstrate Kryptonite on the automotive grade Infineon AURIX TC399 processor with a wide range of programs that includes a commercial real-time automotive application. We show that, while being easily scalable and tunable, Kryptonite creates WCPI environments increasing the runtime by up to 58% for benchmark applications and 26% for the automotive application.	Article	OCT 2023	10.1145/3609128	[]	[]	-1	-1	-1
P	RAPPERPORT J; SOLOTAROFF J	Method for generating promotions leveraging            existing contracts retailer enters into in order to            obtain promotional variable values, involves collecting            transaction log data, and updating learning model based            on transaction log data						NOVELTY - The method involves receiving a contract at an offer generation system. Data is extracted from the contract. Multiple test offers are accessed by an offer bank. A subset of the test offers is selected from the offer bank by scoring each test offer. Forecast score is predicted for the test offer by applying a reinforcement learning model to transaction logs related to a product. Penalty is applied to the forecast score based on difference between the test offer and contractual offer. The subset of test offers is assigned to multiple retail locations. Transaction log data is collected from multiple retail locations. A reinforcement learning model is updated based on the transaction log data. USE - Method for generating promotions leveraging existing contracts a retailer enters into in order to obtain promotional variable values. Uses include but are not limited to food, home and personal care, consumer durables such as consumer appliances, consumer electronics, automotive leasing, consumer services such as retail financial services, health care, insurance, home repair, beauty, personal care and travel and hospitality such as hotels, airline flights, and restaurants. ADVANTAGE - The method enables improving sales and providing additional feedback data to refine the possible promotional activities. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) a computer readable storage medium for storing a set of instructions for performing a method for generating promotions leveraging existing contracts a retailer enters into in order to obtain promotional variable values;(2) a system for generating promotions leveraging existing contracts a retailer enters into in order to obtain promotional variable values. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram a method for generating promotions leveraging existing contracts a retailer enters into in order to obtain promotional variable values.				[]	[]	-1	-1	-1
J	Busacca, Fabio; Grasso, Christian; Palazzo, Sergio; Schembra, Giovanni	A Smart Road Side Unit in a Microeolic Box to Provide Edge Computing for Vehicular Applications	2023	IEEE TRANSACTIONS ON GREEN COMMUNICATIONS AND NETWORKING		194	210	Deployment of technologies for Intelligent Transportation Systems (ITS) involves the installation of Road Side Units (RSU) on the roadway and On-Board Units (OBU) inside vehicles. In this direction, 5G technologies will make a great impulse providing low latency communication and computation at the edge of the network. First, this paper defines VMEC-in-a-Box, a smart RSU combined with a MEC station, aimed at providing edge computing for vehicular applications by enabling job offloading from vehicles. VMEC-in-a-Box is equipped with a microeolic power generator to be autonomous and self-consistent even in presence of low levels of wind. The behavior of VMEC-in-a-Box is controlled by artificial intelligence to vary its computing capacity dynamically to pursue the best tradeoff between performance and power consumption, and to cooperate by offloading jobs to each other (horizontal offload) to improve performance and reliability of the system. Hence, the paper defines a Markov model to support decisions to optimize by means of Reinforcement Learning the system behavior according to two reward functions defined at the MEC and at the Vehicular Domains. To the best of our knowledge, this is the first work proposing an integrated framework to maximize reliability and performance at both Vehicular and MEC Domains.	Article	MAR 2023	10.1109/TGCN.2022.3187674	[]	[]	-1	-1	-1
B	McDowell, Journey	Comparison of Modern Controls and Reinforcement Learning for Robust Control of Autonomously Backing Up Tractor-Trailers to Loading Docks	2019						Dissertation/Thesis	Jan 01 2019		[]	[]	-1	-1	-1
B	Song, J.; Lyu, D.; Zhang, Z.; Wang, Z.; Zhang, T.; Ma, L.	When cyber-physical systems meet AI: a benchmark, an evaluation, and a way forward	2022	ICSE-SEIP '22: Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice		343	52	Cyber-Physical Systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc. In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly available. There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains. To bridge this gap, we present a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods. Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify current challenges and future opportunities. Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains. Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS to achieve optimal performance and reliability. Our benchmark, code, detailed evaluation results, and experiment scripts are available on https://sites.google.com/view/ai-cps-benchmark.	Conference Paper	2022	10.1145/3510457.3513049	[]	[]	-1	-1	-1
B	Fusayasu, H.; Heya, A.; Hirata, K.	Modeling and Dynamic Analysis of Three-Degree-of-Freedom Spherical Actuator under Deep Reinforcement Learning Control	2022	2022 23rd International Conference on the Computation of Electromagnetic Fields (COMPUMAG)		4 pp.	4 pp.	Multi-degree-of-freedom (multi-DOF) spherical actuators have been developed for the fields of robotics and industrial machinery. The input current for each coil is calculated using a torque generating equation based on a torque map. If there is a difference between the analyzed and measured torque maps, this modeling error will reduce positioning accuracy. Permanent magnet type actuators can generate unexpected cogging torque due to various manufacturing errors. This manufacturing variation causes a torque map modeling error and significantly deteriorates the performance of conventional control systems using proportional-integral-differential (PID) controllers. Therefore, we propose a method to improve the positioning accuracy by introducing a compensator using reinforcement learning for the torque map modeling error pre-calculated by a 3-D finite element method (3-D FEM). We designed a simulation that assumes manufacturing variations in cogging torque and applied this current compensator to test its performance.	Conference Paper	2022	10.1109/COMPUMAG55718.2022.9826990	[]	[]	-1	-1	-1
B	Jiang, Yu	Vision-Based Eco-Oriented Driving Strategies for Freeway Scenarios Using Deep Reinforcement Learning	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Tang Xiaolin; Chen Jiaxin; Gao Bolin; Yang Kai; Hu Xiaosong; Li Keqiang	Deep Reinforcement Learning-based Integrated Control of Hybrid Electric Vehicles Driven by High Definition Map in Cloud Control System	2022	Journal of Mechanical Engineering		163	177	In the context of the development of intelligence, connectivity, and new energy, the automotive industry combines computer, information communication, artificial intelligence(AI) to achieve integrated development. Based on the new generation of information and communication technology--cloud control system(CCS) of intelligent and connected vehicles(ICVs), the cloud-level automatic driving of new energy vehicles is realized driven by connected data, which provides innovative planning and control ideas for vehicle driving and power systems. Firstly, based on the resource platform of CCS, the latitude, longitude, altitude, and weather of the target road are obtained, and a high definition(HD) path model including slope, curvature, and steering angle is established. Secondly, a deep reinforcement learning(DRL)-based integrated control method for hybrid electric vehicle(HEV) drive by the HD model is proposed. By adopting two DRL algorithms, the speed and steering of the vehicle and the engine and transmission in the powertrain are controlled, and the synchronous learning of four control strategies is realized. Finally, processor-in-the-loop(PIL) tests are performed by using the high-performance edge computing device NVIDIA Jetson AGX Xavier. The results show that under a variable space including 14 states and 4 actions, the DRL-based integrated control strategy realizes the precise control of the speed and steering of the vehicle layer under the high-speed driving cycle of 172 km, and achieves a fuel consumption of 5.53L/100km. Meanwhile, it only consumes 104.14s in the PIL test, which verifies the optimization and real-time performance of the learning-based multi-objective integrated control strategy.	Article	2022		[]	[]	-1	-1	-1
B	Hu, Z.; Huang, R.; Han, C.	Multi-Agent Deep Reinforcement Learning for Charging Coordination of Electric Vehicles	2023	2023 IEEE 3rd International Conference on Industrial Electronics for Sustainable Energy Systems (IESES)		1	6	Virtual power plant (VPP) is a power system control center that mediates among the energy market, power grid, distributed energy resources, power storage units, controllable power loads, and electric vehicles (EV s), As energy consumers and wireless communication users, an EV is one of the key components of VPP systems that pose stringent requirements on both charging system management and communication infrastructure. On one hand, the optimal power flow (OPF) needs to minimize power system loss and voltage variance. On the other hand, communication quality of service (QoS) needs to be accounted for, in terms of data rate and latency in VPP systems. These lead long-term spatial EV charging coordination (i.e., charging station (CS) selection) a non-convex optimization problem, which is yet an open issue. In this regard, by embedding SG millimeter-wave (mm W) access point (AP) into CS, an AP-enabled CS architecture is first devised in this paper to support VPP control information flow and EV communication applications, such as autonomous driving. Based on the AP-enabled CS, an AP-enabled-CS-assisted multi-agent deep reinforcement learning algorithm (ABC-MADRL) is proposed to provide an intelligent long-term EV charging coordination solution. Experimental results illustrate that ABC-MADRL dramatically reduces the power loss and voltage variance by 24.3%, compared to the uncoordinated charging.	Conference Paper	2023	10.1109/IESES53571.2023.10253695	[]	[]	-1	-1	-1
P		Control device for motor vehicle, has microcontroller designed to activate or deactivate two functions e.g. safety function, of quantity of functions of adjustment device based on algorithms that have actual temperature as input variable						NOVELTY - The device has power-metal oxide semiconductor field effect transistors (M1-M4) controlling an electrical parameter e.g. drive current, of a drive motor (Rl). A microcontroller (1) is designed to determine actual temperature of the transistor and to activate or deactivate two functions e.g. safety function, of a quantity of functions of an adjustment device depending on algorithms that have the temperature as an input variable. The microcontroller (1) is electrically connected with the power-metal oxide semiconductor field effect transistors. USE - Used for an adjusting device e.g. window regulator, sun roof, mirror, steering wheel adjustment, belt presenter, lock, rear flap and sliding door, of a motor vehicle (claimed). ADVANTAGE - The device can be adapted to specifications of the automotive engineering. The safety function protects a passenger of the motor vehicle against injuries by the adjustment device to prevent the impairment of the driver during the guidance of the motor vehicle or in special emergency situation e.g. accident situation. DESCRIPTION Of DRAWING(S) - The drawing shows a circuit diagram of a control device.Ground (GND)Power-metal oxide semiconductor field effect transistors (M1-M4)Drive motor (Rl)Temperature sensors (TS1 to TS4)Microcontroller (1)				[]	[]	-1	-1	-1
P	CHEN J; DOU L; XU Y; MA Y; SUN J	Optimal output tracking control method for            enhanced learning of heterogeneous unmanned cluster            system, involves designing data-driven optimal output            tracking control process to obtain optimal            controller						NOVELTY - The method involves establishing (S1) a heterogeneous unmanned cluster system model. A data-based initial stability control strategy learning process is designed (S3) to obtain the initial stability control strategy. The data-driven optimal output tracking control process is designed (S4), based on the initial stable control strategy obtained and the reinforcement learning to obtain the optimal controller. The tracking and control tasks of the heterogeneous unmanned cluster system are implemented based on the optimal controller obtained. USE - Optimal output tracking control method for enhanced learning of heterogeneous unmanned cluster system such as unmanned aircraft, unmanned vehicle and unmanned ship. ADVANTAGE - The optimal output tracking control target of the heterogeneous unmanned cluster system is realized. The state of the leader is accurately estimated within a given time. The convergence time of the designed completely distributed observer with preset time is completely determined by the designer, and the global topology information is not used in the design process. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the optimal output tracking control method for enhanced learning of heterogeneous unmanned cluster system. (Drawing includes non-English language text)S1Step for establishing heterogeneous unmanned cluster system modelS2Step for designing fully distributed observer with preset timeS3Step for designing data-based initial stability control strategy learning process toS4Step for designing data-driven optimal output tracking control process				[]	[]	-1	-1	-1
J		Multi-level Reinforcement Learning for flow control	2021					Flow control is the process of targeted manipulation of fluid flow fields to accomplish a prescribed objective (e.g. reduce drag). Flow control uses information from the flow (provided by sensors) to adapt to incoming perturbations and adjust to changing flow conditions. General flow control is a largely unsolved mathematical problem appearing in many industries, including automotive, aerospace and environmental subsurface flow problems. The missing ingredient for turning flow control into a practical tool is the development of general flow control algorithms that can handle the following: (a) uncertainties in the system perturbations (e.g. the speed and direction of the perturbation), (b) uncertainties in the flow model parameters, (c) sparsity of the observations (i.e. partial and noisy observations) (d) modelling errors due to discretization and parameter upscaling.In this proposal, Reinforcement Learning (RL) algorithms will be utilized to learn general flow control polices using reliable simulated flow environments. From an application point of view, the developed mathematical techniques address flow control in two applications: (a) increasing energy efficiency in transportation trucks by flow control of incompressible Navier-Stokes flow past an obstacle and (b) safe and efficient storage of anthropogenic carbon dioxide (CO2) in deep geological formations using flow control in a Darcy-type subsurface flow. For the first application, road freight transportation accounts for approximately 5% of the UK's carbon footprint and flow control to reduce the aerodynamic drag could significantly improve the fuel efficiency, for example a 15% reduction in drag is equivalent to about 5% in fuel savings. For the CO2 storage application, the produced CO2 by human activities, for example from a power stations or an energy-intensive industries, could be injected into deep saline aquifers as a possible mitigation strategy to reduce anthropogenic emissions of carbon dioxide into the atmosphere. The control of injection strategies in the subsurface storage sites, given the inherent uncertainties in the subsurface properties, would minimize the risk of leakage while maximising the storage capacity.	Awarded Grant	Mar 31 2021		[]	[]	-1	-1	-1
J	Boukhalfa, F.; Alami, R.; Achab, M.; Moulines, E.; Bennis, M.	Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study [arXiv]	2024	Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study [arXiv]				In today's era, autonomous vehicles demand a safety level on par with aircraft. Taking a cue from the aerospace industry, which relies on redundancy to achieve high reliability, the automotive sector can also leverage this concept by building redundancy in V2X (Vehicle-to-Everything) technologies. Given the current lack of reliable V2X technologies, this idea is particularly promising. By deploying multiple RATs (Radio Access Technologies) in parallel, the ongoing debate over the standard technology for future vehicles can be put to rest. However, coordinating multiple communication technologies is a complex task due to dynamic, time-varying channels and varying traffic conditions. This paper addresses the vertical handover problem in V2X using Deep Reinforcement Learning (DRL) algorithms. The goal is to assist vehicles in selecting the most appropriate V2X technology (DSRC/V-VLC) in a serpentine environment. The results show that the benchmarked algorithms outperform the current state-of-the-art approaches in terms of redundancy and usage rate of V-VLC headlights. This result is a significant reduction in communication costs while maintaining a high level of reliability. These results provide strong evidence for integrating advanced DRL decision mechanisms into the architecture as a promising approach to solving the vertical handover problem in V2X.	Preprint	2024		[]	[]	-1	-1	-1
P	ZHANG Y; WANG Y; CONG Y; GAO B; CHEN H	Method for controlling embedded optimization for            unmanned driving, involves combining prediction of            vehicle intention behavior and track, executing action,            and calculating advantage function to update and adjust            actor and critic network						NOVELTY - The method involves driving decision problem for establishing and driving strategy characterization. The motion vector of driving decision of vehicle is represented, based on parameterized driving decision frame. A vehicle control system is configured to perform driving decision, according to designed return function to obtain return signal as training data. The driving strategy is learned and updated by strengthening learning algorithm. The track data is collected under different continuous decision amount, and the lateral parameter and corresponding time parameter are calculated to form lateral parameter sequence. The change of vehicle track is simulated according to decision variable by neural network obtained by training, and prediction of vehicle intention behavior and track are combined. The action is executed to obtain the return value, and an advantage function is calculated to update and adjust the actor and the critic network. USE - Method for controlling embedded optimization for unmanned driving of vehicle. ADVANTAGE - The method enables effective search of the action space in reinforcement learning and makes driving strategies iterate quickly. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a driving control module; and(2) an automatic driving control system. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the automatic driving control system of the unmanned vehicle. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
P	TANG D; HUANG G; WU R; ZHAO S	Three-dimensional trajectory design and resource            scheduling optimization method for complex unmanned            areal vehicle (UAV) network, involves randomly            extracting action from accumulated training experience            pool to randomness training						NOVELTY - The method involves randomly generating an initial position of an unmanned aerial vehicle and a ground user. An unmanned vehicle air-to-ground communication model is generated. The unmanned aerial vehicle communication channel model is generated and the unmanned machine transmission energy consumption model is generated. The unmanned aerial vehicle air-to-ground communication model, the unmanned aerial vehicle communication channel model and the unmanned aerial vehicle transmission energy consumption model parameter are transmitted into the depth reinforcement learning network, for learning and extracting characteristic value. A target system model is trained by a neural network. The target parameter is optimized. An action is randomly extracted from an accumulated training experience pool to randomness the training. USE - Three-dimensional trajectory design and resource scheduling optimization method for complex unmanned areal vehicle (UAV) network. Can be used in football field, rock concerts and traffic hot spots. ADVANTAGE - The method ensures that all the ground users are served, for adjusting the real-time position of the unmanned aerial vehicle to maximize the total downlink capacity of the deep learning and the time consumption of the limited aerial vehicle energy is reduced. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the optimization process. (Drawing includes non-English language text)				[]	[]	-1	-1	-1
J	Chen, Tao; Gao, Ciwei	Intelligent Electric Vehicle Charging Scheduling in Transportation-Energy Nexus with Distributional Reinforcement Learning	2023	IEEE-CAA JOURNAL OF AUTOMATICA SINICA		2171	2173	Dear Editor, This letter is concerned with electric vehicle (EV) charging scheduling problem in transportation-energy nexus using an intelligent decision-making strategy with probabilistic self-adaptability features. In order to accommodate the coupling effects of stochastic EV driving behavior on transport network and distribution network, a risk-captured distributional reinforcement learning solution is presented by using explicit probabilistic information for action and reward function in Markov decision process (MDP) model, where the Bellman equation is extended to a more generalized version. Scheduling EV charging in a transportation-energy nexus, according to both transport and distribution network conditions, is an important topic in recent studies to improve the driving and charging energy efficiency, especially considering the high penetration rate of EV nowadays and even more extremely higher one in the future [1]. In order to accommodate the coupling effects of stochastic EV driving behavior and battery state-of-charge (SoC) on transport and distribution network, various methods have been developed for designing the smart charging scheduling strategy with consideration of electricity price, renewable energy adoption, road conditions and many others.	Article	NOV 2023	10.1109/JAS.2023.123285	[]	[]	-1	-1	-1
B	Turakhia, Shubham	A Deep Reinforcement Learning Approach for Robotic Bicycle Stabilization	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Wang, Xiaohan; Zhang, Lin; Liu, Yongkui; Zhao, Chun; Wang, Kunyu	Solving task scheduling problems in cloud manufacturing via attention mechanism and deep reinforcement learning	2022	JOURNAL OF MANUFACTURING SYSTEMS		452	468	Cloud manufacturing (CMfg) offers a cloud platform for both consumers and providers, and allocating consumer tasks to service providers requires many-to-many scheduling. Deep reinforcement learning (DRL) has been gradually employed to solve CMfg scheduling problems and has achieved satisfactory performance in dynamic and uncertain cloud environments. Nonetheless, we need better scheduling algorithms and more accessible modeling methods to enable practical implementation. In this study, an end-to-end scheduling algorithm is proposed to address task scheduling problems in CMfg. The proposal extracts intercorrelations in enterprise-enterprise and enterprise-task with the multi-head attention mechanism and is trained by DRL. Our proposal has extremely low time-response compared to heuristic algorithms and can generate a scheduling solution in seconds. In contrast to other DRL algorithms, our proposal can achieve better scheduling performances and uses a more accessible modeling method: the objective function alone is sufficient to train the model stably, without the need for a step-based reward function. Applying multi-head attention and DRL to scheduling problems is an exploratory attempt to achieve positive results. Experimental results on a case of automobile structure part processing in CMfg indicated that our proposal showed competitive scheduling performances and running time compared to eight DRL algorithms, two heuristic algorithms, and two priority dispatching rules. Besides, the results proved that our proposal's generalizability and scalability were better than the other eight DRL algorithms.	Article	OCT 2022	10.1016/j.jmsy.2022.08.013	[]	[]	-1	-1	-1
C	Doering, Andre; Dangelmaier, Wilhelm; Danne, Christoph	Using k-means for clustering in complex automotive production systems to support a Q-learning-system	2007	PROCEEDINGS OF THE SIXTH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS		487	+	This work shows the application of k-means clustering to reduce the state space complexity for a q-leaning algorithm in supply networks of serial production systems. An adequate clustering function is introduced and based on several scenarios the results of the clustering are validated with respect to their usability for the q-learning system. In addition, runtime and scalability aspects are evaluated for those scenarios.	Proceedings Paper	2007	10.1109/COGINF.2007.4341928	[]	[]	-1	-1	-1
J	Ren, Xiaoxia; Ye, Jinze; Xie, Liping; Lin, Xinyou	Battery longevity-conscious energy management predictive control strategy optimized by using deep reinforcement learning algorithm for a fuel cell hybrid electric vehicle	2024	ENERGY				Energy management strategies play an essential role in improving fuel economy and extending battery lifetime for fuel cell hybrid electric vehicles. However, the traditional energy management strategy ignores the lifetime of the battery for good fuel economy. To overcome this drawback, a battery longevity-conscious energy manage-ment predictive control strategy is proposed based on the deep reinforcement learning algorithm predictive equivalent consumption minimization strategy (DRL-PECMS) in this study. To begin with, the back-propagation neural network is devised for predicting demand power, and the predictive equivalent consumption minimum strategy (PECMS) is proposed to improve the hydrogen consumption. Then, in order to improve the battery durability performance, the deep reinforcement learning algorithm is utilized to optimize the battery power and improve battery lifetime. Finally, numerical verification and hard-ware in the loop experiments are conducted to validate hydrogen consumption and battery durability performance of the proposed strategy. The validation results show that, compared with CD/CS and SQP(Sequential Quadratic Programming), the PECMS combined can achieve better fuel economy with the fuel consumption reduction by 55.6 % and 5.27 %, which effectively improves the fuel economy. The DRL-PECMS can reduce the effective Ah-throughput by 3.1 % compared with the PECMS. The numerous validations and comparisons demonstrate that the proposed strategy effectively accom-plishes the trade-off optimization between energy consumption and battery durability performance.	Article; Early Access		10.1016/j.energy.2023.129344	[]	[]	-1	-1	-1
P	KOBAYASHI Y; OKAWA W M; YUTA K; MOE O	Thermoplastic resin composition used to form            molded article, comprises carbon nanotubes having            specific average diameter, carbon black having specific            average primary particle diameter, and thermoplastic            resin comprising polyolefin resins, polyamide resin and            polyester resin						NOVELTY - A thermoplastic resin composition comprises 85-90 %mass thermoplastic resin (A), 0.1-5 %mass carbon nanotubes (B) having an average diameter of 1-15 nm, and 5-12% mass carbon black (C) having an average primary particle diameter of 20-50 nm. The component (A) comprises a polyolefin resin (A1) having melt flow rate of 5.0-50 g/10 minutes at 230&#176; C and load of 2.16 kgf, a polyamide resin (A2) having melt flow rate of 5.0-50 g/10 minutes at 240&#176; C and a load of 2.16 kgf, a polyester resin (A3) having melt flow rate of 5.0-50 g/10 minutes at 280&#176; C and load of 1.2 kgf, and a polycarbonate resin (A4) having melt flow rate of 5.0-50 g/10 minutes at 280&#176; C and load of 1.2 kgf. The attenuation amount of the difference between emission direction and exit direction (&#916; RL) is 3 dB or less, and return loss of the difference between emission direction and exit direction (&#916; TL) is 5 dB or less. USE - Thermoplastic resin composition used to form molded article used for electromagnetic wave absorber (claimed). Uses include but are not limited to electrical components, automotive components, medical components, food containers, and electronic devices such as mobile phones and personal computers. ADVANTAGE - The thermoplastic resin composition has excellent dispersibility, and provides a molded article having excellent electromagnetic wave absorbing performance with low reflection loss and low transmission loss and having small angular dependence of the electromagnetic wave absorbing performance. The molded article for a millimeter wave absorber which is excellent in reflection loss and transmission loss in a specific frequency band of 60-90 GHz called millimeter wave. DETAILED DESCRIPTION - A thermoplastic resin composition comprises 85-90 %mass thermoplastic resin (A), 0.1-5 %mass carbon nanotubes (B) having an average diameter of 1-15 nm, and 5-12% mass carbon black (C) having an average primary particle diameter of 20-50 nm. The component (A) comprises a polyolefin resin (A1) having melt flow rate of 5.0-50 g/10 minutes at 230&#176; C and load of 2.16 kgf, a polyamide resin (A2) having melt flow rate of 5.0-50 g/10 minutes at 240&#176; C and a load of 2.16 kgf, a polyester resin (A3) having melt flow rate of 5.0-50 g/10 minutes at 280&#176; C and load of 1.2 kgf, and a polycarbonate resin (A4) having melt flow rate of 5.0-50 g/10 minutes at 280&#176; C and load of 1.2 kgf. The attenuation amount of the difference between reflection loss of emission direction (&#916; RL) is 3 dB or less, and is calculated using the formula: &#916; RL = modulus of (RL(MD)-RL(TD)). The attenuation amount of difference between transmission loss of emission direction (&#916; TL) is 5 dB or less, and is calculated using the formula: &#916; TL = modulus of (TL(MD)-TL(TD)). Where RL(MD) and RL(TD), TL (MD) and TL (TD) are each molded from the thermoplastic resin composition for electromagnetic wave absorbers by an injection molding machine. An electromagnetic wave with a frequency of 76.5 GHz is applied in the thickness direction of the molded material such that the electric field direction of the electromagnetic wave was parallel to the injection direction (MD direction) or perpendicular to the injection direction (TD direction). An INDEPENDENT CLAIM is included for a molded article which is formed from the thermoplastic resin composition.				[]	[]	-1	-1	-1
J	Zhang, Jing; Hou, Lei; Zhang, Bin; Yang, Xin; Diao, Xiaohong; Jiang, Linru; Qu, Feng	Optimal operation of energy storage system in photovoltaic-storage charging station based on intelligent reinforcement learning	2023	ENERGY AND BUILDINGS				Optimizing the energy storage charging and discharging strategy is conducive to improving the economy of the integrated operation of photovoltaic-storage charging. The existing model-driven stochastic optimization methods cannot fully consider the complex operating characteristics of the energy storage system and the uncertainty of photovoltaic power generation and electric vehicle charging load characteristics. Therefore, an optimal operation method for the entire life cycle of the energy storage system of the photovoltaic-storage charging station based on intelligent reinforcement learning is proposed. Firstly, the energy storage operation efficiency model and the capacity attenuation model are finely modeled. Then, the energy storage optimization operation strategy based on reinforcement learning was established with the goal of maximizing the revenue of photovoltaic charging stations, taking into account the uncertainty of electric vehicle charging demand, photovoltaic output, and electricity prices to satisfy the charging requirements and photovoltaic consumption of electric vehicles. A dual delay depth deterministic strategy gradient algorithm is used to solve the problem because of the continuity of decision-making actions for energy storage charging and discharging. The model is trained by the actual historical data, and the energy storage charging and discharging strategy is optimized in real time based on the current period status. Finally, the proposed method and model are tested, and the proposed method is compared with the traditional model-driven method. The results verify the effectiveness of the proposed method and model, and the revenue of optical storage charging stations throughout their energy storage life cycle is improved.	Article; Early Access		10.1016/j.enbuild.2023.113570	[]	[]	-1	-1	-1
J	Cao, X.; Wang, Q.; Li, H.; Ma, H.	Research on Fast CFD Simulation of Automobile Flow Field Based on Artificial Intelligence	2023	Journal of Physics: Conference Series		012011 (7 pp.)	012011 (7 pp.)	Fast automotive aerodynamic evaluation can extremely reduce the design cycles of automotive. This paper establishes a rapid simulation model of the automobile flow field based on the artificial intelligence surrogate model. The end-to-end network is used to map the geometric model, incoming flow conditions, and result. The decoder is used to splice high-dimensional and low-dimensional features to achieve feature sharing. The average MAE error of the optimal model is 5.249%. The average calculation time of a single example is 1.2968s, which is about 0.62% of CFD solver. The simulation result demonstrate that the deep learning method can not only accelerate the calculation, but also can improve the design efficiency of automobile aerodynamic profile with high accuracy.	Conference Paper; Journal Paper	2023	10.1088/1742-6596/2441/1/012011	[]	[]	-1	-1	-1
B	[Anonymous]	2011 IEEE International Symposium on Intelligent Control (ISIC) part of the IEEE Multi-Conference on Systems & Control (MSC)	2011	2011 IEEE International Symposium on Intelligent Control (ISIC) part of the IEEE Multi-Conference on Systems & Control (MSC)				The following topics are dealt with: multiagent system; energy system control; fault diagnostics; industrial systems; embedded control; adaptive control; aerospace application; reinforcement learning; approximation based control; nonzero sum games; robust model based control; fusion plasma control; Tokamaks; automotive applications; control education; repetitive process; iterative learning control; intelligent mechatronics; drill string vibrations; optimization; Kalman filtering; Kalman estimation; optimal control; and renewable energy electric grid integration.	Conference Proceedings	2011		[]	[]	-1	-1	-1
J	Aljohani, Tawfiq M.; Ebrahim, Ahmed; Mohammed, Osama	Real-Time metadata-driven routing optimization for electric vehicle energy consumption minimization using deep reinforcement learning and Markov chain model	2021	ELECTRIC POWER SYSTEMS RESEARCH				A real-time, data-driven electric vehicle (EVs) routing optimization to achieve energy consumption minimization is proposed in this work. The proposed framework utilizes the concept of Double Deep Q-learning Network (DDQN) in learning the maximum travel policy of the EV as an agent. The policy model is trained to estimate the agent's optimal action per the obtained reward signals and Q-values, representing the feasible routing options. The agent's energy requirement on the road is assessed following Markov Chain Model (MCM), with Markov's unit step represented as the average energy consumption that takes into consideration the different driving patterns, agent's surrounding environment, road conditions, and applicable restrictions. The framework offers a better exploration strategy, continuous learning ability, and the adoption of individual routing preferences. A real-time simulation in the python environment that considered real-life driving data from Google's API platform is performed. Results obtained for two geographically different drives show that the proposed energy consumption minimization framework reduced the energy utilization of the EVs to reach its intended destination by 5.89% and 11.82%, compared with Google's proposed routes originally. Both drives started at 4.30 PM on April 25th, 2019, in Los Angeles, California, and Miami, Florida, to reach EV's charging stations that are located six miles away from both of the starting locations.	Article; Early Access		10.1016/j.epsr.2020.106962	[]	[]	-1	-1	-1
J	Nakwattanaset, A.; Suranuntchai, S.	Finite Element Analysis of Reinforcement Rocker Part Made from JAC780Y Steel Sheets	2021	Key Engineering Materials		83	9	The manufacturing industries for automotive parts aim to develop technologies for reducing vehicle weight in order to decrease fuel consumption. However, passive safety function for drivers and passengers must not be impaired or should be even improved. Therefore, advanced high strength steel sheet plays more and more important role in designing automotive components. Nowadays, prediction of formability for sheet metal stamping has high capability more than the past. The major challenge is springback prediction. Moreover, it assists in the tooling design to correctly compensate for springback. Especially in automotive production, springback effects have been generally exhibited distinct after forming process of the high strength steel sheets. The springback effect occurred in the deformed state of metal parts must be taken into account by designing any sheet metal panels. Then, the purpose of the present research is to investigate the springback phenomenon of an automotive part named Reinforcement Rocker RL made from an advanced high strength steel grade JAC780Y, after stamping. In addition, the tools design has been carried out. Finite Element (FE) program known as DYNAFORM (based on LS-DYNA solver), has been applied to analyze and improve the springback effect on such forming part. An anisotropic material model according to type 36 (MAT_036 3-PARAMETER_BARAT) was applied. The results obtained from simulations were compared with required parts in each section. Then, the die surface from compensation in 2nd step forming was modified to use. Finally, the simulation part was verified with the real stamping part. It was found that the finite element simulation showed high capability for prediction and compensation of springback in high strength steel sheets forming.	Journal Paper	2021	10.4028/www.scientific.net/KEM.877.83	[]	[]	-1	-1	-1
C	Ennen, Philipp; Reuter, Sebastian; Vossen, Rene; Jeschke, Sabina	Automated Production Ramp-up Through Self-Learning Systems	2016	3RD ICRM 2016 INTERNATIONAL CONFERENCE ON RAMP-UP MANAGEMENT	Procedia CIRP	57	62	"The ramp-up of production systems is characterised by situations that arise for the first time. Due to the unpredictability of system behaviour in such situations, instabilities occur that lead to reduced production effectiveness. In order to deal with the resulting uncertainty, this paper presents an approach for self-directed systems capable of ""learning"", that is, they adapt their behaviour depending on the signals and changes of the circumfluent world. The advantages of such systems are significant, as they can react to changing products, production equipment and process constraints, and are able to function in exceptional situations. The presented concept makes use of reinforcement learning, one of the most general approaches to learning control. Simulations of three different ramp-up processes are used, where, as a demonstration, robots have to assemble windscreens on a moving truck. (C) 2016 Published by Elsevier B.V."	Proceedings Paper	2016	10.1016/j.procir.2016.05.094	[]	[]	-1	-1	-1
J	El Sallab, A.; Abdou, M.; Perot, E.; Yogamani, S.	End-to-End Deep Reinforcement Learning for Lane Keeping Assist [arXiv]	2016	arXiv		9 pp.	9 pp.	Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications. There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment. Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning. This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks. As this is a relatively new area of research for autonomous driving, we will formulate two main categories of algorithms: 1) Discrete actions category, and 2) Continuous actions category. For the discrete actions category, we will deal with Deep Q-Network Algorithm (DQN) while for the continuous actions category, we will deal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to that, We will also discover the performance of these two categories on an open source car simulator for Racing called (TORCS) which stands for The Open Racing car Simulator. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction with other vehicles. Finally, we explain the effect of some restricted conditions, put on the car during the learning phase, on the convergence time for finishing its learning phase.	Journal Paper	13 Dec. 2016		[]	[]	-1	-1	-1
B	Manring, Levi H.	Using Reinforcement Learning and Bayesian Optimization on Problems in Vehicle Dynamics and Random Vibration Environmental Testing	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Alighanbari, Sina; Azad, Nasser L.	Safe Adaptive Deep Reinforcement Learning for Autonomous Driving in Urban Environments. Additional Filter? How and Where?	2021	IEEE ACCESS		141347	141359	Autonomous driving (AD) provides a reliable solution for safe driving by replacing human drivers responsible for the majority of accidents. The emergence of Machine Learning, specifically Deep Reinforcement Learning (DRL), and its ability to solve complex games proved its potential to address AD challenges. However, model-free methods still suffer from safety-related issues that can be resolved using safe-DRL approaches. The addition of model-based safety filters to the learning-based algorithms provides safety bounds on their performance and constraint satisfaction. In this paper, we investigate the addition of a safety filter based on Model Predictive Control and show an increase in mean testing episode reward by 110% from -75 mean episode reward during testing for 50 episodes for Deep Deterministic Policy Gradient (DDPG) to 7.758. We study the impacts of safety filters (7.758 mean reward), heuristic rules, bounded additive noises (0.49% performance increase comparing to noise-free case), and exploration (3.425 mean reward) on the learning algorithm. We compare the effects of filters in the context of simulated exploration and bounded exploration and prove that bounded exploration results in 9.86% increase in mean reward and 12.95% decrease in std comparing to the other method. Additionally, inspired by Deep Internal Learning and biological mechanisms like brain plasticity, we investigate the idea of using each sample for training only once instead of utilizing stochastic batches which increases the mean testing accumulated reward by 1.87% and leads to the best performance (7.942 mean reward and 0.048 std). Finally, the results demonstrate better automotive results for our proposed method than DDPG. Our proposed method, DDPG with safety filter in bounded exploration and adaptive learning under noisy input conditions, has a success rate of 100% under different traffic densities for the simulation environment used in this paper and our assumptions. The proposed method's automotive results are shown for a braking scenario to avoid collision with other road users.	Article	2021	10.1109/ACCESS.2021.3119915	[]	[]	-1	-1	-1
C	Tsiakoulis, Pirros; Gasic, Milica; Henderson, Matthew; Planells-Lerma, Joaquin; Prombonas, Jorge; Thomson, Blaise; Yu, Kai; Young, Steve; Tzirkel, Eli	Statistical Methods for Building Robust Spoken Dialogue Systems in an Automobile	2013	ADVANCES IN HUMAN ASPECTS OF ROAD AND RAIL TRANSPORTATION	Advances in Human Factors and Ergonomics Series	744	753	We investigate the potential of statistical techniques for spoken dialogue systems in an automotive environment. Specifically, we focus on partially observable Markov decision processes (POMDPs), which have recently been proposed as a statistical framework for building dialogue managers (DMs). These statistical DMs have explicit models of uncertainty, which allow alternative recognition hypotheses to be exploited, and dialogue management policies that can be optimised automatically using reinforcement learning. This paper presents a voice-based in-car system for providing information about local amenities (e.g. restaurants). A user trial is described which compares performance of a trained statistical dialogue manager with a conventional handcrafted system. The results demonstrate the differing behaviours of the two systems and the performance advantage obtained when using the statistical approach.	Proceedings Paper	2013		[]	[]	-1	-1	-1
C	Fu, Fang; Kang, Yunpeng; Zhang, Zhicai; Yu, F. Richard	Transcoding for Live Streaming-based on Vehicular Fog Computing: An Actor-Critic DRL Approach	2020	IEEE INFOCOM 2020 - IEEE CONFERENCE ON COMPUTER COMMUNICATIONS WORKSHOPS (INFOCOM WKSHPS)	IEEE Conference on Computer Communications Workshops	1015	1020	As one of the most frequently launched applications in vehicular networks, live-streaming service is gaining momentum from the rapid development of automotive industry and telecommunication technologies. However, due to the dynamic characteristics of wireless links and topology of vehicular networks, providing high quality, low latency, and low variance video streaming for vehicles is a big challenge. This study aims to maximize video quality while decreasing time-delays and bitrate switches in vehicular fog computing (VFC) systems, by jointly optimizing vehicle scheduling, bitrate selection, computational and spectrum resource allocation. Considering the dynamic characteristics of vehicular networks and available computational/spectrum resource, the above problem is modeled as a Markov decision process (MDP). Since the action space of the MDP is multi-dimensional continuous variables mixed with discrete variables, the study utilizes an actor-critic deep reinforcement learning (DRL) algorithm to resolve the MDP problem. Extensive simulation results based on the dataset of the real world show the effectiveness of the proposed algorithm.	Proceedings Paper	2020		[]	[]	-1	-1	-1
P	KUWANA K; YOSHIDA I; ICHIKAWA H; TOZU K; YOSHIDA T	Vehicle antilocking braking system										[]	[]	-1	-1	-1
J	Zheng, Senjing; Lan, Feiying; Baronti, Luca; Duc Truong Pham; Castellani, Marco	Automatic identification of mechanical parts for robotic disassembly using the PointNet deep neural network	2022	INTERNATIONAL JOURNAL OF MANUFACTURING RESEARCH		1	21	Identification is the first step towards the manipulation of parts for robotic disassembly and remanufacturing. PointNet is a recently developed deep neural network capable of identifying objects from 3D scenes (point clouds) irrespective of their position and orientation. PointNet was used to recognise 12 instances of components of turbochargers for automotive engines. These instances included different mechanical parts, as well as different models of the same part. Point clouds of partial views of the parts were created from CAD models using a purpose-developed depth-camera simulator, reproducing various levels of sensor imprecision. Experimental evidence indicated PointNet can be consistently trained to recognise with accuracy the objects. In the presence of sensor imprecision, the accuracy in the recall phase can be increased adding stochastic error to the training examples. Training 12 independent classifiers, one for each part, did not yield significant improvements in accuracy compared to using one classifier for all the parts. [Submitted 13 September 2019; Accepted 27 March 2020]	Article	2022	10.1504/IJMR.2022.121591	[]	[]	-1	-1	-1
P	BULLARD E; PARAMESWARAN R; SAUL K; SOHMSHETTY R	Method for automating die manufacturing process in            robot-assisted tool and a die building system for            producing automotive body structural components,            involves obtaining die sensor data from die sensors and            image data from image sensors by controllers						NOVELTY - The method (600) involves obtaining die sensor data from die sensors and image data from image sensors by using controllers (604). A command is generated (608) to perform machining tool operations by the controller based on the die sensor data and the image data. A machining tool is controlled (612) through the controller to perform the machining tool operation. A position of a robot arm is selectively adjusted (616) by using the controller based on the machining tool operation. An artificial intelligence system is trained (620). Die manufacturing process is performed (628) based on the machining tool operation, the position of the robot arm, the image data and the die sensor data, where the artificial intelligence system is a reinforcement learning network. USE - Method for automating die manufacturing process in a robot-assisted tool and a die building system for producing automotive body structural components. ADVANTAGE - The method enables utilizing an artificial intelligence system to automatically perform the die manufacturing process based on the machining tool operations, the position of the robot arm, the image data and the die sensor data, so that the machine learning model can be trained in an efficient manner, thus improving quality of the die. The method enables allowing a machine learning module to perform machine learning on the die to improve machine learning models and machine learning algorithms to improve quality of machine learning results. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for automating die manufacturing process. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for automating die manufacturing process. (Drawing includes non-English language text).600Automating die manufacturing process604Obtaining die sensor data from die sensors and image data from image sensors by using controllers608Generating command612Controlling machining tool through controller616Adjusting position of robot arm620Training artificial intelligence system624Judging whether artificial intelligence system is sufficiently trained628Performing die making process				[]	[]	-1	-1	-1
J	Liu, Derong; Javaherian, Hossein; Kovalenko, Olesia; Huang, Ting	Adaptive critic learning techniques for engine torque and air-fuel ratio control	2008	IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART B-CYBERNETICS		988	993	A new approach for engine calibration and control is proposed. In this paper, we present our research results on the implementation of adaptive critic designs for self-learning control of automotive engines. A class of adaptive critic designs that can be classified as'(model-free) action-dependent heuristic dynamic programming is used in this research project. The goals of the present learning control design for automotive engines include improved performance, reduced emissions, and maintained optimum performance under various operating conditions. Using the data from a test vehicle with a V8 engine, we developed a neural network model of the engine and neural network controllers based on the idea of approximate dynamic programming to achieve optimal control. We have developed and simulated self-learning neural network controllers for both engine torque (TRQ) and exhaust air-fuel ratio (AFR) control. The goal of TRQ control and AFR control is to track the commanded values. For both control problems; excellent neural network controller transient performance has been achieved.	Article	AUG 2008	10.1109/TSMCB.2008.922019	[]	[]	-1	-1	-1
P		Automotive hydraulic pressure brake gear										[]	[]	-1	-1	-1
C	Lakhmi, N.; Sahin, E.; Dallery, Y.	Proposition of a Framework for Classifying Returnable Transport Items Closed-Loop/Reverse Logistics Issues	2019	IFAC PAPERSONLINE		1955	1960	In this paper, we propose a conceptual framework for classifying operations management-related issues for returnable transport items (RTI). RTI closed-loop supply chain (CLSC) issues are classified into three categories. Our proposal is based on a global literature review in the field of CLSC and reverse logistics (RL), with a specific focus on transport items reuse. We also derive our work from observations made in the automotive industry. The motivation for dealing with this topic is that more and more companies are adopting RTIs in their supply chain, and despite the growing attention that CLSC and RL research have gained during the last years, the focus is still more oriented toward remanufacturing issues. The objectives of this paper are twofold. Firstly, to provide practitioners with a clear vision on different issues that arise when implementing a RTI system from an operations management point of view, and secondly, to give suggestions to researchers regarding existing gaps that still need to be filled in this field. (C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.	Proceedings Paper	2019	10.1016/j.ifacol.2019.11.489	[]	[]	-1	-1	-1
J	Kang, Yuhan; Wang, Haoxin; Kim, BaekGyu; Xie, Jiang; Zhang, Xiao-Ping; Han, Zhu	Time Efficient Offloading Optimization in Automotive Multi-Access Edge Computing Networks Using Mean-Field Games	2023	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		6460	6473	Emerging connected vehicular services, such as intelligent driving and high-definition (HD) map, are gaining increasing interest with the fast development of multi-access edge computing (MEC). For most time-sensitive and computation-intensive vehicular services, the data offloading process significantly influences the capacity and performance of MEC, especially when the number of connected vehicles is enormous. In this work, we consider data offloading optimization for a large-scale automotive MEC network. The problem is challenging due to the large number of connected vehicles and the complicated interaction between vehicles and edge servers. To tackle the scalability problem, we reformulate the original offloading optimization problem into a Mean-Field-Game (MFG) problem by abstracting the interaction among the connected vehicles as a distribution over their state spaces of task sizes, known as the mean-field term. To solve the problem efficiently, we propose a G-prox Primal-Dual-Hybrid-Gradient (PDHG) algorithm that transforms the MFG problem into a saddle-point problem. Based on our developed MFG model and G-prox PDHG algorithm, we propose the first data offloading scheme whose computation time is independent of the number of connected vehicles in automotive MEC systems. Extensive evaluation results corroborate the superior performance of our proposed scheme compared with the state-of-the-art methods.	Article	MAY 2023	10.1109/TVT.2022.3229888	[]	[]	-1	-1	-1
C	Li, Handong; Dai, Xuewu; Kotter, Richard; Aslam, Nauman; Cao, Yue	A Simulation Environment of Solar-Wind Powered Electric Vehicle Car Park for Reinforcement Learning and Optimization	2023	PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON NEW ENERGY AND ELECTRICAL TECHNOLOGY	Lecture Notes in Electrical Engineering	979	991	In accordance with the United Kingdom's goal to reach net zero by 2050, electric vehicles (EVs) play a crucial role in transportation. However, if the electricity used to charge EVs is derived from fossil fuels, this does not necessarily imply a reduction of overall emissions nationally or globally. To achieve optimal EV charging, a deeper comprehension of the unpredictability of on-site renewable energy sources (ORES) energy output is required. In this paper, the predicted renewable energy generated is used as the actual value for the reinforcement learning algorithm simulation environment. Such a model can represent the relationship between the power generation and the wind speed as well as solar irradiation, which are characterized by significant uncertainties. The uncertainty analysis shows that the wind speed at Newcastle upon Tyne can be modelled as a Weibull distribution with parameters A = 19.98 and B = 1.91. As for energy demand, this paper integrates information from an Oslo (Norway) car parking garage-based set of EV charging stations with EVs' demand statistics. The charging habits of EV users range from 800 min to 1,000 min of parking time, and from 5 kWh to 20 kWh in terms of charging energy. The maximum connection frequency for EV charging is 20 min. In addition, this paper develops methods for stochastic EV charging and parking space occupancy employing actual data. Based on the aforesaid renewable energy generation and the EV charging status, it is possible to develop a decision algorithm to optimal renewable energy efficiency.	Proceedings Paper	2023	10.1007/978-981-99-0553-9_102	[]	[]	-1	-1	-1
C	Pettersson, Lars; Cheng, Shi; Salter, Michael; Rydberg, Anders; Platt, Duncan	Compact Integrated Slot Array Antennas for the 79 GHz Automotive Band	2009	2009 EUROPEAN MICROWAVE CONFERENCE, VOLS 1-3		228	+	This paper presents two compact slot array antennas for the 79 GHz automotive band integrated into a 28 um thick benzocyclobutene (BCB) substrate attached to a 325 um thick 5 20 Omega cm bulk resistivity silicon wafer. The two antennas are a transmit (TX) 1x8 slot array antenna with a size of 1x23 mm and a receive (RX) M slot array antenna with a size of 15x23 mm. Promising performance has been measured with the TX and RX sub-array antennas (gain >4 dBi) with good matching to 50 11 (RL>10 dB) in the frequency range 70-90 GHz. By using a metal cavity, mounted on the back of the antenna, parallel plate modes could be reduced and the gain could also be increased by 2dB. The measured and the simulated results are consistent.	Proceedings Paper	2009		[]	[]	-1	-1	-1
B	Karunakaran, D.; Worrall, S.; Nebot, E.	Efficient statistical validation with edge cases to evaluate Highly Automated Vehicles	2020	2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)		8 pp.	8 pp.	The widescale deployment of Autonomous Vehicles (AV) seems to be imminent despite many safety challenges that are yet to be resolved. It is well known that there are no universally agreed Verification and Validation (VV) methodologies to guarantee absolute safety, which is crucial for the acceptance of this technology. Existing standards focus on deterministic processes where the validation requires only a set of test cases that cover the requirements. Modern autonomous vehicles will undoubtedly include machine learning and probabilistic techniques that require a much more comprehensive testing regime due to the non-deterministic nature of the operating design domain. A rigourous statistical validation process is an essential component required to address this challenge. Most research in this area focuses on evaluating system performance in large scale real-world data gathering exercises (number of miles travelled), or randomised test scenarios in simulation. This paper presents a new approach to compute the statistical characteristics of a systems behaviour by biasing automatically generated test cases towards the worst case scenarios, identifying potential unsafe edge cases. We use reinforcement learning (RL) to learn the behaviours of simulated actors that cause unsafe behaviour measured by the well established RSS safety metric. We demonstrate that by using the method we can more efficiently validate a system using a smaller number of test cases by focusing the simulation towards the worst case scenario, generating edge cases that correspond to unsafe situations.	Conference Paper	2020	10.1109/ITSC45102.2020.9294590	[]	[]	-1	-1	-1
J	Li, Weihan; Cui, Han; Nemeth, Thomas; Jansen, Jonathan; Uenluebayir, Cem; Wei, Zhongbao; Feng, Xuning; Han, Xuebing; Ouyang, Minggao; Dai, Haifeng; Wei, Xuezhe; Sauer, Dirk Uwe	Cloud-based health-conscious energy management of hybrid battery systems in electric vehicles with deep reinforcement learning	2021	APPLIED ENERGY				In order to fulfill the energy and power demand of battery electric vehicles, a hybrid battery system with a high-energy and a high-power battery pack can be implemented as the energy source. This paper explores a cloud-based multi-objective energy management strategy for the hybrid architecture with a deep deterministic policy gradient, which increases the electrical and thermal safety, and meanwhile minimizes the system's energy loss and aging cost. In order to simulate the electro-thermal dynamics and aging behaviors of the batteries, models are built for both high-energy and high-power cells based on the characterization and aging tests. A cloud-based training approach is proposed for energy management with real-world vehicle data collected from various road conditions. Results show the improvement of electrical and thermal safety, as well as the reduction of energy loss and aging cost of the whole system with the proposed strategy based on the collected real-world driving data. Furthermore, processor-in-the-loop tests verify that the proposed strategy can achieve a much higher convergence rate and a better performance in terms of the minimization of both energy loss and aging cost compared with state-of-the-art learning-based strategies.	Article; Early Access		10.1016/j.apenergy.2021.116977	[]	[]	-1	-1	-1
C	Fan, Yuxin; Huang, Jingxuan; Wang, Xinyi; Fei, Zesong	Resource Allocation for V2X Assisted Automotive Radar System based on Reinforcement Learning	2022	2022 14TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS AND SIGNAL PROCESSING, WCSP	International Conference on Wireless Communications and Signal Processing	672	676	Due to the complexity and variability of road traffic scenarios and the existence of blind spots in car radar detection, radar performance of single vehicle is limited. To address this issue, we propose a joint communication and radar sensing (JCR) system for Intelligent Connected Vehicles (ICVs), where communication is used to assist in reducing the miss detection probability. Based on this, we model the time resource allocation problem as a Markov Decision Process (MDP), and design the Q-learning and the Double Deep Q-learning Network (DDQN) algorithms to optimize the allocation of time resources for radar and communication functions dynamically. The simulation results show that compared with the Round-robin algorithm, the Q-learning and the DDQN algorithms can increase the communication data throughput by more than 6.6% and reduce the miss detection probability by more than 29.4%. The miss detection probability of the system using the assisted mode is 10.7%similar to 17.2% lower than that of the system without it.	Proceedings Paper	2022	10.1109/WCSP55476.2022.10039351	[]	[]	-1	-1	-1
P	VIDEIRA F; DESSI B	Optical device for motor vehicle, has translucent screen extending between input face and output face, where cross section of screen along privileged axis has dimension greater than another dimension orthogonal to former dimension						NOVELTY - The device has a translucent screen (EC) extending between an input face (FE) and an output face (FS). The screen has a privileged axis (AP) so that a beam parallel light illuminating the entrance face with an incidence along the privileged axis is refracted in a parallel beam propagating in the screen directly to the face of output. An optical reflector (RL) remote from the screen is positioned and arranged to reflect a light beam from a light source (SL) in the parallel beam. Each light source is offset from the axis, where a cross-section of the screen along the axis has a dimension. USE - Optical device (DO) for a motor vehicle (claimed) e.g. automotive type vehicle for performing a photometric function, such as a regulatory function of reversing and rear fog lights. ADVANTAGE - The optical reflector reduces the loss of light power by using an increased number of light sources or by using more powerful LEDs. The reflector increases the optical path, thus increasing the light energy absorbed by the flat guide. The internal reflections cause a significant diffusion of light through the exit face, since the light rays emitted by the exit face have multiple directions, resulting from internal reflections. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the DO for a motor vehicle.Privileged axis (AP)Translucent screen (EC)Input face (FE)Output face (FS)Optical reflector (RL)Light source (SL)				[]	[]	-1	-1	-1
P	PYE A	Electronic system for use in industrial            applications, has series resistor-inductor network            comprising resistor and inductor connected in series            between end of stripline and reference voltage, where            network controls characteristic of balun						NOVELTY - The system has a first stripline (104) including a first end configured to carry an unbalanced signal. A second stripline (106) is adjacent to the first stripline and includes the first end configured to receive reference voltage and a second end configured to carry a component of a balanced signal. A series resistor-inductor (RL) network comprises a resistor and an inductor (108) electrically connected in series between the second end of the first stripline and the reference voltage, where the series RL network controls a common-mode response characteristic of a coupled-line balun (200). USE - Electronic system for use in various applications e.g. industrial, medical and/and automotive applications. Uses include but are not limited to a smartphone, a laptop computer, a tablet computer, a wearable computing device, an automobile, a camcorder, a camera, a digital camera, a portable memory chip, a washer, and a dryer. ADVANTAGE - The system provides relatively high coupling to a substrate and/or to a backside, and to surrounding circuitry to reduce lower frequency performance by reducing common-mode rejection and degrading phase and amplitude balance. The coupled-line balun provides high common-mode rejection over as large a bandwidth as possible. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for transmitting a signal with a common-mode compensation(2) a coupled-line balun with common-mode compensation. DESCRIPTION Of DRAWING(S) - The drawing shows a circuit diagram of a coupled-line balun with common-mode nulling.Striplines (104, 106)Inductor (108)Compensating series resistor (110)Coupled-line balun (200)Signal source (202)Source impedance (204)				[]	[]	-1	-1	-1
B	Xiaohan Zhang; Yong Xiao; Qiang Li; Saad, W.	Deep Reinforcement Learning for Fog Computing-based Vehicular System with Multi-operator Support	2020	ICC 2020 - 2020 IEEE International Conference on Communications (ICC). Proceedings		6 pp.	6 pp.	This paper studies the potential performance improvement that can be achieved by enabling multi-operator wireless connectivity for cloud/fog computing-connected vehicular systems. Mobile network operator (MNO) selection and switching problem is formulated by jointly considering switching cost, quality-of-service (QoS) variations between MNOs, and the different prices that can be charged by different MNOs as well as cloud and fog servers. A double deep Q network (DQN) based switching policy is proposed and proved to be able to minimize the long-term average cost of each vehicle with guaranteed latency and reliability performance. The performance of the proposed approach is evaluated using the dataset collected in a commercially available city-wide LTE network. Simulation results show that our proposed policy can significantly reduce the cost paid by each fog/cloud-connected vehicle with guaranteed latency services.	Conference Paper	2020	10.1109/ICC40277.2020.9148778	[]	[]	-1	-1	-1
C	Markolf, Lukas; Eilbrecht, Jan; Stursberg, Olaf	Trajectory Planning for Autonomous Vehicles combining Nonlinear Optimal Control and Supervised Learning	2020	IFAC PAPERSONLINE		15608	15614	This paper considers computationally efficient planning of reference trajectories for autonomous on-road vehicles in a cooperative setting. The basic element of the approach is the notion of so-called maneuvers, which allow to cast the nonlinear and non-convex planning task into a highly structured optimal control problem. This can be solved quite efficiently, but not fast enough for online operation when considering nonlinear vehicle models. Therefore, the approach proposed in this paper aims at approximating solutions using a supervised learning approach: First, training data are generated by solving optimal control problems and are then used to train a neural network. As is demonstrated for a cooperative overtaking maneuver, this approach shows good performance, while (contrasting approaches like reinforcement learning) requiring only low training effort. Copyright (C) 2020 The Authors.	Proceedings Paper	2020	10.1016/j.ifacol.2020.12.2495	[]	[]	-1	-1	-1
J	Altenburg, S.; Schuchter, F.; Bause, K.; Albers, A.	Use of AI methods for clutch development in automotive drivetrains	2023	FORSCHUNG IM INGENIEURWESEN-ENGINEERING RESEARCH		571	579	Current development methods for modeling and optimization for vehicle clutches reach their limits in the field of rising expectations of ride comfort and energy efficiency. This article examines the use of AI methods for clutch development and provides an overview based on various application examples in current Mercedes-Benz AG research projects. By means of supervised learning and deep neural networks, a friction coefficient model and a temperature model of a clutch with high accuracy are developed. Reinforcement learning with deep neural networks is used to synthesize controllers for various gear changes. Vehicle measurement data is analyzed using cluster algorithms to derive action recommendations for the application of the engine restart of a hybrid drivetrain. The methods shown increase the automation potential in development and may reduce the effort required to adopt complex development processes for new transmission variants and generations.	Article; Early Access		10.1007/s10010-023-00665-8	[]	[]	-1	-1	-1
J	Pettersson, Lars; Cheng, Shi; Salter, Michael; Rydberg, Anders; Platt, Duncan	Compact integrated slot array antennas for the 79 GHz automotive band	2010	INTERNATIONAL JOURNAL OF MICROWAVE AND WIRELESS TECHNOLOGIES		305	316	This paper presents two compact slot array antennas for the 79 GHz automotive band integrated into a 28 mu m thick benzocyclobutene (BCB) substrate attached to a 325 mu m thick 5-20 Omega cm bulk resistivity silicon wafer. The two antennas are a transmit (TX) 1 x 8 slot array antenna with a size of 1 x 23 mm and a receive (RX) 8 x 8 slot array antenna with a size of 15 x 23 mm. Promising performance has been measured with the TX and RX subarray antennas (gain >4 dBi) with good matching to 50 Omega (RL > 10 dB) in the frequency range 70-90 GHz. By using a metal cavity, mounted on the back of the antenna, potential parallel plate or cavity modes could be reduced and the gain could also be increased by 2 dB. The measured and the simulated results are consistent.	Article	AUG 2010	10.1017/S1759078710000486	[]	[]	-1	-1	-1
C	Hofstetter, Johannes; Bauer, Hans; Li, Wenbin; Wachtmeister, Georg	Energy and Emission Management of Hybrid Electric Vehicles using Reinforcement Learning	2019	IFAC PAPERSONLINE		19	24	The electrification of drivetrains of conventional vehicles plays a decisive role in reducing fuel consumption. At the same time decreasing pollutant emission limits must be met also under real driving conditions. This trade-off between fuel consumption and pollutant emissions needs to be optimized, which results in powertrains with increasing complexity. A holistic energy and emission management is needed to control such systems in a way that the fuel consumption is minimized while emission limits are respected.Mathematical optimization methods are difficult to apply in real-time applications due to high computational and calibration demands. Self-learning algorithms, on the other hand, seem to be a suitable solution for such optimization problems.In this paper a control strategy for a hybrid electrical vehicle is presented, consisting of a decision-making agent, trained on different test drives with Reinforcement Learning. For these, the Proximal Policy Optimization method was applied. The strategy controls the torque-split between the combustion engine and electric motor, the power of an electrically heated catalyst and internal engine measures. The method is demonstrated in a simulation framework based on a Diesel P0-HEV with a SCR exhaust gas aftertreatment system. In comparison to a reference strategy a fuel reduction of 3.1 % averaged over the test data set was achieved. (C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.	Proceedings Paper	2019	10.1016/j.ifacol.2019.12.615	[]	[]	-1	-1	-1
C	Tosic, Predrag T.; Vilalta, Ricardo	Learning and Meta-Learning for Coordination of Autonomous Unmanned Vehicles A Preliminary Analysis	2010	ECAI 2010 - 19TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE	Frontiers in Artificial Intelligence and Applications	163	168	"We study models of coordination, negotiation and collaboration in multi-agent systems (MAS). More specifically, we investigate scalable models and protocols for various distributed consensus coordination problems in large-scale MAS. Examples of such problems include conflict avoidance, leader election and coalition formation. We are particularly interested in application domains where robotic or unmanned vehicle agents interact with each other in real-time, as they try to jointly complete various tasks in complex dynamic environments, and where decisions often need to be made ""on the fly"". Such MAS applications, we argue, necessitate a multi-tiered approach to learning how to coordinate effectively. One such collaborative MAS application domain is ensembles of autonomous micro unmanned aerial vehicles (micro-UAVs). A large ensemble of micro-UAVs on a complex, multi-stage mission comprised of many diverse tasks with varying time and other resource requirements provides an excellent framework for studying multi-tiered learning how to better coordinate. A variety of tasks and their resource demands, complexity and unpredictability of the overall environment, types of coordination problems that the UAVs may encounter in the course of their mission, multiple time scales at which the overall system can use learning and adaptation in order to perform better in the future, and multiple logical and organizational levels at which large ensembles of micro-UAVs can be analyzed and optimized, all suggest the need for a multi-tiered approach to learning. We outline our theoretical and conceptual framework that integrates reinforcement learning and meta-learning, and discuss potential benefits that our framework could provide for enabling autonomous micro-UAVs (and other types of autonomous vehicles) to coordinate more effectively."	Proceedings Paper	2010	10.3233/978-1-60750-606-5-163	[]	[]	-1	-1	-1
J	Xinyu Huang; Lijun He; Wanyue Zhang	Vehicle Speed Aware Computing Task Offloading and Resource Allocation Based on Multi-Agent Reinforcement Learning in a Vehicular Edge Computing Network [arXiv]	2020	arXiv		8 pp.	8 pp.	For in-vehicle application, the vehicles with different speeds have different delay requirements. However, vehicle speeds have not been extensively explored, which may cause mismatching between vehicle speed and its allocated computation and wireless resource. In this paper, we propose a vehicle speed aware task offloading and resource allocation strategy, to decrease the energy cost of executing tasks without exceeding the delay constraint. First, we establish the vehicle speed aware delay constraint model based on different speeds and task types. Then, the delay and energy cost of task execution in VEC server and local terminal are calculated. Next, we formulate a joint optimization of task offloading and resource allocation to minimize vehicles' energy cost subject to delay constraints. MADDPG method is employed to obtain offloading and resource allocation strategy. Simulation results show that our algorithm can achieve superior performance on energy cost and task completion delay.	Journal Paper	14 Aug. 2020		[]	[]	-1	-1	-1
J	Sun, Haochen; Tao, Fazhan; Fu, Zhumu; Gao, Aiyun; Jiao, Longyin	Driving-Behavior-Aware Optimal Energy Management Strategy for Multi-Source Fuel Cell Hybrid Electric Vehicles Based on Adaptive Soft Deep-Reinforcement Learning	2023	IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS		4127	4146	The majority of existing energy management strategies (EMSs), merely considering external driving conditions, often allocate demand power in an irrational way, resulting in a waste of energy and a short service life of power sources. Therefore, it is necessary to integrate driving behavior in EMS to reduce the fuel consumption and improve the lifespan of power sources. In this paper, a driving-behavior-aware adaptive deep-reinforcement-learning (DRL) based EMS is proposed for a three-power-source fuel cell hybrid electric vehicle (FCHEV). To fully utilize each power source, a hierarchical power splitting method is adopted by an adaptive fuzzy filter. Then, a high-performance driving behavior recognizer is employed, and Pontryagin's minimum principle (PMP) method is used to compute the optimal equivalent factor (EF) of each driving behavior. To realize a trade-off between global learning and real-time implementation, an improved multi-learning-space DRL-based algorithm, applying driving-behavior-aware adaptive equivalent consumption minimization strategy (A-ECMS) and soft learning mechanism, is proposed and verified by a series of simulations. Simulation results show that, compared with the benchmark method ECMS, the proposed P-DQL method can reduce the hydrogen consumption by 49.9% on average, and the total cost to use by 31.4% , showing a promising ability to increase fuel economy and reduce hydrogen consumption and the total cost to use of FCHEV.	Article; Early Access		10.1109/TITS.2022.3233564	[]	[]	-1	-1	-1
J	Campi, Tommaso; Cruciani, Silvano; Maradei, Francesca; Feliziani, Mauro	Magnetic Field during Wireless Charging in an Electric Vehicle According to Standard SAE J2954	2019	ENERGIES				The Society of Automotive Engineers (SAE) Recommended Practice (RP) J2954 (November 2017) was recently published to standardize the wireless power transfer (WPT) technology to recharge the battery of an electric vehicle (EV). The SAE J2954 RP establishes criteria for interoperability, electromagnetic compatibility (EMC), electromagnetic field (EMF) safety, etc. The aim of this study was to predict the magnetic field behavior inside and outside an EV during wireless charging using the design criteria of SAE RP J2954. Analyzing the worst case configurations of WPT coils and EV bodyshell by a sophisticated software tool based on the finite element method (FEM) that takes into account the field reflection and refraction of the metal EV bodyshell, it is possible to numerically assess the magnetic field levels in the environment. The investigation was performed considering the worst case configurationa small city car with a Class 2 WPT system of 7.7 kVA with WPT coils with maximum admissible ground clearance and offset. The results showed that the reference level (RL) of the International Commission on Non-Ionizing Radiation Protection (ICNIRP) guidelines in terms of magnetic flux density was exceeded under and beside the EV. To mitigate the magnetic field, the currents flowing through the WPT coils were varied using the inductor-capacitor-capacitor (LCC) compensation instead of the traditional series-series (SS) compensation. The corresponding calculated field was compliant with the 2010 ICNIRP RL and presented a limited exceedance of the 1998 ICNIRP RL. Finally, the influence of the body width on the magnetic field behavior adopting maximum offset was investigated, demonstrating that the magnetic field emission in the environment increased as the ground clearance increased and as the body width decreased.	Article	MAY 1 2019	10.3390/en12091795	[]	[]	-1	-1	-1
J	Hu, Fenghe	Artificial Intelligence Supported Cellular-Connected Extended Reality	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
P	RAMIREDDY V; GROSSMANN M; LANDMANN M; RAMIREDIPENCATESH; MARKUS G; MARCUS L	Method for generating and reporting channel state            information (CSI) report in wireless communications            system, involves transmitting uplink control            information (UCI) including CSI report over uplink (UL)            channel to network node						NOVELTY - The method involves grouping (403) the precoder coefficients of the Rl transmission layers into two coefficient subsets, each coefficient subset comprising a precoder coefficients. An ordering is assigned (404) to coefficient subsets and an ordering to the precoder coefficients within each coefficient subset. The coefficient subsets are divided (405) into CSI groups having associated priority levels. A CSI report is generated (406) which comprising a CSI part 1 and a CSI part 2. The CSI part 1 has a fixed payload size and comprises information indicating the size of the payload of the CSI part 2, where the CSI part 2 includes the precoder coefficients of the two CSI groups. An uplink control information (UCI) including the CSI report over an uplink UL channel transmitted (407) to the network node. USE - Method for generating and reporting CSI report in wireless communication system by UE. ADVANTAGE - The method enables providing a higher level of performance than previous generations of mobile communication system, and providing ubiquitous connectivity for applications as diverse automotive communication, remote control with feedback, video downloads, as well as data applications for Internet-of-Things (loT) devices and machine type communication (MTC) devices. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a method performed by a network node for receiving a CSI report generated by a UE;an user equipment; anda network node. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating method for generating and reporting CSI report in wireless communication system.403Step for grouping the precoder coefficients of the Rl transmission layers into two coefficient subsets404Step for assigning an ordering to coefficient subsets405Step for dividing the coefficient subsets into CSI groups having associated priority levels406Step for generating a CSI report407Step for transmitting an UCI including the CSI report over an uplink UL channel to the network node				[]	[]	-1	-1	-1
J	Zhang, Wei; Wang, Jixin; Xu, Zhenyu; Shen, Yuying; Gao, Guangzong	A generalized energy management framework for hybrid construction vehicles via model-based reinforcement learning	2022	ENERGY				Hybrid construction vehicles (HCVs) have more specific tasks and highly repetitive patterns than on-road ve-hicles. Consequently, they are more suitable for model-based energy management. However, distinctions be-tween work cycles result in adverse scenarios for generalizing model-based energy management. In this study, we solve this problem by proposing a generalized strategy using a model-based reinforcement learning framework. The generalized design highlights three aspects: 1) long-term stability, 2) self-learning ability, and 3) state transition model reuse. A reward function with a trend term is proposed to avoid the cumulative errors between operation cycles and improve the long-term stability of learning. In addition, Gaussian process regression is leveraged to approximate the value function, thereby reducing the computational load and improving the learning efficiency. To further enhance the reusability of the environmental model, a modelling method based on the Gaussian mixture model is put forward. Finally, a generalized HCV energy management framework that includes offline and online learning is designed, where a pre-learning model and an approximation function are adopted for reuse and dynamic learning. Simulation results demonstrate the superiority of the proposed framework to conventional model-based methods in terms of stability, generality, and adaptability, accompanied by a reduction of 5.9% in fuel consumption.	Article; Early Access		10.1016/j.energy.2022.124849	[]	[]	-1	-1	-1
P	CIGNOLI M	Driver circuit for driving resistor-inductor load            in automotive systems, has first comparator for            comparing voltage across load path terminals of            high-side power switch with two thresholds, and second            comparator coupled between control terminal of low-side            power switch and combinational logic						NOVELTY - The circuit (100) has a first combinational logic (105) coupled between an input terminal (101) of a high-side driver and an input terminal of a circuit unit, where the first combinational logic controls operation of a high-side power switch (127). A second combinational logic (107) is coupled between the input terminal of a low-side driver and the input terminal of the circuit unit. A first comparator (119A) compares a first voltage across load path terminals of the high-side power switch with a first threshold and a second threshold. A second comparator (123A) is coupled between the control terminal of a low-side power switch (129) and the second combinational logic, where the second comparators compare a voltage at the control terminal of the low-side power switch with a threshold voltage of the low-side power switch and a full-on voltage of the low-side power switch. USE - Driver circuit e.g. electrical circuit for driving a resistor-inductor (RL) load in automotive systems. Uses include but are not limited to vehicle transmissions, vehicle breaking systems, fuel injection systems, and combustion engine valve controls. ADVANTAGE - The circuit provides the high-side power switch and the low-side driver that are coupled in series with each other, so that the driver circuit can be operated in efficient manner. The circuit designs the large pull-up current to reduce the dead time, and ensures that the channel region of the power switch can be rapidly formed. The circuit allows for precise control of the current slew rate and the voltage slew rate during the on/off transition phase of the power switches. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:(1) an integrated circuit;(2) a method for operating a driver circuit for driving a resistor-inductor load. DESCRIPTION Of DRAWING(S) - The drawing shows a circuit diagram of a driver circuit for driving a resistor-inductor load in automotive systems.100Driver circuit101Input terminal105First combinational logic107Second combinational logic119AFirst comparator123ASecond comparator127High side power switch129Low side power switch				[]	[]	-1	-1	-1
B	Vicencio, D.; Silva, H.; Soares, S.; Filipe, V.; Valente, A.	An Intelligent Predictive Maintenance Approach Based on End-of-Line Test Logfiles in the Automotive Industry	2021	Industrial IoT Technologies and Applications. 4th EAI International Conference, Industrial IoT 2020. Proceedings. Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering (LNICST 365)		121	40	Through technological advents from Industry 4.0 and the Internet of Things, as well as new Big Data solutions, predictive maintenance begins to play a strategic role in the increasing operational performance of any industrial facility. Equipment failures can be very costly and have catastrophic consequences. In its basic concept, Predictive maintenance allows minimizing equipment faults or service disruptions, presenting promising cost savings. This paper presents a data-driven approach, based on multiple-instance learning, to predict malfunctions in End-of-Line Testing Systems through the extraction of operational logs, which, while not designed to predict failures, contains valid information regarding their operational mode over time. For the case study performed, a real-life dataset was used containing thousands of log messages, collected in a real automotive industry environment. The insights gained from mining this type of data will be shared in this paper, highlighting the main challenges and benefits, as well as good recommendations, and best practices for the appropriate usage of machine learning techniques and analytics tools that can be implemented in similar industrial environments.	Conference Paper	2021	10.1007/978-3-030-71061-3_8	[]	[]	-1	-1	-1
P	NAIK V B; TOH E H; QUEK K B E	High sensing margin resistive memory device for            electronic products, has first selector coupled to word            line for coupling write path to storage unit, and            second selector coupled to read line for coupling read            path to storage unit						NOVELTY - The device has a memory cell (300) comprising a storage unit (310) coupled to a cell selector unit (340). The storage unit includes first and second storage elements (3101, 3102). The cell selector unit comprises a first selector (341) with a write select transistor (TW) and a second selector (351) with a first read transistor (TR1) and a second read transistor (TR2). The first selector is coupled to a word line (WL) for selectively coupling a write path to the storage unit. The second selector is coupled to a read line (RL) for selectively coupling a read path to the storage unit. USE - High sensing margin resistive memory device for electronic products. Uses include but are not limited to mobile phones, smart card, mass storage, enterprise storage and industrial and automotive products. ADVANTAGE - The device improves READ and WRITE distributions for high speed and low power applications. The device enables separate tuning of transistors, thus improving high speed and high reliable operations for a magnetoresistive (MRAM) device. The device avoids need of additional power to switch the two storage elements using two separate current pulses so as to reduce power consumption. DETAILED DESCRIPTION - The memory cell is a spin torque transfer-magnetic RAM (STT-MRAM) cell. The first and second storage elements are first and second magnetic tunnel junction elements. INDEPENDENT CLAIMS are also included for the following:(1) a method for operating a memory device(2) a method for forming a memory device. DESCRIPTION Of DRAWING(S) - The drawing shows a circuit view of a memory cell.Read line (RL)Read transistors (TR1, TR2)Write select transistor (TW)Word line (WL)Memory cell (300)Storage unit (310)Storage elements (3101, 3102)Cell selector unit (340)Selectors (341, 351)				[]	[]	-1	-1	-1
B	Fragkos, Georgios	Autonomous Decision-Making in Interdependent Computing Systems Based on Artificial Intelligence	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Ulmschneider, K.; Glimm, B.	Semantic exploitation of implicit patent information	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)		8 pp.	8 pp.	In recent years patents have become increasingly important for businesses to protect their intellectual capital and as a valuable source of information. Patent information is, however, not employed to its full potential and the interpretation of structured and unstructured patent information in large volumes remains a challenge. We address this by proposing an integrated interdisciplinary approach that uses natural language processing and machine learning techniques to formalize multilingual patent information in an ontology. The ontology further contains patent and domain specific knowledge, which allows for aligning patents with technological fields of interest and other business-related artifacts. Our empirical evaluation shows that for categorizing patents according to well-known technological fields of interest, the approach achieves high accuracy with selected feature sets compared to related work focussing on monolingual patents. We further show that combining OWL RL reasoning with SPARQL querying over the patent knowledge base allows for answering complex business queries and illustrate this with real-world use cases from the automotive domain.	Conference Paper	2016	10.1109/SSCI.2016.7849943	[]	[]	-1	-1	-1
J	Mora, C.; Cascini, A.; Gamberi, M.; Regattieri, A.; Bortolini, M.	A planning model for the optimisation of the end-of-life vehicles recovery network	2014	International Journal of Logistics Systems and Management		449	72	European Directive 2000/53 on end-of-life vehicles (ELVs) introduced new constraints, regulations and recommendations on the vehicle design, waste disposal and take-back policy. Automotive original equipment manufacturers (OEMs) are even more involved in the expected vehicle recovery network modifications. As a consequence, one of the most critical and challenging issues for reverse logistics (RL) management is the development of effective tools to support strategic, tactical and operational decisions to yield the maximum economic benefit in compliance with the reference regulations. In this paper, a mixed integer linear programming (MILP) model for ELV closed-loop network design is proposed. The network cost minimisation objective function, the integration of forward and reverse logistics and the inclusion of remanufacturing activities for vehicle module reuse represent the model key features. The proposed model is, further, applied on a realistic Italian case-study and the results of a sensitivity analysis are presented to identify the parameters most affecting the model outcomes.	Journal Paper	2014	10.1504/IJLSM.2014.063980	[]	[]	-1	-1	-1
C	Howard, William W.; Thornton, Charles E.; Martone, Anthony F.; Buehrer, R. Michael	Multi-player Bandits for Distributed Cognitive Radar	2021	2021 IEEE RADAR CONFERENCE (RADARCONF21): RADAR ON THE MOVE	IEEE Radar Conference			With new applications for radar networks such as automotive control and indoor localization, the need for spectrum sharing and general interoperability is expected to rise. This paper describes the application of multi-player bandit algorithms for waveform selection to a distributed cognitive radar network that must coexist with a communications system. Specifically, we make the assumption that radar nodes in the network have no dedicated communication channel. As we will discuss later, nodes can communicate indirectly by taking actions which intentionally interfere with other nodes and observing the resulting collisions. The radar nodes attempt to optimize their own spectrum utilization while avoiding collisions, not only with each other, but with the communications system. The communications system is assumed to statically occupy some subset of the bands available to the radar network. First, we examine models that assume each node experiences equivalent channel conditions, and later examine a model that relaxes this assumption.	Proceedings Paper	2021	10.1109/RadarConf2147009.2021.9455306	[]	[]	-1	-1	-1
J	He, Xiangkun; Lv, Chen	Toward Intelligent Connected E-Mobility: Energy-Aware Cooperative Driving With Deep Multiagent Reinforcement Learning	2023	IEEE VEHICULAR TECHNOLOGY MAGAZINE		101	109	In recent years, electrified mobility (e-mobility), especially connected and autonomous electric vehicles (CAEVs), has been gaining momentum along with the rapid development of emerging technologies such as artificial intelligence (AI) and Internet of Things. The social benefits of CAEVs are manifested in the form of safer transportation, lower energy consumption, and reduced congestion and emissions. Nevertheless, it is highly difficult to design driving policies that ensure road safety, travel efficiency, and energy conservation for all CAEVs in traffic flows, particularly in a mixed-autonomy scenario where both CAEVs and human-driven vehicles (HDVs) are on the road and interact with each other. Here we present a novel deep multiagent reinforcement learning (DMARL)-enabled energy-aware cooperative driving solution, facilitating CAEVs to learn vehicular platoon management policies for guaranteeing overall traffic flow performance. Specifically, with the aid of information communication technology (ICT), CAEVs can share their vehicle state and learned knowledge, such as their state of charge (SoC), speed, and driving policies. Additionally, a cooperative multiagent actor-critic (CMAAC) technique is developed to optimize vehicular platoon management policies that map perceptual information directly to the group decision-making behaviors for the CAEV platoon. The proposed approach is evaluated in highway on-ramp merging scenarios with two different mixed-autonomy traffic flows. The results demonstrate the benefits of our scheme. Finally, we discuss the challenges and potential research directions for the proposed energy-aware cooperative driving solution.	Article; Early Access		10.1109/MVT.2023.3291171	[]	[]	-1	-1	-1
J	Wang, Kunyu; Yang, Rong; Zhou, Yongjian; Huang, Wei; Zhang, Song	Design and Improvement of SD3-Based Energy Management Strategy for a Hybrid Electric Urban Bus	2022	ENERGIES				With the rapid development of machine learning, deep reinforcement learning (DRL) algorithms have recently been widely used for energy management in hybrid electric urban buses (HEUBs). However, the current DRL-based strategies suffer from insufficient constraint capability, slow learning speed, and unstable convergence. In this study, a state-of-the-art continuous control DRL algorithm, softmax deep double deterministic policy gradients (SD3), is used to develop the energy management system of a power-split HEUB. In particular, an action masking (AM) technique that does not alter the SD3 ' s underlying principles is proposed to prevent the SD3-based strategy from outputting invalid actions that violate the system's physical constraints. Additionally, the transfer learning (TL) method of the SD3-based strategy is explored to avoid repetitive training of neural networks in different driving cycles. The results demonstrate that the learning performance and learning stability of SD3 are unaffected by AM and that SD3 with AM achieves control performance that is highly comparable to dynamic planning for both the CHTC-B and WVUCITY driving cycles. Aside from that, TL contributes to the rapid development of SD3. TL can speed up SD3's convergence by at least 67.61% without significantly affecting fuel economy.	Article	AUG 2022	10.3390/en15165878	[]	[]	-1	-1	-1
J	Moghaddam, Valeh; Yazdani, Amirmehdi; Wang, Hai; Parlevliet, David; Shahnia, Farhad	An Online Reinforcement Learning Approach for Dynamic Pricing of Electric Vehicle Charging Stations	2020	IEEE ACCESS		130305	130313	The global market share of electric vehicles (EVs) is on the rise, resulting in a rapid increase in their charging demand in both spatial and temporal domains. A remedy to shift the extra charging loads at peak hours to off-peak hours, caused by charging EVs at public charging stations, is an online pricing strategy. This paper presents a novel combinatorial online pricing strategy that has been established upon a reward-based model to prevent network instability and power outages. In the proposed solution, the utility provides incentives to the charging stations for their contributions in the EVs charging load shifting. Then, a constraint optimization problem is developed to minimize the total charging demand of the EVs during peak hours. To control the EVs charging demands in supporting utility's stability and increasing the total revenue of the charging stations, treated as a multi-agent framework, an online reinforcement learning model is developed which is based on the combination of an adaptive heuristic critic and recursive least square algorithm. The effective performance of the proposed model is validated through extensive simulation studies such as qualitative, numerical, and robustness performance assessment tests. The simulation results indicate significant improvement in the robustness and effectiveness of the proposed solution in terms of utility's power saving and charging stations' profit.	Article	2020	10.1109/ACCESS.2020.3009419	[]	[]	-1	-1	-1
J	Devo, Alessandro; Dionigi, Alberto; Costante, Gabriele	Enhancing continuous control of mobile robots for end-to-end visual active tracking	2021	ROBOTICS AND AUTONOMOUS SYSTEMS				In the last decades, visual target tracking has been one of the primary research interests of the Robotics research community. The recent advances in Deep Learning technologies have made the exploitation of visual tracking approaches effective and possible in a wide variety of applications, ranging from automotive to surveillance and human assistance. However, the majority of the existing works focus exclusively on passive visual tracking, i.e., tracking elements in sequences of images by assuming that no actions can be taken to adapt the camera position to the motion of the tracked entity. On the contrary, in this work, we address visual active tracking, in which the tracker has to actively search for and track a specified target. Current State-of-the-Art approaches use Deep Reinforcement Learning (DRL) techniques to address the problem in an end-to-end manner. However, two main problems arise: (i) most of the contributions focus only on discrete action spaces, and the ones that consider continuous control do not achieve the same level of performance; and (ii) if not properly tuned, DRL models can be challenging to train, resulting in considerably slow learning progress and poor final performance. To address these challenges, we propose a novel DRL-based visual active tracking system that provides continuous action policies. To accelerate training and improve the overall performance, we introduce additional objective functions and a Heuristic Trajectory Generator (HTG) to facilitate learning. Through extensive experimentation, we show that our method can reach and surpass other State-of-the-Art approaches performances, and demonstrate that, even if trained exclusively in simulation, it can successfully perform visual active tracking even in real scenarios. (C) 2021 Elsevier B.V. All rights reserved.	Article; Early Access		10.1016/j.robot.2021.103799	[]	[]	-1	-1	-1
B	Kristjansen, M.; Kulkarni, A.; Jensen, P.G.; Teodorescu, R.; Larsen, K.G.	Dual Balancing of SoC/SoT in Smart Batteries Using Reinforcement Learning in Uppaal Stratego	2023	IECON 2023- 49th Annual Conference of the IEEE Industrial Electronics Society		1	6	Battery packs in electric vehicles are managed by battery management systems that influence the state of charge among the cells in the pack, where such systems have received much attention in research. More recently, balancing the temperature among the cells has become a research topic. In our work, we consider a dual-balancing problem where we aim to balance both the parameters of the state of charge and temperature. We consider a Smart Battery Pack, where individual cells can be bypassed, meaning that no current is going to or from the cell, which allows the cell to cool off while the cell does not charge or discharge. Moreover, a smart battery pack can estimate each cell's characteristics, which, in turn, can be used to define a model of cell and battery pack behavior. We conduct experiments using the model of a battery pack where each cell differs in its configuration as an effect of aging. For such a pack with heterogeneous cells, we use Q- Learning in U ppaal Stratego to synthesize a controller that maximizes the time spent in a balanced state, meaning that all cells' states are within a specific range of each other. We show significant improvements in two aspects compared with two threshold-based controllers that balance either state of charge or temperature. The synthesized controllers are only unbalanced with the state of charge between 1-4% of the time and for temperature between 15-20% of the time. The threshold-based controllers are either unbalanced for the state of charge for as much as 37 % of the time or for temperature for as much as 44 % of the time. Finally, the maximum variations of state of charge and temperature among the cells are decreased.	Conference Paper	2023	10.1109/IECON51785.2023.10311828	[]	[]	-1	-1	-1
B	Wete, E.; Greenyer, J.; Wortmann, A.; Kudenko, D.; Nejdl, W.	MDE and Learning for flexible Planning and optimized Execution of Multi-Robot Choreographies	2023	2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)		1	4	Multi-Robot systems in automotive are safety-critical systems that consist of collaborating-aware robots and components that interact with external components, the environment, or humans at run-time. This implies a significant complexity for the system engineer to design, model, validate the system, and optimize the cycle time, including considering unexpected events at run-time. This paper addresses this challenge by describing a model-driven engineering approach that formally designs the system under the consideration of uncertainties and at run-time optimizes the system actions using learning-based approaches. We implemented this approach in an industrial-inspired case study of a spot-welding multi-robot cell. Based on the system requirements, we generate valid system strategies that consider unexpected events such as robot interruptions and failures. Considering movement and interruption time models, we implemented a reinforcement learning method to optimize system actions at run-time. We show that via simulations and learning, our approach can be used to synthesize time-efficient schedules for robot task assignments that improve the overall cycle time.	Conference Paper	2023	10.1109/ETFA54631.2023.10275559	[]	[]	-1	-1	-1
C	Blau, Benjamin S.; Hildenbrand, Tobias; Armbruster, Matthias; Fassunge, Martin G.; Xu, Yongchun; Knapper, Rico	INCENTIVES AND PERFORMANCE IN LARGE-SCALE LEAN SOFTWARE DEVELOPMENT <i>An Agent</i>-<i>based Simulation Approach</i>	2011	ENASE 2011: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON EVALUATION OF NOVEL APPROACHES TO SOFTWARE ENGINEERING		26	37	The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade. However, a simple transfer of good practices from the automotive industry combined with experiences from agile development on a team level is not possible due to fundamental differences stemming from the particular domain specifics - i.e. different types of products and components (material versus immaterial goods), knowledge work versus production systems as well as established business models. Especially team empowerment and the absence of a a hierarchical control on all levels impacts goal orientation and business optimization. In such settings, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity of central importance.Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes. We (ii) compare different incentive schemes ranging from individual to team-based compensation. Based on our results, we are (iii) able to provide recommendations on how to design such incentives, what their effect is, and how to chose an adequate development structure to foster overall software product development flow by means of more economic decisions and thus resulting in a shorter time to market. For calibrating our simulation, we rely on practical experience from a very large software company piloting and implementing lean and agile for about three years.	Proceedings Paper	2011		[]	[]	-1	-1	-1
J	He, Hongwen; Wang, Yunlong; Li, Jianwei; Dou, Jingwei; Lian, Renzong; Li, Yuecheng	An Improved Energy Management Strategy for Hybrid Electric Vehicles Integrating Multistates of Vehicle-Traffic Information	2021	IEEE TRANSACTIONS ON TRANSPORTATION ELECTRIFICATION		1161	1172	This study aims to answer the key question for hybrid electric vehicles (HEVs) on how to manage the power flow in HEVs with recent intelligent and electrified upgrades in automotive industries. The new energy management strategy (EMS) needs to fuse both the physic and cyber systems, reflecting the dynamic vehicle system in the physical layer, as well as taking full advantage of the outside information in the cyber layer. Given that, this article proposes the cyber-physical system (CPS)-based EMS using deep reinforcement learning (DRL) in two different types of vehicles [hybrid electric bus (HEB) and Prius]. Under the proposed framework, exploratory training is carried out for the EMS which is mediated by DRL algorithms, expert prior knowledge and multistate of traffic information. Then, the prior valid knowledge trained by HEB is applied to Prius based on deep transfer learning, accelerating the new EMS convergence and ensuring the same initial parameters of the two vehicles' deep neural networks. Moreover, the cyber information is decoupled from the vehicle itself, for the first time being visualized for comparative analysis. The results show a significant improvement by considering traffic states (TS) and using dynamic programming (DP) as a benchmark with 6.94% fuel economy improvement for deep deterministic policy gradients (DDPG) test results and 8.12% for deep Q-learning (DQL), respectively. The decoupling analysis distinguished the effect of various TS for the HEB and Prius due to their different characteristics in vehicle service, driving style, and vehicle structures, which further verifies the effectiveness of the proposed EMS.	Article	SEP 2021	10.1109/TTE.2021.3054896	[]	[]	-1	-1	-1
J	Zhang, Zhe; Ding, Haitao; Guo, Konghui; Zhang, Niaona	Eco-Driving Cruise Control for 4WIMD-EVs Based on Receding Horizon Reinforcement Learning	2023	ELECTRONICS				Aiming to improve the distance per charge of four in-wheel independent motor-drive electric vehicles in intelligent transportation systems, a hierarchical energy management strategy that weighs their computational efficiency and optimization performance is proposed. According to the information of an intelligent transportation system, a method combining reinforcement learning with receding horizon optimization is proposed at the upper level, which solves the cruising velocity for eco-driving in a long predictive horizon based on the online construction of a velocity planning problem. At the lower level, a multi-objective optimal torque allocation method that considers energy saving and safety is proposed, where an analytical solution based on the state feedback control was obtained with the vehicle following the optimal speed of the upper level and tracking the centerline of the target path. The energy management strategy proposed in this study effectively reduces the complexity of the intelligent energy-saving control system of the vehicle and achieves a fast solution to the whole vehicle energy optimization problem, integrating macro-traffic information while considering both power and safety. Finally, an intelligent, connected hardware-in-the-loop (HIL) simulation platform is built to verify the method formulated in this study. The simulation results demonstrate that the proposed method reduces energy consumption by 12.98% compared with the conventional constant-speed cruising strategy. In addition, the computational time is significantly reduced.	Article	MAR 2023	10.3390/electronics12061350	[]	[]	-1	-1	-1
P	FARRES L; BLANC X	Electronically controlled pneumatic brake system            for automotive vehicle e.g. truck, has third air supply            circuit that provides same braking performance as first            and second air supply circuits for rear and front axle            at nominal conditions						NOVELTY - The brake system has one or more front axle brake module for providing pneumatic control pressure to the left and right front pneumatic brake actuators and one or more rear axle brake module for providing pneumatic control pressure to the left and right rear pneumatic brake actuators. An air production module (6) selectively provides air under pressure to front and rear axles electronic brake modules through first and second air supply circuits (AC1,AC2) for the rear and front axles, respectively. The first and second air reservoirs (Rl,R2) are respectively coupled to first and second air supply circuits. A third air supply circuit (AC3) is connected to a third reservoir (R3) for providing a redundant pneumatic supply to the front and rear axle brake modules. The third air supply circuit provides same braking performance as the first and second air supply circuits for the rear and front axle at nominal conditions. USE - Electronically controlled pneumatic brake system for automotive vehicle e.g. truck and bus. ADVANTAGE - The redundant electro-pneumatic braking system is provided so as to make available compressed air to pneumatic wheel brake actuators and maintain normal braking capability even though one pneumatic circuit has a problem. The accuracy of the detection of leakage is improved, possible variations of pressure at the trunk portion are taken into account and subtracted from pressure measured at the first and second circuits, thus, giving a better differential mode, and better independence from extraneous effects. A simple and reliable method is provided to perform an early pneumatic isolation of a leaking circuit such that each of the overflow valves is controlled actively and selectively by a relevant control solenoid to close in an early manner a leaking pneumatic circuit, without waiting for the mechanical closure. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method to control overflow valves in an electronically controlled pneumatic brake system for an automotive vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows circuit diagram of an electro-pneumatic braking system for a truck.Air production module (6)First air supply circuit (AC1)Second air supply circuit (AC2)Third air supply circuit (AC3)First air reservoir (R1)Second air reservoir (R2)Third air reservoir (R3)				[]	[]	-1	-1	-1
J	Rehder, Tobias; Koenig, Alexander; Goehl, Michel; Louis, Lawrence; Schramm, Dieter	Lane Change Intention Awareness for Assisted and Automated Driving on Highways	2019	IEEE TRANSACTIONS ON INTELLIGENT VEHICLES		265	276	Today the automotive industry faces a robust trend toward assisted and automated driving. The technology to accomplish this ambition has evolved rapidly over the last few years, and yet there are still a lot of algorithmical challenges left to make an automation of the driving task a safe and comfortable experience. One of the main remaining challenges is the comprehension of the current traffic situation and the anticipation of all traffic participants' future driving behavior, which is needed for the technical system to obtain situation awareness: an indispensable foundation for successful decision-making. In this paper, a prediction framework is presented that is able to infer a driver's maneuver intention. This is achieved via a hybrid Bayesian network whose hidden layers represent a driver's lane contentedness. A pre-training of the network's parameters with simulated data provides for human interpretable parameters even after running the expectation maximization algorithm based on data gathered on German highways. Moreover, the future driving path of any traffic participant is predicted by solving an optimal control problem, whereby the parameters of the optimal control formulation are found via inverse reinforcement learning.	Article	JUN 2019	10.1109/TIV.2019.2904386	[]	[]	-1	-1	-1
P	WADAHARA E; ISHIBASHI S; OHARA H	Conductive fiber reinforced molding material contains electroconductive fiber and resin, and has preset electrical resistance values at longitudinal and arbitrary directions						NOVELTY - A conductive reinforced molding material consisting of electroconductive fiber and resin, has electrical resistance value (RL) in longitudinal direction set to 1x102 ohms or less, and electrical resistance value (RT) in longitudinal and arbitrary directions in flat surface to cross orthogonal direction set to 1x103 ohms or more. USE - For electroconductive molded product (claimed), housings, electric and electronic machines, office automation apparatus, household electric appliances and automotive applications. ADVANTAGE - The electroconductive molded product obtained using conductive fiber reinforced molding material, is excellent in electroconductivity, dynamic characteristics, impact strength and rigidity. The molding material has high productivity. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: (i) Manufacture of conductive fiber reinforced molding material, which involves opening electro conductive fiber, preheating, forming composite of electroconductive fiber and the resin by impregnating resin in electroconductive fiber band, and coating the composite by the resin at velocity of 10 m/minute or more for appropriate time; and (ii) Electroconductive molded product having volume resistivity value of 1000 ohmsasteriskcm or less, formed by molding conductive fiber reinforced molding material by press forming or injection molding.				[]	[]	-1	-1	-1
P	BLEI M; JANSEN C; MERGENTHALER J; CAVALEIRO P; ROLAND K; ALLEF P; SILBER S; REUTER E; SILBER D S; ALLEF D P; ROLAND D K; CAVALEIRO D P; BLEY M; MORGANTLER J	Increasing coverage and/or maintaining application            properties of coating composition e.g. paints or            coating materials during storage, by mixing at least            one rhamnolipid and/or at least one sophorolipid into            coating composition						NOVELTY - Coverage and/or maintaining application properties of a coating composition during storage is increased mixing at least one rhamnolipid (RL) and/or at least one sophorolipid into the coating composition. USE - The method is useful for increasing coverage and/or maintaining application properties of a coating composition during storage, where coating composition is used as paints or coating materials selected from interior wall paints, exterior paints, architectural paints, floor coatings, wood paints, industrial paints, paints for automotive original equipment manufacturer (OEM) or refinishing, primers, primer-services, basecoats, and topcoats (all claimed). ADVANTAGE - The use of rhamnoiipids and/or sophorolipids for increasing the coverage of coating compositions does not alter the amount of pigments of fillers and other nonvolatile components, and causes the rheology profile after storage to have lower variance from the desired rheology profile established than that of a comparable commercial coating composition. Thus, usability of the coating composition for the desired application is maintained.				[]	[]	-1	-1	-1
P	AHMED G; FERIK S E	Adaptive control method for aircraft with slung load, involves transmitting additional anti-swing control input to unmanned aerial vehicle to correct for slung load using relationship among additional longitudinal and lateral displacements						NOVELTY - The method involves calculating neural network estimate of a function. A control input is transmitted for an unmanned aerial vehicle (102) for controlling flight of the unmanned aerial vehicle. Additional anti-swing control input is transmitted to the unmanned aerial vehicle to correct for a slung load using a relationship among additional longitudinal and lateral displacements, longitudinal and lateral feedback gains, load cable length (L), load angle (phi-L) in x-z plane, time and longitudinal and lateral time delays introduced in the feedback of the load angle. USE - Adaptive control method for an unmanned aerial vehicle i.e. aircraft, with a slung load. ADVANTAGE - The method enables performing adaptive control using a double loop architecture including an inner loop with an inner controller responsible for controlling attitude angles and altitude and an outer loop with an outer controller responsible for providing an inner controller of the inner loop with the desired angle values, thus operating the inner controller of the inner loop faster than an outer controller of the outer loop to achieve stability while improving robustness to disturbances and estimation errors by making modification in the tuning algorithm. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer software product comprising a set of instructions for executing an adaptive control method for an unmanned vehicle with a slung load. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an aircraft for modeling dynamics of an aerial vehicle with a slung load.Unit vectors (i-H,k-H)Load cable length (L)Load angle (phi-L)Position vector (RL)Unmanned aerial vehicle (102)				[]	[]	-1	-1	-1
J	Liu, Xing; Wang, Ying; Zhang, Kaibo; Li, Wenhe	Energy Management Strategy Based on Deep Reinforcement Learning and Speed Prediction for Power-Split Hybrid Electric Vehicle with Multidimensional Continuous Control	2023	ENERGY TECHNOLOGY				An efficient energy management strategy (EMS) is significant to improve the economy of hybrid electric vehicles (HEVs). Herein, a power-split HEV model is built and validated against test results, and then the EMS is proposed for this model based on vehicle speed prediction and deep reinforcement learning (DRL) algorithms. The rule-based local controller and global optimal empirical knowledge are introduced to enhance the convergence speed. It is shown in the results that the twin delayed deep deterministic policy gradient algorithm (TD3) achieves more satisfactory performance on converge speed and energy efficiency. The networks of the DRL algorithm with continuous control update more robustly during iterations, in contrast to the discrete ones. Although the power-split HEV with lower control dimension can reduce the learning burden for DRL EMS; however, the multidimensional control space shows greater optimization potential. As a result, the equivalent fuel consumption of TD3-based EMS with multidimensional continuous control differences from the global optimal algorithm only by 4.92%. Herein, it is demonstrated in the results that long short-term memory recurrent neural network (LSTM RNN) performs better for vehicle speed prediction than classical RNN and BP neural network, and the predictive vehicle speed feature helps improve fuel economy by 0.55%.	Article; Early Access		10.1002/ente.202300231	[]	[]	-1	-1	-1
C	Pighetti, Alessandro; Forneris, Luca; Lazzaroni, Luca; Bellotti, Francesco; Capello, Alessio; Cossu, Marianna; De Gloria, Alessandro; Berta, Riccardo	High-Level Decision-Making Non-player Vehicles	2022	GAMES AND LEARNING ALLIANCE, GALA 2022	Lecture Notes in Computer Science	223	233	"Availability of realistic driver models, also able to represent various driving styles, is key to add traffic in serious games on automotive driving. We propose a new architecture for behavioural planning of vehicles, that decide their motion taking high-level decisions, such as ""keep lane"", ""overtake"" and ""go to rightmost lane"". This is similar to a driver's high-level reasoning and takes into account the availability of ever more sophisticated Advanced Driving Assistance Systems (ADAS) in current vehicles. Compared to a low-level decision making system, our model performs better both in terms of safety and average speed. As a significant advantage, the hierarchical approach allows to reduce the number of training steps, which is critical for ML models, by more than one order of magnitude. The developed agent seems to show a more realistic behaviour. We also showed feasibility of training models able to differentiate their performance in a way similar to the driving styles. We believe that such agents could be profitably employed in state of the art SGs for driving, improving the realism of single NPVs and overall traffic."	Proceedings Paper	2022	10.1007/978-3-031-22124-8_22	[]	[]	-1	-1	-1
B	Kuwana, A.; Matsuda, J.-I.; Kobayashi, H.	Analysis of Switching Characteristics of Wide SOA and High Reliability 100 V N-LDMOS Transistor with Dual RESURF and Grounded Field Plate Structure	2021	2021 IEEE 14th International Conference on ASIC (ASICON)		4 pp.	4 pp.	We proposed a wide SOA and high reliability 0.35 mum CMOS compatible 100 V dual RESURF LDMOS transistor with low switching loss and low specific on-resistance for automotive applications. This paper describes detailed switching characteristics by changing load resistance RL and gate resistance RG for actual use which were not investigated. TCAD simulations verified that the total energy loss (total switching loss + conduction loss) of the proposed device is sufficiently smaller (about 30 % down at the maximum) than that of the conventional device in most of the actual use range except for the following region: low duty cycle D < 0.1 and high switching frequency f > 1.1 MHz at a low RG of 1.31 Omegamm2and a high RLof 65.5 Omegamm2under a device layout area of 1 mm2. Also, a unique switching characteristic of the proposed device, or a convex-shape gate plateau, not observed before, is analyzed.	Conference Paper	2021	10.1109/ASICON52560.2021.9620319	[]	[]	-1	-1	-1
J	Correll, Randall; Weinberg, Sean J.; Sanches, Fabio; Ide, Takanori; Suzuki, Takafumi	Quantum Neural Networks for a Supply Chain Logistics Application	2023	ADVANCED QUANTUM TECHNOLOGIES				Problem instances of a size suitable for practical applications are not likely to be addressed during the noisy intermediate-scale quantum (NISQ) period with (almost) pure quantum algorithms. Hybrid classical-quantum algorithms have potential, however, to achieve good performance on much larger problem instances. One such hybrid algorithm on a problem of substantial importance: vehicle routing for supply chain logistics with multiple trucks and complex demand structure is investigated. Reinforcement learning with neural networks with embedded quantum circuits is used. In such neural networks, projecting high-dimensional feature vectors down to smaller vectors is necessary to accommodate restrictions on the number of qubits of NISQ hardware. However, a multi-head attention mechanism is used where, even in classical machine learning, such projections are natural and desirable. Data from the truck routing logistics of a company in the automotive sector is considered, and the methodology is applied by decomposing into small teams of trucks and results are found comparable to human truck assignment.	Article; Early Access		10.1002/qute.202200183	[]	[]	-1	-1	-1
J	Huh, Jaeseok; Park, Jonghun; Shin, Dongmin; Choi, Yerim	A Behavior Optimization Method for Unmanned Combat Aerial Vehicles Using Matrix Factorization	2020	IEEE ACCESS		100298	100307	One of the fundamental technologies for unmanned combat aerial vehicles and combat simulators is behavior optimization, which finds a behavior that maximizes the probability of winning a battle. With the advent of military science, combat logs became available, allowing machine learning algorithms to be used for the behavior optimization. Due to implicit attributes such as the experience of an operator that are not explicitly presented in log data, existing methods for behavior optimization have limitations in performance improvement. Furthermore, specific behaviors occur with low frequency, resulting in a dataset with imbalanced and empty values. Therefore, we apply a matrix factorization (MF) method, which is one of latent factor models and known for sophisticated imputation of empty values, to the behavior optimization problem of unmanned combat aerial vehicles. A situation-behavior matrix, whose elements are ratings indicating the optimality of behaviors in situations, is defined to implement the MF based method. Experiments for performance comparison were conducted on combat logs, in which the proposed method yielded satisfactory results.	Article	2020	10.1109/ACCESS.2020.2998189	[]	[]	-1	-1	-1
J	Huang, Ruchen; He, Hongwen	Naturalistic data-driven and emission reduction-conscious energy management for hybrid electric vehicle based on improved soft actor-critic algorithm	2023	JOURNAL OF POWER SOURCES				Energy management strategies (EMSs) are critical to saving fuel and reducing emissions for hybrid electric vehicles (HEVs). Given that, this article proposes a naturalistic data-driven and emission reduction-conscious EMS based on deep reinforcement learning (DRL) for a power-split HEV. In this article, for the purpose of evaluating the practical fuel economy of an HEV driving in a certain city region, a specific driving cycle is constructed by using a naturalistic data-driven method. Furthermore, to realize the multi-objective optimization in terms of fuel conservation and emission reduction as well as the state of charge (SOC) sustaining, an intelligent EMS based on the improved soft actor-critic (SAC) algorithm with a novel experience replay method is innovatively proposed. Finally, the effectiveness and optimality of the proposed EMS are verified. Simulation results indicate that the constructed driving cycle can effectively reflect the real traffic scenarios of the test region. Moreover, the proposed EMS achieves 95.25% fuel economy performance of the global optimum, improving the fuel economy by 5.29% and reducing the emissions by 10.42% compared with the emission reduction-neglecting EMS based on standard SAC. This article contributes to energy conservation and emission reduction for the transportation industry through advanced DRL methods.	Article; Early Access		10.1016/j.jpowsour.2023.232648	[]	[]	-1	-1	-1
B	Tran, H.-D.; Manzanas Lopez, D.; Johnson, T.	Tutorial: Neural Network and Autonomous Cyber-Physical Systems Formal Verification for Trustworthy AI and Safe Autonomy	2023	2023 International Conference on Embedded Software (EMSOFT)		1	2	This interactive tutorial describes state-of-the-art methods for formally verifying neural networks and their usage within safety-critical cyber-physical systems (CPS). The inclusion of deep learning models in safety-critical applications requires to formally analyze the behavior of the system, including reasoning about the individual components (e.g., controller robustness), and their interactions and effects in the system as a whole. This tutorial begins with a lecture on this emerging research area, followed by demos of these methods implemented in software tools, specifically the Neural Network Verification (NNV) tool. Examples include systems from aerospace, automotive, and beyond.	Conference Paper	2023	10.1145/3607890.3608454	[]	[]	-1	-1	-1
C	Ye, Yiming; Zhang, Jiangfeng; Xu, Bin	A Fast Q-learning Energy Management Strategy for Battery/Supercapacitor Electric Vehicles Considering Energy Saving and Battery Aging	2021	INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND ENERGY TECHNOLOGIES (ICECET 2021)		1639	1644	Electrified powertrain system brings advantages of improved energy efficiency and reduced fossil fuel consumption, which leads to the electrification of powertrain systems as a key target of automotive industry. Advanced battery technology has been exploited in electric vehicle application and has made considerable progress. However, the degradation of batteries in the vehicle operation could cause adverse effects on the performance and lifespan of electric vehicles. Research on battery/supercapacitor hybrid energy storage systems of electric vehicles that considers energy saving and battery degradation is also lacking. This paper presents a fast Q-learning based energy management strategy to maximize energy saving and minimize battery degradation. Besides, battery only electric vehicle is also studied, and acts as a baseline vehicle. An electrified powertrain system model considering the battery degradation effect is established to form the environment for the Q-learning strategy. Under the training and validation driving cycles, the comparison indicates that the fast Q-learning strategy improves the energy efficiency by 3.83%, and the battery degradation is relieved by 26.36%.	Proceedings Paper	2021	10.1109/ICECET52533.2021.9698682	[]	[]	-1	-1	-1
J	Govindan, Kannan; Kadzinski, Milosz; Ehling, Ronja; Miebs, Grzegorz	Selection of a sustainable third-party reverse logistics provider based on the robustness analysis of an outranking graph kernel conducted with ELECTRE I and SMAA	2019	OMEGA-INTERNATIONAL JOURNAL OF MANAGEMENT SCIENCE		1	15	Pressure from legislation and customers has motivated companies to consider reverse logistics (RL) in their operations. Since it is a complex procedure that requires an adequate system, the recent trend consists in outsourcing RL to third-party reverse logistics providers (3PRLPs). This paper provides the background of sustainable triple bottom line theory with focus on economic, environmental, and social aspects under 3PRL concerns. The relevant sustainability criteria are used in a case study conducted in cooperation with an Indian automotive remanufacturing company. To select the most preferred service provider, we use a hybrid method combining a variant of ELECTRE I accounting for the effect of reinforced preference, the revised Simos procedure, and Stochastic Multi-criteria Acceptability Analysis. The incorporated approach exploits all parameters of an outranking model compatible with the incomplete preference information of the Decision Maker. In particular, it derives the newly defined kernel acceptability and membership indices that can be interpreted as a support given to the selection of either a particular subset of alternatives or a single option. The proposed ELECTRE-based method enriches the spectrum of multiple criteria decision analysis approaches that can be used to effectively approach the problem of the 3PRLP selection. As indicated by the extensive review presented in the paper, this application field was so far dominated by Analytic Hierarchy Process and TOPSIS, whose weaknesses can be overcome by applying the outranking methods. (C) 2018 Elsevier Ltd. All rights reserved.	Article	JUN 2019	10.1016/j.omega.2018.05.007	[]	[]	-1	-1	-1
B	Vora, Ankur	Cross-Layer Approaches for 5G Wireless Communication	2019						Dissertation/Thesis	Jan 01 2019		[]	[]	-1	-1	-1
J	Karunakaran, Dhanoop; Perez, Julie Stephany Berrio; Worrall, Stewart	Generating Edge Cases for Testing Autonomous Vehicles Using Real-World Data	2024	SENSORS				In the past decade, automotive companies have invested significantly in autonomous vehicles (AV), but achieving widespread deployment remains a challenge in part due to the complexities of safety evaluation. Traditional distance-based testing has been shown to be expensive and time-consuming. To address this, experts have proposed scenario-based testing (SBT), which simulates detailed real-world driving scenarios to assess vehicle responses efficiently. This paper introduces a method that builds a parametric representation of a driving scenario using collected driving data. By adopting a data-driven approach, we are then able to generate realistic, concrete scenarios that correspond to high-risk situations. A reinforcement learning technique is used to identify the combination of parameter values that result in the failure of a system under test (SUT). The proposed method generates novel, simulated high-risk scenarios, thereby offering a meaningful and focused assessment of AV systems.	Article	JAN 2024	10.3390/s24010108	[]	[]	-1	-1	-1
B	[Anonymous]	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)				The following topics are dealt with: multicore processors; GPU; magnetic recording drives; visual object recognition accelerator; sequential circuits; interconnects; cyclic logic encryptions; analog SAT solver; graph coloring; safety model checking; on-chip bit-flip detectors; EM side-channel attack resilience; deep neural network; monolithic 3D IC; TSV; NoC; offshore oil spill monitoring; deep reinforcement learning; nonlinear circuit simulation; controller area network; model predictive control; automotive cyber-physical systems; computer-aided design; CAD; robust design flow database; RDF; memristor aided logic; optical circuits; DSA-MP lithography; random access memory; multi-level cell STT-MRAM main memory; asymmetric cryptography; SSD flash-translation layers; adaptive distributed mobile learning system; IoT; mobile telemedicine; FPGA; split manufacturing; logic synthesis; self-powered wearable systems; memory access control; arithmetic circuits; high level synthesis; approximate lookup table-based accelerator; ApproxLUT; multilayer time-based spiking neuromorphic architecture; active power grids; thermal analysis; reconfigurable in-memory computing architecture; medical resource availability; intelligent transportation systems; system-on-chips; routing approach; MEDA biochips; VLSI; nonvonNeumann processor; Big Data Analysis; Industry 4.0; FinFET technology; and IC yield improvement.	Conference Proceedings	2017		[]	[]	-1	-1	-1
J	Sanchez, Francisco Sebastian Rodriguez	Fracture behaviour of automotive adhesive joints	2008						Dissertation/Thesis	Jan 01 2008		[]	[]	-1	-1	-1
B	Sithinamsuwan, J.; Hata, K.; Fujimoto, H.; Hori, Y.	Sensorless Automatic Stop Control of Electric Vehicle in Semi-dynamic Wireless Power Transfer System with Two Transmitter Coils	2019	IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society. Proceedings		4323	8	Semi-dynamic wireless power transfer (SDWPT) system is one of the solutions to short driving distance in electric vehicle (EV). This system provides electric power to EVs on the point of stopping. A scheme of sensorless positioning system in SDWPT system via magnetic resonance coupling is proposed in this paper. High transmission efficiency and short vehicular gap position are selected as the final stop position which lead to energy conservation and traffic jam problem reduction. These proposed methods also show more practicality of positioning system for SDWPT system by using command value prediction (CVP) and reinforcement learning algorithm (RLA). Performance of these two proposed method is evaluated by simulations and experiments. The results pointed out that these two proposed methods are able to stop the vehicle at the high transmission efficiency position without any visual sensor.	Conference Paper	2019	10.1109/IECON.2019.8927315	[]	[]	-1	-1	-1
J	Huang, Jiang-Ping; Gao, Liang; Li, Xin-Yu; Zhang, Chun-Jiang	A cooperative hierarchical deep reinforcement learning based multi-agent method for distributed job shop scheduling problem with random job arrivals	2023	COMPUTERS & INDUSTRIAL ENGINEERING				Distributed manufacturing can reduce the production cost through the cooperation among factories, and it has been an important trend in the industrial field. For the enterprises with daily delivered production tasks, the random job arrivals are regular. Thus, the Distributed Job-shop Scheduling Problem (DJSP) with random job arrivals is studied, and it is a typical case from the equipment manufacturing industry. The DJSP involves two coupled decision-making processes, job assigning and job sequencing, and the distributed and uncertain pro-duction environment requires the scheduling method to be more responsive and adaptive. Thus, a Deep Rein-forcement Learning (DRL) based multi-agent method is explored, and it is composed of the assigning agent and the sequencing agent. Two Markov Decision Processes (MDPs) are formulated for the two agents respectively. In the MDP for the assigning agent, fourteen factory-and-job related features are extracted as the state features, seven composite assigning rules are designed as the candidate actions, and the reward depends on the total processing time of different factories. In the MDP of the sequencing agent, five machine-and-job related features are set as the state features, six sequencing rules make up the action space, and the change of the factory makespan is the reward. Besides, to enhance the learning ability of the agents, a Deep Q-Network (DQN) framework with variable threshold probability in the training stage is designed, which can balance the exploi-tation and exploration in the model training. The proposed multi-agent method's effectiveness is proved by the independent utility test and the comparison test that are based on 1350 production instances, and its practical value in the actual production is implied by the case study from an automotive engine manufacturing company.	Article; Early Access		10.1016/j.cie.2023.109650	[]	[]	-1	-1	-1
P	WANG J; WEI H; CHEN M; HE S; CHEN L; LIU X; SUN S; XIE W; LI H	Mobile welding robot, has robot main body that is            provided with movable chassis and welding mechanical            arm, relative position and posture are controlled            between first work piece and second work piece, and            main control system controls position and posture of            welding mechanical arm						NOVELTY - The robot has a robot main body provided with a movable chassis (12) and a welding mechanical arm. A first electric machine is connected with a second electric machine. A relative position and a posture are controlled between a first work piece and a second work piece. A main control system controls a position and the posture of the welding mechanical arm. The welding mechanical arm is hinged with the movable chassis by a welding mechanical arm rotating shaft (101). The welding mechanical arm is provided with a welding mechanical arm big arm and a welding mechanical arm small arm. The welding mechanical arm big arm is hinged with the welding mechanical arm rotating shaft through a welding mechanical arm pitching shaft (102). USE - Mobile welding robot for automatic welding of automobile frame by using technology of reinforcement learning. Can also be used in automotive frame welding field, specifically to a system for automatically adjusting welding mechanical arm of an automobile frame welding system. ADVANTAGE - The robot prevents the automobile frame deformation in the welding process, and can be corrected in time. The welding quality can be monitored in real time, and automatic welding can be realized. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a mobile welding robot.11Movable wheel12Movable chassis101Welding mechanical arm rotating shaft102Welding mechanical arm pitching shaft103Mobile chassis camera104Welding mechanical arm large arm				[]	[]	-1	-1	-1
P	KAUSHIK D; SHARMA A; SHUBHAM; KUMAR S	System for performing automated quality control in            manufacturing processes using machine learning            algorithms, has machine learning module to train and            test machine learning algorithms using extracted            features, and detection module to detect defects and            errors in real-time using trained algorithms						NOVELTY - The system has a data acquisition module for collecting data from a manufacturing process. A feature extraction module extracts relevant features from the data. A machine learning module trains and tests machine learning algorithms using the extracted features. A defect detection module detects defects and errors in real-time using the trained algorithms, where the machine learning algorithm is trained using supervised, unsupervised, or reinforcement learning methods and the machine learning algorithms are trained using deep learning techniques such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks. The defect detection module generates reports and alerts for the manufacturing team to take corrective action. USE - System for performing automated quality control in manufacturing processes using machine learning algorithms. Uses include but are not limited to automotive, aerospace, electronics, and pharmaceuticals. ADVANTAGE - The system improves the efficiency and accuracy of the manufacturing process, leading to higher quality products and reduced waste and rework. The system can adapt to changes in manufacturing process and learn from current data to improve the accuracy of defect detection. The machine learning algorithms can learn from large amounts of data and identify patterns and trends that can be used to predict and prevent defects and errors in the manufacturing processes. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for performing automated quality control in manufacturing processes using machine learning algorithms.				[]	[]	-1	-1	-1
J	Sliwa, Benjamin; Adam, Rick; Wietfeld, Christian	Client-Based Intelligence for Resource Efficient Vehicular Big Data Transfer in Future 6G Networks	2021	IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY		5332	5346	"Vehicular big data is anticipated to become the ""new oil"" of the automotive industry which fuels the development of novel crowdsensing-enabled services. However, the tremendous amount of transmitted vehicular sensor data represents a massive challenge for the cellular network. A promising method for achieving relief which allows to utilize the existing network resources in a more efficient way is the utilization of intelligence on the end-edge-cloud devices. Through machine learning-based identification and exploitation of highly resource efficient data transmission opportunities, the client devices are able to participate in overall network resource optimization process. In this work, we present a novel client-based opportunistic data transmission method for delay-tolerant applications which is based on a hybrid machine learning approach: Supervised learning is applied to forecast the currently achievable data rate which serves as the metric for the reinforcement learning-based data transfer scheduling process. In addition, unsupervised learning is applied to uncover geospatially-dependent uncertainties within the prediction model. In a comprehensive real world evaluation in the public cellular networks of three German Mobile Network Operator (MNO), we show that the average data rate can be improved by up to 223% while simultaneously reducing the amount of occupied network resources by up to 89%. As a side-effect of preferring more robust network conditions for the data transfer, the transmission-related power consumption is reduced by up to 73%. The price to pay is an increased Age of Information (AoI) of the sensor data."	Article	JUN 2021	10.1109/TVT.2021.3060459	[]	[]	-1	-1	-1
J	Mirza, Muhammad Ayzed; Yu, Junsheng; Ahmed, Manzoor; Raza, Salman; Khan, Wali Ullah; Xu, Fang; Nauman, Ali	DRL-driven zero-RIS assisted energy-efficient task offloading in vehicular edge computing networks	2023	JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES				The increasing complexity of modern automotive applications presents difficulties when running them on the on-board units (OBUs) of vehicles. While 5G/6G vehicular edge computing networks (VECNs) offer potential solutions through computation task offloading, ensuring prompt, energy-efficient access to these networks remains a significant challenge. To overcome these challenges, reconfigurable intelligent surfaces (RIS) can play an important role in 6G vehicular networks. With RIS, networks can provide better connectivity, increased data rate and energy efficient access, and communication channel security. In this paper, we utilize zero-energy RIS (ze-RIS) to aid vehicular computation offloading while maximizing the energy and time savings while meeting the task and environmental constraints. A joint power and offloading mechanism controlling DRL-driven RIS-assisted energy efficient task offloading (DREEO) scheme is proposed. DREEO utilizes a hybrid approach that combines binary and partial offloading mechanisms, complemented by an intelligent communication link switching mechanism. This strategy helps in saving both energy and time effectively. An efficiency factor, serving as both a performance indicator and a reward function, is introduced for the DRL agent, considering both saved energy and time. Through extensive evaluations, DREEO scheme shown an increase in task success rate from 2.13% to 7.36% and has improved the efficiency factor from 21.97 to 51.27. Furthermore, compared to other evaluated schemes, the DREEO scheme consistently outperforms them in terms of reward and the TFPS ratio, the DRL properties.	Article	OCT 2023	10.1016/j.jksuci.2023.101837	[]	[]	-1	-1	-1
J	Zhang, G.; Wang, X.; Zhang, Y.; Gao, A.	Research on Cooperative Path Planning Model of Multiple Unmanned Vehicles in Real Environment	2023	Journal of System Simulation		408	22	The cluster combat application of unmanned ground vehicles(UVS) is a hot research issue of the intersection of artificial intelligence and battle command. Aiming at the cooperative path planning multiple unmanned vehicles not meeting the dynamic threat condition requirement, by combining the global path planning algorithm A-STAR with the local path planning algorithm RL, from the perspective of perception to behavioral decision making, the cooperative path planning model of multiple unmanned vehicles is studied. The cooperative combat situation threat algorithm, state and action space, reward function and sphere of influence function are designed, the sub-models of formation configuration strategy generation and dynamic optimization of strike path are carried out, and the cooperative path planning control model of multiple unmanned vehicles based on autonomous learning is constructed and solved. An application example shows that the proposed path planning model can effectively cope with the requirements of multi-unmanned vehicle collaborative path planning task in complex urban environment, and has important theoretical research and practical application value. Â© 2023, The editorial Board of Journal of System Simulation. All right reserved.	Journal Paper	2023	10.16182/j.issn1004731x.joss.21-0947	[]	[]	-1	-1	-1
J	Trang, Nguyen Thi Nha; Li, Yan	Reverse supply chain for end- of- life vehicles treatment: An in- depth content review	2023	RESOURCES CONSERVATION & RECYCLING ADVANCES				Reverse supply chain (RSC) implementation is integral to waste management. Owing to economic, environmental and social benefits, RSC has been adopted to many industries and automotive is no exception. Multiple components and materials found in End-of-life vehicles (ELVs) have potential recycling value but also cause environmental damages if improperly treated. The number of ELVs is accelerating worldwide, especially in developing countries. Research on RSCs for ELVs treatment has been increasing. However, the literature still lacks a comprehensive understanding of this topic. The authors thus conducted an in-depth content review of publications on RSC of ELVs. The methodology applied PRISMA 2020 guideline combined with the model of Mayring (2001). Extensive search strings were utilized to retrieve articles from SCOPUS and Web of Science. After a rigid selection process and an intensive content assessment on 10,140 publications as raw materials, a total of 151 peer-reviewed papers was selected and carefully analyzed. The content categories were developed deductively and inductively. Major categories included research methodology overarching others: research themes, RSC/ELV types, stages in RSC, country specific and stakeholders. Modeling is the most commonly used among the papers, especially in RSC network design and planning. Mixed-integer linear programming prevails over other modeling approaches while AHP is predominant decision-making tool. Studies on legislations, network performance evaluation, and forecasting remain negligible. Categorization and significant achievements of articles are introduced. Future research is suggested upon identified research gaps. The review benefits academicians and practitioners in better comprehension of the field and promising research directions.	Review; Early Access		10.1016/j.rcradv.2022.200128	[]	[]	-1	-1	-1
J	Song, Jiayang; Xie, Xuan; Ma, Lei	SIEGE: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems	2023	IEEE TRANSACTIONS ON SOFTWARE ENGINEERING		4058	4080	Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance. However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties. Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system. Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications. Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs. In this paper, we propose SIEGE, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs. We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems. Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly (https://sites.google.com/view/ai-cps-siege/ home).	Article	AUG 2023	10.1109/TSE.2023.3282981	[]	[]	-1	-1	-1
J	Zhang, Yuanzhi; Zhang, Caizhi; Fan, Ruijia; Deng, Chenghao; Wan, Song; Chaoui, Hicham	Energy management strategy for fuel cell vehicles via soft actor-critic-based deep reinforcement learning considering powertrain thermal and durability characteristics	2023	ENERGY CONVERSION AND MANAGEMENT				Temperature can significantly affect the water equilibrium, electrochemical kinetics and mass transmission in a proton exchange membrane fuel cell (PEMFC) stack, meanwhile it also impacts the lifespan and safety of a lithium-ion battery (LIB). Yet, energy management strategy (EMS) is rarely to synchronously study the durability performances of the LIB and PEMFC stack with their thermal effects in fuel cell vehicles (FCVs) under real-world driving scenarios. Thus, this study proposes a deep reinforcement learning (DRL)-based EMS to minimize tran-sient costs of the LIB and PEMFC stack, which include their state-of-health (SOH) descents and overtemperature penalties. Meanwhile, the transient costs are incorporated into the overall cost, which comprises the hydrogen consumption rate of the PEMFC stack, and penalty of maintaining the LIB state-of-charge (SOC). Moreover, the soft actor-critic (SAC) is applied to the DRL-based EMS due to its advantage of stability across different random environments and no meticulous hyperparameter calibration. Specifically, the proposed EMS intelligently allo-cates the direct current (DC) bus power of FCVs in real time to maximize a multi-objective reward in accordance with FCV states, in which the reward is the negative overall cost. Then, long-term real-world driving scenarios in Chongqing city, China, are used for off-line training and real-time control to advance the adaptability of the proposed EMS. The results show that in comparison with the deep Q-network (DQN)-based EMS considering the powertrain temperature and durability, and the SAC-based EMS neglecting the powertrain temperature and durability, the proposed strategy can actualize overall SOH increments of the powertrain up to 14.01 % and 3.45 %, respectively, and restrict the maximum temperatures of the PEMFC stack and LIB. In addition, the general-ization of the proposed EMS is verified, in which the trained model of the proposed EMS is tested in other FCV and driving cycles, and it can acquire similar effectiveness. Thus, the proposed strategy can enforce the lifespan durability and thermal stability of the powertrain system.	Article; Early Access		10.1016/j.enconman.2023.116921	[]	[]	-1	-1	-1
B	Jerouschek, D.; Tan, O.; Kennel, R.; Taskiran, A.	Modeling Lithium-Ion Batteries Using Machine Learning Algorithms for Mild-Hybrid Vehicle Applications	2021	2021 International Conference on Smart Energy Systems and Technologies (SEST)		6 pp.	6 pp.	The prediction of voltage levels in an automotive 48V mild hybrid power supply system is safety-relevant while also enabling greater efficiency. The high power-to-energy ratio in these power supply systems makes exact voltage prediction challenging, so that a method is established to model the behavior of the lithium-ion batteries by means of a recurrent neural network. The raw data are consequently pre-processed with over- and undersampling, normalization and sequentialization algorithms. The resulting database is used to train the constructed recurrent neural network models, while hyperparameter tuning is carried out with the optimization framework optuna. This training methodology is performed with two battery types. Validation shows a maximum error of 2.34 V for the LTO battery and a maximum error of 3.39 V for the LFP battery. The results demonstrate performance of the proposed methodology in an appropriate error range for utilization as a tool to generate a battery model based on available data.	Conference Paper	2021	10.1109/SEST50973.2021.9543225	[]	[]	-1	-1	-1
R	Zhao, Qianyu; Han, Zhaoyang; Wang, Shouxiang; Dong, Yichao; Qian, Guangchao	Table1_Coordinated control of multiple converters in model-free AC/DC distribution networks based on reinforcement learning.XLSX	2023	Figshare				Taking into account the challenges of obtaining accurate physical parameters and uncertainties arising from the integration of a large number of sources and loads, this paper proposes a real-time voltage control method for AC/DC distribution networks. The method utilizes model-free generation and coordinated control of multiple converters, and employs a combination of agent modeling and multi-agent soft actor critic (MASAC) techniques for modeling and solving the problem. Firstly, a complex nonlinear mapping relationship between bus power and voltage is established by training an power-voltage model, to address the issue of obtaining physical parameters in AC/DC distribution networks. Next, a Markov decision process is established for the voltage control problem, with multiple intelligent agents distributed to control the active and reactive power at each converter, in response to the uncertainties of photovoltaic (PV) and load variations. Using the MASAC method, a centralized training strategy and decentralized execution policy are implemented to achieve distributed control of the converters, with each converter making optimal decisions based on its local observation state. Finally, the proposed method is verified by numerical simulations, demonstrating its sound effectiveness and generalization ability. Copyright: CC BY 4.0	Data set	2023-07-06	https://doi.org/10.3389/fenrg.2023.1202701.s001	[]	[]	-1	-1	-1
B	Chen, Z.	Q-learning and Neural Networks in Automated Driving Systems	2023	2023 2nd International Conference on Data Analytics, Computing and Artificial Intelligence (ICDACAI)		683	6	With the development of artificial intelligence, machine learning, and other disciplines, automation in various fields is becoming popular. Therefore, autonomous driving systems in the automotive field represent a major trend in the future of intelligent transportation systems. This paper focuses on the application of Q-learning and deep neural networks in autonomous driving systems. The simulation in Python is used to simulate the condition of the car driving alone on the road. Firstly, the normal car driving situation was analyzed and the actions and states in Q-learning were set. The action set-up ensures the distance sensors' number and direction. Then, this paper draws the neural network and simulates it to get the data. Secondly, it analyzes the resulting reward scores and iteration times and identifies uncertainties through line graphs. In addition, it observes the simulation video and derives the reasons why uncertainty appears. In the simulation, the Best Score, Average generation time and Genetic distance were obtained from the simulation against the number of Gens. The data for genetic distance was found to increase with the number of times it was generated, indicating that the Q-learning component was successfully run in the simulation. And it was found that the uncertainty was due to the birth point of the vehicle. According to the uncertainty, further development of the simulation could include more road environments and different obstacles.	Conference Paper	2023	10.1109/ICDACAI59742.2023.00136	[]	[]	-1	-1	-1
P	LO BIUNDO G; PACHETTI C; LO B G	Motor vehicle oil pump, has external rotor            constrained to driving body, and internal rotor fitted            onto idle shaft for rotating about axis of rotation,            where internal rotor is rotated by external rotor						NOVELTY - The pump (10) has an external rotor (15) with a radially internal surface that forms a set of lobes. An internal rotor has a radial external surface forming another set lobes are configured to engage the former lobes of the external rotor. A drive portion is rotatable with respect to a pump portion (11) about an axis of rotation (Rl). The external rotor is constrained to the drive portion and an idle shaft is rotatably connected to the pump portion. The internal rotor is fitted onto the idle shaft for rotating about another axis (R2) parallel and eccentric to the former axis. USE - Oil pump in an automotive sector e.g. internal combustion engine and hybrid engine, of a motor vehicle such as car. ADVANTAGE - The vehicle reduces weight, fuel consumption, maintenance and production costs of motor vehicle. It ensures proper oil circulation in the heat engine when the engine is not running but still requires lubrication. It reduces power consumption and provides lower oil flow rates than the main pump. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a section of the oil pump.Motor Vehicle Oil Pump (10)Pump portion (11)Delivery Opening (13)Suction Opening (14)External Rotor (15)				[]	[]	-1	-1	-1
P	BULHELLER J; STEINER M; SCHIEGEL S	Control device for e.g. window regulator, has            temperature sensors that activate or deactivate            functions of adjustment unit depending on algorithms            that are assigned to respective functions, which have            actual temperature as input parameter						NOVELTY - The device has a power semiconductor controlling an electrical value of a power train, and temperature sensors (TS1 - TS4) controlling a set of functions of an adjustment unit. The sensors determine actual temperature of the semiconductor and activate or deactivate the functions of the adjustment unit depending on two algorithms that are assigned to the respective functions, which have the actual temperature as an input parameter. USE - Used for an adjustment unit e.g. window regulator, sunroof, mirror, steering wheel adjuster, belt hand-over, belt buckle, tailgate and sliding door, of a motor vehicle (all claimed) in automotive engineering. ADVANTAGE - The device protects a user or a passenger of the motor vehicle against injury by the adjustment unit, thus preventing impairment of the driver during guiding of the motor vehicle or in an emergency situation such as accident situation. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for controlling an adjustment unit of a motor vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a circuit diagram of a control device.Measuring inputs (In1 - In4)Power-field effect transistors (M1 - M4)Drive motor (RL)Temperature sensors (TS1 - TS4)Microcontroller (1)				[]	[]	-1	-1	-1
B	Pourdanesh, F.; Dinh, T.Q.; Tagliabo, F.; Whiffin, P.	Failure safety analysis of artificial intelligence systems for smart/autonomous vehicles	2021	2021 24th International Conference on Mechatronics Technology (ICMT)		6 pp.	6 pp.	Up to now failures in artificial intelligence systems, specifically machine learning algorithms which are their software components, are considered as systematic failures. The goal of this paper is to introduce a new concept of quantitative failure analysis for machine learning algorithms which can be used in smart/autonomous vehicles to guarantee sufficiently low risk of residual errors in this application. Firstly, a coincidence in evaluating impacts of unpredictable behaviours of machine learning algorithms and hardware components is introduced in order to statistically estimate failure rate based on a given number of data points. Next, a metric utilising this randomic failure rate is proposed to assess functional safety of smart and/or autonomous vehicles and evaluate their safeness according to ISO 26262:2018, and ISO/PAS 21448.	Conference Paper	2021	10.1109/ICMT53429.2021.9687283	[]	[]	-1	-1	-1
B	Jazayeri, Mohammad Sadegh	Predicting Vehicle Trajectories at Intersections Using Advanced Machine Learning Techniques	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
B	Yamamoto, F.; Kokubo, K.; Koakutsu, H.; Takahashi, Y.	A high efficiency power amplifier for car audio	1994	Third International Symposium on Consumer Electronics		283	7 vol.2	This paper describes a simply configured, highly efficient BTL (Balanced Transformer-Less) power amplifier IC for car audio equipment (18W*2CH, Vcc=13.2V, RL=4 Omega). Since this IC employs only one switching regulator circuit, its power loss is as small as 1/2 to 1/3 of that of a conventional class-B BTL amplifier when compared at a practical listening level (2W to 3W).	Conference Paper	1994		[]	[]	-1	-1	-1
J	Ruan, Jiageng; Wu, Changcheng; Liang, Zhaowen; Liu, Kai; Li, Bin; Li, Weihan; Li, Tongyang	The application of machine learning-based energy management strategy in a multi-mode plug-in hybrid electric vehicle, part II: Deep deterministic policy gradient algorithm design for electric mode	2023	ENERGY				Machine learning (ML)-based methods have attracted great attention in the multi-objective optimization prob-lems, which is the key challenge in the energy management strategy (EMS) of the multi-power hybrid system. Our recently published research in this journal verified the effectiveness and feasibility of a Deep Deterministic Policy Gradient (DDPG)-based EMS in the charge-sustaining (CS) stage of a multi-mode plug-in hybrid vehicle (PHEV). However, the application of ML-based-EMS in the charge-depletion (CD) stage and the regenerative braking mode of PHEV are still missing. This study proposes a discrete-continuous hybrid actions-based hier-archical EMS to optimally distribute the dual-motor driving force in battery electric driving and regenerative braking. In the upper layer of EMS, DDPG is trained to learn the torque distribution principles of dual-motor operation to achieve better energy efficiency without losing dynamic performance. Meanwhile, the total recoverable braking torque is also determined by the upper layer EMS considering the braking demand, me-chanical and electrical braking system conditions, vehicle safety, and the provisions of law. In the lower level of EMS, the driving mode is determined under the guidance of energy consumption optimization. The verified results show that the proposed EMS outperforms other deep reinforcement learning (DRL)-based hierarchical and non-hierarchical EMSs.	Article; Early Access		10.1016/j.energy.2023.126792	[]	[]	-1	-1	-1
B	Wang, Pengyue	Data-Driven Framework for Energy Management in Extended Range Electric Vehicles Used in Package Delivery Applications	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
B	Basile, G.; Leccese, S.; Petrillo, A.; Rizzo, R.; Santini, S.	Sustainable DDPG-based Path Tracking For Connected Autonomous Electric Vehicles in extra-urban scenarios	2023	2023 IEEE IAS Global Conference on Renewable Energy and Hydrogen Technologies (GlobConHT)		1	7	This paper addresses the path-tracking control problem for Connected Autonomous Electric Vehicles (CAEVs) moving in a smart Cooperative Connected Automated Mobility (CCAM) environment, where a smart infrastructure suggests the reference behaviour to achieve. To solve this problem, a novel energy-efficient Deep Deterministic Policy Gradient-based (DDPG) Algorithm, able to minimize its energy consumption while guaranteeing the optimal tracking of the suggested path, is proposed. Specifically, in order to improve the power autonomy and the battery state of charge (SOC), a Comprehensive Power-based Electric Vehicle Consumption Model (CPEM) is exploited for the training of the DDPG agent. The training process confirms the capability of DDPG agent into learning the safe eco-driving policy, while a case of study proves the advantages and the performance of the overall closed-loop of the proposed control strategy.	Conference Paper	2023	10.1109/GlobConHT56829.2023.10087542	[]	[]	-1	-1	-1
J	Becsi, Tamas; Szabo, Adam; Kovari, Balint; Aradi, Szilard; Gaspar, Peter	Reinforcement Learning Based Control Design for a Floating Piston Pneumatic Gearbox Actuator	2020	IEEE ACCESS		147295	147312	"Electro-pneumatic actuators play an essential role in various areas of the industry, including heavy-duty vehicles. This article deals with the control problem of an Automatic Manual Transmission, where the actuator of the system is a double-acting floating-piston cylinder, with dedicated inner-position. During the control design of electro-pneumatic cylinders, one must implement a set-valued control on a nonlinear system, when, as in the present case, non-proportional valves provide the airflow. As both the system model itself and the qualitative control goals can be formulated as a Partially Observable Markov Decision Process, Machine learning frameworks are a conspicuous choice for handling such control problems. To this end, six different solutions are compared in the article, of which a new agent named PG-MCTS, using a modified version of the ""Upper Confidence bound for Trees"" algorithm, is also presented. The performance and strategic choice comparison of the six methods are carried out in a simulation environment. Validation tests performed on an actual transmission system and implemented on an automotive control unit to prove the applicability of the concept. In this case, a Policy Gradient agent, selected by implementation and computation capacity restrictions. The results show that the presented methods are suitable for the control of floating-piston cylinders and can be extended to other fluid mechanical actuators, or even different set-valued nonlinear control problems."	Article	2020	10.1109/ACCESS.2020.3015576	[]	[]	-1	-1	-1
C	Natella, Domenico; Vasca, Francesco	A Q-learning Approach for SoftECU Design in Hybrid Electric Vehicles	2020	2020 24TH INTERNATIONAL CONFERENCE ON SYSTEM THEORY, CONTROL AND COMPUTING (ICSTCC)	International Conference on System Theory Control and Computing	763	768	A software electronic control unit (SoftECU) implements simplified versions of the algorithms coded in a corresponding real ECU. In this paper a Q-learning approach for the design of SoftECUs is proposed. The training phase of the SoftECU is based on the mismatch between the commands provided by the real ECU and those coming from the SoftECU model. The technique is applied to the case of an energy management ECU for hybrid electric vehicles. The effectiveness of the SoftECU is validated by considering the new European driving cycle and the worldwide harmonized light vehicles test procedure.	Proceedings Paper	2020	10.1109/icstcc50638.2020.9259643	[]	[]	-1	-1	-1
B	Costa, Francesco	Prolonging Robot Lifespan Using Fatigue Balancing with Reinforcement Learning	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Liebig, S.; Schmitt, A.; Hammerer, H.	High-dynamic high-power E-Motor Emulator for power electronic testing	2018	PCIM Europe 2018		1157	61	In the past decade, inverter testing has become a very important and complex issue during inverter developments. State-of-the-art inverters, key components of electrical power trains, offer multiple functions, high reliability, fault tolerance and they must ensure a high level of functional safety. Especially automotive applications prove to be demanding due to their exceptionally high dynamic requirements, challenging mission profiles, harsh environment and special test cases such as the curb stone edge case with high currents, high dynamic and no speed. Conventional test methods such as motor test beds or active / passive RL-loads are not suitable to verify inverter functionalities in a sufficient manner. This paper presents a new and powerful inverter testing technology: The E-Motor Emulator - testing power electronics without an e-machine.	Conference Paper	2018		[]	[]	-1	-1	-1
C	Ulmschneider, Klaus; Glimm, Birte	Semantic Exploitation of Implicit Patent Information	2016	PROCEEDINGS OF 2016 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)				In recent years patents have become increasingly important for businesses to protect their intellectual capital and as a valuable source of information. Patent information is, however, not employed to its full potential and the interpretation of structured and unstructured patent information in large volumes remains a challenge. We address this by proposing an integrated interdisciplinary approach that uses natural language processing and machine learning techniques to formalize multilingual patent information in an ontology. The ontology further contains patent and domain specific knowledge, which allows for aligning patents with technological fields of interest and other business-related artifacts. Our empirical evaluation shows that for categorizing patents according to well-known technological fields of interest, the approach achieves high accuracy with selected feature sets compared to related work focussing on monolingual patents. We further show that combining OWL RL reasoning with SPARQL querying over the patent knowledge base allows for answering complex business queries and illustrate this with real-world use cases from the automotive domain.	Proceedings Paper	2016		[]	[]	-1	-1	-1
P	ROUSSELET E; GIANONCELLI D	Method for manufacturing double twist mica type fire resistant cable for use in e.g. construction industry, involves laying mica tape on strands of cabled filaments or wires, and making strands to undergo double twist in cabling machine						NOVELTY - The method involves cabling filaments or wires coming from a filament payout device (DR) by a double-twist cabling frame (ALDT) of a cabling machine in an assembly line, and receiving a mica tape from a mica tape payout device in the assembly line in a single step. The mica tape is laid on the strands of the cabled filaments or wires, and the strands are made to undergo a double twist in the cabling machine, so as to output a double twist mica type cable, where the wires are made of copper or other metals. USE - Method for manufacturing a double twist mica type fire resistant cable for use in construction, automotive, aeronautics and aerospace industries (all claimed). ADVANTAGE - The method reduces the number of joints, lowers the stress on the mica tape, improves productivity, improves the finished product quality, and reduces production costs and time. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a machine for assembly of a double twist mica type fire resistant cable(2) a production line for a double twist mica type fire resistant cable. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view illustrating a method for manufacturing a double twist mica type fire resistant cable.Double-twist cabling frame (ALDT)Filament payout device (DR)Linear drives (RL)				[]	[]	-1	-1	-1
C	Rida, Amin; Margomenos, Alexandros; Wu, Terence; Tentzeris, Manos M.	Novel Wideband 3D Transitions on Liquid Crystal Polymer for Millimeter-Wave Applications up to 100 GHz	2009	2009 IEEE/MTT-S INTERNATIONAL MICROWAVE SYMPOSIUM, VOLS 1-3	IEEE MTT-S International Microwave Symposium	953	+	This paper reports on two novel broadband vertical transitions on flexible organic Liquid Crystal Polymer (LCP) substrate which show superior characteristics-Return Loss below -10 dB, Insertion Loss better than -2.5 dB from 60 to 100 GHz. The presented Coplanar Waveguide-Coplanar Waveguide-Microstrip (CPW-CPW-MSTRIP) and CPW-CPW-CPW 3D transitions require strategically placed vias and tapering of the CPW ground planes in order to suppress radiation loss and optimize the performance over a very broad frequency range. These transitions are simple to realize and are compatible with low-cost substrate fabrication guidelines allowing for the easy feeding of embedded IC's in 3D modules, especially in compact automotive radar applications and beam-steering wideband antenna arrays. To estimate the effect of practical fabrication variations, a sensitivity analysis was performed against the lateral misalignment of via to pad defined by commercial fabrication houses and shows a worst case scenario of a 5dB RL and 0.2dB IL degradation compared to the ideal case. The measurement results of the reported transitions exhibit very good performance up to 100 GHz and show reasonable agreement with simulation.	Proceedings Paper	2009	10.1109/MWSYM.2009.5165856	[]	[]	-1	-1	-1
P	PATERSON S	Device for use with a system for measuring an            optical characteristic of a fluid comprises first light            pipe, which defines first light path running from first            pipe input to first pipe output, and second light pipe,            defines second light path						NOVELTY - The device (1000) comprises a first light pipe, which defines a first light path running from a first pipe input to a first pipe output. A second light pipe, defines a second light path separate from the first light path, running from a second pipe input to a second pipe output. A mount (320) is coupled to the first and second light pipes. The mount defines a separation of a first space from a second space. The first light pipe includes a first material with a first predetermined refractive index (Rl) value. The second light pipe includes a second material with a second predetermined value. A sensing portion of the first light pipe is arranged in the first space. The second light pipe is arranged entirely in the second space. USE - Device for use with a system (claimed) for measuring an optical characteristic of a fluid, such as aviation fuel, automotive fuel, diesel fuel. ADVANTAGE - The impact of the variability is minimized, and the performance is improved by the device. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for characterizing a fluid under test in a first space. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the device.Mediums (205)Fin (305)Input pipe portion (310)Mount (320)Active area (412)Device (1000)				[]	[]	-1	-1	-1
J	Wang, Feifan; Ju, Feng	Decomposition-based real-time control of multi-stage transfer lines with residence time constraints	2021	IISE TRANSACTIONS		943	959	It is commonly observed in the food industry, battery production, automotive paint shop, and semiconductor manufacturing that an intermediate product's residence time in the buffer within a production line is controlled by a time window to guarantee product quality. There is typically a minimum time limit reflected by a part's travel time or process requirement. Meanwhile, these intermediate parts are prevented from staying in the buffer for too long by an upper time limit, exceeding which a part will be scrapped or need additional treatment. To increase production throughput and reduce scrap, one needs to control machines' working mode according to real-time system information in the stochastic production environment, which is a difficult problem to solve, due to the system's complexity. In this article, we propose a novel decomposition-based control approach by decomposing a production system into small-scale subsystems based on domain knowledge and their structural relationship. An iterative aggregation procedure is then used to generate a production control policy with convergence guarantee. Numerical studies suggest that the decomposition-based control approach outperforms general-purpose reinforcement learning method by delivering significant system performance improvement and substantial reduction on computation overhead.	Article; Early Access		10.1080/24725854.2020.1803513	[]	[]	-1	-1	-1
J	Veeramani, Satheeshkumar; Muthuswamy, Sreekumar; Sagar, Keerthi; Zoppi, Matteo	Artificial intelligence planners for multi-head path planning of SwarmItFIX agents	2020	JOURNAL OF INTELLIGENT MANUFACTURING		815	832	Sheet metal manufacturing is finding wide applications in automotive and aerospace industries. Handling of giant sheet materials in manufacturing industries is one of the key problems. Utilization of robots, viz SwarmItFIX, will address this problem and automate the fixturing process, which greatly reduces lead time and thus the production cost. Implementation of intelligence into the robots will further improve efficiency in handling and reduce manufacturing inaccuracies. In this work, two different novel planners are proposed which do path planning for the heads of the SwarmItFIX agents. The environment of the problem is modeled as a Markov Decision Problem. The first planner uses the Value Iteration and Policy Iteration (PI) algorithms individually and the second planner performs the Monte Carlo control reinforcement learning. Finally, when the simulation is done and parameters of the proposed three algorithms along with existing Constraint Satisfaction Problem algorithm are compared with each other. It is observed that the proposed PI algorithm returns the plan much faster than the other algorithms. In the near future, the efficient planning model will be tested and implemented into the SwarmItFIX setup at the PMAR laboratory, University of Genoa, Italy.	Article	APR 2020	10.1007/s10845-019-01479-8	[]	[]	-1	-1	-1
P	CORNELISSEN H J; VDOVIN O V; VAN ASSELT R; VAN DER LUBBE M J; CALON G M; HIKMET R A M; VAN BOMMEL T; VAN DER LUBBE M J J	Lighting arrangement for use in e.g. luminaire,            has light transmissive element comprising dome-shaped            body with curved surface, where part of element light            escape part and luminescent material are radiationally            coupled						NOVELTY - The arrangement (1) has a device which comprises a luminescent material comprising element (100) and a light transmissive element (200), where the device includes a device axis (Al). The element comprises the material configured to emit a light upon irradiation with the light, where the element has a length (LI) and a characteristic dimension (Dl) perpendicular to the length. The element is configured at a non zero distance (rl) from the device axis, and the element partly surrounds the axis. The device axis intersects the element, and a portion of a curved surface comprises an element light entrance portion. USE - Lighting arrangement for luminaire (claimed) and spotlight. Can also be used for automotive application and light source with high CRI and relatively low correlated color temperature (CCT). ADVANTAGE - The luminescent material comprising element has the length and the characteristic dimension that are perpendicular to the device axis, thus providing a compact laser-based light generating device, and hence reducing the size and weight of the device. The configuration of the light transmissive element allows the device to be used in a compact manner, and reduces the size of the luminaire or spotlight, thus reducing the manufacturing cost of the luminaries. The arrangement provides a good color rendering and/or R9, and avoids the thermal quenching, and degradation of the phosphors. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an arrangement for luminaire and spotlight.Arrangement (1)Luminescent material comprising element (100)Element escape portion (102)Luminescent material (110)Light transmissive element (200)				[]	[]	-1	-1	-1
P	HUICI F; KUENZER S	Method for building optimized image for            application by e.g. internet-things based device,            involves deriving subsequent configuration file by            using machine learning algorithm based on performance            indicator, and building image for application						NOVELTY - The method (200) involves decomposing an operating system into granular modules. An initial configuration file is derived based on symbols that the application depends on. The initial configuration file is provided to a build system. An initial image including initial modules for the application is build by the build system based on the initial configuration file. Performance indicators for the initial image are gathered (206) by a monitoring system. A subsequent configuration file is derived by using a machine learning algorithm based on the performance indicators. A subsequent image for the application is built by the build system. The subsequent image is a uni-kernel and a specialized operating system image. The performance indicators comprise one of network performance, CPU performance, and memory performance. USE - Method for building an optimized image for an application by a computing system (claimed) e.g. internet-things (IoT) based device, embedded device and automotive industry system. ADVANTAGE - The method enables improving the performance of a computing system while minimizing development time, iteratively measuring the performance of a specialized operating system (OS) image and utilizing that information to drive a model that intelligently builds subsequent, better optimized versions of the image. The method enables adopting a reinforcement learning algorithm to drive a process that iteratively measures the performance of a specialized image and uses that information to drive a model that intelligently decides to build subsequent, perform versions of the image by selecting, which components of the OS and libraries to use. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computing system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a process for building a specialized operating system.Method for building an optimized image for an application by a computing system (200)Step for providing an initial configuration to a build system and a model (202)Step for building an image (204)Step for gathering performance indicators for the image (206)Step for providing performance indicators to the model (208)				[]	[]	-1	-1	-1
P	BLEI M; JANSEN C; MERGENTHALER J; CAVALEIRO P; ROLAND K; ALLEF P; SILBER S; REUTER E; SILBER D S; ALLEF D P; ROLAND D K; CAVALEIRO D P; KATRIN R; BLEY M; MORGANTLER J	Improving speed of wetting of pulverulent            formulation constituents in coating composition and            obtaining improved homogeneous color appearance of dry            paint film hue by mixing rhamnolipid and/or            sophorolipid into coating composition						NOVELTY - The speed of wetting of pulverulent formulation constituents in a coating composition is improved, and improved homogeneous color appearance of a dry paint film hue is obtained by mixing at least one rhamnolipid (RL) and/or at least one sophorolipid into a coating composition. USE - The method is useful for improving the speed of wetting of pulverulent formulation constituents in a coating composition and obtaining improved homogeneous color appearance of a dry paint film hue, where coating composition is a paint or coating material selected from interior wall paint, exterior paint, architectural paint, floor coating, wood paint, industrial paint, automotive original equipment manufacturing (OEM) or refinishing paint, primer, primer-surfacer, basecoat, and topcoat (all claimed). ADVANTAGE - The use of RLs and/or sophorolipids in coating compositions leads to a reduction in dust nuisance during incorporation and to the increase in occupational safety in the production of coating compositions, with assurance or improvement of the retention of a homogeneous color appearance of the dry coating film. The dry coating film, through the use of RLs and/or sophorolipids, has an improved homogeneous colour appearance with regard to difference in hue, ascertained from the delta E, and visual assessment, compared to a conventional coating film without RLs and/or sophorolipids. A further advantage is the shortening of the time taken to introduce the particles into the application medium, which translates to optimization of the process duration.				[]	[]	-1	-1	-1
P	DESCHAMPS H; GRANJOUX M; MARTEAU B	Crossed-coil digital logometer controller with            corrective interpolator converts input frequency into            digital signal by counting clock pulses during input            period for programmable division										[]	[]	-1	-1	-1
J	Xu, Bin; Shi, Junzhe; Li, Sixu; Li, Huayi; Wang, Zhe	Energy consumption and battery aging minimization using a Q-learning strategy for a battery/ultracapacitor electric vehicle	2021	ENERGY				Propulsion system electrification revolution has been undergoing in the automotive industry. The electrified propulsion system improves energy efficiency and reduces the dependence on fossil fuel. However, the batteries of electric vehicles experience degradation process during vehicle operation. Research considering both battery degradation and energy consumption in battery/ultracapacitor electric vehicles is still lacking. This study proposes a Q-learning-based strategy to minimize battery degradation and energy consumption. Besides Q-learning, two rule-based energy management methods are also proposed and optimized using Particle Swarm Optimization algorithm. A vehicle propulsion system model is first presented, where the severity factor battery degradation model is considered and exper-imentally validated with the help of Genetic Algorithm. In the results analysis, Q-learning is first explained with the optimal policy map after learning. Then, the result from a vehicle without ultra-capacitor is used as the baseline, which is compared with the results from the vehicle with ultracapacitor using Q-learning, and two rule-based methods as the energy management strategies. At the learning and validation driving cycles, the results indicate that the Q-learning strategy slows down the battery degradation by 13-20% and increases the vehicle range by 1.5-2% compared with the baseline vehicle without ultracapacitor. (c) 2021 Elsevier Ltd. All rights reserved.	Article; Early Access		10.1016/j.energy.2021.120705	[]	[]	-1	-1	-1
J		Assuring Safety and Trustworthiness of Deep Neural Networks by Making Them More Interpretable	2018					Deep reinforcement learning (DRL) is one of the key techniques leading to the current wave of artificial intelligence. However, the concerns on its safety and trustworthiness have been raised, particularly when it is applied to safety critical applications such as autonomous driving cars and un-manned aerial vehicles (UAVs). Additional to ensure its functional correctness, techniques are urgently needed to explain the black-box model and its decision making. An interpretable model can significantly improve human operators' trust on the DRL model. In this project, we aim to develop a set of interpretability techniques for DRL models. We will start by developing novel adversarial attacks for DRL models to study their robustness, and then investigate how to utilise the obtained adversarial examples for interpretability. The success of this project may significantly improve the safety and trustworthiness of the DRL models, and enable its wide applications to the automotive and aerospace industries in the UK. EPSRC research area: artificial intelligence techniques	Awarded Grant	Sep 30 2018		[]	[]	-1	-1	-1
J	Castano, Fernando; Strzelczak, Stanislaw; Villalonga, Alberto; Haber, Rodolfo E.; Kossakowska, Joanna	Sensor Reliability in Cyber-Physical Systems Using Internet-of-Things Data: A Review and Case Study	2019	REMOTE SENSING				Nowadays, reliability of sensors is one of the most important challenges for widespread application of Internet-of-things data in key emerging fields such as the automotive and manufacturing sectors. This paper presents a brief review of the main research and innovation actions at the European level, as well as some on-going research related to sensor reliability in cyber-physical systems (CPS). The research reported in this paper is also focused on the design of a procedure for evaluating the reliability of Internet-of-Things sensors in a cyber-physical system. The results of a case study of sensor reliability assessment in an autonomous driving scenario for the automotive sector are also shown. A co-simulation framework is designed in order to enable real-time interaction between virtual and real sensors. The case study consists of an IoT LiDAR-based collaborative map in order to assess the CPS-based co-simulation framework. Specifically, the sensor chosen is the Ibeo Lux 4-layer LiDAR sensor with IoT added capabilities. The modeling library for predicting error with machine learning methods is implemented at a local level, and a self-learning-procedure for decision-making based on Q-learning runs at a global level. The study supporting the experimental evaluation of the co-simulation framework is presented using simulated and real data. The results demonstrate the effectiveness of the proposed method for increasing sensor reliability in cyber-physical systems using Internet-of-Things data.	Review	OCT 2019	10.3390/rs11192252	[]	[]	-1	-1	-1
P	KUNIYOSHI F; EGUCHI T	Manufacture of rare-earth-transition-metal-boron            sintered magnet for motor, involves heating sintered            magnet material containing rare earth element and            transition-metal comprising iron, cobalt, aluminum,            manganese or silicon and iron, and alloy and diffusing            metal in alloy into magnet material						NOVELTY - Manufacture of rare-earth-transition-metal-boron sintered magnet involves preparing a rare-earth-transition-metal-boron sintered magnet material comprising rare earth element comprising at least one selected from neodymium, praseodymium and cerium, transition-metal comprising at least one iron, cobalt, aluminum, manganese, and silicon and iron, preparing an alloy comprising metal (R1) comprising metal (RH) comprising terbium and dypsorium and rare earth element (RL) other than metal (RH), comprising at least one of neodymium, praseodymium and lanthanum, and metal (M) comprising at least one of boron and carbon and at least one selected from aluminum, copper, zinc, gallium, iron, cobalt and nickel, heating the sintered magnet material and the alloy, and diffusing metal (R1) and metal (M) into the sintered magnet material. In the alloy, the content of metal (R1) is 70-95 %mass or less of the entire alloy. The content of lanthanum in metal (R1) is 5-40%, USE - Manufacture of rare-earth-transition-metal-boron sintered magnet. Uses include but are not limited to motors for automotive field such as electric vehicles, hybrid vehicle and plug-in hybrid electric vehicle, wind power generation applications and home appliances. ADVANTAGE - The prepared rare-earth-transition-metal-boron sintered magnet has high residual magnetic flux density and high coercive force while using lanthanum as a diffusion source. DETAILED DESCRIPTION - Manufacture of rare-earth-transition-metal-boron sintered magnet involves preparing a rare-earth-transition-metal-boron sintered magnet material comprising rare earth element comprising at least one selected from neodymium, praseodymium and cerium, transition-metal comprising at least one iron, cobalt, aluminum, manganese, and silicon and iron, preparing an alloy comprising metal (R1) comprising metal (RH) comprising terbium and dypsorium and rare earth element (RL) other than metal (RH), comprising at least one of neodymium, praseodymium and lanthanum, and metal (M) comprising at least one of boron and carbon and at least one selected from aluminum, copper, zinc, gallium, iron, cobalt and nickel, heating the sintered magnet material and the alloy at 700-1100&#8451; in a vacuum or inert gas atmosphere, and diffusing metal (R1) and metal (M) into the sintered magnet material. In the alloy, the content of metal (R1) is 70-95 %mass or less of the entire alloy. The content of lanthanum in metal (R1) is 5-40%, and the content of metal (M) is 5-30 %mass or less of the entire alloy. The total content of boron and carbon in metal (M) is 5-20%.				[]	[]	-1	-1	-1
J		Event Triggered Unknown Networked Control System Design by using Adaptive Dynamic Programming	2014					This project will develop new universal control designs for Network Control Systems (NCS) such as the new distributed control systems needed for the smart grid and modern automotive systems. Unlike many traditional NCS systems, these will be designed from the start so as to maximize performance over time, fully accounting for communication delays in networks and unexpected events, as well as nonlinearities, continuous random disturbances and other issues which this PI has addressed in the past. He intends to develop rigorous mathematical proofs that these new designs will always lead to stable operation. He will also maintain a pipeline to immediate practical applications, through his existing Industry-University Center on Intelligent Maintenance. The designs are intended to be general, learning-based and massively parallel, in a way which may also help us understand how mammal brains can perform tasks beyond the scope of more traditional types of design.<br/><br/>These designs will be an extension of work previously funded by NSF, described in the Handbook of RLADP, edited by Lewis and Liu. ADP, like linear programming, is a class of challenges, not just one specific method, though there are relations between the successful methods. ADP includes all the methods aimed at the general problem of multistage optimization in the face of nonlinearity and stochastic disturbance. In order to cope with general nonlinear tasks, this project will include neural networks which have been proved to be more effective as approximators of general nonlinear functions than more traditional systems.  The chief novelty here is the ability to combine these capabilities with the presence of communication delays and random discrete events, which are an important practical issue in most NCS.	Awarded Grant	Aug 01 2014		[]	[]	-1	-1	-1
J	Wang, Chun; Liu, Fengchen; Tang, Aihua; Liu, Rui	A dynamic programming-optimized two-layer adaptive energy management strategy for electric vehicles considering driving pattern recognition	2023	JOURNAL OF ENERGY STORAGE				To achieve optimal real-time power allocation in electric vehicles, a two-layer adaptive dynamic programming (DP) optimization energy management strategy (EMS) has been proposed. The upper layer uses learning vector quantization (LVQ) to produce real-time driving pattern recognition (DPR) results. The method determines 10 characteristic parameters for training the recognition network and the length of the sampling window is 120 s. The typical driving cycles are divided into different levels of blocks to identify the real-time DPR level. The lower layer adopts the optimization strategy of DP extraction to adjust the power distribution between the battery pack and the supercapacitor pack according to the recognition results. DP is used to define a cost function to minimize the energy loss of the hybrid energy storage system (HESS) and optimize the battery usage range in the system. The near-optimal real-time EMS is extracted by analyzing the DP control behavior of the battery under the layered state of charge (SOC). The simulation results indicate that the proposed new rule control based on DP optimization (NRB) EMS improves the system efficiency by 10 % compared with the original rule-based (RB) EMS under different temperatures and DPR levels. In addition, the system efficiency gap is controlled at 3 % compared with DP.	Article; Early Access		10.1016/j.est.2023.107924	[]	[]	-1	-1	-1
J	Tambon, Florian; Laberge, Gabriel; An, Le; Nikanjam, Amin; Mindom, Paulina Stevia Nouwou; Pequignot, Yann; Khomh, Foutse; Antoniol, Giulio; Merlo, Ettore; Laviolette, Francois	How to certify machine learning based safety-critical systems? A systematic literature review	2022	AUTOMATED SOFTWARE ENGINEERING				"Context Machine Learning (ML) has been at the heart of many innovations over the past years. However, including it in so-called ""safety-critical"" systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches. Objective This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question ""How to Certify Machine Learning Based Safety-critical Systems?"". Method We conduct a Systematic Literature Review (SLR) of research papers published between 2015 and 2020, covering topics related to the certification of ML systems. In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted. Results The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of ML models. It also emphasized the need to further develop connections between academia and industries to deepen the domain study. Finally, it also illustrated the necessity to build connections between the above mentioned main pillars that are for now mainly studied separately. Conclusion We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions."	Review	NOV 2022	10.1007/s10515-022-00337-x	[]	[]	-1	-1	-1
B	Hovgard, Mattias	Energy Reduction of Robot Stations with Uncertainties	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Liorni, Ilaria; Lisewski, Tomasz; Capstick, Myles H.; Kuehn, Sven; Neufeld, Esra; Kuster, Niels	Novel Method and Procedure for Evaluating Compliance of Sources With Strong Gradient Magnetic Fields Such as Wireless Power Transfer Systems	2020	IEEE TRANSACTIONS ON ELECTROMAGNETIC COMPATIBILITY		1323	1332	To date, the development of valid and globally universally accepted recognized methods for the accurate exposure assessment of wireless power transfer (WPT) technologies is lagging behind the rapid emergence of high power systems in the energy and automotive sectors. WPT systems based on inductive and magnetic resonance technologies generate strong but rapidly decaying magnetic fields, which often exceed reference levels (RL) in the immediate vicinity of the WPT coils by up to a factor of >100. Compliance testing of WPT systems with limits derived for homogeneous exposure can lead to estimations of exposure that exceed by up to 40 dB assessments of the fields induced in the human body, i.e., the basic restrictions (BR) defined by the international safety guidelines. Testing compliance with the BR is impractical for regulatory purposes due to the high costs of resources of determining the maximum exposure conditions. This paper presents a novel compliance testing method that mitigates the overestimation of the exposure while maintaining the simplicity of the testing procedure. This is achieved by using coupling transformation functions to correlate not only the amplitude and frequency but also the gradient of the incident field with the BR. These novel conservative coupling functions have been determined by means of a large scale numerical study in the frequency range 3 kHz-10 MHz supported by a physics-based approximation. The here proposed compliance testing method is still conservative in comparison to the compliance toward BR of localized sources. However, in comparison to today's practice of applying RL directly, the overestimation is strongly reduced for high gradient fields (G(n) > 50 T/m/T), e.g., by more than 3000 times for field gradients of about 200 T/m/T. We have validated the method by numerical analysis of human exposure to actual WPT sources. The adoption of the new method will help to accelerate the introduction of high-power wireless charging devices in the global market.	Article	AUG 2020	10.1109/TEMC.2019.2924519	[]	[]	-1	-1	-1
B	Zhang, Tyler	Integrated Corridor Management for Connected Vehicles and Park and Ride Structures	2023						Dissertation/Thesis	Jan 01 2023		[]	[]	-1	-1	-1
B	Xu, J.; Ma, J.	Auto Parts Defect Detection Based on Few-shot Learning	2022	2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)		943	6	In the process of auto parts products from manufacturing to quality inspection, it is inevitable that there will be defects on the surface due to aging of machine tools or human factors. How to better apply computer vision technology to defect detection of auto parts is still an issue discussed by the industry and academia. For the condition that sufficient labeled samples is usually not available in the industrial field, this paper uses an improved few-shot metric learning model to identify auto parts samples. According to the traditional few-shot learning models lack of the considerations of channel level and internal relations of support set, this paper adds ECA module and Transformer module to few-shot learning. Add more attention to the key information of support set and query set samples at channel level while increasing the attention to the intrinsic feature of support set, to improve the classification effect.	Conference Paper	2022	10.1109/CVIDLICCEA56201.2022.9823993	[]	[]	-1	-1	-1
C	Garg, Prasoon; Silvas, Emilia; Willems, Frank	Potential of Machine Learning Methods for Robust Performance and Efficient Engine Control Development	2021	IFAC PAPERSONLINE		189	195	Increasingly strict legislation for greenhouse gas and real-world pollutant emissions makes it necessary to develop fuel-efficient and robust control solutions for future automotive engines. Today's engine control development relies on traditional map-based and model-based control approaches. Due to growing system complexity and real-world requirements, these expert-intensive and time-consuming approaches are facing a turning point, which will lead to unacceptable development time and costs in the near future. Artificial Intelligence (AI) is a disruptive technology, which has interesting features that can tackle these challenges. AI-based methods have received growing interest due to the increasing availability of data and the success of AI applications for complex problems. This paper presents an overview of the state-of-the-art in Machine Learning (ML)-based methods that are applied for engine control development with focus on the time-consuming calibration process. The overview here shows that the vast majority of studies concentrates on regression modelling to model complex processes, to reduce the number of model parameters and to develop real-time, ECU implementable models. The identified promising directions for future ML-based engine control research include the application of reinforcement learning methods to on-line optimize engine performance and guarantee robust performance and unsupervised learning methods for data quality monitoring. Copyright (C) 2021 The Authors.	Proceedings Paper; Early Access		10.1016/j.ifacol.2021.10.162	[]	[]	-1	-1	-1
P	MEHR E; ELLIS T; NOONE J	Method for real-time adaptive control of            post-design free form deposition process or post-design            joining process, involves providing instructions to            perform post-design free form deposition process or            post-design joining process						NOVELTY - The method involves providing an input design geometry for an object. A processor is programmed to predict an optimal set of process control parameters for initiating a free form deposition process or joining process. The predicted optimal set of process control parameters are derived using a machine learning algorithm. The processor is programmed to remove noise from the object property data provided by the sensors providing it to the machine learning algorithm. The processor is programmed to provide instructions to perform the post-design free form deposition process or post-design joining process to fabricate the object. The machine learning algorithm adjusts the process control parameters in real-time while physically performing the free form deposition process or the joining process. USE - Method for real-time adaptive control of post-design free form deposition process or post-design joining process used in automotive industry, aeronautics industry, medical device industry and consumer electronics industry. ADVANTAGE - The method for automated object defect classification and adaptive real-time control of additive manufacturing and welding apparatus can provide for rapid optimization and adjustment of the process control parameters used in response to changes in process or environmental parameters, thus, the improved process yield, process throughput, and quality of the parts that are produced. The dimensions of the individual layers can be accurately predicted. The method automatically adjusts the layer thickness to improve the accuracy of the fabricated part or to improve the build time. The post-process machining remove the extra materials and improve the dimensional accuracy of the part requires extra time and adds to the cost. The adaptive medial axis transformation (MAT) path planning algorithms are able to automatically generate path patterns with varying step-over distances by analyzing the part geometry to achieve better part quality, accuracy at the boundary, and efficient use of material. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for controlling a post-design free form deposition process or a post-design joining process. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of an action prediction-reward loop for a reinforcement learning algorithm.				[]	[]	-1	-1	-1
B	McMurrough, Christopher Dale	Real time hardware and software systems for micro air vehicle flight control testing	2010						Dissertation/Thesis	Jan 01 2010		[]	[]	-1	-1	-1
P	LASAGNI A F; BIEDA M	Optical arrangement for laser interference structuring of sample, has positioning mirrors which are arranged for reflecting beams on common target point so that optical path length of beams between beam splitter and common target is same						NOVELTY - The arrangement has a beam splitter (1), on which a laser beam along main optical axis (H) is radiated to the arrangement and is divided into transmitted beam portion (tl) and reflected beam portion (rl). A lower positioning mirror (2t) is arranged in the beam path of transmitted beam portion and upper positioning mirror (2r) is arranged in beam path of reflected beam portion. The positioning mirrors are arranged and aligned for reflecting the two beams on a common target point (Z) so that optical path length of two beams between beam splitter and common target is the same. USE - Optical arrangement for laser interference structuring of sample such as metal substrate, plastic substrate or semiconductor substrate used in solar industry, automotive industry, medical industry and aircraft industry. ADVANTAGE - The optical arrangement avoids the reduction in structuring efficiency of asymmetrical and/or inhomogeneous structures, so that improved interference structuring can be attained. The individual optical elements used for guiding the beam of array are positioned and aligned so that time difference between the laser pulses of two partial beams, which impinge on the sample, is avoided and no geometrical path difference of the two partial beams exists. The individual mirrors are arranged in such a way that optical path difference between the two partial beams is avoided. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method for laser interference structuring of sample. DESCRIPTION Of DRAWING(S) - The drawing shows a plan view of the optical arrangement.Beam splitter (1)Upper positioning mirror (2r)Lower positioning mirror (2t)Main optical axis (H)Common target point (Z)Reflected beam portion (r1)Transmitted beam portion (t1)				[]	[]	-1	-1	-1
B	Mustapha, N.A.C.; Zahirul Alam, A.H.M.; Khan, S.; Azman, A.W.	Parametric analysis of single boost converter for energy harvester	2015	2015 IEEE 3rd International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)		4 pp.	4 pp.	This paper presents a conventional DC-DC boost converter for low and wide voltage supply range, suitable for energy harvesting purpose. The output voltage can be increased by controlling the transistor switching frequency, duty cycle, inductance, load capacitor, rise, and fall time. Both computer simulation and experiment results are performed in details. Experiment results have shown an error less than 6 % with the simulation. A linear trend of output voltage in the range of 4 V to 49 V is successfully converted from 100 mV to 1.5 V input voltage using low switching frequency of 2 kHz. The circuit parameter for this voltage range are L = 100 muH, D = 50 %, tr = tf = 2.9 mus considering CL= 10 muF, and RL= 10 kOmega. This circuit is suitable for medium voltage range application such as in automotive, aircraft, industry, and wireless measurement system.	Conference Paper	2015	10.1109/ICSIMA.2015.7559014	[]	[]	-1	-1	-1
J	Kapoor, Rahul; Wilde, Daniel	Peering into a crystal ball: Forecasting behavior and industry foresight	2023	STRATEGIC MANAGEMENT JOURNAL		704	736	Research Summary What makes some managers and entrepreneurs better at forecasting the industry context than others? We argue that, regardless of experience or expertise, a learning-based forecasting behavior in which individuals attend to and incorporate new relevant information from the environment into an updated belief that aligns with the Bayesian belief updating process is likely to generate superior industry foresight. However, the effectiveness of such a cognitively demanding process diminishes under high levels of uncertainty. We find support for these arguments using an experimental design of forecasting tournaments in the managerially relevant context of the global automotive industry from 2016 to 2019. The study provides a novel account of individual-level forecasting behavior and its effectiveness in an evolving industry and suggests important implications for managers and entrepreneurs. Managerial Summary How a focal industry will evolve is a key forecasting problem faced by managers and entrepreneurs as they seek to identify opportunities and make strategic decisions. However, developing superior industry foresight in the face of significant change, and limited and often contradictory information, can be especially challenging. We study how individuals forecast the ongoing transformation of the global automotive industry with respect to electrification and autonomy, using a novel research design of forecasting tournaments. A forecasting process in which individuals update their beliefs by neither ignoring prior information nor overacting to new information helps to generate superior industry foresight. There was a significant penalty to forecasting accuracy when individuals did not update their beliefs at all, or when they updated, but overreacted to new information.	Article; Early Access		10.1002/smj.3450	[]	[]	-1	-1	-1
J	Go Takami	AI-based plant control	2020	Yokogawa Technical Report (English Edition)		33	6	Expectations for machine learning and artificial intelligence (AI) are growing globally and related investment has been increasing in a diverse range of businesses. Machine learning is used for autonomous car driving and robot control, and the use of its applications is rapidly increasing in factory automation (FA). On the other hand, practical process control techniques based on machine learning and AI have not yet been developed for process automation (PA) although data analysis using process data is becoming more common. PID control still remains the main technique and advanced control techniques by experts are used when complicated control is required. Not to simulate but to apply machine learning technology to actual equipment, we performed AI-based control of a three-tank-level control system, which is a popular educational kit for process control. This paper introduces the details of the experiment and the machine learning technology used to control this system.	Journal Paper	2020		[]	[]	-1	-1	-1
B	Zhou, Yanlin	Modern Autonomous Driving	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J	Manimuthu, Arunmozhi; Venkatesh, V. G.; Shi, Yangyan; Sreedharan, V. Raja; Koh, S. C. Lenny	Design and development of automobile assembly model using federated artificial intelligence with smart contract	2022	INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH		111	135	With smart sensors and embedded drivers, today's automotive industry has taken a giant leap in emerging technologies like Machine learning, Artificial intelligence, and the Internet of things and started to build data-driven decision-making strategies to compete in global smart manufacturing. This paper proposes a novel design framework that uses Federated learning-Artificial intelligence (FAI) for decision-making and Smart Contract (SC) policies for process execution and control in a completely automated smart automobile manufacturing industry. The proposed design introduces a novel element called Trust Threshold Limit (TTL) that helps moderate the excess usage of embedded equipment, tools, energy, and cost functions, limiting wastages in the manufacturing processes. This research highlights the use cases of AI in decentralised Blockchain with smart contracts, the company's trading policies, and its advantages for effectively handling market risk assessments during socio-economic crisis. The developed model supported by real-time cases incorporated cost functions, delivery time and energy evaluations. Results spotlight the use of FAI in decision accuracy for the developed smart contract-based Automobile Assembly Model (AAM), thereby qualitatively limiting the threshold level of cost, energy and other control functions in procurement assembly and manufacturing. Customisation and graphical user interface with cloud integration are some challenges of this model.	Article; Early Access		10.1080/00207543.2021.1988750	[]	[]	-1	-1	-1
J	Zheng, Jinkai; Zhang, Yao; Luan, Tom H.; Mu, Phil K.; Li, Guanjie; Dong, Mianxiong; Wu, Yuan	Digital Twin Enabled Task Offloading for IoVs: A Learning-Based Approach	2024	IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING		659	672	This article explores the optimal offloading strategy in the Internet of Vehicles (IoVs), which is challenged by three issues. First, the resources of edge servers are shared by multiple vehicles, leading to random changes over time. Second, as a vehicle would drive across consecutive edge servers, the offloading strategy needs to consider the overall edge resources along the trip. Third, at each vehicle, the computing tasks arrive continuously when driving. This dictates the offloading strategy to consider not only the current status but also the futuristic computing tasks. To tackle these issues, we propose a digital twin (DT) network framework. A DT network maintains DTs in the cyber-space to synchronize the real-world activities of vehicles. Therefore, task offloading decisions can be benefited by combining both the global information aggregated from neighbor twins and historical information uploaded by vehicles. With comprehensive information, the optimal offloading strategy can be determined. We characterize the offloading problem as a Markov Decision Process (MDP) and develop an A3C-based decision-making algorithm, which can learn optimal offloading actions that minimize the long-term system costs. Extensive experiments demonstrate the performance of our proposal in terms of fast convergence and low system costs when compared with other approaches.	Article	JAN 2024	10.1109/TNSE.2023.3303461	[]	[]	-1	-1	-1
J	Li, Han; Liu, Zhao; Zhu, Ping	An improved multi-objective optimization algorithm with mixed variables for automobile engine hood lightweight design	2021	JOURNAL OF MECHANICAL SCIENCE AND TECHNOLOGY		2073	2082	Engine hood is one of the important parts of the vehicles, which has influences on the lightweight, structural safety, pedestrian protection, and aesthetics. The optimization design of engine hood is a high-dimensional, multi-objective, and mixed-variable optimization problem. In order to reduce the physical test investment in the development and improve the efficiency of optimization, this article proposes a data-driven method for optimal hood design. A newly proposed single-objective optimization algorithm is improved by several strategies for multi-objective constrained problem with mixed variables. Then the hood is optimized through the specially designed machine learning model. Finally, both the hood's weight and pedestrian injury are reduced while maintaining structural stiffness and frequency in the desired range. The comparative study and final hood optimization results prove the effectiveness of the proposed method.	Article; Early Access		10.1007/s12206-021-0423-5	[]	[]	-1	-1	-1
B	S, N.F.; Naeemi, S.; Shamchi, S.H.; Nahvi, A.	Autonomous Merging onto the Highway Using LSTM Neural Network	2023	2023 11th RSI International Conference on Robotics and Mechatronics (ICRoM)		574	9	Merging onto highways is a complex task for autonomous vehicles and can cause catastrophic crashes, especially when considering the high speeds of other vehicles in the highway. While methods like path tracking and reinforcement learning are commonly used to execute this task, they may not accurately imitate the behavior of an expert driver and simultaneously maintain accuracy, safety and passenger comfort. In this paper, we present an autonomous driver model that uses an LSTM neural network trained on data gathered from two expert drivers executing the maneuver in a driving simulator. We extracted five features from the dataset, and used them to train the neural network model. The model approximates the proper steering wheel angle at each time step and feeds it to a PD controller which sets the physical steering wheel angle by managing the input voltage of a DC motor mounted on the driving simulator's steering rod. The neural network model achieved an R2 score of 0.82057 and successfully guided the vehicle to merge onto the highway and reach the second lane without collisions in multiple trials in the simulator. The results of this research shows the potential of using expert driver data and neural networks to improve the accuracy and safety of merging onto highways in autonomous vehicles. As autonomous vehicles become more prevalent in the automotive industry, it is essential to develop reliable and safe highway-merging strategies. Our model provides a promising solution to this challenge and can be further developed and integrated into autonomous vehicles to enhance their capabilities and ensure safe and efficient merging onto highways.	Conference Paper	2023	10.1109/ICRoM60803.2023.10412407	[]	[]	-1	-1	-1
B	Bouton, Maxime	Safe and Scalable Planning Under Uncertainty for Autonomous Driving	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
B	Mohanti, Subhramoy	Distributed Beamforming with Unmanned Vehicles and Edge Network Resource Orchestration for Wireless IoT : A Systems Perspective	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Lobo, Deepan	Outdoor Operations of Multiple Quadrotors in Windy Environment	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Mustapha, N.A.C.; Zahirul Alam, A.H.M.; Khan, S.; Azman, A.W.	Parametric sweep analysis of medium voltage range boost converter for energy harvester application	2015	2015 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE)		1	4	This paper presents a parametric sweep analysis discussion on proposed DC-DC boost converter circuit for low and wide voltage supply range. Analysis is initially done using computer simulation and then tested with experimental work. Results are combined and discussed in details. In this work, effect of parameter such as input voltage, switching frequency and inductance is presented in details. A linear conversion has been observed in this work. Low DC input voltage of 100 mV to 1.5 V is used and successfully converts to up to 50 V in linear inclination, considering CL = 10 muF, and RL = 10 kOmega. The circuit parameter for this voltage range are L = 100 muH, D = 50 %, and 2 kHz frequency operation. This circuit can be used for energy harvesting purpose and medium voltage application such as aircraft, wireless measurement system and automotive.	Conference Paper	2015	10.1109/WIECON-ECE.2015.7443887	[]	[]	-1	-1	-1
P	QIN K; SU Y; JIA J; CAO T; LI Z; LIAO C; LIAN M	Method for designing PEEK resin gradient honeycomb            wave absorbing structure based on equivalent            electromagnetic parameter analysis used in aircraft            service environment involves constructing wave            absorption structure equivalent electromagnetic            parameter analysis model based on symmetrical            structure						NOVELTY - The method involves constructing a wave absorption structure equivalent electromagnetic parameter analysis model based on a symmetrical structure according to a transmission line theory and microwave theory. A bottom metal plate in a traditional wave absorption structure is cancelled in the equivalent electromagnetic parameter analysis model. A bottom surface of the wave-absorbing structure is used as a reference. The air domain and the wave absorbing structure are spatially mirrored symmetrical. The top part of the lower air area is collected. An electromagnetic simulation model of a symmetrical gradient honeycomb wave absorbing structure is established. A reflection coefficient is obtained in scattering coefficient and transmission coefficient. USE - Method for designing PEEK (Colorless organic polymer thermoplastic) resin gradient honeycomb wave absorbing structure based on equivalent electromagnetic parameter analysis used in aircraft service environment, and can also be used in automotive industry. ADVANTAGE - The reflection loss of the series PEEK (Colorless organic polymer thermoplastic) resin base gradient honeycomb wave absorbing structure is quickly solved with better absorbing effect, and the efficient wave-absorbing bandwidth is wider as the target optimization design PEEK (Colorless organic polymer thermoplastic)-reinforced epoxy resin gradient honey comb microwave absorbing structure. The experiment loss and high accuracy are avoided to obtain 2-18 GHz wave band efficient wave absorbing bandwidth is 14 GHz, and the reflection loss mean RL is -17.39dB. Thus, the method provides a universal efficient gradient structure equivalent electromagnetic parameter inversion method, and provides a better wave absorbing effect on 2-18 GHz wave band and wider efficient wave-absorbing bandwidth. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a gradient honeycomb wave-absorbing structure of the method for designing PEEK resin gradient honeycomb wave absorbing structure based on equivalent electromagnetic parameter analysis (Drawing includes non-English language text).				[]	[]	-1	-1	-1
J	Pamula, Teresa; Pamula, Danuta	Prediction of Electric Buses Energy Consumption from Trip Parameters Using Deep Learning	2022	ENERGIES				The energy demand of electric buses (EBs) is a very important parameter that should be considered by transport companies when introducing electric buses into the urban bus fleet. This article proposes a novel deep-learning-based model for predicting energy consumption of an electric bus traveling in an urban area. The model addresses two important issues: accuracy and cost of prediction. The aim of the research was to develop the deep-learning-based prediction model, which requires only the data readily available to bus fleet operators, such as location of the bus stops (coordinates, altitude), route traveled, schedule, travel time between stops, and to find the most suitable type and configuration of neural network to evaluate the model. The developed prediction model was assessed with different types of deep neural networks using real data collected for several bus lines in a medium-sized city in Poland. Conducted research has shown that the deep learning network with autoencoders (DLNA) neural network allows for the most accurate energy consumption estimation of 93%. The proposed model can be used by public transport companies to plan driving schedules and energy management when introducing electric buses.	Article	MAR 2022	10.3390/en15051747	[]	[]	-1	-1	-1
B	Reindorp, Matthew J.	Industrial flexibility in theory and practice	2009						Dissertation/Thesis	Jan 01 2009		[]	[]	-1	-1	-1
J	Sliwa, B.; Adam, R.; Wietfeld, C.	Client-based Intelligence for Resource Efficient Vehicular Big Data Transfer in Future 6G Network [arXiv]	2021	arXiv		15 pp.	15 pp.	"Vehicular big data is anticipated to become the ""new oil"" of the automotive industry which fuels the development of novel crowdsensing-enabled services. However, the tremendous amount of transmitted vehicular sensor data represents a massive challenge for the cellular network. A promising method for achieving relief which allows to utilize the existing network resources in a more efficient way is the utilization of intelligence on the end-edge-cloud devices. Through machine learning-based identification and exploitation of highly resource efficient data transmission opportunities, the client devices are able to participate in overall network resource optimization process. In this work, we present a novel client-based opportunistic data transmission method for delay-tolerant applications which is based on a hybrid machine learning approach: Supervised learning is applied to forecast the currently achievable data rate which serves as the metric for the reinforcement learning-based data transfer scheduling process. In addition, unsupervised learning is applied to uncover geospatially-dependent uncertainties within the prediction model. In a comprehensive real world evaluation in the public cellular networks of three German Mobile Network Operators (MNOs), we show that the average data rate can be improved by up to 223 % while simultaneously reducing the amount of occupied network resources by up to 89 %. As a side-effect of preferring more robust network conditions for the data transfer, the transmission-related power consumption is reduced by up to 73 %. The price to pay is an increased Age of Information (AoI) of the sensor data."	Journal Paper	17 Feb. 2021		[]	[]	-1	-1	-1
J	Xu, Fengxiang	On modelling strategy of the weld line for tailor-welded structures under quasi-static and dynamic scenarios	2016	INTERNATIONAL JOURNAL OF VEHICLE DESIGN		230	247	Tailor-welded blanks (TWBs) have been widely applied in the automotive industry owing to better balance between light weight and crashworthiness. The obvious feature of the TWBs is the existence of weld lines. This paper numerically compares the modelling strategies of the weld line under quasi-static and dynamic events. A novel method with crossover scheme is proposed. For the quasi-static tests, those existing modelling methods such as rigid links (RL), onefold beam (OB)/twofold beam (TB), shell (SH)/solid model (SN) and new crossover model are compared. Note that the SH/SN model is considered to be accurate and as the baseline model. Through the deformation analysis and equivalent plastic strains under the static scenario, it indicates that the novel scheme is superior to other modelling methods. Then, the method is further applied to dynamic impacting. Those results provide important information for numerical modelling especially when TWBs are fabricated through different materials with a relatively higher thickness ratio.	Article	2016	10.1504/IJVD.2016.10001630	[]	[]	-1	-1	-1
P	OBORIL F; PASCH F; SCHOLL K; BUERKLE C; NATARAJAN V	Machine-readable medium for facilitating reinforced learning by using safety factor of robot in e.g. robotic datacenters, has set of instructions for updating robot failure count corresponding to robot in response to determining that simulated action does not match robot action						NOVELTY - The medium has a set of instructions for collecting activity data for a robot (802), where the robot is operated according to a path control plan generated using reinforcement learning with a safety factor as a reward function. A detection is made to check whether a safety event with a robot action is occurred with the robot and an object (804). A recreation of the safety event is simulated (806) to determine whether a simulated action matches the robot action using activity data. A robot failure count corresponding to the robot is updated (808) in response to determining that the simulated action does not match the robot action. USE - Machine-readable medium for facilitating reinforced learning by using a safety factor of a robot in various applications. Uses include but are not limited to smart manufacturing assembly lines, multi-robot automotive component assemblies, computer and consumer electronics fabrications, smart retail and warehouse logistics, and robotic datacenters. ADVANTAGE - The medium enables utilizing a cloud computing as a model of service delivery for enabling convenient on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or interaction with a provider of service. The medium allows a cloud consumer to unilaterally provision computing capabilities such as server time and network storage without requiring human interaction with a service`s provider. A cloud computing environment is allowed to offer infrastructure, platforms and/or software as services for which the cloud consumer does not need to maintain resources on a local computing device. The capabilities are rapidly and elastically provisioned and rapidly released to rapidly scale in. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for: (1) a device for facilitating reinforced learning by using a safety factor of a robot in a network; (2) an apparatus for facilitating reinforced learning by using a safety factor of a robot. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a process for facilitating reinforced learning by using a safety factor.Step for collecting activity data for robot (802)Step for detecting that key event with robot action is occurred with robot and object (804)Step for simulating recreation of key event using telemetry data (806)Step for updating robot failure count corresponding to robot in response to determining that simulated action does not match robot action (808)Step for storing or outputting updated robot failure count (810)				[]	[]	-1	-1	-1
B	Li, Huayi	Advances in Game-Theoretic, Set-Theoretic and Optimal Control to Enhance Mobility	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Alotaibi, Jamal	SDN-Enabled Efficient Resource Utilization in a Secure, Trustworthy and Privacy Preserving IOV-Fog Environment	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Hao, Liangliang	Stochastic Optimal Control based Reinforcement Learning for Electric Vehicle Charging and Residential Demand Response: Efficiency and Scalability	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Patel, M. S. Sunitha; Srinath, S.	Improved Spatial Invariance for Vehicle Platoon Application using New Pooling Method in Convolution Neural Network	2022	INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS		469	476	imbalanced dataset is a prominent concern for automotive deep learning researchers. The proposed work provides a new mixed pooling strategy with enhanced performance for imbalanced vehicle dataset based on Convolution Neural Network (CNN). Pooling is crucial for improving spatial invariance, processing time, and overfitting in CNN architecture. Max and average pooling are often utilized in contemporary research articles. Both techniques of pooling have their own advantages and disadvantages. In this study, the advantages of both pooling algorithms are evaluated for the classification of three vehicles: car, bus, and truck for imbalanced datasets. For each epoch, the performance of max pooling, average pooling, and the new mixed pooling method was assessed using ROC, F1-score, and error rate. Comparing the performance of the max-pooling method to that of the average pooling method, it has been found that the max-pooling method is superior. The performance of the proposed mixed pooling approach is superior to that of the maximum pooling and average pooling methods. In terms of Receiver Operating Characteristics (ROC), the proposed mixed pooling technique is approximately 2 per cent better than the maximum pooling method and 8 per cent better than the mixed pooling method. Using a new pooling technique, the classification performance with an imbalanced dataset is improved, and also a novel mixed pooling method is proposed for the classification of vehicles.	Article	JUL 2022		[]	[]	-1	-1	-1
B	Mustapha, N.A.C.; Alam, A.H.M.Z.; Khan, S.; Azman, A.W.	Boost converter for low voltage energy harvesting applications: Basic component selection	2013	2013 IEEE International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)		4 pp.	4 pp.	Regulating energy from tiny sources such from ambient vibration to supply a continuous energy to power an electronics device is a beneficial and challenging work to do. Having very small energy usually less than 1 V, needs a boost circuit to power up the device. This paper shows that the passive components of a conventional boost converter play an important role to boost very small voltage. This paper also presents that switching frequency effects to boost voltage. Transient analyses have been performed using PSpice simulation tool to check the circuit response to various parameter changes. The proposed circuit with components L=160 muH, RL=10 kOmega and switching frequency fS=1 kHz with duty cycle D=0.5 results optimum performance of the circuit. This voltage converter is suitable for vibration energy harvesting having low frequency applications in kHz range. The application of this converter is expected to be in automotive, healthcare and industrial field.	Conference Paper	2013	10.1109/ICSIMA.2013.6717922	[]	[]	-1	-1	-1
J	Kirchgaessner, Wilhelm; Wallscheid, Oliver; Boecker, Joachim	Data-Driven Permanent Magnet Temperature Estimation in Synchronous Motors With Supervised Machine Learning: A Benchmark	2021	IEEE TRANSACTIONS ON ENERGY CONVERSION		2059	2067	Monitoring the magnet temperature in permanent magnet synchronous motors (PMSMs) for automotive applications is a challenging task for several decades now, as signal injection or sensor-based methods still prove unfeasible in a commercial context. Overheating results in severe motor deterioration and is thus of high concern for the machine's control strategy and its design. Lack of precise temperature estimations leads to lesser device utilization and higher material cost. In this work, several machine learning (ML) models are empirically evaluated on their estimation accuracy for the task of predicting latent high-dynamic magnet temperature profiles, specifically, ordinary least squares, support vector regression, k-nearest neighbors, randomized trees, and neural networks. Having test bench data available, it is shown that ML approaches relying merely on collected data meet the estimation performance of classical thermal models built on thermodynamic theory. Through benchmarking, this work reveals the potential of simpler ML models in terms of regression accuracy, model size, and their data demand in comparison to parameter-heavy deep neural networks, which were investigated in the literature before. Especially linear regression and simple feed-forward neural networks with optimized hyperparameters mark strong predictive quality at low to moderate model sizes.	Article	SEP 2021	10.1109/TEC.2021.3052546	[]	[]	-1	-1	-1
B	Al Ja'idi, MohammadH. A.	Design and Development of Adaptive EV Charging Management for Urban Traffic Environments	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
C	Perin, Giovanni; Nophut, David; Badia, Leonardo; Fitzek, Frank H. P.	Maximizing Airtime Efficiency for Reliable Broadcast Streams in WMNs with Multi-Armed Bandits	2020	2020 11TH IEEE ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON)		472	478	Wireless broadcast routing is a complex problem, shown in the literature to be NP-complete. Current protocols implement either heuristics to find solutions that are not guaranteed to be optimal or classic flooding. However, many future use cases, like automotive applications, industrial robotics, and multimedia broadcast, will require efficient yet reliable methods. In this work, we use contextual multi-armed bandits together with opportunistic routing (OR) and network coding (NC) to find approximately optimal solutions to the problem of broadcast routing in a distributed fashion. Each router independently learns its own transmission credit, i.e., the number of packets to forward for each innovative packet received, so that the airtime cost, subject to latency constraints, is minimized. Results show that the proposed solutions, particularly the deep learning based one, vastly improve the overall reliability, while performing close to MORE multicast in terms of airtime and to B.A.T.M.A.N. in latency, both being the best candidates in the respective discipline among the tested ones.	Proceedings Paper	2020		[]	[]	-1	-1	-1
P	AKIYAMA Y; MIZUSHIRO K; MIZUSHIRO T; KEN M; YOSHIKUNI A	Moulded articles with improved weld line strength            formed from polyphenylene ether blends contg.            particulate polyolefin and styrene! butadiene! styrene            resin										[]	[]	-1	-1	-1
P	TAKAHASHI K	Active suspension for motor vehicle										[]	[]	-1	-1	-1
P	CHANDRASHEKAR S; SELVAGANAPATHY S; HENTTONEN T; DECARREAU G	User equipment (UE) in wireless communication            network for performing radio link monitoring (RLM) in            multiple transmitter receiver point (mTRP) scenario,            has processor that is configured to receive response            indicative of target cell among non-serving cell from            serving distributed unit (DU)						NOVELTY - The UE has processor and a memory including a computer program code. The processor is configured to receive configuration information indicative of beam corresponding to a serving cell of a serving DU of the network node and beam corresponding to non-serving cell of a non-serving DU of the network node from a centralized unit (CU) of a network node. The processor is configured to perform RLM over the serving cell by using the beam of the serving cell. The processor is configured to transmit a request for switching the RLM from the serving cell to non-serving cell to the serving DU. The processor is configured to receive a response indicative of a target cell among the non-serving cell from the serving DU. The processor is configured to switch the RLM from the serving cell to the target cell. USE - User equipment (UE) such as mobile station, mobile terminal, mobile subscriber unit, mobile phone, cellular phone, smart phone, cordless phone, personal digital assistant (PDA), wireless communication device, laptop computer, tablet computer, gaming device, netbook, smartbook, ultra book, medical mobile device or equipment, biometric sensor, smart watch, smart glass, smart wrist band, driver-assistance system, smart meter or sensor, unmanned vehicle, industrial manufacturing equipment, a global positioning system (GPS) device, an Internet-of-Things (loT) device, Industrial loT (I loT) device, a machine-type communication (MTC) device, group of Massive loT (MIoT) or Massive MTC (mMTC) devices or sensor in wireless communication network for performing radio link monitoring (RLM) in multiple transmitter receiver point (mTRP) scenario. ADVANTAGE - The CU facilitate efficient RLM for the UE in both intra-DU and inter-DU scenarios and allow the serving DU to efficiently select a new in response to an RL quality degradation occurred in the serving cell. The probability of the RLF decreases and mobility robustness is improved if the RLF is decided by performing the RLM over both serving and non-serving cells. The more efficient uplink or downlink beam management allows for increased intra-cell and inter-cell mobility and larger number of transmission configuration indicator (TCI) states. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:a CU of network node;a serving DU of network node;a method for operating serving DU of network node;a method for operating CU of network node;a computer program product comprising computer-readable storage medium storing program for performing RLM in mTRP scenario; anda method for operating UE in wireless communication network. DESCRIPTION Of DRAWING(S) - The drawing shows the interaction diagram that explains the interaction between a UE and logical entities of a disaggregated network node.800Method for performing interaction between a UE and logical entities of a disaggregated network nodeS802Step for transmitting report to DUS804Step for forwarding report to CUS806,S808Step for receiving UE context setup response from DUS820Step for transmitting a report to DU				[]	[]	-1	-1	-1
J	Dowdeswell, B.; Sinha, R.; MacDonell, S.G.	Finding faults: A scoping study of fault diagnostics for Industrial Cyber-Physical Systems [arXiv]	2021	arXiv		19 pp.	19 pp.	Context: As Industrial Cyber-Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems.(Abridged). [10.1016/j.jss.2020.110638].	Journal Paper	13 Jan. 2021		[]	[]	-1	-1	-1
B	Wang, Baoqian	Enabling Technologies and Applications for Networked Airborne Computing	2023						Dissertation/Thesis	Jan 01 2023		[]	[]	-1	-1	-1
B	Sahba, Amin	Intelligent Flow Control of Connected Driverless Vehicles in Smart City Intersections	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
B	Siew, Peng Mun	Multiple Target Tracking Using Random Finite Sets	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
B	Raphael, Jeffery	An Exploration of Traffic Signal Control Using Multi-Agent Market-Based Mechanisms	2018						Dissertation/Thesis	Jan 01 2018		[]	[]	-1	-1	-1
J	Jess, Brandi; Brusey, James; Rostagno, Matteo Maria; Merlo, Alberto Maria; Gaura, Elena; Gyamfi, Kojo Sarfo	Fast, Detailed, Accurate Simulation of a Thermal Car-Cabin Using Machine-Learning	2022	FRONTIERS IN MECHANICAL ENGINEERING-SWITZERLAND				Car-cabin thermal systems, including heated seats, air-conditioning, and radiant panels, use a large proportion of the energy budget of electric vehicles and thus reduce their effective range. Optimising these systems and their controllers might be possible with computationally efficient simulation. Unfortunately, state-of-the-art simulators are either too slow or provide little resolution of the cabin's thermal environment. In this work, we propose a novel approach to developing a fast simulation by machine learning (ML) from measurements within the car cabin over a number of trials within a climatic wind tunnel. A range of ML approaches are tried and compared. The best-performing ML approach is compared to more traditional 1D simulation in terms of accuracy and speed. The resulting simulation, based on Multivariate Linear Regression, is fast (5 microseconds per simulation second), and yields good accuracy (NRMSE 1.8%), which exceeds the performance of the traditional 1D simulator. Furthermore, the simulation is able to differentially simulate the thermal environment of the footwell versus the head and the driver position versus the front passenger seat, but unlike a traditional 1D model cannot support changes to the physical structure. This fast method for obtaining computationally efficient simulators of car cabins will accelerate adoption of techniques such as Deep Reinforcement Learning for climate control.	Article	MAR 10 2022	10.3389/fmech.2022.753169	[]	[]	-1	-1	-1
B	Kaur, H.; Goyal, M.K.	Secure Data Communication in Vehicular Networks for Disaster Rescue	2022	2022 International Conference on Futuristic Technologies (INCOFT)		1	6	Ground recovery automobiles operating in the regions impacted by the catastrophe should coordinate their efforts and communicate a large amount of data with one another. This data should include rescue orders, details on the state of the roadways, and data on prior rescue missions. This will guarantee that you are able to drive safely and effectively respond. Unmanned aerial vehicles (UAVs), which can be used to conduct instant recovery services in places that are destructed and assist in sharing data for ground Vehicular networks, can be utilised when connectivity investments are damaged as a consequence of climate change. UAVs can help share data for field Iot environments and can conduct recovery services in areas that have been damaged. However, in a tragedy scenario involving UAV-assisted IoV, there are possible safety threats on intelligence sharing among machines and UAVs. These threats are the outcome of an unreliable distributed environment, unreliable improper behavior tracing, and minimal information. These issues could compromise the integrity of shared data. The interconnection of the relevant vehicles and unmanned aerial vehicles (UAVs) makes these dangers possible. In this article, we create a lightweight vehicular blockchain-enabled secure (LVBS) data exchange architecture for the Automotive Internet of Things that is helped by unmanned aerial vehicles (UAVs) to assist in hurricane relief. The concerns which have been voiced will be addressed more effectively as a result of this. To begin, we will discuss the new UAV and smart contracts collaboration high - altitude system architectures that we have developed for usage in places that have been affected by natural disasters. A resource consensus method is developed for the lighter vehicular ecosystem in the second step of our project. This not only makes it possible for us to track and record data exchanges in a manner that is both safe and immutable, but it also helps us arrive at a majority more quickly while ensuring that its degree of safety is not compromised. In the end, comprehensive models are tested, which demonstrate that LVBS has the ability to effectively increase the safety of the consensual phase and stimulate the data exchange of a great quality. This is shown by the fact that LVBS has been shown to have this capacity.	Conference Paper	2022	10.1109/INCOFT55651.2022.10094552	[]	[]	-1	-1	-1
J	Frost, GeoffP.	Stochastic optimisation of vehicle suspension control systems via learning automata	1998						Dissertation/Thesis	Jan 01 1998		[]	[]	-1	-1	-1
J		ERI: Improving the Learning Efficiency of Adaptive Optimal Control Systems in Information-Limited Environments	2022					This Engineering Research Initiation (ERI) grant will fund research that enables efficient, on-the-fly learning of optimal control strategies for complex engineering systems operating in uncertain environments, with application to connected and autonomous vehicles, thereby promoting the progress of science and advancing the national prosperity and welfare. Many emerging control systems in the artificial intelligence, automotive, robotic, and energy fields require that optimal actions be identified and executed across a network of components in the absence of detailed system knowledge and based on limited input data. Learning-based approaches have been developed to meet this requirement, but are challenged by very slow rates of learning, restrictive requirements on the control policy used to initiate the learning process, and complications due to sensor limitations and sparse data sharing between individual components. This project will overcome these challenges by building a new learning-efficient control framework that integrates advantages of existing methods and demonstrates new solutions for handling missing data streams and optimizing the communication structure between system components. When applied to networks of autonomous vehicles in complex traffic scenarios, the framework may enable improvements in roadway safety and reduction in road fatalities. The broader impacts of this project include outreach efforts to the public intended to show how artificial intelligence and automatic control can be safely leveraged, as well as training and preparation of undergraduate and graduate students to pursue further education and advanced STEM careers.&lt;br/&gt;&lt;br/&gt;This research aims to make fundamental contributions to the development of a learning-efficient, adaptive optimal control framework for nonlinear dynamical systems with completely unknown system models and under conditions of partial observability, and to enable the application of this framework to networked control systems with nontrivial communication topologies. It will achieve this outcome by developing a new hybrid iterative form of reinforcement learning that achieves a quadratic rate of convergence even if a system model and an initial admissible control policy are unavailable. Inspired by ideas from hierarchical reinforcement learning, a two-layer learning-efficient method will be created to enable simultaneous learning of robust distributed control strategies for individual network agents and an optimal network communication topology, including in the presence of communication delays between agents. Micro-traffic simulations and physical experiments will be used to test the theoretical framework in the context of collision avoidance in several formation control scenarios.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	Awarded Grant	Jan 01 2022		[]	[]	-1	-1	-1
B	Zhou, Pengzhan	Building High Performance Wireless Rechargeable Sensor Networks and Vehicular Networks by Algorithm Designs	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Luo Yutao; Shi Zixian; Liang Weiqiang	Autonomous Driving Dynamic-programming Algorithm Based on Improved Artificial Potential Field	2022	China Journal of Highway and Transport		279	292	"Decision-making algorithms for autonomous driving can be divided into end-to-end and sequential planning algorithms. Sequential algorithms are adopted by most OEMs because of their interpretability and robustness. Planning module is the core of the sequential algorithm. It receives information from a perception module and high-definition map and outputs the driving trajectories or actions. Artificial potential field (APF) method, which is widely used in planning algorithms for autonomous driving, is becoming increasingly popular owing to its excellent planning efficiency and information extraction capability. However, APF does not consider destination factor, and single-point destination gravitational field causes a large force, resulting in incorrect directions for long-distance cases, and cannot cope with complex traffic environments. In response to these problems, this study proposed ""Driving Intention & Risk Field"" (IRF) to model traffic factors including destination, vehicles, and road boundaries and consider their characteristics separately and then in combination. A global gravitational field considering the global route was created, and a global planned path was discretized into equidistant path points. The path points within the range of the interest area were dynamically selected to construct a global gravitational field. To verify the performance of the IRF, an IRF-SAC decision-planning algorithm platform was built, and highway, urban crossroad, and roundabout scenes were set in a CARLA simulation environment. The research results show that compared with NF-SAC and FSM, the IRF-SAC algorithm significantly improves safety, comfort, and vehicle-passing efficiency. In the highway scenario, IRF-SAC achieves high accuracy and robustness in path tracking, and the maximum displacement errors are reduced by 44.8% and 70.2% compared with the FSM and NF-SAC algorithms, respectively. In the crossroad scenario, the average risk coefficients are reduced by 12.0% and 20.6%, and root mean squares of the longitudinal acceleration are reduced by 13.2% and 44.9%, compared with the NF-SAC and FSM algorithms, respectively. Moreover, the driving time is reduced by 39.2% compared with the FSM algorithm. In the roundabout scenario, the average risk coefficient is reduced by 31.7% and 52.9%, and the root mean square of the longitudinal acceleration is reduced by 27.0% and 19.0%, compared with the NF-SAC and FSM algorithms, respectively."	Article	2022		[]	[]	-1	-1	-1
C	Phukan, Ripun; Zhao, Xingchen; Chang, Che-wei; Dong, Dong; Burgos, Rolando; Mustapha, Debbou; Platt, Arnaud	A Compact Integrated DM-CM Filter with PCB Embedded DC Current Sensor for High Altitude High Current Applications	2022	2022 IEEE/AIAA TRANSPORTATION ELECTRIFICATION CONFERENCE AND ELECTRIC AIRCRAFT TECHNOLOGIES SYMPOSIUM (ITEC+EATS 2022)		923	928	This paper presents a novel multi-turn solution for integrated DM-CM filter used in DC-fed motor drive system. The proposed solution is targeted for high-current and high-altitude applications by stacking multiple PCB(s) and interconnects. A ladder network is proposed to realize the multi-turn solution, while edge plating is proposed for filter grounding. A Printed Circuit Board (PCB) based Active Magneto Resistive sensor (AMR) is embedded within the PCB dielectric to measure DC side current. PCB based feedthrough connection is implemented to improve DC filter attenuation characteristics at high frequency. Challenges pertaining to partial discharge (PD) free operation, high current and grounding of the filter are discussed. The filter and enclosure are tested in a high-altitude chamber for partial discharge. The integrated AMR sensor is tested using Silicon Carbide (SiC) enabled three level inverter under open loop RL load tests. The proposed structure is highly competitive compared to the state-of-the-art technologies and is recommended for low weight requirements.	Proceedings Paper	2022	10.1109/ITEC53557.2022.9814074	[]	[]	-1	-1	-1
J	Lall, Pradeep; Hande, Madhura; Bhat, Chandan; Lee, Jay	Prognostics Health Monitoring (PHM) for Prior Damage Assessment in Electronics Equipment Under Thermo-Mechanical Loads	2011	IEEE TRANSACTIONS ON COMPONENTS PACKAGING AND MANUFACTURING TECHNOLOGY		1774	1789	Methodologies for prognostication and health monitoring (HM) can significantly impact electronic reliability for applications in which even minimal risk of failure may be unbearable. Presently, HM approaches such as the built-in self-test are based on reactive failure diagnostics and unable to determine residual-life (RL) or estimate residual-reliability. Prognostics health-monitoring (PHM) approach presented in this paper is different from state-of-art diagnostics and resides in the prefailure-space of the electronic system, in which no macro-indicators such as cracks or delamination exist. Applications for the presented PHM framework include, consumer and defense applications such as automotive safety systems including front and rear impact protection systems, chassis-control systems, x-by-wire systems, and defense applications such as avionics systems, naval electronic warfare systems. The presented PHM methodologies enable the estimation of prior damage in deployed electronics by the interrogation of the system state for systems in which the prior stress-history may be unknown or unavailable. The primary focus is on thermo-mechanical stresses. The presented methodologies will trigger repair or replacement, significantly prior to failure. The approach involves the use of condition monitoring devices which can be interrogated for damage proxies at finite time-intervals. The system's residual life is computed based on residual-life computation algorithms. Previously, we have developed several leading indicators of failure. In this paper, a mathematical approach has been presented to calculate the prior damage in electronics subjected to cyclic and isothermal thermo-mechanical loads. Electronic components operating in a harsh environment may be subjected to both temperature variations in addition to thermal aging during use-life. Data have been collected for leading indicators of failure for 95.5Sn4Ag0.5Cu first-level interconnects under both single and sequential applications of cyclic and isothermal thermo-mechanical loads. Methodology for the determination of prior damage history has been presented using non-linear least-squares method based on interrogation techniques. The methodology presented used the Levenberg-Marquardt Algorithm. The test vehicle includes various area-array packaging architectures soldered on immersion Ag finish, subjected to thermal cycling in the range of -40 degrees C to 125 degrees C and isothermal aging at 125 degrees C.	Article	NOV 2011	10.1109/TCPMT.2011.2160542	[]	[]	-1	-1	-1
J		Adaptive dynamic programming for uncertain nonlinear systems through coupling of nonlinear analysis and data-based learning	2015					Optimal control methods provide a means to associate a user-defined cost with control actions or decisions. These methods have made pervasive impacts in a wide class of application domains. The shift towards autonomy by the automotive industry in examples such as replacing mechanical systems with computer controlled electronic control systems, has resulted in efficiencies in fuel injection, braking, throttling, etc. For electric vehicles, fuel economy, drivability, and emission control are functions of the engine design and the control strategies. For robotic systems, the desire to achieve optimal behavior is essential for efficient task execution. Likewise, aerospace systems have always been a mainstream application domain for optimal control methods because there has always been (and always will be) a tight coupling between performance and energy/fuel costs. As the cost of energy and the awareness of the environmental impacts of producing energy have risen, optimal control may now play a timely role in a broader spectrum of application domains. The Energy Efficiency and Renewable Energy office of the U.S. Department of Energy indicates that as much as 10-20 percent of American energy use could be saved by optimizing industrial systems. It is self-evident that optimal control solutions can have significant impacts in a wide range of industries, but the development of optimal control solutions for real engineering systems is limited by numerous technical barriers. The most fundamental and open-ended problems arise from barriers associated with developing optimal solutions in the presence of uncertainty. The driving question in this project is how to arbitrate between gaining knowledge about a system while simultaneously making the optimal control decision for engineering systems that are uncertain and complex.<br/><br/>The technical aims of this project are motivated by the hypothesis and observations from preliminary efforts that nonlinear analysis methods can be exploited to design real-time approximate optimal solutions, while concurrent background processing methods can be used to update the optimal control approximation for improved performance. Intellectual merits in this project are realized through the development of classes of closed-loop controllers with associated stability analysis, and advanced function approximation methods, that ensure sufficient exploration of the system response while learning the approximate optimal control solution. Outcomes of this research would allow for optimal control implementation in a broader class of application domains where the system exhibits nonlinear behaviors and uncertainty. The broad impact of this framework is a merger of methods that bridge the gap between the computational intelligence community and control systems community to enable data-based learning methods to optimize control performance.	Awarded Grant	Aug 01 2015		[]	[]	-1	-1	-1
B	Erian, Karim H.	Autonomous Control of an All-Terrain Vehicle Using Embedded Systems and Artificial Intelligence Techniques	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
P	BROCAS A; BOURRIGAUD S; LABARTHE C; SAURY P; ANNE-LAURE B; SYLVAIN B; CELINE L; PATRICK S; LABARTHESELINE; PATRIQUE S	Suspension polymerization of (meth)acrylic and/or            styrenic monomer used for producing polymer beads used            e.g. in three-dimensional printing, involves            polymerizing alkoxyamine(s) and acrylic and/or styrenic            monomer with nitroxide in stirred reactor, and            introducing methacrylic and/or styrenic monomer						NOVELTY - Suspension polymerization of a (meth)acrylic and/or styrenic monomer involves introducing 0.5-4 wt.% suspending agent constituting an aqueous phase, and an organic phase comprising at least one alkoxyamine and at least one acrylic and/or styrenic monomer with a nitroxide in a molar ratio of 1/50-1/1000, to a stirred reactor comprising water, polymerizing the monomers in the suspension up to a minimum mass conversion rate of 80% at 15-140 degrees C, and introducing at least one methacrylic and/or styrenic monomer and at least one mercaptan into the polymerized suspension, polymerizing under agitation of the monomers up to a minimum conversion rate of 95% at 15-140 degrees C, introducing a soluble initiator into the aqueous phase to complete the polymerization up to a minimum mass conversion rate of 99% with a rate of 0.1-2%, and filtering, washing, and drying to obtain beads of a composition comprising at least one block copolymer. The alkoxyamine carrying at least one nitroxide has a structure (I). USE - Suspension polymerization of (meth)acrylic and/or styrenic monomer used for producing polymer beads used in three-dimensional printing, as additive for polymers, and for manufacturing molded article, injection-compression article, and extrusion article (all claimed). Uses include but are not limited to automotive component, aeronautical component, aerospace component, prostheses, hearing system, cellular fabric, textile, clothing, fashion article, decoration component, boxes for electronics, home automation component, lighting component, etc. ADVANTAGE - The method produces polymer beads which are thermally stable and have excellent impact resistance, optical properties, and heat resistance, and easy transformation conditions or optimal mechanical performance. DETAILED DESCRIPTION - Suspension polymerization of a (meth)acrylic and/or styrenic monomer involves introducing 0.5-4 wt.% suspending agent constituting an aqueous phase, and an organic phase comprising at least one alkoxyamine and at least one acrylic and/or styrenic monomer with a nitroxide in a molar ratio of 1/50-1/1000, to a stirred reactor comprising water, polymerizing the monomers in the suspension up to a minimum mass conversion rate of 80% at 15-140 degrees C, and introducing at least one methacrylic and/or styrenic monomer and at least one mercaptan with a mercaptan/nitroxide molar ratio of 2/1000-8/1000 into the polymerized suspension, polymerizing under agitation of the monomers up to a minimum conversion rate of 95% at 15-140 degrees C, introducing a soluble initiator into the aqueous phase to complete the polymerization up to a minimum mass conversion rate of 99% with a rate of 0.1-2%, and filtering, washing, and drying to obtain beads of a composition comprising at least one block copolymer. The mass ratio of aqueous phase and organic phase is 3-10. The alkoxyamine carrying at least one nitroxide is formula (I). The mass ratio of acrylic and/or styrenic monomer and methacrylic and/or styrenic monomers is 25/75-70/30.Rl' = monovalent group with molar mass of more than 15.42 g/mol;Ra',Rb' = 1-40C alkyl;andor Ra'+Rb' = ring optionally substituted by hydroxy, alkoxy or amino.				[]	[]	-1	-1	-1
B	Shibata, M.; Hoshi, N.	Novel inverter topologies for two-wheel drive electric vehicles with two permanent magnet synchronous motors	2007	2007 European Conference on Power Electronics and Applications		1	10	Novel topology of an inverter that can independently control two permanent magnet synchronous motors (PMSMs) is proposed. In conventional, 12 switching devices, i.e. two three-phase three-leg inverters, are required to control two PMSMs; thus, the equipment size becomes large. The proposed inverter requires the same number of switches as the conventional inverter, i.e. only 6 switching devices. In this paper, the circuit configuration and control principle of the proposed inverter are introduced; and a controller was constructed using an FPGA. Moreover, the basic characteristics of the inverter, which is connected with two Y-connected RL loads, are shown through computer simulations and experiments. As a result, it was verified through the simulations and the experiments that the proposed inverter could output different amplitude and frequency for each load.	Conference Paper	2007	10.1109/EPE.2007.4417667	[]	[]	-1	-1	-1
J	Khan, Samir; Tsutsumi, Seiji; Yairi, Takehisa; Nakasuka, Shinichi	Robustness of AI-based prognostic and systems health management	2021	ANNUAL REVIEWS IN CONTROL		130	152	Prognostic and systems Health Management (PHM) is an integral part of a system. It is used for solving reliability problems that often manifest due to complexities in design, manufacturing, operating environment and system maintenance. For safety-critical applications, using a model-based development process for complex systems might not always be ideal but it is equally important to establish the robustness of the solution. The information revolution has allowed data-driven methods to diffuse within this field to construct the requisite process (or system models) to cope with the so-called big data phenomenon. This is supported by large datasets that help machine-learning models achieve impressive accuracy. AI technologies are now being integrated into many PHM related applications including aerospace, automotive, medical robots and even autonomous weapon systems. However, with such rapid growth in complexity and connectivity, a systems' behaviour is influenced in unforeseen ways by cyberattacks, human errors, working with incorrect or incomplete models and even adversarial phenomena. Many of these models depend on the training data and how well the data represents the test data. These issues require fine-tuning and even retraining the models when there is even a small change in operating conditions or equipment. Yet, there is still ambiguity associated with their implementation, even if the learning algorithms classify accordingly. Uncertainties can lie in any part of the AI-based PHM model, including in the requirements, assumptions, or even in the data used for training and validation. These factors lead to sub-optimal solutions with an open interpretation as to why the requirements have not been met. This warrants the need for achieving a level of robustness in the implemented PHM, which is a challenging task in a machine learning solution. This article aims to present a framework for testing the robustness of AI-based PHM. It reviews some key milestones achieved in the AI research community to deal with three particular issues relevant for AI-based PHM in safety-critical applications: robustness to model errors, robustness to unknown phenomena and empirical evaluation of robustness during deployment. To deal with model errors, many techniques from probabilistic inference and robust optimisation are often used to provide some robustness guarantee metric. In the case of unknown phenomena, techniques include anomaly detection methods, using causal models, the construction of ensembles and reinforcement learning. It elicits from the authors' work on fault diagnostics and robust optimisation via machine learning techniques to offer guidelines to the PHM research community. Finally, challenges and future directions are also examined; on how to better cope with any uncertainties as they appear during the operating life of an asset.	Review; Early Access		10.1016/j.arcontrol.2021.04.001	[]	[]	-1	-1	-1
J		EAGER: Real-Time: Reinforcement, Meta, and Episodic Learning for Control under Uncertainty	2018					Machine learning and artificial intelligence are among the most important general purpose technologies for the coming decades, with potential to transform all aspects of society from health, to manufacturing, to business, to education, and to security. In the last decade, there have been very important and impressive advances in machine learning driven by the use of deep neural networks, innovative training algorithms, computational resources including specialized hardware (graphics processors, tensor processing units), and large datasets. Some of these developments have connections to emerging understanding from neuroscience on how the human brain learns to makes decisions in real-time. However, there are major challenges in the use of these techniques in real-time control and decision making for engineering systems where stability, reliability, and safety are paramount concerns. This project aims to connect major advances in machine learning and neuroscience to control systems and thereby advance myriad application domains. Modern engineered systems are increasingly complicated. They comprise large heterogeneous distributed networks of (IoT) connected devices, systems, and human/social agents, e.g., transportation, energy, water, manufacturing, health and agriculture. A major challenge is performance, stability and reliability of these systems under large uncertainties. The goal is to expand our understanding and integration of learning and control to derive principles and algorithms for the development of learning-based control systems for a variety of engineering applications.  <br/><br/>While there are significant historical connections between reinforcement learning and stochastic dynamic control, the potential for leveraging ongoing and future advances in machine learning for control remains significantly under- explored. The field of control systems has deep and solid theoretical and mathematical foundations with comprehensive and well-established frameworks for linear, nonlinear, robust, adaptive, stochastic, distributed, and model-predictive control systems. Equally importantly, control systems have applications in multiple domains, such as aerospace, automotive, manufacturing, energy, transportation, agriculture, water, and many other engineered and socio-technical systems. Despite this rich spectrum of theoretical foundations and important applications, the domain of applicability of traditional control techniques is limited to situations where good mathematical models of the underlying systems are available, and where the environmental uncertainty is not too large. This exploratory research project is aimed at overcoming these limitations via novel problem formulations in systems and control inspired by new insights coming from recent developments in machine learning.  A key focus will be on novel control architectures inspired by neuroscience and reinforcement learning. Besides architectural innovations, the project will explore questions of stability, performance, and uncertainty by integrating ideas from rapid (one-shot) learning, meta-learning, and episodic control into control algorithms. The ideas from this project will be at the core of a new graduate level course in learning for control which will be taught at the University of California, Irvine. The resulting course materials will be made available to the research community and will benefit interested graduate students across the nation. In addition, short courses will be offered at major professional conferences, e. g., American Control Conference, IEEE Conference on Decision and Control.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	Awarded Grant	Oct 01 2018		[]	[]	-1	-1	-1
B	Qi, Xuewei	Next generation intelligent driver-vehicle-infrastructure cooperative system for energy efficient driving in connected vehicle environment	2016						Dissertation/Thesis	Jan 01 2016		[]	[]	-1	-1	-1
J	Niu, Junyan; Zhuang, Weichao; Ye, Jianwei; Song, Ziyou; Yin, Guodong; Zhang, Yuanjian	Optimal sizing and learning-based energy management strategy of NCR/LTO hybrid battery system for electric taxis	2022	ENERGY				This paper proposes an offline sizing method and an online energy management strategy for the electric vehicle with semi-active hybrid battery system (HBS). The semi-active HBS is composed by Nickel Cobalt Rechargeable (NCR) and lithium titanate (LTO) batteries with a bi-directional DC/DC converter. First, the vehicle dynamics and the HBS are modelled. Second, a hierarchical optimal sizing method is proposed to minimize the distance-based cost (DBC) of electric taxi in a variety of driving cycles. The lower layer optimizes the energy management strategy (EMS) with dynamic programming (DP), while the upper layer optimizes the sizes of HBS for minimum DBC. Based on the sizing results, the DBC decreases firstly and then increases with the increasing LTO size. In addition, the results of DP indicate the SOC of the LTO batteries works between 50% and 80% for optimal NCR lifespan. Third, by using the rule extracted from DP, a learning-based EMS, i.e., deep deterministic policy gradient (DDPG), is proposed with excellent real-time control potential. Finally, the simulation results show that the proposed DDPG EMS achieves the improved performance than fuzzy logic control EMS and closed result with what can be achieved through DP, yet the computation time is much less. (c) 2022 Elsevier Ltd. All rights reserved.	Article; Early Access		10.1016/j.energy.2022.124653	[]	[]	-1	-1	-1
J	Cheng, Xingxing; Zhang, Rongquan; Bu, Siqi	A data-driven approach for collaborative optimization of large-scale electric vehicles considering energy consumption uncertainty	2023	ELECTRIC POWER SYSTEMS RESEARCH				With the explosive growth of electric vehicles (EVs), it is an urgent task to incorporate low-carbon EVs with advanced optimization strategies to achieve orderly charging-discharging and economical operations for EVs. Nevertheless, current charging-discharging optimization strategies for EVs may be impractical since their energy consumption uncertainty and the driving trip cost are generally not considered. Therefore, a collaborative optimization model for large-scale EV charging-discharging with energy consumption uncertainty in this paper is proposed to simultaneously maximize passenger revenue and reduce the costs of the driving, charging-discharging, and battery depletion. Subsequently, a data-driven approach is developed to tackle the model. In this approach, an uncertainty predictor based on wavelet transform, deep deterministic policy gradient, and quantile regression is first applied to estimate the energy consumption uncertainty. Then, an adaptive learning rate firefly algorithm is presented to identify the most satisfactory solution for the optimization model. Finally, taking the actual data of 300 EVs in a city in China as case studies, the simulation results reveal that the proposed method is effective and has high application significance.	Article; Early Access		10.1016/j.epsr.2023.109461	[]	[]	-1	-1	-1
B	Mirbakhsh, Ardeshir	A Self-Learning Intersection Control System for Connected and Automated Vehicles	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
J	Ren Bingtao; Deng Weiwen; Bai Xuesong; Li Jiangkun; Zong Ruixue; Zhu Bing; Ding Juan	Technologies of virtual scenario construction for intelligent driving testing	2021	Journal of Image and Graphics		1	12	With the continuous improvement of vehicle intelligence,the interaction of vehicles with the surrounding environment through perception is increasing. The environment that needs to be dealt with,including many factors such as roads,surrounding traffic and weather conditions,is becoming increasing complex. Limited by the development cycle and cost,especially safety factors and the consideration of complex and diverse working conditions,traditional open road or closed field tests are difficult to meet the requirements of intelligent driving testing. Therefore,simulation test based on digital virtual technology has become a new important means for intelligent driving testing and verification. The simulation test mainly adopts a combination of accurate physical modeling,efficient numerical simulation,and high-fidelity image rendering to realistically construct human-vehicle environment models,including vehicles,roads,weather and lighting,and traffic, and various types of vehicles. The construction of virtual scenarios is a key technology simulation and is particularly important for improving the pressure and acceleration of intelligent driving testing. The virtual scenarios can meet the needs of a large number of diverse test samples to reflect the complex and changeable application environment of intelligent driving. They can also provide a large number of labeled datasets for machine learning that can contain rich data with boundary feature scenario content and lay a solid data foundation for deep learning perception and reinforcement learning planning algorithms. Therefore,the simulation scenario construction technology for intelligent driving test has been investigated worldwide in the current automotive intelligence. As an emerging technology,it still faces many challenges,and its methods need to be studied in depth. This paper systematically expounds the progress and current situation of domestic and foreign studies in simulation scenario construction technology,including automatic scenario construction methods and traffic simulation modeling methods,and focuses on some issues worthy of in-depth study. In the research of scenario construction methods, the key elements and characteristics of the limited scenarios can reflect the infinite richness and complex driving environment. A deep understanding of the network structure and mutual coupling of the scenario is essential for the research of virtual scenario construction. Establishing a description method of the scenario limit and boundary characteristics to form the scenario automated generation method can maximize the potential of accelerated testing of intelligent driving. Researchers have promoted the rapid development of scenario generation technology from different perspectives. However,they often use parameter traversal search ideas to determine the system state space. The development and testing is time consuming and labor intensive due to the unlimited expansion of scenario search. The construction of a scenario with dangerous characteristics requires in-depth exploration of the safety boundary of the ego vehicle driving. Thus,the constructed corner scenario can provide effective information corresponding to real driving for realizing the enhanced generation of the corner characteristics of the scenario. This condition responds to the accelerated testing of intelligent driving systems above level four. In terms of traffic modeling methods,a deep understanding of the driving behavior and interaction characteristics of vehicles is the basis and primary task. Determining the influence law of vehicle driving motion in data information and establishing the traffic model with random dangerous characteristics are the key to realize intelligent driving testing.	Review	2021		[]	[]	-1	-1	-1
B	Elmalaki, Salma Hosni Emam M.	Software Mechanisms for Pervasive and Autonomous Computing	2018						Dissertation/Thesis	Jan 01 2018		[]	[]	-1	-1	-1
B	Shou, Zhenyu	Harnessing Big Data for the Sharing Economy in Smart Cities	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J	Li, Bei; Li, Jiangchen; Jian, Bingcong	Price-Guided Cooperation of Combined Offshore Wind and Hydrogen Plant, Hydrogen Pipeline Network, Power Network, and Transportation Network	2023	IEEE ACCESS		124368	124385	Nowadays, the combined offshore wind farm and hydrogen production plant (COWHP) has been installed all around the world and will play an important role in reducing the carbon dioxide emissions. However, the cooperation of the COWHP, the hydrogen pipeline transportation, the power network transmission, and the hydrogen fuel cell electric vehicles (HFCEVs) scheduling is still an essential problem. This problem falls into the category of mixed-integer nonlinear optimization, which involves a significant number of decision variables and constraints. Existing methods have often struggled to effectively solve this problem due to its inherent complexity and non-linear nature. In this paper, a price-guided method is proposed to cooperate with this complex system. First, the COWHP model, the hydrogen pipeline network scheduling model, the power network flow model, the HFCEVs refuelling microgrid model, and the HFCEV traffic flow model in real-world transportation networks are presented. Second, the price-based cooperation model is proposed. Third, price strategies are deployed to cooperate with the complex system. The decision making trial and evaluation laboratory-the technique for order preference by similarity to ideal solution (Dematel-TOPSIS) method is deployed to evaluate the real-time performance of different strategies. The simulation results reveal that in the small vehicle flow transportation network cases, deep deterministic policy gradient (DDPG) has the best real-time evaluation performance; whereas for the large vehicle flow transportation network cases, long short term memory (LSTM) has the best real-time performance. From the posterior evaluation view, LSTM has the best performance for all cases to reduce total operation cost, and also reduce the total waiting time.	Article	2023	10.1109/ACCESS.2023.3330161	[]	[]	-1	-1	-1
B	Qin, Mian	Toolpath Generation and Optimization for Metal Additive Manufacturing	2022						Dissertation/Thesis	Jan 01 2022		[]	[]	-1	-1	-1
B	Iqbal, Sardar Sarwat	Impact of heat treatment and oxidation of Carbon-carbon composites on microstructure and physical properties	2011						Dissertation/Thesis	Jan 01 2011		[]	[]	-1	-1	-1
B	Balu, Aditya	Deep Learning and GPU-Accelerated Algorithms for Computer-Aided Engineering	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J	Sinha, Sumit	Building Transformative Framework for Isolation and Mitigation of Quality Defects in Multi-Station Assembly Systems Using Deep Learning	2021						Dissertation/Thesis	Jan 01 2021		[]	[]	-1	-1	-1
J		Investigations into the Design Rules for the Control of Wire Arc Additive Manufacturing	2020					This award advances the understanding of defect formation in wire arc additive manufacturing, leading to improved processing and greater process control. This research addresses surface waviness and nonuniform wall thickness challenges in wire arc additive manufacturing, making it acceptable for many U.S. industries, including aerospace, defense, and automotive, and thereby benefiting the nations economy and well-being.  Wire arc additive manufacturing uses a welding arc as the energy source in making three-dimensional objects with high throughput. This process can be easily integrated with existing computer-numerical-control routers, or robot arms. Thus, the knowledge and methodology developed through this project can be directly transferred to small- and medium-sized enterprises interested in making or repairing metallic parts. Additionally, this project develops the professional skills of K-12, undergraduate, and graduate students, including women and underrepresented minorities, by training them through a unique set of integrated education and multidisciplinary research opportunities in the areas of wire arc additive manufacturing and data analytics. Accordingly, students are familiarized with emerging technology-intensive manufacturing and data science disciplines, thereby preparing a future workforce equipped with these new skills and knowledge. &lt;br/&gt;&lt;br/&gt;The two overarching research goals of this project are to (1) gain fundamental knowledge about surface waviness and effective wall thickness formation mechanisms in wire arc additive manufacturing and (2) investigate the design rules for the monitoring and control of the manufacturing process. An integrated experimental-characterization and theoretical-modeling framework are considered to achieve these goals. The surface quality of wire arc additively manufactured structures is controlled by fine-tuning and balancing the surface tension, arc force, droplet impact, gravity, buoyancy, and friction for different manufacturing conditions. The project framework consists of four components: (1) measurement and analysis of real-time process signatures (e.g., voltage and current) and visualization data of the arc, droplet, and weld pool features; (2) data measurements and analysis of defect (e.g., balling effect and voids) generation and propagation as a function of process parameters, wall thickness and inclination, and multi-bead/multilayer deposition; (3) characterization of the surface tension-based computational fluid dynamics model and its verification and validation through experiments; and (4) establishment of design rules using data-driven, low-fidelity surrogate models. Machine learning algorithms (e.g., convolutional neural network, AnoGAN, transfer learning, and reinforcement learning) are developed in detail for defect detections and classifications as well as process control. This integrated framework provides unique, transformative, and efficient opportunities to synergistically understand the arc, droplet, and weld pool characteristics and defect formation in wire arc additive manufacturing.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	Awarded Grant	Jul 01 2020		[]	[]	-1	-1	-1
J	Ender, J.; Fiorentini, S.; de orio, R.L.; Hadamek, T.; Bendra, M.; Goes, W.; Selberherr, S.; Sverdlov, V.	Advances in modeling emerging magnetoresistive random access memories: from finite element methods to machine learning approaches	2022	Proceedings of SPIE		1215708 (14 pp.)	1215708 (14 pp.)	Emerging spin transfer torque magnetoresistive random access memories (STT MRAM) are nonvolatile and offer high speed and endurance. They are promising for stand-alone and embedded applications in the automotive industry, microcontrollers, Internet of Things, frame buffer memory, and slow SRAM. The MRAM cell usually includes a CoFeB fixed reference layer and a free magnetic layer (FL), separated by a tunnel barrier. To design ultra-scaled MRAM cells it is necessary to accurately model the torques acting on the magnetization in composite magnetic layers with one or several nonmagnetic inclusions between the ferromagnetic parts. The magnetization dynamics is governed by the Landau-Lifshitz- Gilbert (LLG) equation supplemented with the corresponding torques. The torques depend on nonequilibrium spin accumulation generated by an electric current. The electric current and the spin accumulation also depend on the magnetization. Therefore, the LLG and the spin-charge transport equations are coupled and must be solved simultaneously. We apply the finite element method (FEM) to numerically solve this coupled system of partial differential equations. To develop an open source solver, we use well-developed C++ FEM libraries. The computationally most expensive part is the demagnetizing field calculation. It is performed by a hybrid finite element-boundary element method. This confines the simulation domain for the field evaluation to the ferromagnets only. Advanced compression algorithms for large, dense matrices are used to optimize the performance of the demagnetizing field calculation in complex structures. To evaluate the torques acting on the magnetization, a coupled spin and charge transport approach is implemented. For the computation of the torques acting in a magnetic tunnel junction (MTJ), a magnetization-dependent resistivity of the tunnel barrier is introduced. A fully three-dimensional solution of the equations is performed to accurately model the torques acting on the magnetization. The use of a unique set of equations for the whole memory cell including the FL, fixed layer, contacts, and nonmagnetic spacers is one of the advantages of our approach. To incorporate the temperature increase due to the electric current, we solve the heat transport equation coupled to the electron, spin, and magnetization dynamics, and we demonstrate that the FL temperature is highly inhomogeneous due to a non-uniform magnetization of the FL during switching. Spin-orbit torque (SOT) MRAM is fast-switching and thus well suitable for caches. By means of micromagnetic simulations, we demonstrate the purely electrical switching of a perpendicular FL by the SOTs due to two orthogonal short current pulses. To further optimize the pulse sequence, a machine learning approach based on reinforcement learning is employed. Importantly, a neural network trained on a fixed material parameter set achieves switching for a wide range of material parameter variations.	Conference Paper; Journal Paper	2022	10.1117/12.2624595	[]	[]	-1	-1	-1
B	Shabanisamghabady, Mitra	Dislocation Slip and Deformation Twinning in Face Centered Cubic Low Stacking Fault Energy High Entropy Alloys	2020						Dissertation/Thesis	Jan 01 2020		[]	[]	-1	-1	-1
J		Drift Counteraction Control: Theory and Applications	2014					Drift Counteraction Control: Theory and Applications<br/>In many existing and emerging engineering systems there is an inherent tendency of process variables to drift. In some systems this drift is caused by large, persistent disturbances due to interactions with the external environment.  In other systems, similar drift is caused by finite resources (fuel, energy, component life etc.) being continuously depleted.  The operating objectives for such systems with drift are reflected in a set of constraints that must be satisfied for as long as possible by countering the drift, rather than in terms of usual set-point command tracking requirements.  This research project advances theory and methods for designing control algorithms that perform drift counteraction thereby establishing a foundation for addressing practical drift counteraction applications, and for incorporating drift counteraction technology into industrial products.   These advances will represent contributions to several areas of control theory including stochastic control, set-theoretic control, game-theoretic control, constrained control, and nonlinear control. The developments in theory will proceed in close synergy with the demonstration of their benefits in several engineering applications to automotive and aerospace systems and dynamic motion simulators. The research results will be integrated into university courses for graduate students, short courses for industrial and academic audiences, and into the computational software package implementing drift counteraction control design techniques.  <br/>The drift counteraction control is based primarily on constraints and does not use conventional set-points, differently from most of the traditional control theory.  The development of effective techniques for drift counteraction will be based on treating external disturbances causing drift and constraint violation first in the stochastic and then in the deterministic setting. In the stochastic setting, advances in Stochastic Drift Counteraction Optimal Control (SDCOC) that maximizes the expected cost before the imposed constraints are violated will be pursued.  Stability and boundness conditions for closed-loop trajectories of systems operating under SDCOC control laws when disturbances settle to constant values or vary within a subset of the full range will be derived. Sub-optimality and convergence properties of receding horizon and reinforcement learning variants of SDCOC will be studied. Advances in SDCOC computational procedures will be made, firstly, by exploiting decomposition of the system dynamics into slow and fast subsystems and, secondly, based on Markov Chain models that use Fuzzy Encoding (MCFE).  By combining SDCOC and MCFE, evidence based control framework for drift counteraction in uncertain systems will be defined and complemented with estimation algorithms.  In the deterministic setting, set-theoretic, game-theoretic and disturbance estimation/cancellation based approaches to drift counteraction problems will be pursued.  The advances will focus on formulation and derivation of algorithms, the improvements in their off-board and on-board computations, and characterization of closed-loop properties (constraint violation time, ability to prevent constraint violation over an infinite time interval, trajectory bounds, etc.).  Synergistically with the theoretical advances, several practical applications of drift counteraction control will be considered. These applications will include increasing comfort and fuel efficiency of adaptive cruise control systems for conventional and hybrid passenger vehicles, extending range and life of electric and hybrid electric vehicles and their batteries, improving the ability to replicate motions in small scale dynamic motion simulators, and improving the capability and efficiency of spacecraft attitude control using momentum exchange devices.  For each application, its requirements will be reflected in a drift counteraction problem formulation, appropriate models will be established, drift counteraction control algorithms will be designed, and performance benefits will be quantified. To ensure practical relevance of the results and facilitate their experimental validation in the vehicle adaptive cruise control case, interactions and collaborations with industry partners will be leveraged.	Awarded Grant	Aug 15 2014		[]	[]	-1	-1	-1
J	Maasoumy, M.; Sangiovanni-Vincentelli, A.	Smart Connected Buildings Design Automation: Foundations and Trends	2016	Foundations and Trends in Electronic Design Automation		1	143	Buildings are the result of a complex integration of multi-physics subsystems. Besides the obvious civil engineering infrastructure, thermal, electrical, mechanical, control, communication and computing subsystems must co-exist and be operated so that the overall operation is smooth and efficient. This is particularly important for commercial buildings but is also very relevant for residential buildings especially apartment buildings. Unfortunately, the design and deployment of these subsystems is rarely synchronized: lighting, security, heating, ventilation and air conditioning systems are often designed independently. However, simply putting together a collection of sub-systems, albeit optimized, has led to the inefficient buildings of today. Worldwide, buildings consume 42% of all electrical power - more than any other asset - and it can be proven that much of this can be reduced if a holistic approach to design, deployment, and operation is taken. Government agencies, academic institutions, building contractors and owners have realized the significant impact of buildings on the global environment, the electrical grid, and the mission of their organizations. However, the economic impact for all constituencies is still difficult to assess. Government regulations can play a fundamental role, as it has been the case for the transportation industry where regulations on emission and fuel consumption have been the single most important factor of innovation in automotive design. We are convinced that by leveraging technology and utilizing a system-level approach to buildings, they will provide comfort, safety and functionality while minimizing energy cost, supporting a robust electric grid and mitigating environmental impact. Realizing this vision requires adding intelligence from the beginning of the design phase, to deployment, from commissioning to operation, all the way to the end of the buildings life cycle. In this issue, we attempt to provide an as-complete-as-possible overview of the activities in the field of smart connected building design automation that attempts to make the vision a reality. The overarching range of such activities includes developing simulation tools for modeling and the design of buildings, and consequently control algorithms proposed to make buildings smarter and more efficient. Furthermore, we will review real-world and large-scale implementation of such control strategies on physical buildings. We then present a formal co-design methodology to design buildings, taking the view that buildings are prime examples of cyber-physical systems where the virtual and physical worlds meet as more traditional products such as thermostats are able to connect online and perform complicated computational tasks to control building temperature effectively. We complete the presentation describing the growing role of buildings in the operation of the smart grid where buildings are not only consumers of energy, but are themselves also providers of services and energy to the grid. The audiences for this monograph are industry professionals and researchers who work in the area of smart buildings, smart cities, and smart grid, with emphasis on energy efficiency, simulation tools, optimal control, and cyber-physical systems for the emerging power markets.	Journal Paper	9 March 2016	10.1561/1000000043	[]	[]	-1	-1	-1
J	Bazhinov, Oleksiy; Gerlici, Juraj; Kravchenko, Oleksandr; Haiek, Yevhen; Bazhynova, Tetiana; Zaverukha, Ruslan; Kravchenko, Kateryna	Development of a Method for Evaluating the Technical Condition of a Car's Hybrid Powertrain	2021	SYMMETRY-BASEL				The article presents the results of a study performed and substantiated based on the principles of a new method of diagnostics of technical conditions of a hybrid powertrain regardless of the structural diagram and design features of a hybrid vehicle. The presented new technology of the diagnostics of hybrid powertrains allows an objective complex assessment of their technical condition by diagnostic parameters in contrast to existing diagnostic methods. In the proposed method, a mechanism for the general standardization of diagnostic parameters has been developed as well as for determining the numerical values of the parameters of the powertrain. The control subset was used to control the learning error. As a result of debugging the system, the scatter of experimental and calculated points has decreased, which confirms the quality of debugging the tested fuzzy model. As a result of training the artificial neural network, the standard deviation of the error in the control sample was 0.012 center dot P-k. A symmetry method of diagnostics of the technical state of a hybrid propulsion system was developed based on the concept of a neural network together with a neuro-fuzzy control with an adaptive criteria based on the method of training a neural network with reinforcement. The components of the vector functional include the criteria for control accuracy, the use of traction battery energy, and the degree of toxicity of exhaust gases. It is proposed to use the principle of symmetry of the guaranteed result and the linear inversion of the vector criterion into a supercriterion to determine the technical state of a hybrid powertrain on a set of Pareto-optimal controls under unequal conditions of optimality.	Article	DEC 2021	10.3390/sym13122356	[]	[]	-1	-1	-1
