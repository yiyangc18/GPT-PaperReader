title: Dissipation of Stop-and-Go Waves of Mixed Autonomous Vehicle Flow with Reinforcement Learning
abstract: Traffic congestion is a common phenomenon in cities, and improving traffic efficiency has become an urgent problem to be solved.
Many researchers have proposed feasible solutions from different perspectives, such as traffic flow, signal lights, etc.
This paper proposes a method of dissipating stop-and-go wave based on reinforcement learning (RL).
Specifically, we take the mixed autonomous vehicle flow as the research object, using RL to train the driving strategy of connected autonomous vehicle (CAV), and dissipating the stop-and-go waves in vehicle flow by adjusting the CAV's driving behavior.

We propose the concept of "Equivalent Density Difference" as a model to describe the difference of traffic flow dynamics before and after a specific vehicle within a certain range, and use this index to design RL model.

The proposed method combines the advantages of data-driven and model-driven, improving the training efficiency of RL.
Experimental results show that this method can increase the system-level speed and improve the stability of the mixed autonomous vehicle flow.

title: RISE: A Velocity Control Framework with Minimal Impacts based on Reinforcement Learning
abstract: Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade.
However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes.
In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle).
In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles.
To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle.
Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles.
To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph.
Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.

title: Research on Decision-making Method for Autonomous Driving Behavior of Connected and Automated Vehicle
abstract: Aiming at the problem of direct conflict between autonomous vehicles and other humandriven vehicles at intersections,an autonomous vehicle behavior decision model is built,and deep reinforcement learning is used to train autonomous vehicles when passing road intersections,allowing autonomous vehicles to make autonomous decisions and achieve fast control of complex scenarios, and the comparison with the non-dominated sorting genetic algorithm-â…¡verifies the stability of the autonomous vehicle.
The simulation results show that the autonomous vehicle behavior decision-making method using the depth deterministic strategy gradient algorithm has better output speed to ensure the smooth changes of the throttle and brake values,and effectively solve the safety and comfort problems of autonomous vehicles.


title: An Efficiency Enhancing Methodology for Multiple Autonomous Vehicles in an Urban Network Adopting Deep Reinforcement Learning
abstract: To reduce the impact of congestion, it is necessary to improve our overall understanding of the influence of the autonomous vehicle.
Recently, deep reinforcement learning has become an effective means of solving complex control tasks.
Accordingly, we show an advanced deep reinforcement learning that investigates how the leading autonomous vehicles affect the urban network under a mixed-traffic environment.
We also suggest a set of hyperparameters for achieving better performance.
Firstly, we feed a set of hyperparameters into our deep reinforcement learning agents.
Secondly, we investigate the leading autonomous vehicle experiment in the urban network with different autonomous vehicle penetration rates.
Thirdly, the advantage of leading autonomous vehicles is evaluated using entire manual vehicle and leading manual vehicle experiments.
Finally, the proximal policy optimization with a clipped objective is compared to the proximal policy optimization with an adaptive Kullback-Leibler penalty to verify the superiority of the proposed hyperparameter.

We demonstrate that full automation traffic increased the average speed 1.27 times greater compared with the entire manual vehicle experiment.
Our proposed method becomes significantly more effective at a higher autonomous vehicle penetration rate.
Furthermore, the leading autonomous vehicles could help to mitigate traffic congestion.

title: Adaptive Stress Testing with Reward Augmentation for Autonomous Vehicle Validation
abstract: Determining possible failure scenarios is a critical step in the evaluation of autonomous vehicle systems.
Real world vehicle testing is commonly employed for autonomous vehicle validation, but the costs and time requirements are high.
Consequently, simulation driven methods such as Adaptive Stress Testing (AST) have been proposed to aid in validation.
AST formulates the problem of finding the most likely failure scenarios as a Markov decision process, which can be solved using reinforcement learning.
In practice, AST tends to find scenarios where failure is unavoidable and tends to repeatedly discover the same types of failures of a system.
This work addresses these issues by encoding domain relevant information into the search procedure.
With this modification, the AST method discovers a larger and more expressive subset of the failure space when compared to the original AST formulation.
We show that our approach is able to identify useful failure scenarios of an autonomous vehicle policy.

title: Autonomous RL: Autonomous Vehicle Obstacle Avoidance in a Dynamic Environment using MLP-SARSA Reinforcement Learning
abstract: This paper presents a Multi-Layer Perceptron-State Action Reward State Action (MLP-SARSA) based reinforcement learning methodology for dynamic obstacle detection and avoidance for autonomous vehicle navigation.

MLP-SARSA is an on-policy reinforcement learning approach, which gains information and rewards from the environment and helps the autonomous vehicle to avoid dynamic moving obstacles.
MLP with SARSA provides a significant advantage over dynamic environment compared to other traditional reinforcement algorithms.
In this study, a MLP-SARSA model is trained in a complex urban simulation environment with dynamic obstacles using the pygame library.
Experimental results show that the trained MLP-SARSA can navigate the autonomous vehicle in a dynamic environment with more confidences than traditional Q-learning and SARSA reinforcement algorithms.

title: Research on Decision Model of Autonomous Vehicle Based on Deep Reinforcement Learning
abstract: The Deep Q Network (DQN) model has been widely used in autonomous vehicle lane change decision in highway scenes, but the traditional DQN has the problems of overestimation and slow convergence speed.
Aiming at these problems, an autonomous vehicle lane changing decision model based on the improved DQN is proposed.
First, the obtained state values are input into two neural networks with the same structure and different parameter update frequencies to reduce the correlation between empirical samples, and then the hybrid strategy based on e-greedy and Boltzmann is used to make the vehicles explore the environment.

Finally, the model is trained and tested in the experimental scene built by the NGSIM dataset.
The experimental results show that the Double Deep Q Network (DDQN) model based on the hybrid strategy improves the success rate of the autonomous vehicle's lane-changing decision and the convergence speed of the network.


title: Connected Autonomous Vehicle Platoon Control Through Multi-agent Deep Reinforcement Learning
abstract: The rise of the artificial intelligence (AI) brings golden opportunity to accelerate the development of the intelligent transportation system (ITS).
The platoon control of connected autonomous vehicle (CAV) as the key technology exhibits superior for improving traffic system.
However, there still exist some challenges in multi-objective platoon control and multi-agent interaction.
Therefore, this paper proposed a connected autonomous vehicle latoon control approach with multi-agent deep reinforcement learning (MADRL).
Finally, the results in stochastic mixed traffic flow based on SUMO (simulation of urban mobility) platform demonstrate that the proposed method is feasible, effective and advanced.

title: Deep reinforcement learning approach for autonomous vehicle systems for maintaining security and safety using LSTM-GAN
abstract: The success of autonomous vehicles (AV hs) depends upon the effectiveness of sensors being used and the accuracy of communication links and technologies being employed.
But these sensors and communication links have great security and safety concerns as they can be attacked by an adversary to take the control of an autonomous vehicle by influencing their data.
Especially during the state estimation process for monitoring of autonomous vehicles' dynamics system, these concerns require immediate and effective solution.
In this paper we present a new adversarial deep reinforcement learning algorithm (NDRL) that can be used to maximize the robustness of autonomous vehicle dynamics in the presence of these attacks.
In this approach the adversary tries to insert defective data to the autonomous vehicle's sensor readings so that it can disrupt the safe and optimal distance between the autonomous vehicles traveling on the road.

The attacker tries to make sure that there is no more safe and optimal distance between the autonomous vehicles, thus it may lead to the road accidents.
Further attacker can also add fake data in such a way that it leads to reduced traffic flow on the road.
On the other hand, autonomous vehicle will try to defend itself from these types of attacks by maintaining the safe and optimal distance i.e.
by minimizing the deviation so that adversary does not succeed in its mission.
This attacker-autonomous vehicle action reaction can be studied through the game theory formulation with incorporating the deep learning tools.
Each autonomous vehicle will use Long-Short-Term-Memory (LSTM)-Generative Adversarial Network (GAN) models to find out the anticipated distance variation resulting from its actions and input this to the new deep reinforcement learning algorithm (NDRL) which attempts to reduce the variation in distance.

Whereas attacker also chooses deep reinforcement learning algorithm (NDRL) and wants to maximize the distance variation between the autonomous vehicles.
(c) 2020 Elsevier Inc. All rights reserved.

title: Research on decision algorithm for autonomous vehicle lane change based on vision DQN
abstract: Unmanned driving technology is current research hotspot in the field of artificial intelligence(AI).
Traditional deep Q network(DQN)has problems such as slow convergence speed in the vision-based lane change decision.
Aiming at this problem,a vision-based DQN autonomous vehicle lane-changing decision model is proposed.
Visual perception based on the attention mechanism is fused with DQN to focus the network on important image features,and introduces the Q-Masking mechanism to reduce the complexity of decision-making and speed up the convergence speed of DQN training.

Finally,a DQN-based speed decision algorithm is proposed to form a complete lane changing decision model,which is trained and tested in a simulation environment.
The experimental results show that the proposed model can realize a faster lane change decision-making strategy,and accelerate the convergence speed of DQN,at the same time.

title: Don't Get into Trouble! Risk-aware Decision-Making for Autonomous Vehicles
abstract: Risk is traditionally described as the expected likelihood of an undesirable outcome, such as a collision for an autonomous vehicle.
Accurately predicting risk or potentially risky situations is critical for the safe operation of an autonomous vehicle.
This work combines use of a controller trained to navigate around individuals in a crowd and a risk-based decision-making framework for an autonomous vehicle that integrates high-level risk-based path planning with a reinforcement learning-based lowlevel control.

We evaluated our method using a high-fidelity simulation environment.
We show our method results in zero collisions with pedestrians and predicted the least risky path, time to travel, or day to travel in approximately 72% of traversals.
This work can improve safety by allowing an autonomous vehicle to one day avoid and react to risky situations.

title: Double Deep Q-Learning and Faster R-CNN-Based Autonomous Vehicle Navigation and Obstacle Avoidance in Dynamic Environment
abstract: Autonomous vehicle navigation in an unknown dynamic environment is crucial for both supervised- and Reinforcement Learning-based autonomous maneuvering.
The cooperative fusion of these two learning approaches has the potential to be an effective mechanism to tackle indefinite environmental dynamics.
Most of the state-of-the-art autonomous vehicle navigation systems are trained on a specific mapped model with familiar environmental dynamics.
However, this research focuses on the cooperative fusion of supervised and Reinforcement Learning technologies for autonomous navigation of land vehicles in a dynamic and unknown environment.
The Faster R-CNN, a supervised learning approach, identifies the ambient environmental obstacles for untroubled maneuver of the autonomous vehicle.
Whereas, the training policies of Double Deep Q-Learning, a Reinforcement Learning approach, enable the autonomous agent to learn effective navigation decisions form the dynamic environment.
The proposed model is primarily tested in a gaming environment similar to the real-world.
It exhibits the overall efficiency and effectiveness in the maneuver of autonomous land vehicles.

title: Heuristic Reinforcement Learning Based Overtaking Decision for an Autonomous Vehicle
abstract: This paper proposes an intelligent overtaking decision based on the heuristic reinforcement learning method for an autonomous vehicle.
The proposed overtaking control focuses on the safety and efficiency of the autonomous vehicle driving.
Firstly, the overtaking problem is modeled and the adaptive safe driving area is constructed.
Then, a heuristic reinforcement learning method called Heu-Dyna is developed to derive the optimal overtaking decision, which introduces the heuristic planning function.
Besides, the generalized correlation coefficient is designed to evaluate the training perfection of the control strategy.
The simulation results show that the performance of the proposed method on the rapidity and optimality is superior to the Q-learning method and the Dyna method.
Furthermore, the adaptability of the proposed method is validated by applying different driving conditions.
Copyright (C) 2021 The Authors.

title: Deep Reinforcement Learning for Collision Avoidance of Autonomous Vehicle
abstract: To save training efforts, reinforcement learning approach is applied to the autonomous vehicle for obstacle avoidance.
Therefore, this study is aimed to let the autonomous vehicle to learn from mistakes and readdress its movement accuracy for collision avoidance in working environment.
An enhanced learning method Q-learning is used to record and update the Q values for different movement through a table that the autonomous vehicle can use it to determine how and where to move.
The Q table is learned through the deep learning neural network which may encounter innumerable situations from the environments and the different actions performed by the autonomous vehicle.
In the experiments, the depth camera is adopted as the input device to be not affected by light intensity and road color.
The Q table is ready to use after 9000 epochs or about 3.5 hours training.
Let the autonomous vehicle run for 3 minutes at a time in three different environments with lights on and off 10 times each.
The success rate of obstacle avoidance is as high as 95% which proves the feasibility of proposed approach.

title: A Route Planning for Autonomous Vehicle in 5G and Edge Computing Environment
abstract: Route planning have a huge impact on the safe driving and energy efficiency of autonomous vehicle, which is an inevitable part of Internet of Vehicle.
In the 5G edge environment, autonomous vehicle needs to communicate with other vehicles and base stations, route planning needs to consider the communication overhead of autonomous vehicle in addition to the driving distance.

However, the current route planning methods only consider the shortest path, which cannot comprehensively consider the distance overhead and time cost.
To cope with these challenges, this paper proposes an algorithm based on reinforcement learning to solve autonomous vehicle route planning through 5G networks and edge computing(RLVRP), which can obtain the goal of minimizing the driving distance based on minimizing the task processing time and server response time.

In addition, we update our route planning strategy according to the dynamic change of network resources.
Extensive experimental results show that the proposed algorithm greatly reduces service delay compared with state-of-the-art baselines.

title: Skills to Drive: Successor Features for Autonomous Highway Pilot
abstract: Reinforcement learning applications are spreading among different domains, including autonomous vehicle control.
The diverse situations that can happen during, for instance, at a highway commute are infinite, and with labeled data, the perfect coverage of all use-cases sounds ambitious.
However, with the complex tasks and complicated scenarios faced during an autonomous vehicle system design, the credit assignment problem arises.
How to construct appropriate objectives for the artificial intelligence to learn and the preferences between the different goals also matter of the designer's choice.
This work attempts to tackle the problem by utilizing successor features and providing a possible decomposition of the reward functions, guiding the agent's actions.
This method makes the training easier for the agent and enables immediate, profound performance on new combined tasks.
Furthermore, with the optimal composition, the desired behavior can be fine-tuned, and as an auxiliary gain, the decomposition empowers different driving styles and makes driving preferences rapidly changeable.

We introduce the adaptation of FastRL algorithm to autonomous vehicle domain, meanwhile developing a stabilizing way of using Successor Features, namely DoubleFastRL.
We compare our solution for a highway driving scenario with basic agents such as Q-learning having multi-objective training.

title: Decision-Making for Oncoming Traffic Overtaking Scenario using Double DQN
abstract: Great progress has been made in the field of machine learning in recent years.
And learning-based methods have been widely utilized for developing highly autonomous vehicle.
To this end, we introduce a reinforcement learning based intelligent autonomous vehicle decision making method for oncoming overtaking scenario.
The goal of reinforcement learning is to learn how to take optimal decision in corresponding observations through interactions with the environment using a reward function to estimate whether the decision is good or not.

A Double Deep Q-learning (Double DQN) agent was used to learn policies (control strategies) for both longitudinal speed and lane change decision.
Prioritized Experience Replay (PER) was used to accelerate convergence of the policies.
A two-way 3-car scenario with oncoming traffic was established in SUMO (Simulation of Urban Mobility) to train and test the policies.

title: Highway Traffic Modeling and Decision Making for Autonomous Vehicle Using Reinforcement Learning
abstract: This paper studies the decision making problem of autonomous vehicles in traffic.
We model the interaction between an autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an experienced driver as the target to be learned.

The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles.
By designing the reward function of the MDP, the desired, driving behavior of the autonomous vehicle is obtained using reinforcement learning.
Simulated results demonstrate the desired driving behaviors of an autonomous vehicle.

title: Navigation of Autonomous Vehicles using Reinforcement Learning with Generalized Advantage Estimation
abstract: This study proposes a reinforcement learning ap-proach using Generalized Advantage Estimation (GAE) for autonomous vehicle navigation in complex environments.
The method is based on the actor-critic framework, where the actor network predicts actions and the critic network estimates state values.
GAE is used to compute the advantage of each action, which is then used to update the actor and critic networks.
The approach was evaluated in a simulation of an autonomous vehicle navigating through challenging environments and it was found to effectively learn and improve navigation performance over time.
The results suggest GAE as a promising direction for further research in autonomous vehicle navigation in complex environments.

title: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions
abstract: Autonomous vehicles must operate safely in their dynamic and continuously-changing environment.
However, the operating environment of an autonomous vehicle is complicated and full of various types of uncertainties.
Additionally, the operating environment has many configurations, including static and dynamic obstacles with which an autonomous vehicle must avoid collisions.
Though various approaches targeting environment configuration for autonomous vehicles have shown promising results, their effectiveness in dealing with a continuous-changing environment is limited.
Thus, it is essential to learn realistic environment configurations of continuously-changing environment, under which an autonomous vehicle should be tested regarding its ability to avoid collisions.
Featured with agents dynamically interacting with the environment, Reinforcement Learning (RL) has shown great potential in dealing with complicated problems requiring adapting to the environment.
To this end, we present an RL-based environment configuration learning approach, i.e., DeepCollision, which intelligently learns environment configurations that lead an autonomous vehicle to crash.
DeepCollision employs Deep Q-Learning as the RL solution, and selects collision probability as the safety measure, to construct the reward function.
We trained four DeepCollision models and conducted an experiment to compare them with two baselines, i.e., random and greedy.
Results show that DeepCollision demonstrated significantly better effectiveness in generating collisions compared with the baselines.
We also provide recommendations on configuring DeepCollision with the most suitable time interval based on different road structures.

title: Hierarchical reinforcement learning method for autonomous vehicle behavior planning [arXiv]
abstract: In this work, we propose a hierarchical reinforcement learning (HRL) structure which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals.
In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other similar tasks with the same sub-goals.

The states are defined as processed observations which are transmitted from the perception system of the autonomous vehicle.
A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure.
Compared to traditional RL methods, our algorithm is more sample-efficient since its modular design allows reusing the policies of sub-goals across similar tasks.
The results show that the proposed method converges to an optimal policy faster than traditional RL methods.

title: Obstacle Avoidance System on Autonomous Car Using D3QN
abstract: An autonomous vehicle's triumphant and safe navigation, which can circumvent obstacles, necessitates a skill set encompassing steering wheel control, sometimes called obstacle avoidance.
One potential approach to address this issue is using a simulation framework wherein an automobile is subjected to various barriers.
During this simulation, the sensory input of the car, as well as its corresponding actions, are recorded and analyzed.
An alternative approach involves allowing the vehicle to autonomously acquire knowledge to optimize its performance towards the desired objective.
The Dueling Deep Double QNetworks (D3QN) approach is a strategy that enables the model to autonomously learn and optimize its performance to attain the most favorable conclusion.
The D3QN architecture is a computational framework incorporating Dueling and double-Q processes.
The implementation of D3QN is anticipated to result in a reduction in the training time required for an autonomous vehicle.
This study is expected to substitute for training an autonomous vehicle.

title: Discrete-time Finite Horizon Adaptive Dynamic Programming for Autonomous Vehicle Control
abstract: Nowadays, the autonomous vehicle control has become more and more critical, especially for the trajectory tracking problem.
The existing model predictive control method applied to autonomous vehicle can reduce the tracking error.
However, it requires online calculation for the model recurrence in long horizon, which causes heavy computational burden.
This paper presents a novel discrete-time model predictive control solving framework.
A finite horizon adaptive dynamic programming method was proposed to estimate the control policy for trajectory tracking.
Under the constraint of self consistent condition, the control policy is fitted through deep neural network as a parameterized function to obtain faster solution.
The performance of the proposed method is tested and verified under double lane change conditions via CarSim.
The experimental results show that the system can accurately track given reference trajectory with 5000Hz frequency, which is three times faster than that solved by least squares estimation method.

title: Deep Reinforcement Learning for Collision Avoidance of Autonomous Vehicle
abstract: To save training efforts, reinforcement learning approach is applied to the autonomous vehicle for obstacle avoidance.
Therefore, this study is aimed to let the autonomous vehicle to learn from mistakes and readdress its movement accuracy for collision avoidance in working environment.
An enhanced learning method Q-learning is used to record and update the Q values for different movement through a table that the autonomous vehicle can use it to determine how and where to move.
The Q table is learned through the deep learning neural network which may encounter innumerable situations from the environments and the different actions performed by the autonomous vehicle.
In the experiments, the depth camera is adopted as the input device to be not affected by light intensity and road color.
The Q table is ready to use after 9000 epochs or about 3.5 hours training.
Let the autonomous vehicle run for 3 minutes at a time in three different environments with lights on and off 10 times each.
The success rate of obstacle avoidance is as high as 95% which proves the feasibility of proposed approach.

title: Learning Robust Control Policies for End-to-End Autonomous Driving From Data-Driven Simulation
abstract: In this work, we present a data-driven simulation and training engine capable of learning end-to-end autonomous vehicle control policies using only sparse rewards.
By leveraging real, human-collected trajectories through an environment, we render novel training data that allows virtual agents to drive along a continuum of new local trajectories consistent with the road appearance and semantics, each with a different view of the scene.

We demonstrate the ability of policies learned within our simulator to generalize to and navigate in previously unseen real-world roads, without access to any human control labels during training.
Our results validate the learned policy onboard a full-scale autonomous vehicle, including in previously un-encountered scenarios, such as new roads and novel, complex, near-crash situations.
Our methods are scalable, leverage reinforcement learning, and apply broadly to situations requiring effective perception and robust operation in the physical world.

title: Enhanced Image Preprocessing Method for an Autonomous Vehicle Agent System
abstract: Excessive training time is a major issue face when training autonomous vehicle agents with neural networks by using images as input.
This paper proposes a deep time-economical Q network (DQN) input image preprocessing method to train an autonomous vehicle agent in a virtual environment.
The environmental information is extracted from the virtual environment.
A top-view image of the entire environment is then redrawn according to the environmental information.
During training of the DQN model, the top-view image is cropped to place the vehicle agent at the center of the cropped image.
The current frame top-view image is combined with the images from the previous two training iterations.
The DQN model use this combined image as input.
The experimental results indicate higher performance and shorter training time for the DQN model trained with the preprocessed images compared with that trained without preprocessing.

title: Game Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems
abstract: Autonomous driving has been the subject of increased interest in recent years both in industry and in academia.
Serious efforts are being pursued to address legal, technical, and logistical problems and make autonomous cars a viable option for everyday transportation.
One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience.

Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where interactions among multiple drivers and vehicles occur simultaneously.

Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help to decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests.

In this paper, we present a game theoretic traffic model that can be used to: 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system.

We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.


title: Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation
abstract: Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia.
This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events.
Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition.
Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged.

A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.


title: Research on Decision Model of Autonomous Vehicle Based on Deep Reinforcement Learning
abstract: The Deep Q Network (DQN) model has been widely used in autonomous vehicle lane change decision in highway scenes, but the traditional DQN has the problems of overestimation and slow convergence speed.
Aiming at these problems, an autonomous vehicle lane changing decision model based on the improved DQN is proposed.
First, the obtained state values are input into two neural networks with the same structure and different parameter update frequencies to reduce the correlation between empirical samples, and then the hybrid strategy based on s-greedy and Boltzmann is used to make the vehicles explore the environment.

Finally, the model is trained and tested in the experimental scene built by the NGSIM dataset.
The experimental results show that the Double Deep Q Network (DDQN) model based on the hybrid strategy improves the success rate of the autonomous vehicle's lane-changing decision and the convergence speed of the network.


title: Decision-Making for Complex Scenario using Safe Reinforcement Learning
abstract: In recent years, machine learning is widely used in many fields.
Compared with the rule-based method, machine learning plays a more excellent role in the decision-making of the autonomous vehicle.
Some complex situations are often met in our daily life.
To this end, Safe reinforcement learning(RL) is introduced to ensure that safer actions are selected.
Constant Turn Rate and Acceleration(CTRA) model is first used to predict the future trajectories of surrounding vehicles.
Then Double Deep Q-Learning(DDQN) method is used to make decisions and ensure the autonomous vehicle can move at the desired speed as much as possible.
In order to achieve a safer decision-making, some safety rules are introduced.
Finally, the algorithm is demonstrated in Simulation of Urban Mobility(SUMO) and has been proved to have an outstanding performance on such a complex scenario.

title: Towards Automated Safety Coverage and Testing for Autonomous Vehicles with Reinforcement Learning [arXiv]
abstract: The kind of closed-loop verification likely to be required for autonomous vehicle (AV) safety testing is beyond the reach of traditional test methodologies and discrete verification.
Validation puts the autonomous vehicle system to the test in scenarios or situations that the system would likely encounter in everyday driving after its release.
These scenarios can either be controlled directly in a physical (closed-course proving ground) or virtual (simulation of predefined scenarios) environment, or they can arise spontaneously during operation in the real world (open-road testing or simulation of randomly generated scenarios).

In AV testing, simulation serves primarily two purposes: to assist the development of a robust autonomous vehicle and to test and validate the AV before release.
A challenge arises from the sheer number of scenario variations that can be constructed from each of the above sources due to the high number of variables involved (most of which are continuous).
Even with continuous variables discretized, the possible number of combinations becomes practically infeasible to test.
To overcome this challenge we propose using reinforcement learning (RL) to generate failure examples and unexpected traffic situations for the AV software implementation.
Although reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving.


title: Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning
abstract: Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion.
They represent the main trend in future intelligent transportation systems.
This paper concentrates on the planning problem of autonomous vehicles in traffic.
We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MOP) and consider the driving style of an expert driver as the target to be learned.

The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles.
The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques.

Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning.
The unknown reward function of the expert driver is approximated using a deep neural-network (DNN).
We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function.

Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques.
(C) 2019 Elsevier B.V. All rights reserved.

title: SMART: A Decision-Making Framework with Multi-modality Fusion for Autonomous Driving Based on Reinforcement Learning
abstract: Decision-making in autonomous driving is an emerging technology that has rapid progress over the last decade.
In single-lane scenarios, autonomous vehicles should simultaneously optimize their velocity decisions and steering angle decisions to achieve safety, efficiency, comfort, small impacts on rear vehicles, and small offsets to the lane center line.

Previous studies, however, have typically optimized these two decisions separately, ignoring the potential relationship between them.
In this work, we propose a decision-making framework, named SMART (deci S ion- M aking fr A mework based on R einforcemen T learning), to optimize the velocity and steering angle of the autonomous vehicle in parallel.

In order for the autonomous vehicle to effectively perceive the curvature of the lane and interactions with other vehicles, we adopt a graph attention mechanism to extract and fuse the features from different modalities (i.
e.
, sensor-collected vehicle states and camera-collected lane information).

Then a hybrid reward function takes into account aspects of safety, efficiency, comfort, impact, and lane centering to instruct the autonomous vehicle to make optimal decisions.
Furthermore, our framework enables the autonomous vehicle to adaptively choose the duration of an action, which helps the autonomous vehicle pursue higher reward values.
Extensive experiments evidence that SMART significantly outperforms the existing methods in multiple metrics.

title: Stability Analysis for Autonomous Vehicle Navigation Trained over Deep Deterministic Policy Gradient
abstract: The Deep Deterministic Policy Gradient (DDPG) algorithm is a reinforcement learning algorithm that combines Q-learning with a policy.
Nevertheless, this algorithm generates failures that are not well understood.
Rather than looking for those errors, this study presents a way to evaluate the suitability of the results obtained.
Using the purpose of autonomous vehicle navigation, the DDPG algorithm is applied, obtaining an agent capable of generating trajectories.
This agent is evaluated in terms of stability through the Lyapunov function, verifying if the proposed navigation objectives are achieved.
The reward function of the DDPG is used because it is unknown if the neural networks of the actor and the critic are correctly trained.
Two agents are obtained, and a comparison is performed between them in terms of stability, demonstrating that the Lyapunov function can be used as an evaluation method for agents obtained by the DDPG algorithm.

Verifying the stability at a fixed future horizon, it is possible to determine whether the obtained agent is valid and can be used as a vehicle controller, so a task-satisfaction assessment can be performed.

Furthermore, the proposed analysis is an indication of which parts of the navigation area are insufficient in training terms.

title: Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning [arXiv]
abstract: In this paper, we demonstrate the first successful zero-shot transfer of an autonomous driving policy directly from simulator to a scaled autonomous vehicle under stochastic disturbances.
Using adversarial multi-agent reinforcement learning, in which an adversary perturbed both the inputs and outputs of the autonomous vehicle during training, we train an autonomous vehicle to manage a roundabout merge in the presence of an adversary in simulation.

At test time in hardware, the adversarial policy successfully reproduces the simulated ramp metering behavior and outperforms both a human driving baseline and adversary-free trained policies.
Finally, we demonstrate that the addition of adversarial training considerably improves the stability and robustness of policies being transferred to the real world.
Supplementary information and videos can be found at: (https://sites.google.com/view/ud-ids-lab/arlv).

title: Uncertainty-Aware Reinforcement Learning for Safe Control of Autonomous Vehicles in Signalized Intersections
abstract: This paper proposes a reinforcement learning approach for the control of autonomous vehicles at signalized intersections.
The proposed method is a modified version of the Q-learning approach that takes into account the risky scenarios that might arise in the control of an autonomous vehicle due to the inherent uncertainties in the system.

The proposed algorithm enables robust and risk-aware decision-making in uncertain and sensitive environments.
The proposed algorithm is evaluated in a simulated autonomous vehicle scenario, where it outperforms the standard Q-learning in terms of safety.

title: Anti-lock braking systems
abstract: The data provided summarises the model used to describe the dynamics of the anti-lock braking system.
This includes the specifics of the model-free control method.
The results of the systems are provided as well.
Copyright: CC BY 4.0

title: Hierarchical Reinforcement Learning Method for Autonomous Vehicle Behavior Planning
abstract: Behavioral decision making is an important aspect of autonomous vehicles (AV).
In this work, we propose a behavior planning structure based on hierarchical reinforcement learning (HRL) which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals.

In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other tasks with the same sub-goals.

A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure.
Compared to traditional RL methods, our algorithm is more sample-efficient, since its modular design allows reusing the policies of sub-goals across similar tasks for various transportation scenarios.
The results show that the proposed method converges to an optimal policy faster than traditional RL methods.

title: SLAV-Sim: A Framework for Self-Learning Autonomous Vehicle Simulation
abstract: With the advent of autonomous vehicles, sensors and algorithm testing have become crucial parts of the autonomous vehicle development cycle.
Having access to real-world sensors and vehicles is a dream for researchers and small-scale original equipment manufacturers (OEMs) due to the software and hardware development life-cycle duration and high costs.

Therefore, simulator-based virtual testing has gained traction over the years as the preferred testing method due to its low cost, efficiency, and effectiveness in executing a wide range of testing scenarios.

Companies like ANSYS and NVIDIA have come up with robust simulators, and open-source simulators such as CARLA have also populated the market.
However, there is a lack of lightweight and simple simulators catering to specific test cases.
In this paper, we introduce the SLAV-Sim, a lightweight simulator that specifically trains the behaviour of a self-learning autonomous vehicle.
This simulator has been created using the Unity engine and provides an end-to-end virtual testing framework for different reinforcement learning (RL) algorithms in a variety of scenarios using camera sensors and raycasts.


title: Learning to Navigate Intersections with Unsupervised Driver Trait Inference
abstract: Navigation through uncontrolled intersections is one of the key challenges for autonomous vehicles.
Identifying the subtle differences in hidden traits of other drivers can bring significant benefits when navigating in such environments.
We propose an unsupervised method for inferring driver traits such as driving styles from observed vehicle trajectories.
We use a variational autoencoder with recurrent neural networks to learn a latent representation of traits without any ground truth trait labels.
Then, we use this trait representation to learn a policy for an autonomous vehicle to navigate through a T-intersection with deep reinforcement learning.
Our pipeline enables the autonomous vehicle to adjust its actions when dealing with drivers of different traits to ensure safety and efficiency.
Our method demonstrates promising performance and outperforms state-of-the-art baselines in the T-intersection scenario.

title: Autonomous Vehicles Roundup Strategy by Reinforcement Learning with Prediction Trajectory
abstract: Autonomous vehicles are increasingly applied on many situations, but their autonomous decision-making ability needs to be improved.
Multi-Agent Deep Deterministic Policy Gradient(MADDPG) adopts the method of centralized evaluation and decentralized execution, so that the autonomous vehicle can obtain the whole-field status information and make decisions through the companion information.

In the process of autonomous vehicle training, we introduce artificial potential field, action guidance and other methods to alleviate the problem of sparse rewards.
At the same time, we add a repulsion function to consider the relationship between team vehicles.
Extended Kalman Filter(EKF) is also applied to predict the autonomous vehicle trajectory, changing the training network state input information.
At the same time, secondary correction of the predicted autonomous vehicle trajectory is made to change the prediction range with the training time, and improve the training convergence speed while the speed of opposite agents increases.

Simulation experiments show that the convergence speed and win rate ofMADDPG algorithm based on trajectory prediction and artificial potential field is significantly improved, and it also has strong adaptability to various task scenarios.


title: A Comparison of Autonomous Vehicle Navigation Simulators Under Regulatory and Reinforcement Learning Constraints
abstract: The transition from conventional vehicles to autonomous vehicles is regulated thorough ADAS (Advanced Driver Assistance Systems) functionalities.
The combination of different ADAS functions allows vehicles navigate on a highway autonomously, but at the same time, following the traffic rules and regulations requirements, and also guaranteeing safety on the road.

The practical objective in this article is to implement a Reinforcement Learning method whose actions are based in these regulated functions for autonomous vehicles navigation.
With this aim, a study of the state-of-the-art of autonomous vehicles simulators has been completed.
Hence, the algorithm will be tested using a five-lane highway simulator, previously selected.
Results and performance of the model through experimentation will be presented and evaluated using the simulator for different network architectures.

title: Uncertainty-Aware Reinforcement Learning for Safe Control of Autonomous Vehicles in Signalized Intersections
abstract: This paper proposes a reinforcement learning approach for the control of autonomous vehicles at signalized intersections.
The proposed method is a modified version of the Q-learning approach that takes into account the risky scenarios that might arise in the control of an autonomous vehicle due to the inherent uncertainties in the system.

The proposed algorithm enables robust and risk-aware decision-making in uncertain and sensitive environments.
The proposed algorithm is evaluated in a simulated autonomous vehicle scenario, where it outperforms the standard Q-learning in terms of safety.

title: Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach
abstract: Reinforcementlearning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants.
However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures.

In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations.
Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles.
A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties.
Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently.
Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds.

Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities.
The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.


title: Deep Reinforcement Learning-based Optimal Time-constrained Intercept Guidance
abstract: In this work, we design the guidance strategy for an autonomous vehicle to rendezvous with a stationary target at a time specified a priori.
We first design the suitable guidance law for the autonomous vehicle to steer it on the desired trajectory towards the target.
In particular, our aim is to drive the necessary error variable to zero in an optimal fashion by minimizing a meaningful cost function.
In essence, the proposed method provides an optimal transient performance for the autonomous vehicle to rendezvous with the stationary target precisely at the desired time.
As the optimal transient behavior depends on the design parameters, especially the controller gain, we leverage the proximal policy optimization in the reinforcement learning framework to obtain the required design parameter.

We show through simulations that the proposed technique is energy-efficient when compared to other finite/fixed-time convergent guidance laws.

title: Autonomous Vehicle Driving Path Control with Deep Reinforcement Learning
abstract: Autonomous vehicle (AV) uses the artificial intelligence (AI) technologies to control the vehicle without human intervention.
The implementation of AV has the advantages over the human- driven vehicle such as reducing the road traffic deaths that caused by human errors, increasing the traffic efficiency and minimizing the carbon emission to save the environment.

The main objective of this paper is to develop an AV that keeps a safe distance while following the lead car and remains at the centerline of road.
The proposed Deep Reinforcement Learning (DRL) algorithm for the autonomous driving simulation is Deep Deterministic Policy Gradient (DDPG).
In this paper, the DDPG model for the path following control, reward function, actor network and critic network are created.
The DDPG agent has been trained until 1650-episode rewards have been received.
After the training, the proposed DDPG agent has been simulated to verify the performance.
Then, the values of the two hyperparameters, which are mini-batch size and actor learning rate, are tuned to obtain the shortest training time.

title: ROBUST DEEP REINFORCEMENT LEARNING FOR UNDERWATER NAVIGATION WITH UNKNOWN DISTURBANCES
abstract: We study an underwater navigation problem, where an Underwater Autonomous Vehicle must reach a target position in the presence of a disturbance that may be unknown.
In order to deal with this problem, we make use of Deep Reinforcement Learning tools, and more concretely, we make use of robust control ideas, which allow training an agent in the presence of uncertainty.

We propose a robust Proximal Policy Optimization agent and train it using simulations of an underwater medium: this agent shows an excellent performance when facing unknown disturbances, being able to approach the performance of the optimal agent which had an exact knowledge of the underwater disturbance.


title: Reinforcement Learning-Based Navigation Approach for a Downscaled Autonomous Vehicle in Simplified Urban Scenarios
abstract: This paper presents the implementation of a reinforcement learning based navigation architecture for autonomous vehicles in urban scenarios.
These types of scenarios represent a challenging task due to the presence of dynamic and static road elements.
This work validates the use and feasibility of high-level reinforcement learning controllers in the autonomous vehicle software pipeline.
Tests are performed using a 1:10 downscaled autonomous prototype on a track with one main and two secondary roads.
The platform is equipped with a LIDAR, inertial measurement units, a stereo camera and motor drives for steering and propulsion.
Experiments yield favorable outcomes in terms of collision avoidance, lane keeping and navigational comfort.

title: Fault-Tolerant Multiplayer Tracking Control for Autonomous Vehicle via Model-Free Adaptive Dynamic Programming
abstract: This article investigates the completely unknown autonomous vehicle tracking issues with actuator faults through model-free adaptive dynamic programming (MFADP) approaches.
Because partial parameters are measured difficultly or inaccurately, the model-based control theories are imperfect for the vehicles.
Therefore, the proposed multiplayer optimal control method in this work, which is not necessary to know the prior system knowledge, achieves the purpose of unknown vehicle tracking control via a novel MFADP theory.

Besides, the control strategies are robust, which contain adaptive regulators to eliminate the disturbance of the vehicle systems caused by actuator faults, modeling errors, and curvature interference.

To reduce the computational burden of control, a single neural network (NN) architecture is constructed with minimal computational cost and fast response speed.
In addition, the convergence analysis of the NN structure, the stability and robustness analysis of identification, and the control schemes in this work are supplied.
Finally, two driving scenario simulations are shown to prove the effectiveness of the established controller.

title: Safe Reinforcement Learning on Autonomous Vehicles [arXiv]
abstract: There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications.

Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems.
We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle.

[IROS 2018].

title: RISE: A Velocity Control Framework with Minimal Impacts based on Reinforcement Learning
abstract: Velocity control in autonomous driving is an emerging technology that has achieved rapid progress over the last decade.
However, existing velocity control models are developed in single-lane scenarios and ignore the negative impacts caused by harsh velocity changes.
In this work, we propose a velocity control framework based on reinforcement learning, called RISE (contRol velocIty for autonomouS vEhicle).
In multi-lane circumstances, RISE improves velocity decisions regarding the autonomous vehicle itself, while minimizing impacts on rear vehicles.
To achieve multiple objectives, we propose a hybrid reward function to rate each velocity decision from four aspects: safety, efficiency, comfort, and negative impact to guide the autonomous vehicle.
Among these reward factors, the negative impact is used to penalize the harsh actions of the autonomous vehicle, thus prompting it to reduce the negative impacts on its rear vehicles.
To detect the latent perturbations among surrounding vehicles in multiple lanes, we propose an attention-based encoder to learn the positions and interactions from an impact graph.
Extensive experiments evidence that RISE enables safe driving, and outperforms state-of-the-art methods in efficiency, comfort, and alleviating negative impacts.

title: Learning Autonomous Vehicle Safety Concepts from Demonstrations
abstract: Evaluating the safety of an autonomous vehicle (AV) depends on the behavior of surrounding agents which can be heavily influenced by factors such as environmental context and informally-defined driving etiquette.

A key challenge is in determining a minimum set of assumptions on what constitutes reasonable foreseeable behaviors of other road users for the development of AV safety models and techniques.
In this paper, we propose a data-driven AV safety design methodology that first learns "reasonable" behavioral assumptions from data, and then synthesizes an AV safety concept using these learned behavioral assumptions.

We borrow techniques from control theory, namely high-order control barrier functions and Hamilton-Jacobi reachability, to provide inductive bias to aid interpretability, verifiability, and tractability of our approach.

In our experiments, we learn an AV safety concept using demonstrations collected from a highway traffic-weaving scenario, compare our learned concept to existing baselines, and showcase its efficacy in evaluating real-world driving logs.


title: Autonomous vehicle driving path control with deep reinforcement learning
abstract: Autonomous vehicle (AV) uses the artificial intelligence (AI) technologies to control the vehicle without human intervention.
The implementation of AV has the advantages over the human-driven vehicle such as reducing the road traffic deaths that caused by human errors, increasing the traffic efficiency and minimizing the carbon emission to save the environment.

The main objective of this paper is to develop an AV that keeps a safe distance while following the lead car and remains at the centerline of road.
The proposed Deep Reinforcement Learning (DRL) algorithm for the autonomous driving simulation is Deep Deterministic Policy Gradient (DDPG).
In this paper, the DDPG model for the path following control, reward function, actor network and critic network are created.
The DDPG agent has been trained until 1650-episode rewards have been received.
After the training, the proposed DDPG agent has been simulated to verify the performance.
Then, the values of the two hyperparameters, which are mini-batch size and actor learning rate, are tuned to obtain the shortest training time.

title: Embed Trajectory Imitation in Reinforcement Learning: A Hybrid Method for Autonomous Vehicle Planning
abstract: Learning-based autonomous vehicle trajectory planning methods have shown excellent performance in a variety of complex traffic scenarios.
However, the existing imitation learning (IL) and reinforcement learning (RL) algorithms still have their limitations, such as poor safety and generalizability for IL, and low data efficiency for RL.
To leverage their respective advantages and mitigate the limitations, this paper proposes a novel hybrid RL algorithm for autonomous vehicle planning, where IL is embedded in it to guide its exploration with expert knowledge.

Different from existing approaches, we use multi-step trajectory prediction instead of behavior cloning as the IL method integrated with online RL.
Through such design, we make a further step in the research about how expert demonstration can be helpful to RL.
Moreover, we conduct parallel training and testing of the algorithm based on real-world driving data.
Experimental results demonstrate that our proposed approach outperforms standalone IL and RL methods, and performs better than RL methods enhanced by behavior cloning.

title: Combined Longitudinal and Lateral Control of Autonomous Vehicles based on Reinforcement Learning
abstract: In this paper, in order for the autonomous vehicle to keep a desired distance from the preceding vehicle and stay in the lane, a data-driven optimal control approach is proposed.
Firstly, the dynamics of the autonomous vehicle is derived.
In order to overcome the cutting-edge limitation, a virtual preceding vehicle is defined which is perpendicular to the preceding vehicle.
The tracking error is defined as the deviation between the look ahead point of the autonomous vehicle and the virtual preceding vehicle.
Then, the error system is derived.
Secondly, based on the error system, in order to minimize the cost determined by the tracking error and the energy consumption, the Hamilton-Jacobi-Bellman (HJB) equation is established.
A model-based policy iteration technique is proposed to solve the HJB equation.
Thirdly, a two-phase data-driven policy iteration algorithm is proposed and implemented by using adaptive dynamic programming (ADP).
The efficacy of the proposed data-driven optimal control approach is validated by computer simulations.

title: Don't Get into Trouble! Risk-aware Decision-Making for Autonomous Vehicles
abstract: Risk is traditionally described as the expected likelihood of an undesirable outcome, such as a collision for an autonomous vehicle.
Accurately predicting risk or potentially risky situations is critical for the safe operation of an autonomous vehicle.
This work combines use of a controller trained to navigate around individuals in a crowd and a risk-based decision-making framework for an autonomous vehicle that integrates high-level risk-based path planning with a reinforcement learning-based low-level control.

We evaluated our method using a high-fidelity simulation environment.
We show our method results in zero collisions with pedestrians and predicted the least risky path, time to travel, or day to travel in approximately 72% of traversals.
This work can improve safety by allowing an autonomous vehicle to one day avoid and react to risky situations.

title: Multi-Agent Reinforcement Learning for Highway Platooning
abstract: The advent of autonomous vehicles has opened new horizons for transportation efficiency and safety.
Platooning, a strategy where vehicles travel closely together in a synchronized manner, holds promise for reducing traffic congestion, lowering fuel consumption, and enhancing overall road safety.
This article explores the application of Multi-Agent Reinforcement Learning (MARL) combined with Proximal Policy Optimization (PPO) to optimize autonomous vehicle platooning.
We delve into the world of MARL, which empowers vehicles to communicate and collaborate, enabling real-time decision making in complex traffic scenarios.
PPO, a cutting-edge reinforcement learning algorithm, ensures stable and efficient training for platooning agents.
The synergy between MARL and PPO enables the development of intelligent platooning strategies that adapt dynamically to changing traffic conditions, minimize inter-vehicle gaps, and maximize road capacity.

In addition to these insights, this article introduces a cooperative approach to Multi-Agent Reinforcement Learning (MARL), leveraging Proximal Policy Optimization (PPO) to further optimize autonomous vehicle platooning.

This cooperative framework enhances the adaptability and efficiency of platooning strategies, marking a significant advancement in the pursuit of intelligent and responsive autonomous vehicle systems.

title: Deep Q-network implementation for simulated autonomous vehicle control
abstract: Deep reinforcement learning is poised to be a revolutionised step towards newer possibilities in solving navigation and autonomous vehicle control tasks.
Deep Q-network (DQN) is one of the more popular methods of deep reinforcement learning that allows the agent that controls the vehicle to learn through its mistakes based on its actions and interactions with the environment.

This paper presents the implementation of DQN to an autonomous self-driving vehicle control in two different simulated environments; first environment is in Python which is a simple 2D environment and then advanced to Unity software separately which is a 3D environment.

Based on the scores and pixel inputs, the agent in the vehicle learns and adapts to its surrounding.
It develops the best solution strategy to direct itself in the environment where its task is to manoeuvre the vehicle from point to point on a simulated highway scenario.
The implemented DQN technique approximates the action value function with convolutional neural network.
This evaluates the Q-function for the Q-learning architecture and updates the action value function.
This paper shows that DQN is an effective learning method for the agent of an autonomous vehicle.
In both simulated environments, the autonomous vehicle gradually learnt the manoeuvre operations and progressively gained the ability to successfully navigate itself and avoid obstacles without prior information of the surrounding.


title: Proximal Policy Optimization Through a Deep Reinforcement Learning Framework for Multiple Autonomous Vehicles at a Non-Signalized Intersection
abstract: Advanced deep reinforcement learning shows promise as an approach to addressing continuous control tasks, especially in mixed-autonomy traffic.
In this study, we present a deep reinforcement-learning-based model that considers the effectiveness of leading autonomous vehicles in mixed-autonomy traffic at a non-signalized intersection.
This model integrates the Flow framework, the simulation of urban mobility simulator, and a reinforcement learning library.
We also propose a set of proximal policy optimization hyperparameters to obtain reliable simulation performance.
First, the leading autonomous vehicles at the non-signalized intersection are considered with varying autonomous vehicle penetration rates that range from 10% to 100% in 10% increments.
Second, the proximal policy optimization hyperparameters are input into the multiple perceptron algorithm for the leading autonomous vehicle experiment.
Finally, the superiority of the proposed model is evaluated using all human-driven vehicle and leading human-driven vehicle experiments.
We demonstrate that full-autonomy traffic can improve the average speed and delay time by 1.38 times and 2.55 times, respectively, compared with all human-driven vehicle experiments.
Our proposed method generates more positive effects when the autonomous vehicle penetration rate increases.
Additionally, the leading autonomous vehicle experiment can be used to dissipate the stop-and-go waves at a non-signalized intersection.

title: Lane Changing of Autonomous Vehicle Based on TD3Algorithm in Human-machine Hybrid Driving Environment
abstract: Improving the human acceptance of autonomous vehicles in the future is important,and deep reinforcement learning is a key technology for their acceptance.
To solve the lane-changing decision problem in a human-machine hybrid driving traffic flow,this study used the deep reinforcement-learning algorithm Twin Delayed Deep Deterministic Policy Gradient (TD3) to realize the free-lane-changing behavior of an autonomous vehicle.
First,the theoretical framework of reinforcement learning based on the Markov decision process was introduced.
Then,according to the driving data from actual conditions in the NGSIM dataset,a six-lane moderate traffic-congestion simulation scene was established using the autonomous driving simulator NGSIM-ENV.
Other non-autonomous vehicles were controlled according to the recorded data in NGSIM.
For lane-changing decision making in a continuous action space,the TD3 algorithm was used to develop a lane-changing model to control the driving behavior of the autonomous vehicle.
In the proposed lane-changing model,the state space that contained the selfvehicle and environment information and the action space,which included the vehicle acceleration and heading angle,were established.
Simultaneously,a reward function in the reinforcement learning was designed to consider factors such as safety,driving efficiency,and comfort.
Finally,in the NGSIM-ENV simulation platform,the lane-changing behavior of the autonomous vehicles based on the TD3 algorithm was compared with that in the driving data of human drivers.
The average driving velocity was found to increase by 4.
8%,and the driving safety and comfort were improved.
The simulation results verify the effectiveness of the lane-changing model after the model training is completed.
Furthermore,safe,comfortable,and reasonable lane-changing behavior in a complex traffic environment can be realized.


title: WISEMOVE: A Framework to Investigate Safe Deep Reinforcement Learning for Autonomous Driving
abstract: WISEMOVE is a platform to investigate safe deep reinforcement learning (DRL) in the context of motion planning for autonomous driving.
It adopts a modular architecture that mirrors our autonomous vehicle software stack and can interleave learned and programmed components.
Our initial investigation focuses on a state-of-the-art DRL approach from the literature, to quantify its safety and scalability in simulation, and thus evaluate its potential use on our vehicle.

title: Testing autonomous vehicle lane keeping assist system in BeamNG simulator
abstract: In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator.
Road topologies were generated by our tool RIGAA (available at:Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: Creative Commons Attribution 4.
0 International Open Access

title: Intelligent Safety Decision-Making for Autonomous Vehicle in Highway Environment
abstract: Safe driving policies is the key technology to realize the adaptive cruise control of autonomous vehicle in highway environment.
In this paper, the reinforcement learning method is applied to autonomous driving's decision-making.
To solve the problem that present reinforcement learning methods are difficult to deal with the randomness and uncertainty in driving environment, a model-free method for analyzing the Lyapunov stability and H infinity performance is applied to Actor-Critic algorithm to improve the stability and robustness of reinforcement learning.

The safety of taking an action is judged by setting a safety threshold, thus improving the safety of behavioral decisions.
Our method also designs a set of reward functions to better meet the safety and efficiency of driving decisions in the highway environment.
The results show that the method can provide safe driving strategies for driverless vehicles in both normal road conditions and environments with unexpected situations, enabling the vehicles to drive safely.


title: Autonomous Vehicle for Obstacle Detection and Avoidance Using Reinforcement Learning
abstract: Obstacle detection and avoidance during navigation of an autonomous vehicle is one of the challenging problems.
Different sensors like RGB camera, Radar, and Lidar are presently used to analyze the environment around the vehicle for obstacle detection.
Analyzing the environment using supervised learning techniques has proven to be an expensive process due to the training of different obstacle for different scenarios.
In order to overcome such difficulty, in this paper Reinforcement Learning (RL) techniques are used to understand the uncertain environment based on sensor information to make the decision.
Policy free, model-free Q-learning based RL algorithm with the multilayer perceptron neural network (MLP-NN) is applied and trained to predict optimal vehicle future action based on the current state of the vehicle.

Further, the proposed Q-Learning with MLP-NN based approach is compared with the state of the art, namely, Q-learning.
A simulated urban area obstacles scenario is considered with the different number of ultrasonic radar sensors in detecting obstacles.
The experimental result shows that Q-learning with MLP-NN along with the ultrasonic sensors is proven to be more accurate than conventional Q-learning technique with the ultrasonic sensors.
Hence it is demonstrated that combining Qlearning with MLP-NN will improve in predicting obstacles for autonomous vehicle navigation.

title: A Review on Intelligent PID Controllers in Autonomous Vehicle
abstract: In recent times, autonomous vehicle is gaining popularity in several of applications ranging from conventional transport system to nuclear reactor.
The importance of autonomous vehicle increases manifold for those applications where it is not possible or very much risky for human to reach out.
Many Artificial Intelligence techniques have been successfully applied in this field.
In this paper, an adequate number of recent machine learning-based techniques to design intelligent PID controllers in autonomous vehicles have been reviewed and suitably compared with some other competitive approaches.

Particularly, the parameters tuning ofKp,Ki,Kd as well as the rise time characteristics by two methods, namely Ziegler-Nichols and genetic algorithm (GA), have been discussed.
Also, we have compared the properties of rise time and percentage overshoot by conventional PID controllers and fuzzy-based PID controllers.

title: Enhancing Mixed Traffic Flow Safety via Connected and Autonomous Vehicle Trajectory Planning with a Reinforcement Learning Approach
abstract: The longitudinal trajectory planning of connected and autonomous vehicle (CAV) has been widely studied in the literature to reduce travel time or fuel consumptions.
The safety impact of CAV trajectory planning to the mixed traffic flow with both CAV and human-driven vehicle (HDV), however, is not well understood yet.
This study presents a reinforcement learning modeling approach, named Monte Carlo tree search-based autonomous vehicle safety algorithm, or MCTS-AVS, to optimize the safety of mixed traffic flow, on a one-lane roadway with signalized intersection control.

Crash potential index (CPI) is defined to quantitively measure the safety performance of the mixed traffic flow.
The CAV trajectory planning problem is firstly formulated as an optimization model; then, the solution procedure based on reinforcement learning is proposed.
The tree-expansion determination module and rollout termination module are developed to identify and reduce the unnecessary tree expansion, so as to train the model more efficiently towards the desired direction.

The case study results showed that the proposed algorithm was able to reduce the CPI by 76.
56%, when compared with a benchmark model without any intelligence, and 12.
08%, when compared with another benchmark model that the team developed earlier.

These results demonstrated the satisfactory performance of the proposed algorithm in enhancing the safety of the mixed traffic flow.

title: Autonomous Vehicles Perception (AVP) Using Deep Learning: Modeling, Assessment, and Challenges
abstract: Perception is the fundamental task of any autonomous driving system, which gathers all the necessary information about the surrounding environment of the moving vehicle.
The decision-making system takes the perception data as input and makes the optimum decision given that scenario, which maximizes the safety of the passengers.
This paper surveyed recent literature on autonomous vehicle perception (AVP) by focusing on two primary tasks: Semantic Segmentation and Object Detection.
Both tasks play an important role as a vital component of the vehicle's navigation system.
A comprehensive overview of deep learning for perception and its decision-making process based on images and LiDAR point clouds is discussed.
We discussed the sensors, benchmark datasets, and simulation tools widely used in semantic segmentation and object detection tasks, especially for autonomous driving.
This paper acts as a road map for current and future research in AVP, focusing on models, assessment, and challenges in the field.

title: Autonomous Vehicle Routing Optimization in a Competitive Environment: A Reinforcement Learning Application
abstract: This paper presents a multiagent approach to identify the shortest path for the intelligent agents (i.
e.
, autonomous vehicles) traveling through the transportation network using a Q-learning algorithm.

In addition, this paper discusses how a machine achieves an optimal solution in the case that there are a large number of intelligent agents trying to minimize their travel times simultaneously.
The Q-learning algorithm with a reverse order Q matrix is updated to reach the optimal path within a grid network, during which different coefficients have been evaluated and analyzed based on their impact on the algorithm's performance.

The reinforcement learning algorithm is then applied to the multiagent system with different market penetration of autonomous vehicle agents.
The results revealed that as the percentage of intelligent agents increases, it is more difficult to converge to an optimal solution.
A physical interpretation is that when all of the agents are trying to maximize their utility, and if the intelligent agents have conflicted interests with each other, the system does not converge to a global optimum.

We found that the final converged total travel time reaches its minimum when 75% of the agents are intelligent autonomous vehicles.
The critical threshold lies between 50% and 75%, below which the system performance improves marginally over time.

title: Copy-CAV: V2X-enabled wireless towing for emergency transport
abstract: As smart connected vehicles become increasingly common and pave the way for the autonomous vehicles of the future, their ability to provide enhanced safety and assistance services has improved.
One such service is the emergency transport of drivers in medical distress: as a positive solution of the distress is typically more likely after timely response, an autonomous vehicle could cut on emergency response times, and thus play a key role in saving the life of its driver.
In this paper, we show how such an autonomous emergency transport service can be run from a wireless cellular network, and discuss the importance of having a human in the loop in order to expedite driving.

We present a Monte-Carlo-based driver assessment system that the network can use when selecting the most suitable candidate to wirelessly tow an autonomous vehicle with an incapacitated driver.
We show that this mechanism results in a selection policy that ensures better cohesion between the vehicles, thereby significantly improving service reliability by reducing the chances of disruptions by intervening traffic.


title: A Method of Deep Reinforcement Learning for Simulation of Autonomous Vehicle Control
abstract: Nowadays autonomous driving is expected to revolutionize the transportation sector.
Carmakers, researchers, and administrators have been working on this field for years and significant progress has been made.
However, the doubts and challenges to overcome are still huge, regarding not only complex technologies but also human awareness, culture, current traffic infrastructure.
In terms of technical perspective, the accurate detection of obstacles, avoiding adjacent obstacles, and automatic navigation through the environment are some of the difficult problems.
In this paper, an approach for solving those problems is proposed by using of Policy Gradient to control a simulated car via reinforcement learning.
The proposed method is worked effectively to train an agent to control the simulated car in Unity ML-agents Highway, which is a simulating environment.
This environment is chosen from some criteria of an environment simulating autonomous vehicle.
The testing of the proposed method got positive results.
Beside the average speed was w ell, the agent successfully learned the turning operation, progressively gaining the ability to navigate larger sections of the simulated raceway without crashing.

title: Weights-varying MPC for Autonomous Vehicle Guidance: a Deep Reinforcement Learning Approach
abstract: Model Predictive Control (MPC) can achieve excellent results for complex control tasks like path-following of autonomous vehicles.
However, its performance depends on the right choice of a cost function for its internal optimization problem.
Optimizing the cost function to different objectives is challenging and time-consuming.
In this paper, we propose to automatically learn context-dependent optimal weights for the cost function with Deep Reinforcement Learning and to adapt the weights online.
We show that our approach outperforms the results of a human expert.

title: Varied Realistic Autonomous Vehicle Collision Scenario Generation
abstract: Recently there has been an increase in the number of available autonomous vehicle (AV) models.
To evaluate and compare the safety of the various models the AVs need to be tested in several diverse safety-critical scenarios.
We propose the Adversarial Test Case Generator (ATCG) that differently from previous test case generators allows for the generation of realistic collision scenarios with varied AV and pedestrian behaviour models, on varied scenes and with varied traffic density.

Given a top-view image and the semantic segmentation of a traffic scene, the ATCG learns to place multiple AVs and goal-reaching pedestrians in the scene such that collisions occur.
Pedestrians in previous multi-agent traffic scenario generation works are confined to unrealistic behaviours such as seeking collisions with the AV or ignoring the AV.
Although such scenarios with multiple suicidal pedestrians are collision prone it is unlikely in reality that all pedestrians act abnormally.
In realistic collision scenarios the generated pedestrians' behaviours must resemble real pedestrians.
The ATCG is a team of Reinforcement Learning (RL) agents and can be easily extended with additional RL agents to produce more complex scenes allowing for advanced AVs to be tested.

title: Decision-Making for Oncoming Traffic Overtaking Scenario using Double DQN
abstract: Great progress has been made in the field of machine learning in recent years.
And learning-based methods have been widely utilized for developing highly autonomous vehicle.
To this end, we introduce a reinforcement learning based intelligent autonomous vehicle decision making method for oncoming overtaking scenario.
The goal of reinforcement learning is to learn how to take optimal decision in corresponding observations through interactions with the environment using a reward function to estimate whether the decision is good or not.

A Double Deep Q-learning (Double DQN) agent was used to learn policies (control strategies) for both longitudinal speed and lane change decision.
Prioritized Experience Replay (PER) was used to accelerate convergence of the policies.
A two-way 3-car scenario with oncoming traffic was established in SUMO (Simulation of Urban Mobility) to train and test the policies.

title: Impact-aware Maneuver Decision with Enhanced Perception for Autonomous Vehicle
abstract: Autonomous driving is an emerging technology that has developed rapidly over the last decade.
There have been numerous interdisciplinary challenges imposed on the current transportation system by autonomous vehicles.
In this paper, we conduct an algorithmic study on the autonomous vehicle decision-making process, which is a fundamental problem in the vehicle automation field and the root cause of most traffic congestion.

We propose a perception-and-decision framework, called HEAD, which consists of an enHanced pErception module and a mAneuver Decision module.
HEAD aims to enable the autonomous vehicle to perform safe, efficient, and comfortable maneuvers with minimal impact on other vehicles.
In the enhanced perception module, a graph-based state prediction model with a strategy of phantom vehicle construction is proposed to predict the one-step future states for multiple surrounding vehicles in parallel, which deals with sensor limitations such as limited detection range and poor detection accuracy under occlusions.

Then in the maneuver decision module, a deep reinforcement learning-based model is designed to learn a policy for the autonomous vehicle to perform maneuvers in continuous action space w.r.t.
a parameterized action Markov decision process.
A hybrid reward function takes into account aspects of safety, efficiency, comfort, and impact to guide the autonomous vehicle to make optimal maneuver decisions.
Extensive experiments offer evidence that HEAD can advance the state of the art in terms of both macroscopic and microscopic effectiveness.

title: NeurIPS 2022 Competition: Driving SMARTS [arXiv]
abstract: Driving SMARTS is a regular competition designed to tackle problems caused by the distribution shift in dynamic interaction contexts that are prevalent in real-world autonomous driving (AD).
The proposed competition supports methodologically diverse solutions, such as reinforcement learning (RL) and offline learning methods, trained on a combination of naturalistic AD data and open-source simulation platform SMARTS.

The two-track structure allows focusing on different aspects of the distribution shift.
Track 1 is open to any method and will give ML researchers with different backgrounds an opportunity to solve a real-world autonomous driving challenge.
Track 2 is designed for strictly offline learning methods.
Therefore, direct comparisons can be made between different methods with the aim to identify new promising research directions.
The proposed setup consists of 1) realistic traffic generated using real-world data and micro simulators to ensure fidelity of the scenarios, 2) framework accommodating diverse methods for solving the problem, and 3) baseline method.

As such it provides a unique opportunity for the principled investigation into various aspects of autonomous vehicle deployment.

title: Reinforcement Learning for Autonomous Vehicle using MPC in Highway Situation
abstract: Path planning for Autonomous Vehicle(AV) is a challenging problem, as the vehicle is required to obey the traffic rules while avoiding the collision with the other vehicles.
Model Predictive Control(MPC) is one of the popular approach for proposing a feasible and stable path by reflecting vehicle dynamics in solving objective function and constraining the expected future control input.

However, one of the drawbacks with this approach is that the demanded computational power increases proportionally to the number of considered future inputs.
This paper presents a path planning algorithm using Reinforcement Learning(RL).
RL is similar to MPC in finding the optimal solution that maximizes the reward function which can be seen as intrinsic objective function.
In that respect, adequate employment of MPC path in training resulted in improved efficiency and performance.
Through the simulations, proposed method showed 98% of similarity with path of MPC and reduced computation time by 91.13% on average, thus it is qualified for real-time path planning.

title: Improving environmental awareness for autonomous vehicles
abstract: Autonomous vehicles (AVs) have multiple tasks with different priorities and safety levels where classic supervised learning techniques are no longer applicable.
Thus, reinforcement learning (RL) algorithms become increasingly appropriate for this domain as the RL algorithms can act on complex problems and adapt their responses in the face of unforeseen situations and environments.

The RL agent aims to perform the action that guarantees the optimal reward with the best score.
The problem with this approach is if the agent finds a possible optimal action with a reasonable premium and gets stuck in this mediocre strategy, which at the same time is neither the best nor the worst solution.

Therefore, the agent avoids performing a more extensive exploration to find new paths and learn alternatives to generate a higher reward.
To alleviate this problem, we research the behavior of two types of noise in AVs training.
We analyze the results and point out the noise method that most stimulates exploration.
A vast exploration of the environment is highly relevant to AVs because they know more about the environment and learn alternative ways of acting in the face of uncertainties.
With that, AVs can expect more reliable actions in front of sudden changes in the environment.
According to our experiments' results in a simulator, we can see that noise allows the autonomous vehicle to improve its exploration and increase the reward.

title: Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation [arXiv]
abstract: Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia.
This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events.
Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition.
Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged.

A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.


title: Object Tracking Methods:A Review
abstract: Object tracking is one of the most important tasks in computer vision that has many practical applications such as traffic monitoring, robotics, autonomous vehicle tracking, and so on.
Different researches have been done in recent years, but because of different challenges such as occlusion, illumination variations, fast motion, etc.
researches in this area continues.
In this paper, various methods of tracking objects are examined and a comprehensive classification is presented that classified tracking methods into four main categories of feature-based, segmentation-based, estimation-based, and learning-based methods that each of which has its own sub-categories.

The main focus of this paper is on learning-based methods, which are classified into three categories of generative methods, discriminative methods, and reinforcement learning.
One of the sub-categories of the discriminative model is deep learning.
Because of high-performance, deep learning has recently been very much considered.yyyyyy

title: Adaptive Resilient Event-Triggered Control Design of Autonomous Vehicles With an Iterative Single Critic Learning Framework
abstract: This article investigates the adaptive resilient event-triggered control for rear-wheel-drive autonomous (RWDA) vehicles based on an iterative single critic learning framework, which can effectively balance the frequency/changes in adjusting the vehicle's control during the running process.

According to the kinematic equation of RWDA vehicles and the desired trajectory, the tracking error system during the autonomous driving process is first built, where the denial-of-service (DoS) attacking signals are injected into the networked communication and transmission.

Combining the event-triggered sampling mechanism and iterative single critic learning framework, a new event-triggered condition is developed for the adaptive resilient control algorithm, and the novel utility function design is considered for driving the autonomous vehicle, where the control input can be guaranteed into an applicable saturated bound.

Finally, we apply the new adaptive resilient control scheme to a case of driving the RWDA vehicles, and the simulation results illustrate the effectiveness and practicality successfully.

title: An Artificial Intelligent-Driven Semantic Communication Framework for Connected Autonomous Vehicular Network
abstract: Semantic communication will considerably enhance transmission efficiency by exploring and only transmitting semantic information.
However, most of the previous work in this field is limited to particular applications such as text, audio, or images and does not consider task-oriented communications, where the effectiveness of the transmitted information must be taken into account for completing a specific task.

This paper focuses on developing a semantic communication framework for a high altitude platform (HAP)-supported fully connected autonomous vehicle network.
A system model is proposed in which the traffic infrastructure (TI) transmits its semantic information to the macro base station (MBS) whenever it observes a connected and autonomous vehicle (CAV).
The semantic information has been extracted using a convolutional autoencoder (CAE) as the encoder of CAE gives a smaller representation of the input data.
Then, after receiving the semantic concept, the MBS decides on an appropriate action for the CAVs.
A proximal policy optimization (PPO) algorithm in the MBS for interpreting and making a decision for the semantic concepts.
Simulation results show that the proposed method can reduce up to 63.26% of the communication cost.

title: Rule-constrained reinforcement learning control for autonomous vehicle left turn at unsignalized intersection
abstract: Controlling an autonomous vehicle's unprotected left turn at an intersection is a challenging task.
Traditional rule-based autonomous driving decision and control algorithms struggle to construct accurate and trustworthy mathematical models for such circumstances, owing to their considerable uncertainty and unpredictability.

To overcome this problem, a rule-constrained reinforcement learning (RCRL) control method is proposed in this work for autonomous driving.
To train a reinforcement learning controller with rule constraints, outcomes of the path planning module are used as a goal condition in the reinforcement learning framework.
Since they include vehicle dynamics, the proposed approach is safer and more reliable compared to end-to-end learning, thereby ensuring that the generated trajectories are locally optimal while adjusting to unpredictable situations.

In the experiments, a highly randomized two-way four-lane intersection is established based on the CARLA simulator to verify the effectiveness of the proposed RCRL control method.
Accordingly, the results show that the proposed method can provide real-time safe planning and ensure high passing efficiency for autonomous vehicles in the unprotected left turn task.

title: End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning
abstract: Navigating through intersections is one of the main challenging tasks for an autonomous vehicle.
However, for the majority of intersections regulated by traffic lights, the problem could be solved by a simple rule-based method in which the autonomous vehicle behavior is closely related to the traffic light states.

In this work, we focus on the implementation of a system able to navigate through intersections where only traffic signs are provided.
We propose a multi-agent system using a continuous, model-free Deep Reinforcement Learning algorithm used to train a neural network for predicting both the acceleration and the steering angle at each time step.

We demonstrate that agents learn both the basic rules needed to handle intersections by understanding the priorities of other learners inside the environment, and to drive safely along their paths.
Moreover, a comparison between our system and a rule-based method proves that our model achieves better results especially with dense traffic conditions.
Finally, we test our system on real world scenarios using real recorded traffic data, proving that our module is able to generalize both to unseen environments and to different traffic conditions.

title: Dual-Loop Adaptive Dynamic Programming for Autonomous Vehicle Trajectory Following Control Against Actuator Faults
abstract: This article presents a novel control strategy based on dual-loop adaptive dynamic programming (ADP) to optimize the tracking performance and ensure the security of autonomous vehicles when actuator faults occur.

The proposed dual-loop ADP of controller, composed of two online policy iteration algorithms and an adaptive observer for fault-tolerant control (FTC), guarantees the online tracking accuracy of underactuated systems with uncertain faults, which is difficult to control by single ADP.

In addition to actuator faults, the proposed controller can also address the external disturbances such as icy roads and uncertainty of wheel cornering stiffness, which means the controller possesses better robustness.

The autonomous vehicle is simulated in multiple driving scenarios with different types of faults, demonstrating the effectiveness of the control strategy.

title: Reinforcement-learning-aided adaptive control for autonomous driving with combined lateral and longitudinal dynamics
abstract: This paper presents a deep reinforcement learning-aided controller for a 3-DOF autonomous vehicle with combined lateral and longitudinal dynamics.
In this scheme, the active disturbance rejection control (ADRC) gives full play to its advantages of being model-free and being able to estimate and compensate for internal uncertainties and external disturbances in real-time, and deep deterministic policy gradient (DDPG) fully considers safety, comfort, economy, and combines driving demand with state, action, reward to achieve real-time adaptive adjustment of control parameters.

Thus, the adaptive controller can better deal with uncertainties from modeling, parameters, and driving environment, and self-learning and adaptation ability is obtained simultaneously.
Moreover, simulation results illustrate that the adaptive controller performs satisfactorily for different driving operations and environments due to the online tuning and optimization of control parameters.


title: Generating Socially Acceptable Perturbations for Efficient Evaluation of Autonomous Vehicles
abstract: Deep reinforcement learning methods have been considered and implemented for autonomous vehicle's decision-making in recent years.
A key issue is that deep neural networks can be fragile to adversarial attacks through unseen inputs, and thus the reinforcement learning policy, that uses deep neural network would be also fragile to malicious attacks or benign but out of distribution perturbations.

In this paper, we address the latter issue: we focus on generating socially acceptable perturbations (SAP), so that the autonomous vehicle (AV agent under evaluation), instead of the challenging vehicle (challenger), is primarily responsible for the crash.

In our process, one challenger is added to the environment and trained by deep reinforcement learning to generate the desired perturbation.
The reward is designed so that the challenger aims to fail the AV agent in a socially acceptable way.
After training the challenger, the AV agent policy is evaluated in both the original naturalistic environment and the environment with one challenger.
The results show that the AV agent policy which is safe in the naturalistic environment has many crashes in the perturbed environment.

title: An Artificial Intelligent-Driven Semantic Communication Framework for Connected Autonomous Vehicular Network
abstract: Semantic communication will considerably enhance transmission efficiency by exploring and only transmitting semantic information.
However, most of the previous work in this field is limited to particular applications such as text, audio, or images and does not consider task-oriented communications, where the effectiveness of the transmitted information must be taken into account for completing a specific task.

This paper focuses on developing a semantic communication framework for a high altitude platform (HAP)-supported fully connected autonomous vehicle network.
A system model is proposed in which the traffic infrastructure (TI) transmits its semantic information to the macro base station (MBS) whenever it observes a connected and autonomous vehicle (CAV).
The semantic information has been extracted using a convolutional autoencoder (CAE) as the encoder of CAE gives a smaller representation of the input data.
Then, after receiving the semantic concept, the MBS decides on an appropriate action for the CAVs.
A proximal policy optimization (PPO) algorithm in the MBS for interpreting and making a decision for the semantic concepts.
Simulation results show that the proposed method can reduce up to 63.26% of the communication cost.

title: Route Optimization of Unmanned Aerial Vehicle by using Reinforcement Learning
abstract: The study proposes the machine learning based algorithm for autonomous vehicle.
The dynamic characteristic of unmanned aerial vehicle and real time disturbances such as wind current, obstacles are considered.
The novelty of the work lies in the introduction of reinforcement learning for achieving optimized path which can be followed by the unmanned aerial vehicles to complete the tour from initial to destination point.

The feasible optimal route may be found by incorporating the L algorithm with a reasonable optimality and computational cost when map and current field data are given.

title: A Reinforcement Learning Approach to Jointly Adapt Vehicular Communications and Planning for Optimized Driving
abstract: Our premise is that autonomous vehicles must optimize communications and motion planning jointly.
Specifically, a vehicle must adapt its motion plan staying cognizant of communications rate related constraints and adapt the use of communications while being cognizant of motion planning related restrictions that may be imposed by the on-road environment.

To this end, we formulate a reinforcement learning problem wherein an autonomous vehicle jointly chooses (a) a motion planning action that executes on-road and (hi a communications action of querying sensed information from the infrastructure.

The goal is to optimize the driving utility of the autonomous vehicle.
We apply the Q-learning algorithm to make the vehicle learn the optimal policy, which makes the optimal choice of planning and communications actions at any given time.
We demonstrate the ability of the optimal policy to smartly adapt communications and planning actions, while achieving large driving utilities, using simulations.

title: Design and Implementation of Shortest Path Line Follower Autonomous Rover Using Decision Making Algorithms
abstract: For an autonomous rover, navigating through an unknown environment achieving the target location is the key feature of the rover.
Also, path finding is the main problem in the navigation of autonomous vehicles.
This is generally done via a dynamic control paradigm, which requires local translation from defined states to actions.
Reinforcement learning can be used to develop a control strategy that has learning capabilities in an uncertain environment.
The result analysis shows that an autonomous vehicle that is trained by reinforcement can roam freely in a useful or "reasonably" manner, and therefore its training process is increased significantly.

title: Application of Neural Networks for Design and Development of Low-cost Autonomous Vehicles
abstract: 

title: Reinforcement Learning-based Event-Triggered Model Predictive Control for Autonomous Vehicle Path Following
abstract: Event-triggered model predictive control (MPC) has been proposed in literature to alleviate the high computational requirement of MPC.
Compared to conventional time-triggered MPC, event-triggered MPC solves the optimal control problem only when an event is triggered.
Several event-trigger policies have been studied in literature, typically requiring a prior knowledge of the MPC closed-loop system behavior.
This paper addresses such limitation by investigating the use of model-free reinforcement learning (RL) to trigger MPC.
Specifically, the optimal event-trigger policy is learnt by an RL agent through interactions with the MPC closed-loop system, whose dynamical behavior is assumed to be unknown to the RL agent.
A reward function is defined to balance the closed-loop control performance and event frequency.
As an illustrative example, the autonomous vehicle path following problem is used to demonstrate the applicability of using RL to learn and execute trigger policy for event-triggered MPC.

title: Autonomous Driving in Roundabout Maneuvers Using Reinforcement Learning with Q-Learning
abstract: Navigating roundabouts is a complex driving scenario for both manual and autonomous vehicles.
This paper proposes an approach based on the use of the Q-learning algorithm to train an autonomous vehicle agent to learn how to appropriately navigate roundabouts.
The proposed learning algorithm is implemented using the CARLA simulation environment.
Several simulations are performed to train the algorithm in two scenarios: navigating a roundabout with and without surrounding traffic.
The results illustrate that the Q-learning-algorithm-based vehicle agent is able to learn smooth and efficient driving to perform maneuvers within roundabouts.

title: Collective trust estimation in multi-agent systems
abstract: In previous work, a multi-layered neural network trust model, dubbed NeuroTrust, was introduced.
This trust model was also implemented in an autonomous vehicles convoy simulation, in which speed and gap distance depended on trust.
It has been shown that, in time, through on-line reinforcement learning, this trust model produces better results for significant performance metrics in the respective autonomous vehicle convoy when compared to a baseline trust algorithm.

In this paper, the NeuroTrust model is expanded to leverage the experience of multiple decision-making agents.
A trust aggregation method is proposed for NeuroTrust and is simulated for multiple autonomous vehicle convoy scenarios.
It is shown that the NeuroTrust model tends to optimize faster by leveraging each agent's experience.

title: Machine Learning for Vehicle Behavioural Control
abstract: With the advancement of Artificial Intelligence, a revolution in the automotive field has taken place in last few decades.
With the development of Advanced Driver Assistance Systems (ADAS), various sensors were available, which paved theway for fully- autonomous vehicles.
The first autonomous car has appeared in 1980's, since then a lot of research has taken place in order to develop an autonomous vehicle, which can drive the passengers all by itself without any human-driver intervention.

This project work is aimed at developing a simulation model of an Autonomous Vehicle.
The so developed Autonomous Vehicle is able to navigate through a pre-defined path avoiding obstacles, walkers, and other vehicles autonomously without any human control.
For attaining this driving autonomy, a Reinforcement Learning Algorithm was developed and trained using Deep Q- Learning technique and in-loop-simulations on CARLA-simulator.
Upon training the agent for 20,000 episodes of simulation, it is overt from simulation results that the learning accuracy of the agent aggrandized, the average reward nabbed by the agent amplified, the agent's training loss subsided, which reinforce that the agent started to acquire more positive average-reward than the negative average-reward.

At the 17,150th, episode of training, the agent concomitantly set forth peak learning accuracy, diminutive training loss and best average-reward.
This tacit validate that the agent is adept in maneuvering without colliding with other vehicles, walkers, and with no lane invasions, autonomously from its experience earned from episodic in-loop-simulations.


title: Safe Reinforcement Learning on Autonomous Vehicles
abstract: There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications.

Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems.
We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle.


title: Testing autonomous vehicle lane keeping assist system in BeamNG simulator
abstract: In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator.
Road topologies were generated by our tool RIGAA (available at:Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: Creative Commons Attribution 4.
0 International Open Access

title: Real-time Dynamic Route Generation Algorithm for Demand-responsive Driverless Transit Operation (DRDTO) Applied to Corridors to Consider U-Turns
abstract: Recently, autonomous vehicle is receiving much attention in various sectors including transportation and public transportation.
This study presents an adaptive routing algorithm for the real-time demand response service of autonomous transit vehicles.
The experimental setting includes the real-time demand occurring randomly over time within the study area.
Each demand (service call) request the transit service from designated origin to destination.
The routing algorithm is designed to make U-turn and skip stops to improve service and reduce the wait time of users.
This study adapts Reinforcement Learning, one of the machine learning techniques, namely reinforcement learning, which can precede a complex calculation process in an offline process.
Simulation experiments conducted on a testbed of the Chungju campus of the Korea National University of Transportation.
The simulation results show that the proposed routing algorithm can improve the adaptive transit service over the fixed operation in selected performance indicators.

title: Service Chaining Offloading Decision in the EdgeAI: A Deep Reinforcement Learning Approach
abstract: Many mission critical devices are increasing with upcoming 56 network to fulfill a low latency for a real time network service on smart factory, autonomous vehicle, etc.
Distributed cloud computing system also has a key role to execute the various mobile devices, because, an edge computing is the nearest from the mobile devices to provide low latency and computation energy consumption.

In this paper, we consider the autonomous vehicles with video live streaming services.
Especially, the vehicles require a low transmission delay as within 10 ms.
 To reduce a latency with low energy consumption, we propose a service chaining offloading decision with a deep reinforcement learning.

We split tasks of the vehicle per service function blocks which have their own role.
So it can do partial offloading and user association in a On-Device Edge of the vehicle and in the SBS at the same time.
We can get results that service chaining offloading decision gives more optimal energy consumption with low-latency to autonomous vehicle users.

title: Towards End-to-End Chase in Urban Autonomous Driving Using Reinforcement Learning
abstract: This paper addresses the challenging task of developing an autonomous chase protocol.
First, training of an autonomous vehicle capable of driving autonomously from point A to B was developed to proceed with a chase protocol as a second step.
A dedicated driving setup, based on a discrete action space and a single RGB camera, was developed through a series of experiments.
A dedicated curriculum learning agenda allowed to train the model capable of performing all fundamental road maneuvers.
Several reward functions were proposed, which enabled effective training of the agent.
In the subsequent experiments, we selected the reward function and model that produced the most significant outcome, guaranteeing that the chasing car was within 25 m of a runaway car for 63% of the episode duration.

To the best of our knowledge, this work is the first one that addressed the task of the chase in urban driving using the Reinforcement Learning approach.

title: A Markov Decision Process Model for a Reinforcement Learning-based Autonomous Pedestrian Crossing Protocol
abstract: Autonomous Traffic Management (ATM) systems empowered with Machine Learning (ML) technics are a promising solution for eliminating traffic light and decreasing traffic congestion in the future.
However, few efforts have focused on integrating pedestrians in ATM, namely the static programming-based cooperative protocol called Autonomous Pedestrian Crossing (APC).
In this paper, we model a Markov Decision Process ( MDP) to enable a Deep Reinforcement Learning (DRL)-based version of APC protocol that is able to dynamically achieve the same objectives (i.e.
decreasing traffic delay at the crossing area).
Using concrete state space, action set and reward functions, our model forces the Autonomous Vehicle (AV) to "think" and behave according to APC architecture.
Compared to the traditional programming APC system, our approach permits the AV to learn from its previous experiences in non-signalized crossing and optimize the distance and the velocity parameters accordingly.


title: Velocity control in a right-turn across traffic scenario for autonomous vehicles using kernel-based reinforcement learning
abstract: Recently, advanced control methods like machine leaning are increasingly applied to autonomous vehicle.
This paper focuses on velocity control in a right-turn traffic scenario.
A Markov Decision Processes(MDPs) is modeled and the actor-critic reinforcement learning architecture is employed.
Then the kernel-based least squares policy iteration algorithm(KLSPI) is applied.
Simulation results show that the proposed method can perform different policy in different cases, which preliminarily verify the rationality.

title: Deep Reinforcement Learning Controller for 3D Path Following and Collision Avoidance by Autonomous Underwater Vehicles
abstract: Control theory provides engineers with a multitude of tools to design controllers that manipulate the closed-loop behavior and stability of dynamical systems.
These methods rely heavily on insights into the mathematical model governing the physical system.
However, in complex systems, such as autonomous underwater vehicles performing the dual objective of path following and collision avoidance, decision making becomes nontrivial.
We propose a solution using state-of-the-art Deep Reinforcement Learning (DRL) techniques to develop autonomous agents capable of achieving this hybrid objective without having a priori knowledge about the goal or the environment.

Our results demonstrate the viability of DRL in path following and avoiding collisions towards achieving human-level decision making in autonomous vehicle systems within extreme obstacle configurations.


title: Using Physiological Metrics to Improve Reinforcement Learning for Autonomous Vehicles
abstract: Thanks to recent technological advances Autonomous Vehicles (AVs) are becoming available at some locations.
Safety impacts of these devices have, however, been difficult to assess.
In this paper we utilize physiological metrics to improve the performance of a reinforcement learning agent attempting to drive an autonomous vehicle in simulation.
We measure the performance of our reinforcement learner in several aspects, including the amount of stress imposed on potential passengers, the number of training episodes required, and a score measuring the vehicle's speed as well as the distance successfully traveled by the vehicle, without traveling off-track or hitting a different vehicle.

To that end, we compose a human model, which is based on a dataset of physiological metrics of passengers in an autonomous vehicle.
We embed this model in a reinforcement learning agent by providing negative reward to the agent for actions that cause the human model an increase in heart rate.
We show that such a "passenger-aware" reinforcement learner agent does not only reduce the stress imposed on hypothetical passengers, but, quite surprisingly, also drives safer and its learning process is more effective than an agent that does not obtain rewards from a human model.


title: Deep reinforcement learning for autonomous vehicles: lane keep and overtaking scenarios with collision avoidance
abstract: Numerous accidents and fatalities occur every year across the world as a result of the reckless driving of drivers and the ever-increasing number of vehicles on the road.
Due to these factors, autonomous cars have attracted enormous attention as a potentially game-changing technology to address a number of persistent problems in the transportation industry.
Autonomous vehicles need to be modeled as intelligent agents with the capacity to observe, and perceive the complex and dynamic environment on the road, and decide an action with the highest priority to the lives of people in every scenarios.

The proposed deep deterministic policy gradient-based sequential decision algorithm models the autonomous vehicle as a learning agent and trains it to drive on a lane, overtake a static and a moving vehicle, and avoid collisions with obstacles on the front and right side.

The proposed work is simulated using a TORC simulator and has shown the expected performance under the above-said scenarios.

title: Learning to Navigate Intersections with Unsupervised Driver Trait Inference
abstract: Navigation through uncontrolled intersections is one of the key challenges for autonomous vehicles.
Identifying the subtle differences in hidden traits of other drivers can bring significant benefits when navigating in such environments.
We propose an unsupervised method for inferring driver traits such as driving styles from observed vehicle trajectories.
We use a variational autoencoder with recurrent neural networks to learn a latent representation of traits without any ground truth trait labels.
Then, we use this trait representation to learn a policy for an autonomous vehicle to navigate through a T-intersection with deep reinforcement learning.
Our pipeline enables the autonomous vehicle to adjust its actions when dealing with drivers of different traits to ensure safety and efficiency.
Our method demonstrates promising performance and outperforms state-of-the-art baselines in the T-intersection scenario.

title: A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios
abstract: In recent years, control under urban intersection scenarios has become an emerging research topic.
In such scenarios, the autonomous vehicle confronts complicated situations since it must deal with the interaction with social vehicles timely while obeying the traffic rules.
Generally, the autonomous vehicle is supposed to avoid collisions while pursuing better efficiency.
The existing work fails to provide a framework that emphasizes the integrity of the scenarios while deploying and testing reinforcement learning(RL) methods.
Specifically, we propose a benchmark for training and testing RL-based autonomous driving agents in complex intersection scenarios, which is called RL-CIS.
Then, a set of baselines consisting various algorithms are deployed.
The test benchmark and baselines provide a fair and comprehensive training and testing platform for the study of RL for autonomous driving in the intersection scenario, advancing RL-based methods for autonomous driving control.

The code of our proposed framework can be found at https://github.com/liuyuqi123/ComplexUrbanScenarios.

title: Experience Filter: Using Past Experiences on Unseen Tasks or Environments [arXiv]
abstract: One of the bottlenecks of training autonomous vehicle (AV) agents is the variability of training environments.
Since learning optimal policies for unseen environments is often very costly and requires substantial data collection, it becomes computationally intractable to train the agent on every possible environment or task the AV may encounter.

This paper introduces a zero-shot filtering approach to interpolate learned policies of past experiences to generalize to unseen ones.
We use an experience kernel to correlate environments.
These correlations are then exploited to produce policies for new tasks or environments from learned policies.
We demonstrate our methods on an autonomous vehicle driving through T-intersections with different characteristics, where its behavior is modeled as a partially observable Markov decision process (POMDP).

We first construct compact representations of learned policies for POMDPs with unknown transition functions given a dataset of sequential actions and observations.
Then, we filter parameterized policies of previously visited environments to generate policies to new, unseen environments.
We demonstrate our approaches on both an actual AV and a high-fidelity simulator.
Results indicate that our experience filter offers a fast, low-effort, and near-optimal solution to create policies for tasks or environments never seen before.
Furthermore, the generated new policies outperform the policy learned using the entire data collected from past environments, suggesting that the correlation among different environments can be exploited and irrelevant ones can be filtered out.


title: Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages
abstract: The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control.
However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles.
In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent.

By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy.
This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle.
We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance.

Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone.


title: Hierarchical Motion Planning and Tracking for Autonomous Vehicles Using Global Heuristic Based Potential Field and Reinforcement Learning Based Predictive Control
abstract: The autonomous vehicle is widely applied in various ground operations, in which motion planning and tracking control are becoming the key technologies to achieve autonomous driving.
In order to further improve the performance of motion planning and tracking control, an efficient hierarchical framework containing motion planning and tracking control for the autonomous vehicles is constructed in this paper.

Firstly, the problems of planning and control are modeled and formulated for the autonomous vehicle.
Then, the logical structure of the hierarchical framework is described in detail, which contains several algorithmic improvements and logical associations.
The global heuristic planning based artificial potential field method is developed to generate the real-time optimal motion sequence, and the prioritized Q-learning based forward predictive control method is proposed to further optimize the effectiveness of tracking control.

The hierarchical framework is evaluated and validated by the numerical simulation, virtual driving environment simulation and real-world scenario.
The results show that both the motion planning layer and the tracking control layer of the hierarchical framework perform better than other previous methods.
Finally, the adaptability of the proposed framework is verified by applying another driving scenario.
Furthermore, the hierarchical framework also has the ability for the real-time application.

title: Autonomous Car-Following Approach Based on Real-time Video Frames Processing
abstract: Car-following theory has received considerable attention from the transportation fields in the last decade.
Autonomous vehicles are designed to provide convenient and safe driving by avoiding accidents caused by driver errors.
However, it is always important to enhance the recognition of driver's driving-style in our roads.
With car following models, automated vehicles can imitate the human behavior driving and assure a high safety level on road.
All the built-in technologies must be integrated and complemented to achieve these goals.
Automated object detection and the following process are one of the main research tasks that must be undertaken for this purpose.
In this paper, we design a car following framework for autonomous vehicles using the reinforcement learning technique.
The objective is to follow one leader car based on image-frames.
To this end, we propose to employ the YOLOv3 object detector to detect vehicles the Q-learning technique to train the follower vehicle to autonomously navigate and follow the leader.
Simulation results show the convergence of the model and investigate the behavior of the image-based autonomous vehicle in following its leader.

title: Testing autonomous vehicle lane keeping assist system in BeamNG simulator
abstract: In this video, you can see some of the failures revealed while testing the driving agent on various road topologies in BeamNG simulator.
Road topologies were generated by our tool RIGAA (available at: Reinforcement learning informed evolutionary search for autonomous systems testing | Zenodo) Copyright: CC BY 4.0

title: Autonomous Highway Driving using Deep Reinforcement Learning
abstract: The operational space of an autonomous vehicle (AV) can be diverse and vary significantly.
Due to this, formulating a rule based decision maker for selecting driving maneuvers may not be ideal.
Similarly, it may not be efficient to solve optimal control problem in real-time for a predefined cost function.
In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.
e.
, an autonomous vehicle, learns to make decisions by directly interacting with the simulated traffic.

Here the decision maker is a deep neural network that provides an action choice for a given system state.
We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density.

title: Predicting Parameters for Modeling Traffic Participants
abstract: Accurately modeling the behavior of traffic participants is essential for safely and efficiently navigating an autonomous vehicle through heavy traffic.
We propose a method, based on the intelligent driver model, that allows us to accurately model individual driver behaviors from only a small number of frames using easily observable features.
On average, this method makes prediction errors that have less than 1 meter difference from an oracle with full-information when analyzed over a 10-second horizon of highway driving.
We then validate the efficiency of our method through extensive analysis against a competitive data-driven method such as Reinforcement Learning that may be of independent interest.

title: Reinforcement Learning based Autonomous Vehicle for Exploration and Exploitation of Undiscovered Track
abstract: This research focuses on autonomous traversal of land vehicles through exploring undiscovered tracks and overcoming environmental barriers.
Most of the existing systems can only operate and traverse in a distinctive mapped model especially in a known area.
However, the proposed system which is trained by Deep Reinforcement learning can learn by itself to operate autonomously in extreme conditions.
The dynamic double deep Q-learning (DDQN) model enables the proposed system not be confined to only in known environments.
The ambient environmental obstacles are identified through Faster R-CNN for smooth movement of the autonomous vehicle.
The exploration and exploitation strategies of DDQN enables the autonomous agent to learn proper decisions for various dynamic environments and tracks.
The proposed model is tested in a gaming environment.
It shows the overall effectiveness in traversing of autonomous land vehicles.
The goal is to integrate Deep Reinforcement learning and Faster R-CNN to make the system effective to traverse through undiscovered paths by detecting obstacles.

title: Comparing Model-based and Data-driven Controllers for an Autonomous Vehicle Task
abstract: The advent of autonomous vehicles comes with many questions from an ethical and technological point of view.
The need for high performing controllers, which show transparency and predictability is crucial to generate trust in such systems.
Popular data-driven, black box-like approaches such as deep learning and reinforcement learning are used more and more in robotics due to their ability to process large amounts of information, with outstanding performance, but raising concerns about their transparency and predictability.

Model-based control approaches are still a reliable and predictable alternative, used extensively in industry but with restrictions of their own.
Which of these approaches is preferable is difficult to assess as they are rarely directly compared with each other for the same task, especially for autonomous vehicles.
Here we compare two popular approaches for control synthesis, model-based control i.e.
Model Predictive Controller (MPC), and data-driven control i.e.
Reinforcement Learning (RL) for a lane keeping task with speed limit for an autonomous vehicle; controllers were to take control after a human driver had departed lanes or gone above the speed limit.
We report the differences between both control approaches from analysis, architecture, synthesis, tuning and deployment and compare performance, taking overall benefits and difficulties of each control approach into account.


title: Test Method for Decision Planning of Autonomous Vehicles Based on DQN Algorithm
abstract: In February 2020, Beijing, China andCalifornia, USA respectively released road test reports of 2019 for autonomous vehicles.
Beijing and California respectively represent the highest level of testing and application of autonomous vehicles in the two countries.
This article will compare the test items, evaluation criteria and technical defects of each autonomous vehicle company in the road test reports of China and the United States, also analyze the existing problems, and propose an idea for the construction of a comprehensive test site for autonomous vehicles.

This article aims to solve the prominently exposed problems in decision-making and planning in autonomous vehicles with DQN algorithm-base vehicle fleet, and to look forward to the future development trend of autonomous driving testing.


title: The Design of Performance Guaranteed Autonomous Vehicle Control for Optimal Motion in Unsignalized Intersections
abstract: The design of the motion of autonomous vehicles in non-signalized intersections with the consideration of multiple criteria and safety constraints is a challenging problem with several tasks.
In this paper, a learning-based control solution with guarantees for collision avoidance is proposed.
The design problem is formed in a novel way through the division of the control problem, which leads to reduced complexity for achieving real-time computation.
First, an environment model for the intersection was created based on a constrained quadratic optimization, with which guarantees on collision avoidance can be provided.
A robust cruise controller for the autonomous vehicle was also designed.
Second, the environment model was used in the training process, which was based on a reinforcement learning method.
The goal of the training was to improve the economy of autonomous vehicles, while guaranteeing collision avoidance.
The effectiveness of the method is presented through simulation examples in non-signalized intersection scenarios with varying numbers of vehicles.

title: Interaction-aware Decision Making with Adaptive Strategies under Merging Scenarios
abstract: In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants.
Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness.
Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise.
Many traditional methods have been proposed to solve decision making problems under merging scenarios.
However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios.
In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios.

A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically.

A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency.
An exemplar merging scenario was used to implement and examine the proposed method.

title: GridSim: A Vehicle Kinematics Engine for Deep Neuroevolutionary Control in Autonomous Driving
abstract: Current state of the art solutions in the control of an autonomous vehicle mainly use supervised end-to-end learning, or decoupled perception, planning and action pipelines.
Another possible solution is deep reinforcement learning, but such a method requires that the agent interacts with its surroundings in a simulated environment.
In this paper we introduce GridSim, which is an autonomous driving simulator engine running a car-like robot architecture to generate occupancy grids from simulated sensors.
We use GridSim to study the performance of two deep learning approaches, deep reinforcement learning and driving behavioral learning through genetic algorithms.
The deep network encodes the desired behavior in a two elements fitness function describing a maximum travel distance and a maximum forward speed, bounded to a specific interval.
The algorithms are evaluated on simulated highways, curved roads and inner-city scenarios, all including different driving limitations.

title: The Open-Source TEXPLORE Code Release for Reinforcement Learning on Robots
abstract: The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations on-line.
RL is a paradigm for learning sequential decision making tasks, usually formulated as a Markov Decision Process (MDP).
For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time.
In addition, the algorithm must learn efficiently in the face of noise, sensor/actuator delays, and continuous state features.
In this paper, we present the texplore ROS code release, which contains texplore, the first algorithm to address all of these challenges together.
We demonstrate texplore learning to control the velocity of an autonomous vehicle in real-time.
texplore has been released as an open-source ROS repository, enabling learning on a variety of robot tasks.

title: Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning [arXiv]
abstract: To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment.
Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances.
This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario.
We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance.
The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.

[Intelligent Vehicles Symposium (IV), 2018 IEEE].

title: Path tracking control of an autonomous vehicle with model-free adaptive dynamic programming and RBF neural network disturbance compensation
abstract: The performance of the model-based controller is always affected by the uncertainty and nonlinearity of the model parameters in the vehicle path tracking process.
To address this issue, a novel path tracking controller based on model-free adaptive dynamic programming (ADP) is proposed for autonomous vehicles in this paper.
To be specific, the proposed controller obtains information from the online state and front-wheel angle input data which are repeatedly used to calculate the controller gain iteratively.
So, this controller features not requiring accurate knowledge of vehicle model parameters for controller development.
Meanwhile, the path tracking performance of the autonomous vehicle will be inevitably disturbed by unknown nonlinear external disturbance.
To approximate this disturbance, the learning characteristics of Radial Basis Function Neural Network (RBFNN) are applied to generate compensation for the front-wheel angle.
Afterward, the weight updating law of RBFNN is derived by Lyapunov function to ensure the stability and convergence of the whole system.
Finally, Hardware in the loop (HIL) test results demonstrate that the proposed ADP-RBF controller can improve the comprehensive performance of the vehicle path tracking control system and achieve the balance between path tracking accuracy and minimum sideslip angle.


title: Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages [arXiv]
abstract: The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control.
However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles.
In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent.

By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy.
This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle.
We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance.

Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone.

[Sensors 2021, 21, 2032 doi:10.3390/s21062032].

title: UCRLF: unified constrained reinforcement learning framework for phase-aware architectures for autonomous vehicle signaling and trajectory optimization
abstract: Signaling and trajectory optimization work as contention and researchers have debated on what should be the best for the vehicle, but it seems that both components are complement to each other and there can be combined situations with bounds where maximum optimization can be achieved.

This paper introduces a novel approach called Phase-Aware Deep Learning and Constrained Reinforcement Learning for optimization and constant improvement of signal and trajectory for autonomous vehicle operation modules for an intersection.

It deals with all the components required for the signaling system to operate, communicate and also navigate the vehicle with proper trajectory so that it faces less waiting time and the overall system operates with minimum waiting time and comparable throughput rate.

We have done analysis on the operating time and the vehicle movement as these are vital for pollution and energy consumption.
Our methodologies are not only efficient in time and computation but also have incorporated highly optimized data representation to reduce the overhead of maintaining and accessing the data.
This ensures very efficient time complexity and theoretical computation time and better lower bounds.
Constrained Reinforcement Learning concept is the main contribution of this work and it helped in decreasing 84% of the waiting time for the vehicles.

title: Multi-Agent Advantage Actor-Critic Learning For Message Content Selection in Cooperative Perception Networks
abstract: Recent advancements in autonomous vehicle perception haveexposed limitations of onboard sensors such as radar, lidar, and cameras, which road obstacles and adverse weather conditions can impede.
Connected and Autonomous Vehicles (CAVs) are leveraging wireless communications to share perception information through a process called Cooperative Perception (CP), aiming to provide a more comprehensive understanding of their environment.

However, this can result in excessive redundant and useless information in the network, as the same road objects may be detected and exchanged simultaneously by multiple CAVs.
This not only consumes more network resources but also may overload the communication channel, reducing the delivery of perception information to CAVs and ultimately decreasing the overall CP awareness in the network.

This paper introduces MCORM, a multi-agent learning method based on the advantage actor-critic algorithm to maximize object usefulness and reduce redundancy in the network.
Our evaluations demonstrate that through this method, CAVs learn optimal CP message content selection policies that maximize usefulness.
Further more, our proposal proves to be more effective in mitigating object redundancy and improving network reliability in comparison to existing approaches.

title: Target-Oriented Maneuver Decision for Autonomous Vehicle: A Rule-Aided Reinforcement Learning Framework
abstract: Autonomous driving systems (ADSs) have the potential to revolutionize transportation by improving traffic safety and efficiency.
As the core component of ADSs, maneuver decision aims to make tactical decisions to accomplish road following, obstacle avoidance, and efficient driving.
In this work, we consider a typical but rarely studied task, called Target-Lane-Entering (TLE), where an autonomous vehicle should enter a target lane before reaching an intersection to ensure a smooth transition to another road.

For navigation-assisted autonomous driving, a maneuver decision module chooses the optimal timing to enter the target lane in each road section, thus avoiding rerouting and reducing travel time.
To achieve the TLE task, we propose a ruLe-aided reINforcement lEarning framework, called LINE, which combines the advantages of RL-based policy and rule-based strategy, allowing the autonomous vehicle to make target-oriented maneuver decisions.

Specifically, an RL-based policy with a hybrid reward function is able to make safe, efficient, and comfortable decisions while considering the factors of target lanes.
Then a strategy of rule revision aims to help the policy learn from intervention and block the risk of missing target lanes.
Extensive experiments based on the SUMO simulator confirm the effectiveness of our framework.
The results show that LINE achieves state-of-the-art driving performance with over 95% task success rate.

title: Driving in Dense Traffic with Model-Free Reinforcement Learning
abstract: Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads.
This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through.
However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap.
The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road.
In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle.
The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes.
Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to.
We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation.
As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.

title: Pedestrian Collision Avoidance for Autonomous Vehicles at Unsignalized Intersection Using Deep Q-Network [arXiv]
abstract: Prior research has extensively explored Autonomous Vehicle (AV) navigation in the presence of other vehicles, however, navigation among pedestrians, who are the most vulnerable element in urban environments, has been less examined.

This paper explores AV navigation in crowded, unsignalized intersections.
We compare the performance of different deep reinforcement learning methods trained on our reward function and state representation.
The performance of these methods and a standard rule-based approach were evaluated in two ways, first at the unsignalized intersection on which the methods were trained, and secondly at an unknown unsignalized intersection with a different topology.

For both scenarios, the rule-based method achieves less than 40\% collision-free episodes, whereas our methods result in a performance of approximately 100\%.
Of the three methods used, DDQN/PER outperforms the other two methods while it also shows the smallest average intersection crossing time, the greatest average speed, and the greatest distance from the closest pedestrian.


title: Driving in Dense Traffic with Model-Free Reinforcement Learning
abstract: Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads.
This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through.
However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap.
The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road.
In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle.
The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes.
Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to.
We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation.
As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.

title: Attentional policies for cross-context multi-agent reinforcement learning [arXiv]
abstract: Many potential applications of reinforcement learning in the real world involve interacting with other agents whose numbers vary over time.
We propose new neural policy architectures for these multi-agent problems.
In contrast to other methods of training an individual, discrete policy for each agent and then enforcing cooperation through some additional inter-policy mechanism, we follow the spirit of recent work on the power of relational inductive biases in deep networks by learning multi-agent relationships at the policy level via an attentional architecture.

In our method, all agents share the same policy, but independently apply it in their own context to aggregate the other agents' state information when selecting their next action.
The structure of our architectures allow them to be applied on environments with varying numbers of agents.
We demonstrate our architecture on a benchmark multi-agent autonomous vehicle coordination problem, obtaining superior results to a full-knowledge, fully-centralized reference solution, and significantly outperforming it when scaling to large numbers of agents.


title: Enhancing active disturbance rejection design via deep reinforcement learning and its application to autonomous vehicle
abstract: Taking the velocity regulation of autonomous driving as an example, this paper developed an enhancing active disturbance rejection design employing the deep reinforcement learning-deep deterministic policy gradient (DDPG).

In this scheme, the active disturbance rejection control (ADRC) is adopted to online estimate and compensate disturbances and uncertainties, and feasible regions of control parameters are obtained through the Lyapunov method.

Then DDPG is combined with ADRC to online adaptively tune control parameters in response to the changing environments, where safety, comfort, and energy-saving are considered in the reward design, and mapping relation between the defined action and state is constructed for the maximal reward.

Besides, numerical simulations demonstrate the better performance and stronger robustness of the enhancing design when facing uncertainties, sensor noise, and mechanical faults, and comfort and energy consumption have also been improved to some extent compared with the general ADRC and model predictive control (MPC).


title: Autonomous Vehicle Simulation Using Deep Reinforcement Learning
abstract: The reinforcement learning algorithms have been proven to be extremely accurate in performing a variety of tasks.
These algorithms have outperformed humans in traditional games.
This paper proposes a reinforcement learning based approach to autonomous driving.
The autonomous vehicles must be able to deal with all external situations to ensure safety and to avoid undesired circumstances such as collisions.
Thus, we propose the use of deep deterministic policy gradient (DDPG) algorithm which is able to work in a complex and continuous domain.
To avoid physical damage and reduce costs, we choose to use a simulator to test the proposed approach.
The CARLA simulator would be used as the environment.
To fit the DDPG algorithm to the CARLA environment, our network architecture consists of critic and actor networks.
The performance would be evaluated based on rewards generated by the agent while driving in the simulated environment.

title: Hierarchical framework integrating rapidly-exploring random tree with deep reinforcement learning for autonomous vehicle
abstract: This paper proposes a systematic driving framework where the decision making module of reinforcement learning (RL) is integrated with rapidly-exploring random tree (RRT) as motion planning.
RL is used to generate local goals and semantic speed commands to control the longitudinal speed of a vehicle while rewards are designed for the driving safety and the traffic efficiency.
Guaranteeing the driving comfort, RRT returns a feasible path to be followed by the vehicle with the speed commands.
The scene decomposition approach is implemented to scale the deep neural network (DNN) to environments with multiple traffic participants and double deep Q-networks (DDQN) with prioritized experience replay (PER) is utilized to accelerate the training process.

To handle the disturbance of the perception of the agent, we use an ensemble of neural networks to evaluate the uncertainty of decisions.
It has shown that the proposed framework can tackle unexpected actions of traffic participants at an intersection yielding safe, comfort and efficient driving behaviors.
Also, the ensemble of DDQN with PER is proved to be superior over standard DDQN in terms of learning efficiency and disturbance vulnerability.

title: Lane-keeping Control of Autonomous Vehicles Using Reinforcement Learning and Predictive Safety Filter *
abstract: The urgent need for both safety and high-performance in the motion planning and decision-making system of the autonomous vehicle presents a significant challenge for learning-based technologies.
In this paper, we introduces a lane-keeping control structure that combines reinforcement learning and a predictive safety filter to ensure the vehicle's safety motion.
A vehicle agent under the racetrack environment is trained by the soft actor critic algorithm, to achieve high-performance continuous lane-keeping motion.
Additionally, we establish the safety task for lane-keeping, which results in a learning-based safety filter, to avoid violation of driving on the outer lane.
Through simulations, we demonstrate the effectiveness of our method in ensuring the safe and trustworthy control of autonomous vehicle lane-keeping.

title: Learning-Based Safe Control for Robot and Autonomous Vehicle Using Efficient Safety Certificate
abstract: Energy-function-based safety certificates can provide demonstrable safety for complex automatic control systems used in safety control tasks.
However, recent studies on learning-based energy function synthesis have only focused on feasibility, which can lead to over-conservatism and reduce controller efficiency.
In this study, we propose using magnitude regularization techniques to enhance the efficiency of safe controllers by reducing conservativeness within the energy function while maintaining promising demonstrable safety guarantees.

Specifically, we measure conservativeness by the magnitude of the energy function and reduce it by adding a magnitude regularization term to the integrated loss.
We present the SafeMR algorithm, which is synthesized using reinforcement learning (RL) to unify the learning process of the safety controller and the energy function.
To verify the effectiveness of the algorithm, we conducted two sets of experiments, one in a robot-based environment and the other in an autonomous vehicle environment.
The experimental results demonstrate that the proposed approach reduces the conservativeness of the energy function and outperforms the baseline in terms of controller efficiency for the robot, while ensuring safety.


title: RIGAA at the SBFT 2023 Tool Competition - Cyber-Physical Systems Track
abstract: Testing and verification of autonomous systems is critically important.
In the context of SBFT 2023 CPS testing tool competition, we present our tool RIGAA for generating virtual roads to test an autonomous vehicle lane keeping assist system.
RIGAA combines reinforcement learning as well as evolutionary search to generate test scenarios.
It has achieved the second highest final score among 5 other submitted tools.

title: Driving in Dense Traffic with Model-Free Reinforcement Learning [arXiv]
abstract: Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads.
This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through.
However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap.
The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road.
In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle.
The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes.
Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to.
We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation.

title: A Deep Q Learning-Model Predictive Control Approach to vehicle routing and control with platoon constraints
abstract: In this paper we deal with the control of platoon of autonomous vehicles driving in Urban Road Networks (URNs).
We exploit the idea of using Deep Reinforcement Learning (DRL) as the path planner of the proposed architecture.
The advantage of this solution is the capability to deal with the actual traffic congestion while driving the autonomous vehicle to its destination.
In particular, the high-level routing decisions are translated into manipulable set-points for Receding Horizon controllers by making more computational affordable and efficient the control action on the vehicle dynamics.

Feasibility and asymptotic closed-loop stability are formally proved.
Some simulations on a platoon, consisting of three agents described by double-integrator models, are provided to show the effectiveness of the overall architecture.

title: Deep reinforcement learning-based long-range autonomous valet parking for smart cities
abstract: In this paper, to reduce the congestion rate at the city center and increase the traveling quality of experience (QoE) of each user, the framework of long-range autonomous valet parking is presented.
Here, an Autonomous Vehicle (AV) is deployed to pick up, and drop off users at their required spots, and then drive to the car park around well-organized places of city autonomously.
In this framework, we aim to minimize the overall distance of AV, while guarantee all users are served with great QoE, i.
e.
, picking up, and dropping off users at their required spots through optimizing the path planning of the AV and number of serving time slots.

To this end, we first present a learning-based algorithm, which is named as Double-Layer Ant Colony Optimization (DLACO) algorithm to solve the above problem in an iterative way.
Then, to make the fast decision, while considers the dynamic environment (i.
e.
, the AV may pick up and drop off users from different locations), we further present a deep reinforcement learning-based algorithm, i.
e.
, Deep Q-learning Network (DQN) to solve this problem.

Experimental results show that the DL-ACO and DQN-based algorithms both achieve the considerable performance.

title: DAQL-enabled autonomous vehicle navigation in dynamically changing environment
abstract: In this chapter we have presented a multiple goal reinforcement learning framework and illustrated on a two-goal problem in autonomous vehicle navigation.
In general, DAQL can be applied in any goals that environmental response is available, whereas QL would suffice if environmental response is not available or can be ignored.
A proportional goal fusion function was used to maintain balance between the two goals in this case.
Extensive simulations have been carried out to evaluate its performance under different obstacle behaivors and sensing accuracy.
The results showed that the proposed method is characterized by its ability to (1) deal with single obstacles at any speed and from any directions; (2) deal with two obstacles approaching from different directions; (3) cope with large sensor noise; (4) navigate in high obstacle density and high relative velocity environment.

Detailed comparison of the proposed method with the R&G method reveals that improvements by the proposed method in path time and the number of collision-free episodes are substantial.

title: Autonomous highway driving using reinforcement learning with safety check system based on time-to-collision
abstract: Decision making is an essential component of autonomous vehicle technology and received significant attention from academic and industry organizations.
One of the promising approaches in designing a decision-making method is Reinforcement Learning (RL).
To apply an RL algorithm to an autonomous driving problem, a feature representation of the state must first be chosen.
The most commonly used representation is the spatial-temporal state feature.
However, if the number or order of the surrounding vehicle changes, the feature representation will be affected.
In this paper, we utilize time-to-collision (TTC) as the feature representation and propose a TTC-based safety check system.
The action output by the RL controller would be replaced with a safer action chosen by the safety check system when an agent detects a potential collision, i.e., the TTC is below the time threshold.
A ramp merging task is used to illustrate the effect.
Simulation results show that the proposed method can effectively improve the arrival rate and reduce the collision rate, even in the case of dense traffic situations.
Furthermore, we also conducted experiments to examine the performance of the safety check system with different time thresholds.

title: Computation offloading strategy based on deep reinforcement learning for connected and autonomous vehicle in vehicular edge computing
abstract: Connected and Automated Vehicle (CAV) is a transformative technology that has great potential to improve urban traffic and driving safety.
Electric Vehicle (EV) is becoming the key subject of next-generation CAVs by virtue of its advantages in energy saving.
Due to the limited endurance and computing capacity of EVs, it is challenging to meet the surging demand for computing-intensive and delay-sensitive in-vehicle intelligent applications.
Therefore, computation offloading has been employed to extend a single vehicle's computing capacity.
Although various offloading strategies have been proposed to achieve good computing performace in the Vehicular Edge Computing (VEC) environment, it remains challenging to jointly optimize the offloading failure rate and the total energy consumption of the offloading process.

To address this challenge, in this paper, we establish a computation offloading model based on Markov Decision Process (MDP), taking into consideration task dependencies, vehicle mobility, and different computing resources for task offloading.

We then design a computation offloading strategy based on deep reinforcement learning, and leverage the Deep Q-Network based on Simulated Annealing (SA-DQN) algorithm to optimize the joint objectives.
Experimental results show that the proposed strategy effectively reduces the offloading failure rate and the total energy consumption for application offloading.

title: Cognitive Reinforcement Learning For Autonomous Driving
abstract: Most existing reinforcement learning methods are combined with deep neural networks in autonomous driving.
There is a well-known trouble named 'black-box' of deep neural networks, which lacks of interpretability, occurring in the decision-making process.
That will affect the safety of autonomous vehicle.
In this work, we propose a cognitive reinforcement learning framework.
This framework illustrates a cognitive model that transfers the state space to cognitive signals.
Subsequencely, based on these signals, the model simulates the human decision-making process.
As a result, the outcome of decision provides reward to the reinforcement learning.
The computational experiments conducted in CARLA demonstrate that our framework performs equally well as the conventional reinforcement learning methods and provides interpretability under the same circumstance in the reinforcement learning.


title: Combating Stop-and-Go Wave Problem at a Ring Road Using Deep Reinforcement Learning Based Autonomous Vehicles
abstract: With the rapid development of artificial intelligence, autonomous driving has recently attracted considerable attention.
This paper aims to use an autonomous vehicle to improve road flow by solving the stop-and-go-wave problem on a ring road.
We design a special model of Markov decision process model to solve stop-and-go-wave and use three deep reinforcement learning algorithms to train autonomous vehicles: Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3).

We then compare their driving patterns and performances.
We confirmed that an autonomous vehicle on the ring road could control the flow of multiple non-autonomous vehicles with an extensive simulation study, thus successfully solving the stop-and-go wave problem.


title: Design and Implementation of Intelligent Agent Training Systems for Virtual Vehicles
abstract: This paper presents the results of the design, simulation, and implementation of a virtual vehicle.
Such a process employs the Unity videogame platform and its Machine Learning-Agents library.
The virtual vehicle is implemented in Unity considering mechanisms that represent accurately the dynamics of a real automobile, such as motor torque curve, suspension system, differential, and anti-roll bar, among others.

Intelligent agents are designed and implemented to drive the virtual automobile, and they are trained using imitation or reinforcement.
In the former method, learning by imitation, a human expert interacts with an intelligent agent through a control interface that simulates a real vehicle; in this way, the human expert receives motion signals and has stereoscopic vision, among other capabilities.

In learning by reinforcement, a reward function that stimulates the intelligent agent to exert a soft control over the virtual automobile is designed.
In the training stage, the intelligent agents are introduced into a scenario that simulates a four-lane highway.
In the test stage, instead, they are located in unknown roads created based on random spline curves.
Finally, graphs of the telemetric variables are presented, which are obtained from the automobile dynamics when the vehicle is controlled by the intelligent agents and their human counterpart, both in the training and the test track.


title: Hybrid deep reinforcement learning model for safe and efficient autonomous vehicles at pedestrian crossings
abstract: The autonomous vehicle is a particularly promising area of application for machine learning algorithms.
Autonomous driving is challenging due to the complexity of the road network, the hardly predictable human behaviors, either pedestrians or drivers and the imperative need to ensure the safety of all road users.

Deep Reinforcement Learning algorithms have garnered significant interest as a viable option for controlling autonomous vehicles.
They allow defining a control policy by solely describing the observation space, the action space, and a reward function.
Yet, the most common solutions face difficulties when the action space is hybrid (simultaneously continuous and discrete), and the reward is sparse, which is the case when a selection between choices must be made (like yielding or not), and the trajectory of the vehicle must be adapted according to this decision.

This paper addresses these challenges by focusing on controlling an autonomous vehicle at pedestrian crossings.
The vehicle adapts its speed (continuous decision) and the displayed signal (discrete decision) according to the pedestrian's state.
The unique characteristic of the speed profile is to facilitate safe pedestrian crossings without requiring the vehicle to come to a complete stop.
This raises the consistency problem between the speed profile and the signals issued by the vehicle.
This paper introduces a new hybrid DRL model capable of making both decisions.
It discusses the training method, the construction and necessary constraints on the reward, as well as the success of generalization to different speed limitations.

title: Framework of Active Obstacle Avoidance for Autonomous Vehicle Based on Hybrid Soft Actor-Critic Algorithm
abstract: In this paper, a framework of active obstacle avoidance for autonomous vehicles based on the hybrid soft actor-critic (SAC) algorithm is proposed.
In the stage of local path planning, a comprehensive cost function considering collision risks, deviation from the global route, road lines crossing, and driving comfortability is developed to provide a local optimal path avoiding both static and dynamic obstacles considering multiple predicting timesteps.

Then, a path tracking controller on the foundation of a hybrid SAC algorithm is designed to mitigate the problem of high sample complexity caused by random initialization of parameters in conventional reinforcement learning approaches.

Model predictive control (MPC) plays a guiding role by applying its control action to combine with the action of SAC online to obtain a more effective state and reward information for training.
The mechanism of the combination of MPC with SAC to balance the exploration and reliability is explained in detail.
In order to improve the convergence rate and learning efficiency, a dual actor network structure for two different control actions is adopted.
With considerations of various relevant factors influencing the control effect, the reward for the hybrid SAC algorithm is designed carefully.
Finally, the results of simulation experiments illustrate that the proposed approach performs effectively with the assurance of safety and driving comfortability.
In summary, the hybrid SAC algorithm with dual actor networks performs better than other algorithms for comparison in all test scenarios in this paper.

title: A Hybrid Tactical Decision-Making Approach in Automated Driving Combining Knowledge-Based Systems and Reinforcement Learning
abstract: Decision-making in automated driving is influenced both by objective traffic rules and subjective perceptions and goals of the driver.
Thus, a suitable representation of the environment of the autonomous vehicle is required to model complex traffic situations and extract key features.
To achieve this objective, this work uses an ontology-based situation interpretation (OBSI) to model traffic situations.
The resulting semantic state representation is used to train models of vehiclecontrolling agents using reinforcement learning.
Based on our simulations, it can be shown that the semantic preprocessing of traffic situations significantly improves the agent's performance regarding safety and driving style.

title: A platform for sharing artificial intelligence algorithms in autonomous driving: an overview of enhanced LAOP
abstract: To solve the Autonomous Vehicle (AV) problem, an artificial learning model that translates sensor data into controls must be built.
This way, the vehicle can react to its changing environment.
To the best of our knowledge, there are no platforms where researchers can both develop new machine learning models, train them and compare them directly to others.
We believe that such a platform can greatly impact the field of artificial intelligence and AV.
In this paper, we propose an enhanced version of the Learning Algorithm Optimization Platform LAOP, called LAOP 2.0.
It helps researchers in the field of artificial intelligence develop their models by allowing them to easily test and compare them.
We also introduce the Learning Algorithm Sharing Platform (LASP), which makes it easy to share deep learning algorithms.
As a demonstration of the versatility of our platform, we compared two learning algorithms.
The first one, Fully Connected Neural Network (FUCONN), uses reinforcement learning to train itself.
The second one, Mimicking HUman Behaviour (MHUB), uses supervised learning to adjust its weights and learns from human input.
We demonstrate through extensive simulations that FUCONN outperforms MHUB after being trained.

title: Motion Prediction on Self-driving Cars: A Review [arXiv]
abstract: The autonomous vehicle motion prediction literature is reviewed.
Motion prediction is the most challenging task in autonomous vehicles and self-drive cars.
These challenges have been discussed.
Later on, the state-of-the art has reviewed based on the most recent literature and the current challenges are discussed.
The state-of-the-art consists of classical and physical methods, deep learning networks, and reinforcement learning.
prons and cons of the methods and gap of the research presented in this review.
Finally, the literature surrounding object tracking and motion will be presented.
As a result, deep reinforcement learning is the best candidate to tackle self-driving cars.

title: Autonomous Vehicle Emergency Obstacle Avoidance Maneuver Framework at Highway Speeds
abstract: An autonomous vehicle (AV) uses high-level decision making and lower-level actuator controls, such as throttle (acceleration), braking (deceleration), and steering (change in lateral direction) to navigate through various types of road networks.

Path planning and path following for highway driving are currently available in series-produced highly automated vehicles.
In addition to these, emergency collision avoidance decision making and maneuvering are another key and essential feature that is needed in a series production AV at highway driving speeds.
For reliability, low cost, and fast computation, such an emergency obstacle avoidance maneuvering system should use well-established conventional methods as opposed to data-driven neural networks or reinforcement learning methods, which are currently not suitable for use in highway AV driving.

This paper presents a novel Emergency Obstacle Avoidance Maneuver (EOAM) methodology for AVs traveling at higher speeds and lower road surface friction, involving time-critical maneuver determination and control.

The proposed EOAM framework offers usage of the AV's sensing, perception, control, and actuation system abilities as one cohesive system to avoid an on-road obstacle, based first on performance feasibility and second on passenger comfort, and it is designed to be well integrated within an AV's high-level control and decision-making system.

To demonstrate the efficacy of the proposed method, co-simulation including the AV's EOAM logic in Simulink and a vehicle model in CarSim is conducted with speeds ranging from 55 to 165 km/h and on road surfaces with friction ranging from 1.
0 to 0.
1.

The results are analyzed and interpreted in the context of an entire AV system, with implications for future work.

title: A Reinforcement Learning Benchmark for Autonomous Driving in Intersection Scenarios
abstract: In recent years, control under urban intersection scenarios has become an emerging research topic.
In such scenarios, the autonomous vehicle confronts complicated situations since it must deal with the interaction with social vehicles timely while obeying the traffic rules.
Generally, the autonomous vehicle is supposed to avoid collisions while pursuing better efficiency.
The existing work fails to provide a framework that emphasizes the integrity of the scenarios while deploying and testing reinforcement learning(RL) methods.
Specifically, we propose a benchmark for training and testing RL-based autonomous driving agents in complex intersection scenarios, which is called RL-CIS.
Then, a set of baselines consisting various algorithms are deployed.
The test benchmark and baselines provide a fair and comprehensive training and testing platform for the study of RL for autonomous driving in the intersection scenario, advancing RL-based methods for autonomous driving control.

The code of our proposed framework can be found at https://github.com/liuyufi123/ComplexUrbanScenarios.

title: A Data-Driven Optimal Control Decision-Making System for Multiple Autonomous Vehicles
abstract: With the fast development and rising popularity of autonomous vehicle (AV) technology, multiple AVs may soon be driving on the same road simultaneously.
Such multi-AV coexistence driving situations will lead to new and persistent challenges.
Therefore, improvements on making control decisions for multiple AVs becomes necessary for continued driving safety.
In this paper, we propose a multi-AV decision making system (MADM), which considers multi-AV coexistence driving situations during the decision-making process.
In MADM, we first build a policy formation method to generate policies that learn the driving behaviors of an expert based on the expert's driving trajectory data.
We then develop a multi-AV decision-making method, which adjusts the formed policies through multi-agent reinforcement learning.
The adjusted policies make control decisions for multiple AVs with safety guarantee.
We used a real-world traffic dataset to evaluate the decision making performance of MADM in comparison with several state-of-the-art methods.
Experimental results show that MADM reduces emergency rate by as high as 51% when compared with existing methods.

title: RACE: Reinforced Cooperative Autonomous Vehicle Collision Avoidance
abstract: With the rapid development of autonomous driving, collision avoidance has attracted attention from both academia and industry.
Many collision avoidance strategies have emerged in recent years, but the dynamic and complex nature of driving environment poses a challenge to develop robust collision avoidance algorithms.
Therefore, in this paper, we propose a decentralized framework named RACE: Reinforced Cooperative Autonomous Vehicle Collision AvoidancE.
Leveraging a hierarchical architecture we develop an algorithm named Co-DDPG to efficiently train autonomous vehicles.
Through a security abiding channel, the autonomous vehicles distribute their driving policies.
We use the relative distances obtained by the opponent sensors to build the VANET instead of locations, which ensures the vehicle's location privacy.
With a leader-follower architecture and parameter distribution, RACE accelerates the learning of optimal policies and efficiently utilizes the remaining resources.
We implement the RACE framework in the widely used TORCS simulator and conduct various experiments to measure the performance of RACE.
Evaluations show that RACE quickly learns optimal driving policies and effectively avoids collisions.
Moreover, RACE also scales smoothly with varying number of participating vehicles.
We further compared RACE with existing autonomous driving systems and show that RACE outperforms them by experiencing 65% less collisions in the training process and exhibits improved performance under varying vehicle density.


title: Uncertainty-Aware Model-Based Reinforcement Learning: Methodology and Application in Autonomous Driving
abstract: To further improve learning efficiency and performance of reinforcement learning (RL), a novel uncertainty-aware model-based RL method is proposed and validated in autonomous driving scenarios in this paper.

First, an action-conditioned ensemble model with the capability of uncertainty assessment is established as the environment model.
Then, a novel uncertainty-aware model-based RL method is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL's learning efficiency and performance.

The proposed method is then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios.
Validation results suggest that the proposed method outperforms the model-free RL approach with respect to learning efficiency, and model-based approach with respect to both efficiency and performance, demonstrating its feasibility and effectiveness.


title: Plastic Grabber: Underwater Autonomous Vehicle Simulation for Plastic Objects Retrieval Using Genetic Programming
abstract: We propose a path planning solution using genetic programming for an autonomous underwater vehicle.
Developed in ROS Simulator that is able to roam in an environment, identify a plastic object, such as bottles, grab it and retrieve it to the home base.
This involves the use of a multi-objective fitness function as well as reinforcement learning, both required for the genetic programming to assess the model's behaviour.
The fitness function includes not only the objective of grabbing the object but also the efficient use of stored energy.
Sensors used by the robot include a depth image camera, claw and range sensors that are all simulated in ROS.

title: Reinforcement Learning Based Route And Stop Planning For Autonomous Vehicle Shuttle Service
abstract: AV shuttles can be a complement to the existing bus services to fill the gap between the mobility demand of a city population and the supply of the existing bus services.
In contrast to traditional approaches, this problem requires the consideration of new factors due to new objectives of the AV shuttle service such as mobility service for elderly and physically disabled persons, first/last mile transits, lessening of parking woes of the commuters, small local shop connectivity, and etc.

In this work, we propose methods for route and stop planning for AV shuttle service in an urban city.
We utilize a large scale human mobility dataset collected from users' cellphones' GPS sensors in Richmond city (as a city example) to identify the different potential transportation demands of AV shuttles.

We propose an optimization problem-based solution and a deep reinforcement learning (RL) based solution.
We conduct comprehensive evaluation based on our human mobility dataset along with real-world road network information.
Our evaluation shows the proposed RL based system outperforms the comparison methods in terms of addressing the transportation needs as mentioned above by a significant margin while also minimizing the travel time on roads.


title: Interaction-aware decision making with adaptive strategies under merging scenarios [arXiv]
abstract: In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants.
Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness.
Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise.
Many traditional methods have been proposed to solve decision making problems under merging scenarios.
However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios.
In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios.

A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically.

A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency.
An exemplar merging scenario was used to implement and examine the proposed method.

title: Reinforcement-Learning-Based Decision and Control for Autonomous Vehicle at Two-Way Single-Lane Unsignalized Intersection
abstract: Intersections have attracted wide attention owing to their complexity and high rate of traffic accidents.
In the process of developing L3-and-above autonomous-driving techniques, it is necessary to solve problems in autonomous driving decisions and control at intersections.
In this article, a decision-and-control method based on reinforcement learning and speed prediction is proposed to manage the conjunction of straight and turning vehicles at two-way single-lane unsignalized intersections.

The key position of collision avoidance in the process of confluence is determined by establishing a road-geometry model, and on this basis, the expected speed of the straight vehicle that ensures passing safety is calculated.

Then, a reinforcement-learning algorithm is employed to solve the decision-control problem of the straight vehicle, and the expected speed is optimized to direct the agent to learn and converge to the planned decision.

Simulations were conducted to verify the performance of the proposed method, and the results show that the proposed method can generate proper decisions for the straight vehicle to pass the intersection while guaranteeing preferable safety and traffic efficiency.


title: A Survey of the State-of-the-Art Reinforcement Learning-Based Techniques for Autonomous Vehicle Trajectory Prediction
abstract: Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems.
However, for AVs to effectively navigate the road, they must possess the capability to predict the future trajectories of nearby traffic participants, similar to the predictive driving abilities of human drivers.

Reinforcement Learning (RL) has emerged as a promising approach for learning complex decision-making policies in dynamic environments.
This survey explores the application of RL approaches in trajectory prediction, focusing on inverse reinforcement learning, deep reinforcement learning, and imitation learning.
It provides an in-depth analysis of the underlying principles, algorithms, and architectures employed in these methods, highlighting their respective strengths and limitations.
Moreover, the survey addresses the current challenges in the field and presents potential future research directions, offering valuable insights to readers.

title: Adversarial jaywalker modeling for simulation-based testing of Autonomous Vehicle Systems
abstract: We present an approach for creating adversarial jaywalkers, autonomous pedestrian models which intentionally act to create unsafe situations involving other vehicles.
An adversarial jaywalker employs a hybrid state-model with social forces and state transition rules.
The parameters (for social forces and state transitions) of this model are tuned via reinforcement learning to create risky situations faster with synthetic yet plausible behavior.
The resulting jaywalkers are capable of realistic behavior while still engaging in sufficiently risky actions to be useful for testing.
These adversarial pedestrian models are useful in a wide range of scenario-based tests for autonomous vehicles.

title: Autonomous Driving Policy Continual Learning With One-Shot Disengagement Case
abstract: Disengagement cases during naturalistic driving are rare or even one-shot, but valuable for autonomous driving.
The autonomous vehicles are necessary to continually learn from these disengagement cases, to improve the policy for better performance when next time meeting these cases.
Manually adjusting the policy or adding the rules to fix these disengagement cases may cause engineering burden and may contradict other driving functions.
To this end, this work proposes a continually learning agent which can automatically get improved once encountering a disengagement case.
The main idea is to establish a disengagement-imagination environment, and then train the policy using imagination data for performance improvement, named disengagement-case imagination augmented continual learning (DICL).

In the imagination environment, the surrounding objects are designed to first follow the recorded trajectory, and then switch to the interactive models for the policy training.
The switch point is carefully designed to make the imagination contain the disengagement reasons but avoid overfitting the collected driving case.
This method is evaluated by the real autonomous driving disengagement data, collected from an open-road-testing autonomous vehicle.
The results show that the DICL agent can automatically learn to handle the emerging disengagement case and similar cases.
This work provides a possible way to make the AV agents automatically get improvement during road testing.

title: Machine Self-confidence in Autonomous Systems via Meta-analysis of Decision Processes [arXiv]
abstract: Algorithmic assurances from advanced autonomous systems assist human users in understanding, trusting, and using such systems appropriately.
Designing these systems with the capacity of assessing their own capabilities is one approach to creating an algorithmic assurance.
The idea of `machine self-confidence' is introduced for autonomous systems.
Using a factorization based framework for self-confidence assessment, one component of self-confidence, called `solver-quality', is discussed in the context of Markov decision processes for autonomous systems.

Markov decision processes underlie much of the theory of reinforcement learning, and are commonly used for planning and decision making under uncertainty in robotics and autonomous systems.
A `solver quality' metric is formally defined in the context of decision making algorithms based on Markov decision processes.
A method for assessing solver quality is then derived, drawing inspiration from empirical hardness models.
Finally, numerical experiments for an unmanned autonomous vehicle navigation problem under different solver, parameter and environment conditions indicate that the self-confidence metric exhibits the desired properties.

Discussion of results, and avenues for future investigation are included.

title: Distributional Deep Reinforcement Learning with a Mixture of Gaussians
abstract: In this paper, we propose a novel distributional reinforcement learning (RL) method which models the distribution of the sum of rewards using a mixture density network.
Recently, it has been shown that modeling the randomness of the return distribution leads to better performance in Atari games and control tasks.
Despite the success of the prior work, it has limitations which come from the use of a discrete distribution.
First, it needs a projection step and softmax parametrization for the distribution, since it minimizes the KL divergence loss.
Secondly, its performance depends on discretization hyperparameters such as the number of atoms and bounds of the support which require domain knowledge.
We mitigate these problems with the proposed parameterization, a mixture of Gaussians.
Furthermore, we propose a new distance metric called the Jensen-Tsallis distance, which allows the computation of the distance between two mixtures of Gaussians in a closed form.
We have conducted various experiments to validate the proposed method, including Atari games and autonomous vehicle driving.

title: Quantum-based Offloading Strategy for Intelligent Vehicle Network
abstract: Role of RSUs become essential in enhancing the service reliability rate of the end vehicle to strengthen autonomous vehicle technology.
Computation-intensive services are delay-sensitive, and most existingmethodsattempted comprehensively to meet the application deadline but have not reached the expectations due to classical computing.
In this regard, we design a novel offloading decision-making method based on quantum theory through reinforcement learning.
Grover's algorithm is employed to select a feasible device based on cost and energy usage probability ratio.
Theoretical and mathematical validations and simulation outcomes confine the impact of novel decision-making methods on the statistical constraints of the heterogeneous framework.

title: Feedback Forecasting based Deep Deterministic Policy Gradient Algorithm for Car-Following of Autonomous Vehicle
abstract: Deep reinforcement learning has been applied to the car-following control of autonomous driving in recent years.
However, the safety of car-following through reinforcement learning remains a problem.
To address this problem, a feedback forecasting based deep reinforcement learning algorithm is proposed.
A differential equation model is used to predict the collision of adjacent vehicles.
The output action of reinforcement learning is supervised by the feedback single to achieve a safe distance.
As a result, the performance of autonomous driving safety is improved while others are slightly affected.
The simulation results, based on NGSIM data, show that the predict collision avoidance algorithm is feasible and effective in the car-following control of autonomous driving.

title: A Markov Decision Process Model for a Reinforcement Learning-based Autonomous Pedestrian Crossing Protocol
abstract: Autonomous Traffic Management (ATM) systems empowered with Machine Learning (ML) technics are a promising solution for eliminating traffic light and decreasing traffic congestion in the future.
However, few efforts have focused on integrating pedestrians in ATM, namely the static programming-based cooperative protocol called Autonomous Pedestrian Crossing (APC).
In this paper, we model a Markov Decision Process (MDP) to enable a Deep Reinforcement Learning (DRL)-based version of APC protocol that is able to dynamically achieve the same objectives (i.e.
decreasing traffic delay at the crossing area).
Using concrete state space, action set and reward functions, our model forces the Autonomous Vehicle (AV) to "think" and behave according to APC architecture.
Compared to the traditional programming APC system, our approach permits the AV to learn from its previous experiences in non-signalized crossing and optimize the distance and the velocity parameters accordingly.


title: Reinforcement-learning-aided adaptive control for autonomous driving with combined lateral and longitudinal dynamics
abstract: This paper presents a deep reinforcement learning-aided controller for a 3-DOF autonomous vehicle with combined lateral and longitudinal dynamics.
In this scheme, the active disturbance rejection control (ADRC) gives full play to its advantages of being model-free and being able to estimate and compensate for internal uncertainties and external disturbances in real-time, and deep deterministic policy gradient (DDPG) fully considers safety, comfort, economy, and combines driving demand with state, action, reward to achieve real-time adaptive adjustment of control parameters.

Thus, the adaptive controller can better deal with uncertainties from modeling, parameters, and driving environment, and self-learning and adaptation ability is obtained simultaneously.
Moreover, simulation results illustrate that the adaptive controller performs satisfactorily for different driving operations and environments due to the online tuning and optimization of control parameters.


title: Planning How to Learn
abstract: When a robot uses an imperfect system model to plan its actions, a key challenge is the exploration-exploitation trade-off between two sometimes conflicting objectives: (i) learning and improving the model, and (ii) immediate progress towards the goal, according to the current model.

To address model uncertainty systematically, we propose to use Bayesian reinforcement learning and cast it as a partially observable Markov decision process (POMDP).
We present a simple algorithm for offline POMDP planning in the continuous state space.
Offline planning produces a POMDP policy, which can be executed efficiently online as a finite-state controller.
This approach seamlessly integrates planning and learning: it incorporates learning objectives in the computed plan, which then enables the robot to learn nearly optimally online and reach the goal.
We evaluated the approach in simulations on two distinct tasks, acrobot swing-up and autonomous vehicle navigation amidst pedestrians, and obtained interesting preliminary results.

title: Adaptive Dynamic Programming-Based Active Disturbance Rejection Control for Autonomous Vehicle Tracking
abstract: Nowadays, autonomous driving technology is more closely connected with human society, but the complex urban road environment puts higher requirements on the autonomous vehicle's robustness and fast response ability.

To solve the problems mentioned above, this paper proposes a trajectory tracking control method with anti-interference and high real-time performance.
The active disturbance rejection control approach is utilized to reject external disturbance and internal model uncertainties with the model predictive control controller.
To speed up the computation, parameterized adaptive dynamic programming is used to estimate the optimal control sequence.
A double-lane change demonstration is used to test and verify the proposed method.
The experimental results show that our method improves the anti-interference ability of the system while speeding up the control speed.
The proposed method can maintain the lateral error within 0.09m even with imposing 0.05rad disturbance to the input, and the single-step control time is half the traditional MPC.

title: Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle [arXiv]
abstract: With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack.

This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following.
Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation.

Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed.
Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following.
Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.

title: Path Tracking Control and Identification of Tire Parameters using On-line Model-based Reinforcement Learning
abstract: Path tracking control for autonomous vehicle using model predictive control (MPC) algorithm maintains maneuverability by calculating a sequence of control input which minimizes a tracking error.
The weakness of this method is that the performance of MPC may decrease significantly when the priori prediction model is not accurate.
Therefore, it is important to keep the vehicle stable when MPC having model error.
This paper uses an on-line model-based reinforcement learning (RL) to decrease the path error by learning unknown parameters and updating a prediction model.
To validate, two kinds of path tracking simulation are conducted: one is the comparison the performance between on-line model-based RL and MPC with model error.
The other one is about the test when the model used in MPC and the true dynamics, which actually received input, have different tire model.
The model-based RL method succeeds to learn unknown tire parameters and maintains their maneuverability in both simulations.

title: Car-following method based on inverse reinforcement learning for autonomous vehicle decision-making
abstract: There are still some problems need to be solved though there are a lot of achievements in the fields of automatic driving.
One of those problems is the difficulty of designing a car-following decision-making system for complex traffic conditions.
In recent years, reinforcement learning shows the potential in solving sequential decision optimization problems.
In this article, we establish the reward function R of each driver data based on the inverse reinforcement learning algorithm, and r visualization is carried out, and then driving characteristics and following strategies are analyzed.

At last, we show the efficiency of the proposed method by simulation in a highway environment.

title: Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research [arXiv]
abstract: Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner.
However, realistic simulation requires accurate modeling of nuanced and complex multi-agent interactive behaviors.
To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing.
Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios.
It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows.
To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation.
To supplement Waymax, we benchmark a suite of popular imitation and reinforcement learning algorithms with ablation studies on different design decisions, where we highlight the effectiveness of routes as guidance for planning agents and the ability of RL to overfit against simulated agents.


title: AST-SafeSec: Adaptive Stress Testing for Safety and Security Co-Analysis of Cyber-Physical Systems
abstract: Cyber-physical systems are becoming more intelligent with the adoption of heterogeneous sensor networks and machine learning capabilities that deal with an increasing amount of input data.
While this complexity aims to solve problems in various domains, it adds new challenges for the system assurance.
One issue is the rise in the number of abnormal behaviors that affect system performance due to possible sensor faults and attacks.
The combination of safety risks, which are usually caused by random sensor faults and security risks that can happen during any random system state, makes the full coverage testing of the cyber-physical system challenging.

Existing techniques are inadequate to deal with complex safety and security co-risks against cyber-physical systems.
In this paper, we propose AST-SafeSec, an analysis methodology for both safety and security aspects that utilizes reinforcement learning to identify the most likely adversarial paths at various normal or failure states of a cyber-physical system that can influence system behavior through its sensor data.

The methodology is evaluated using an autonomous vehicle scenario by incorporating a security attack into the stochastic sensor elements of a vehicle.
Evaluation results show that the methodology analyzes the interaction of malicious attacks with random faults and identifies the incident caused by the interactions and the most likely path that leads to the incident.


title: Machine learning for cooperative driving in a multi-lane highway environment
abstract: Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them.
In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment.
A driving algorithm is designed using deep Q learning, a type of reinforcement learning.
In order to train and test driving algorithms, we deploy a simulated traffic system, using SUMO (Simulation of Urban Mobility).
The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles.
Furthermore, the impact of limited communication range and random packet loss is investigated.
Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high.
We propose directions for additional research to improve the performance of the algorithm.

title: High-Speed Autonomous Drifting With Deep Reinforcement Learning
abstract: Drifting is a complicated task for autonomous vehicle control.
Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely.
We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic.
The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed.
After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map.
The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.

title: Predictive cruise control of connected and autonomous vehicles via reinforcement learning
abstract: Predictive cruise control concerns designing controllers for autonomous vehicles using the broadcasted information from the traffic lights such that the idle time around the intersection can be reduced.

This study proposes a novel adaptive optimal control approach based on reinforcement learning to solve the predictive cruise control problem of a platoon of connected and autonomous vehicles.
First, the reference velocity is determined for each autonomous vehicle in the platoon.
Second, a data-driven adaptive optimal control algorithm is developed to estimate the gains of the desired distributed optimal controllers without the exact knowledge of system dynamics.
The obtained controller is able to regulate the headway, velocity, and acceleration of each vehicle in a suboptimal sense.
The goal of trip time reduction is achieved without compromising vehicle safety and passenger comfort.
Numerical simulations are presented to validate the efficacy of the proposed methodology.

title: Learning from Suboptimal Demonstration via Trajectory-Ranked Adversarial Imitation
abstract: Robots trained by Imitation Learning(IL) are used in many tasks(e.g., autonomous vehicle manipulation).
Generative Adversarial Imitation Learning (GAIL) assumes that the demonstration set used for training is of high quality.
However, such demonstrations are difficult and expensive to obtain.
GAIL-related methods fail to learn effective strategies if non-high quality demonstrations are used because the performance of agents trained by this method is limited by the demonstrator's operations.

Our idea is to enable the agent to learn strategy with better performance than the demonstrator from a suboptimal demonstration set, which contains non-high quality demonstrations that are easier to obtain.

Inspired by this, we propose the Trajectory-Ranked Adversarial Imitation Learning (TRAIL) method.
First, for demonstration set processing, we introduce a ranking process and define the concept of Performance Relative Advantage of suboptimal demonstrations to specify the ranking order.
Second, for model training, we reconstruct the objective function of GAIL and use an experience replay buffer, enabling the agent to learn implicit features and ranking information from the ranked suboptimal demonstration set and possess the ability to outperform the demonstrator.

Experiments show that in Mujoco's tasks, our method can learn from a suboptimal demonstration set and can achieve better performance than baseline methods.

title: Improved Robustness and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning
abstract: To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment.
Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances.
This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario.
We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance.
The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.


title: Research on decision-making of autonomous vehicle following based on reinforcement learning method
abstract: Purpose Over the past decades, there has been significant research effort dedicated to the development of autonomous vehicles.
The decision-making system, which is responsible for driving safety, is one of the most important technologies for autonomous vehicles.
The purpose of this study is the use of an intensive learning method combined with car-following data by a driving simulator to obtain an explanatory learning following algorithm and establish an anthropomorphic car-following model.

Design/methodology/approach This paper proposed car-following method based on reinforcement learning for autonomous vehicles decision-making.
An approximator is used to approximate the value function by determining state space, action space and state transition relationship.
A gradient descent method is used to solve the parameter.
Findings The effect of car-following on certain driving styles is initially achieved through the simulation of step conditions.
The effect of car-following initially proves that the reinforcement learning system is more adaptive to car following and that it has certain explanatory and stability based on the explicit calculation of R.
 Originality/value The simulation results show that the car-following method based on reinforcement learning for autonomous vehicle decision-making realizes reliable car-following decision-making and has the advantages of simple sample, small amount of data, simple algorithm and good robustness.


title: Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks
abstract: In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs).
In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation.
First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength).
Second, quality of service (QoS) which is used to measure the network's performance for data forwarding (e.g., delay and packet losses).
As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data.
We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-Q-Networks (DQN).
Additionally, we compare the performance of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.
We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios.

Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance.
Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.

title: Prediction-aware and Reinforcement Learning based Altruistic Cooperative Driving [arXiv]
abstract: Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs.
In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes.
Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents behaviors and use that to forecast what might happen in the future.

Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness.

In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction.
We formulate the AV decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework.

We also propose a Hybrid Predictive Network (HPN) that anticipates future observations.
The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN).
Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy.

We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.

title: Integrating reinforcement-learning-based vehicle dispatch algorithm into agent-based modeling of autonomous taxis
abstract: While increasing number of studies are using agent-based models to study the potential environmental, economic, and social impacts of shared autonomous vehicles, the idle vehicle dispatching in these models is often simplified to heuristic rules.

Because the system performance of an autonomous taxi fleet can be significantly affected by the vehicle dispatching algorithm, refining the vehicle dispatching can help us better evaluate the potential benefits and impacts of shared autonomous vehicle systems.

The recent development of reinforcement-learning-based vehicle dispatching algorithms provides opportunities to improve autonomous vehicle system modeling with efficient and scalable vehicle dispatching.

This study integrates a reinforcement learning algorithm into to an agent-based simulation model of a ride hailing system.
Using an autonomous taxi fleet in New York City as a case study, we compared the system performance of using the proposed Deep Q-Network (DQN) method for dispatching decision with common rule-based and heuristic dispatch algorithms from relevant literatures.

The results show that (1) DQN dispatches vehicles more conservatively (with less dispatching activities and distances) but achieved similar (slightly lower) rider service level with proactive dispatch methods; and (2) DQN outperformed all other dispatch methods evaluated in this study with significantly higher dispatch efficiency, as measured by the ratio of the number of extra riders served due to dispatching to the extra fleet dispatch distance.


title: A Reinforcement Learning Approach for Adaptive Covariance Tuning in the Kalman Filter
abstract: State estimation and localization for the autonomous vehicle are essential for accurate navigation and safe maneuvers.
The commonly used method is Kalman filtering, but its performance is affected by the noise covariance.
An inappropriate set value will decrease the estimation accuracy and even makes the filter diverge.
The noise covariance estimation problem has long been considered a tough issue because there is too much uncertainty in where the noise comes from and therefore unable to model it systematically.
In recent years, Deep Reinforcement Learning (DRL) has made astonishing progress and is an excellent choice for tackling the problem that cannot be solved by conventional techniques, such as parameter estimation.

By finely abstracting the problem as an MDP, we can use the DRL methods to solve it without too many prior assumptions.
We propose an adaptive covariance tuning method applied to the Error State Extend Kalman Filter by taking advantage of DRL, called Reinforcement Learning Aided Covariance Tuning.
The preliminary experiment result indicates that our method achieves a 14.
73% estimation accuracy improvement on average compared with the vanilla fixed-covariance method and bound the estimation error within 0.
4 m.


title: Adaptive Optimal Control of Connected Vehicles
abstract: In this paper, a data-driven non-model-based approach is proposed for the adaptive optimal control of connected vehicles, comprised of n human-driven vehicles only transmitting motional data and an autonomous vehicle in the tail receiving the broadcasted data from preceding vehicles by wireless vehicle-to-vehicle (V2V) communication devices.

An optimal control problem is formulated to minimize the errors of distance and velocity and to optimize the fuel usage.
By employing adaptive dynamic programming (ADP) technique, optimal controllers are obtained by online approximation for the connected vehicles without knowing the system dynamics.
The effectiveness of the proposed approach is demonstrated via online learning control of the connected vehicles in two scenarios.

title: Hierarchical Joint Control for Urban Mixed-Autonomy Traffic Optimization
abstract: With the fast development of autonomous vehicle technologies, the vehicle fleet will be made up of a mixture of human-driven vehicles and autonomous vehicles (AVs) in the coming 20-30 years.
To efficiently utilize abundant data to deal with the mixed-autonomy traffic control problem, this paper formulates and approaches the problem using a deep reinforcement learning framework (DRL).
DRL is a promising data-driven approach for traffic signal control and AVs control in a large-scale grid.
However, traffic control based DRL is quite challenging since the complexity of control and the large search space of the policy.
To deal with these issues, we propose a hierarchical joint control framework based on prior knowledge.
Specifically, traffic signals at intersections and AVs are controlled by their local controllers, set according to well-adjusted policies; while the coordination of the traffic signals at intersections and the coordination among AVs are determined by two master controllers, respectively.

Thus, the control of the whole grid is handled by two master controllers.
In this way, the dimension of the action space is greatly decreased and the control operates much smoother.
We verify our method by implementing a series of experiments in SUMO.
The numerical experiments demonstrate the potential of the mixed-autonomy traffic control, compared with traditional traffic signal systems without AVs.
We also demonstrate that our method is easy to train and operates robustly.

title: Reinforcement Learning with Human Feedback for Realistic Traffic Simulation [arXiv]
abstract: In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems.
A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity.

This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models.
This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models.
To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency.
We also introduce the first dataset for realism alignment in traffic modeling to support such research.
Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.


title: Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies
abstract: Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles.
However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety.
As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand.
This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level.
In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning.
The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test.
We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide.
Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies.
We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.


title: Reinforcement Learning-Based On-Ramp Merging Decision-Making for Autonomous Vehicles
abstract: On-ramp merging is a complex and high-accidents scenario.
The complexity of on-ramp merging scenario is mainly reflected in the aspects of varied merging environment and uncertain driving behavior of the surrounding vehicles, so it is a challenging scenario for autonomous vehicles.

In this paper, a deep Q-learning network (DQN)-based merging decision-making method is proposed for autonomous vehicles.
First, the models of vehicle dynamics and on-ramp merging scenarios are built, and the merging motivation is quantitative for designing the reinforcement learning.
Second, the actions of the autonomous vehicle are defined, and a value-based DQN network for the merging decision-making is designed by simultaneously considering the driving safety, efficiency and comfort.

Finally, in order to evaluate the performance of DQN-based merging decision-making method, a dynamic programming (DP)-based merging decision-making method is selected as the benchmarked method.
The validation results demonstrate the autonomous vehicle by the proposed DQN-based merging decision-making method possesses good performance in safety, efficiency and comfort at on-ramp merging scenarios.


title: Example-guided learning of stochastic human driving policies using deep reinforcement learning
abstract: Deep reinforcement learning has been successfully applied to the generation of goal-directed behavior in artificial agents.
However, existing algorithms are often not designed to reproduce human-like behavior, which may be desired in many environments, such as human-robot collaborations, social robotics and autonomous vehicles.

Here we introduce a model-free and easy-to-implement deep reinforcement learning approach to mimic the stochastic behavior of a human expert by learning distributions of task variables from examples.
As tractable use-cases, we study static and dynamic obstacle avoidance tasks for an autonomous vehicle on a highway road in simulation (Unity).
Our control algorithm receives a feedback signal from two sources: a deterministic (handcrafted) part encoding basic task goals and a stochastic (data-driven) part that incorporates human expert knowledge.

Gaussian processes are used to model human state distributions and to assess the similarity between machine and human behavior.
Using this generic approach, we demonstrate that the learning agent acquires human-like driving skills and can generalize to new roads and obstacle distributions unseen during training.

title: Analysis and Self-driving Algorithm Decision Mode Design
abstract: The countries around the world are closely monitoring the legislative progress of road traffic safety and autonomous vehicles.
This paper aims to focus on the innovation of autonomous vehicle algorithm and technical progress, analyzing the ethical dilemmas and finding out the most proper data counting mode.
The ethical choice of autonomous driving systems will be hugely controversial around the world for a long period.
The behavior of intelligent robots in moral dilemmas can be simulated using a fairly simple value-based calculating mode, which is attributed by participants to the makers of intelligent operating systems.

Using the fully deterministic model, all possible paths are searched and the best path is determined using the digital computation function in this paper.
The algorithm design mode of this paper is that the matrix algorithm should be adopted as the core algorithm mode, regression algorithm, reinforcement algorithm as the auxiliary algorithm modes.

title: A Game Theoretical Model of Traffic with Multiple Interacting Drivers for Use in Autonomous Vehicle Development
abstract: This paper describes a game theoretical model of traffic where multiple drivers interact with each other.
The model is developed using hierarchical reasoning, a game theoretical model of human behavior, and reinforcement learning.
It is assumed that the drivers can observe only a partial state of the traffic they are in and therefore although the environment satisfies the Markov property, it appears as non-Markovian to the drivers.

Hence, each driver implicitly has to find a policy, i.e.
a mapping from observations to actions, for a Partially Observable Markov Decision Process.
In this paper, a computationally tractable solution to this problem is provided by employing hierarchical reasoning together with a suitable reinforcement learning algorithm.
Simulation results are reported, which demonstrate that the resulting driver models provide reasonable behavior for the given traffic scenarios.

title: Heuristics-oriented overtaking decision making for autonomous vehicles using reinforcement learning
abstract: This study presents a three-lane highway overtaking strategy for an automated vehicle, which is based on a heuristic planning reinforcement learning algorithm.
The proposed decision-making controller focuses on keeping the autonomous vehicle operating safely and efficiently.
First, the modelling of the overtaking driving scenario is introduced and the reference approaches named intelligent driver model and minimise overall braking induced by lane changes are formulated.
Second, the Dyna-H algorithm, which combines the modified Q-learning algorithm with a heuristic planning policy, is utilised for highway overtaking decision-making.
Three different heuristic strategies are formulated to improve learning efficiency and compare performance.
This algorithm is applied to determine the lane change and speed selection for an ego vehicle in the environment with uncertainties.
Finally, the performance of Dyna-H is estimated in the autonomous overtaking scenario by comparing it with the reference and traditional learning methods.
Furthermore, the Dyna-H-enabled decision-making strategies are validated and analysed in an open-sourcing driving dataset.
Results prove that the proposed decision-making strategy could produce superior performance in convergence rate and control.

title: Reinforcement Learning for Connected Autonomous Vehicle Localization via UAVs
abstract: In precision farming, a very promising scenario is represented by a connected and autonomous vehicle (CAV) moving in a cultivated field and collecting high-resolution videos and hyperspectral images, requiring both localization and broadband communication.

An effective approach to provide both localization and wideband communication exploits unmanned aerial vehicles (UAVs) that may act as relays to ensure seamless connectivity with a base station (BS).
In this paper, we propose a reinforcement learning (RL)-based algorithm to find the best spatial configuration of a swarm of UAVs to localize a CAV in an unknown environment and assist the communication with a BS.

The UAVs cooperate to estimate the position of the CAV exploiting only the received signal strength (RSS).
A reward function, based on the distance between the UAVs and the CAV, and the estimated geometric diluition of precision (GDOP), is designed.
Numerical results show how the proposed multi-agent Q-learning allows the UAVs to reach low root mean square error (RMSE) in the target localization, even without previous knowledge about the environment.


title: A Decision-Making Model for Autonomous Vehicles at Intersections Based on Hierarchical Reinforcement Learning
abstract: By aiming at addressing the left-turning problem of an autonomous vehicle considering the oncoming vehicles at an urban unsignallized intersection, a hierarchical reinforcement learning is proposed and a two-layer model is established to study behaviors of left-turning driving.

Compared with the conventional decision-making models with a fixed path, the proposed multi-paths decision-making algorithm with horizontal and vertical strategies can improve the efficiency of autonomous vehicles crossing intersections while ensuring safety.


title: Towards Mitigating Probable Road Mishaps through DQN Based Deep Reinforcement Learning
abstract: Road accident is a widespread problem in many urban areas nowadays.
Reckless Driving (Over-Speed) is one of the attributes leading to the enormous destruction of pedestrian life.
Autonomous vehicle speed control is an important research area in the Intelligent Transportation System (ITS) to mitigate road accidents in densely populated areas.
Many existed works utilized Reinforcement Learning (RL) to control the vehicle speed intelligently.
However, controlling the over-speed of the vehicle using RL under the speed limit of the different zones (i.
e.
, Town, Rural Main Road, Highways, and so forth) is also another demanding task because of the speed limit variation.

Therefore, we propose a Deep-Q-Network (DQN) based Deep Reinforcement Learning (Deep RL) method to overcome the over-speed problem in various speed limit areas.
In this paper, the proposed system will reduce the vehicle speed intelligently if the driver passes over the speed limit in a particular zone.
The simulation results show that our proposed solution can successfully reduce the vehicle speed under various zone-based speed limits.

title: Research on autonomous vehicle U-turn problem based on hierarchical reinforcement learning
abstract: The U-turn task is one of the contents of autonomous driving research,and most of the solutions under the standard roads in cities cannot be implemented on non-standard roads.
Aiming at solving this problem,this paper established a vehicle U-turn dynamical model and designed a multi-scale convolutional neural network to extract feature maps as the input of the agent.
In addition,for the sparse reward problem in the U-turn task,this paper proposed a hierarchical proximal policy optimization algorithm that combined hierarchical reinforcement learning and proximal policy optimization algorithm.
In experiments with simple and complex scena-rios,this algorithm learns policies faster and has a higher success rate of U-turn compared to other algorithms.


title: REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES, 53-57. SI
abstract: Reinforcement learning (RL) is one of the most emerging domains of artificial intelligence.
It is widely used in almost all sort of applications, including medical field, stock market, forecasting, and engineering field.
One of the most effective uses of RL is in the control engineering domain owing to its learning by trial, for example, in building autonomous system like autonomous vehicle and robotics.
In this paper, we have focused on the applications of RL in various control engineering problems.
Stability of controller (or agent) using RL paradigm is a very crucial task due to exploration-exploitation policy used by any RL.
Also, the unavailability of exact model of system or environment may lead to unsafe behaviour of the agent.
Therefore, this paper focuses mainly on the stability aspect in RL-based controller.
Many concepts are used to study and analyse the stability of a system viz Lyapunov method and Barrier function.
This paper surveys the detailed application of these well-established stability certifier methods in various model-free and model-based RL framework.

title: Reinforcement Learning-Based Guidance of Autonomous Vehicles
abstract: Reinforcement learning (RL) has attracted significant research efforts to guide an autonomous vehicle (AV) for a collision-free path due to its advantages in investigating interactions among multiple vehicles and dynamic environments.

This study deploys a Deep Q-Network (DQN) based RL algorithm with reward shaping to control an ego AV in an environment with multiple vehicles.
Specifically, the state space of the RL algorithm depends on the desired destination, the ego vehicle's location and orientation, and the location of other vehicles in the system.
The training time of the proposed RL algorithm is much shorter than most current image-based algorithms.
The RL algorithm also provides an extendable framework to include a varying number of vehicles in the environment and can be easily adapted to different maps without changing the setup of the RL algorithm.

Three scenarios were simulated in the Cars Learn to Act (CARLA) simulator to examine the effects of the proposed RL algorithm on guiding the ego AV interacting with multiple vehicles on straight and curvy roads.

Our results showed that the ego AV could learn to reach its destination within 5000 episodes for all scenarios tested.

title: Learning from experience for rapid generation of local car maneuvers
abstract: Being able to rapidly respond to the changing scenes and traffic situations by generating feasible local paths is of pivotal importance for car autonomy.
We propose to train a deep neural network (DNN) to plan feasible and nearly-optimal paths for kinematically constrained vehicles in a small constant time.
Our DNN model is trained using a novel weakly supervised approach and a gradient-based policy search.
On real and simulated scenes and a large set of local planning problems, we demonstrate that our approach outperforms the existing planners with respect to the number of successfully completed tasks.
While the path generation time is about 40 ms, the generated paths are smooth and comparable to those obtained from conventional path planners.

title: Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning
abstract: With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation.
However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users.
For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style.
Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions.
We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy.

The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.

title: Comfort Improvement for Autonomous Vehicles Using Reinforcement Learning with In-Situ Human Feedback
abstract: In this paper, a reinforcement learning-based method is proposed to adapt autonomous vehicle passengers' expectation of comfort through in-situ human-vehicle interaction.
Ride comfort has a significant influence on the user's experience and thus acceptance of autonomous vehicles.
There is plenty of research about the motion planning and control of autonomous vehicles.
However, limited studies have explicitly considered the comfort of passengers in autonomous vehicles.
This paper studies the comfort of humans in autonomous vehicles longitudinal autonomous driving.
The paper models and then improves passengers' feelings about autonomous driving behaviors.
This proposed approach builds a control and adaptation strategy based on reinforcement learning using human's in-situ feedback on autonomous driving.
It also proposes an adaptation of humans to autonomous vehicles to account for improper human driving expectations.
The proposed approaches are implemented and tested with human-in-the-loop experiments and the results demonstrate that the proposed approaches can successfully adapt the vehicle behaviors, improve the ride comfort of humans in autonomous vehicles, and also correct improper human driving expectations.


title: DeepCollision: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions
abstract: With the aim to test autonomous driving systems, we propose a novel reinforcement learning (RL)-based approach named DeepCollision to learn operating environment configurations of autonomous vehicles, including formalizing environment configuration learning as an MDP and adopting DQN algorithm as the RL solution;DeepCollisionlearns environment configurations to maximize collisions of an Autonomous Vehicle Under Test (AVUT).

This dataset contains: algorithms- The algorithm of DeepCollision, which includes the network architecture and the DQN hyperparameter settings; pilot-study- All the raw data and plots for the pilot study; formal-experiment- A dataset contains all the raw data for analysis and the scenarios with detailed demand values; rest-api- The REST API endpoints for environment configuration and oneexampleto show the usage of the APIs.

Copyright: Creative Commons Attribution 4.0 International Open Access

title: Game-Theoretic Cooperative Lane Changing Using Data-Driven Models
abstract: Self-driving vehicles are being increasingly deployed in the wild.
One of the most important next hurdles for autonomous driving is how such vehicles will optimally interact with one another and with their surroundings.
In this paper, we consider the lane changing problem that is fundamental to road-bound multi-vehicle systems, and approach it through a combination of deep reinforcement learning (DRL) and game theory.

We introduce a proactive-passive lane changing framework and formulate the lane changing problem as a Markov game between the proactive and passive vehicles.
Based on different approaches to carry out DRL to solve the Markov game, we propose an asynchronous lane changing scheme as in a single-agent RL setting and a synchronous cooperative lane changing scheme that takes into consideration the adaptive behavior of the other vehicle in a vehicle's decision.

Experimental results show that the synchronous scheme can effectively create and find proper merging moment after sufficient training.
The framework and solution developed here demonstrate the potential of using reinforcement learning to solve multi-agent autonomous vehicle tasks such as the lane changing as they are formulated as Markov games.


title: Using V2X and Reinforcement Learning to Improve Autonomous Vehicles Algorithms with CARLA
abstract: 

title: Reinforcement learning with non-uniform state representations for adaptive search [arXiv]
abstract: Efficient spatial exploration is a key aspect of search and rescue.
In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher.
This should allow an autonomous vehicle find one or more lost targets as rapidly as possible.
We do this by performing non-uniform sampling of the search region.
The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning.
We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target.
Key features of our search algorithm are the ability to employ a very general non-deterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions.

One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics.
[doi:10.1109/SSRR.2018.8468649].

title: Deep Q-Network (DQN) Approach for Automatic Vehicles Applied in the Intelligent Transportation System (ITS)
abstract: This paper presents the design of an intelligent controller applying reinforcement learning using a deep Q-network (DQN) algorithm for autonomous vehicles.
The deep Q-network (DQN) algorithm is an online, model-free reinforcement learning approach.
A DQN agent is a value-based reinforcement learning agent that teaches a critic to predict future rewards or returns.
Deep Q-network is to replace the action-state Q table with a neural network.
This solution applies to building a self-propelled agent capable of correcting static and moving obstacles according to the physical environment.
As a result, the autonomous vehicle can move and avoid collisions with obstacles.
The correctness of the theory is demonstrated through MATLAB simulation.

title: Autonomous Vehicle Fleet Operations and Planning With User Activity Scheduling Constraints
abstract: 

title: Autonomous Highway Driving using Deep Reinforcement Learning [arXiv]
abstract: The operational space of an autonomous vehicle (AV) can be diverse and vary significantly.
This may lead to a scenario that was not postulated in the design phase.
Due to this, formulating a rule based decision maker for selecting maneuvers may not be ideal.
Similarly, it may not be effective to design an a-priori cost function and then solve the optimal control problem in real-time.
In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.
e.
, an autonomous vehicle, learns to make decisions by directly interacting with simulated traffic.

The decision maker for AV is implemented as a deep neural network providing an action choice for a given system state.
In a critical application such as driving, an RL agent without explicit notion of safety may not converge or it may need extremely large number of samples before finding a reliable policy.
To best address the issue, this paper incorporates reinforcement learning with an additional short horizon safety check (SC).
In a critical scenario, the safety check will also provide an alternate safe action to the agent provided if it exists.
This leads to two novel contributions.
First, it generalizes the states that could lead to undesirable "near-misses" or "collisions ".
Second, inclusion of safety check can provide a safe and stable training environment.
This significantly enhances learning efficiency without inhibiting meaningful exploration to ensure safe and optimal learned behavior.
We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density in a highway setting.

title: Using Physiological Metrics to Improve Reinforcement Learning for Autonomous Vehicles
abstract: Thanks to recent technological advances Autonomous Vehicles (AVs) are becoming available at some locations.
Safety impacts of these devices have, however, been difficult to assess.
In this paper we utilize physiological metrics to improve the performance of a reinforcement learning agent attempting to drive an autonomous vehicle in simulation.
We measure the performance of our reinforcement learner in several aspects, including the amount of stress imposed on potential passengers, the number of training episodes required, and a score measuring the vehicle's speed as well as the distance successfully traveled by the vehicle, without traveling off-track or hitting a different vehicle.

To that end, we compose a human model, which is based on a dataset of physiological metrics of passengers in an autonomous vehicle.
We embed this model in a reinforcement learning agent by providing negative reward to the agent for actions that cause the human model an increase in heart rate.
We show that such a "passenger-aware" reinforcement learner agent does not only reduce the stress imposed on hypothetical passengers, but, quite surprisingly, also drives safer and its learning process is more effective than an agent that does not obtain rewards from a human model.


title: Integrating Vehicle Positioning and Path Tracking Practices for an Autonomous Vehicle Prototype in Campus Environment
abstract: This paper presents the implementation of an autonomous electric vehicle (EV) project in the National Taiwan University of Science and Technology (NTUST) campus in Taiwan.
The aim of this work was to integrate two important practices of realizing an autonomous vehicle in a campus environment, including vehicle positioning and path tracking.
Such a project is helpful to the students to learn and practice key technologies of autonomous vehicles conveniently.
Therefore, a laboratory-made EV was equipped with real-time kinematic GPS (RTK-GPS) to provide centimeter position accuracy.
Furthermore, the model predictive control (MPC) was proposed to perform the path tracking capability.
Nevertheless, the RTK-GPS exhibited some robust positioning concerns in practical application, such as a low update rate, signal obstruction, signal drift, and network instability.
To solve this problem, a multisensory fusion approach using an unscented Kalman filter (UKF) was utilized to improve the vehicle positioning performance by further considering an inertial measurement unit (IMU) and wheel odometry.

On the other hand, the model predictive control (MPC) is usually used to control autonomous EVs.
However, the determination of MPC parameters is a challenging task.
Hence, reinforcement learning (RL) was utilized to generalize the pre-trained datum value for the determination of MPC parameters in practice.
To evaluate the performance of the RL-based MPC, software simulations using MATLAB and a laboratory-made, full-scale electric vehicle were arranged for experiments and validation.
In a 199.27 m campus loop path, the estimated travel distance error was 0.82% in terms of UKF.
The MPC parameters generated by RL also achieved a better tracking performance with 0.
227 m RMSE in path tracking experiments, and they also achieved a better tracking performance when compared to that of human-tuned MPC parameters.


title: Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoon
abstract: This paper proposes a model-based deep reinforcement learning (DRL) algorithm for cooperative adaptive cruise control (CACC) of connected vehicles.
Differing from most existing CACC works, we consider a platoon consisting of both human-driven and autonomous vehicles.
The humandriven vehicles are heterogeneous and connected via vehicleto-vehicle (V2V) communication and the autonomous vehicles are controlled by a cloud-based centralized DRL controller via vehicle-to-cloud (V2C) communication.

To overcome the safety and robustness issues of RL, the algorithm informs lowerlevel controllers of desired headway signals instead of directly controlling vehicle accelerations.
The lower-level behavior is modeled according to the optimal velocity model (OVM), which determines vehicle acceleration according to a headway input.
Numerical experiments show that the model-based DRL algorithm outperforms its model-free version in both safety and stability of CACC.
Furthermore, we study the impact of different penetration ratios of autonomous vehicles on the safety, stability, and optimality of the CACC policy.

title: CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning
abstract: Autonomous navigation in off-road environments has been extensively studied in the robotics field.
However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an under-explored area.
In this paper, we propose CoverNav, a novel Deep Reinforcement Learning (DRL) based algorithm, for identifying covert and navigable trajectories with minimal cost in off-road terrains and jungle environments in the presence of observers.

CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination.
Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low-cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information.

If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, trees, etc.)
and use them as shelters to hide behind.
We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL-based navigation algorithm.

Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects.
We observe competitive performance comparable to state-of-the-art (SOTA) methods without compromising accuracy.

title: Intelligent Cooperative Collision Avoidance at Overtaking and Lane Changing Maneuver in 6G-V2X Communications
abstract: The rapid growth in Autonomous Vehicle (AV) technology endeavors increased attention towards road safety in recent days.
Particularly, a higher number of road accidents occurs when the AV tries to overtake or change the lane.
To cut down the number of accidents and improve traffic reliability, the AV should be capable of making intelligent decisions and communicating those to other AVs.
Therefore, in this paper, a Cooperative Collision avoidance scheme for AVs at Overtaking and Lane Changing maneuver (CCAV-OLC) is proposed.
The Inverse Reinforcement Learning (IRL) in the CCAV-OLC scheme, processes on the given number of expert demonstrations for automatically acquiring the reward function, and thereby imitating actual human driving strategy and decisions.

However, the adaptability of IRL to a high-dimensional AV environment restricts the performance of the CCAV-OLC scheme.
To overcome this, the IRL in CCAV-OLC leverages the Gaussian Process (GP) regression model (IRL-GP), which enables data-efficient Bayesian prediction even when the number of demonstrations is very low.

After taking intelligent decisions in overtaking and lane changing maneuvers, the AVs cooperatively communicate and exchange the decisions with each other by 6th Generation Vehicle-to-Everything (6G-V2X) communications, which further improves the accuracy and lessens the time taken for making optimal decisions.

The experimental results show that the AVs clone the expert's optimal driving strategy and avoid the collisions to a greater extent.

title: Decision-Making of an Autonomous Vehicle when Approached by an Emergency Vehicle using Deep Reinforcement Learning
abstract: Autonomous Vehicles (AVs) are the future of road transportation which can increase safety, efficiency, and productivity.
Decision-making of AVs in a highway environment with different goals like overtaking, staying in a lane, and merging have been the focus of many studies.
In this study, we want to address a new edge case in autonomous driving when the AV (ego) needs to make the best lateral and longitudinal decisions when approached by an emergency vehicle (emg).
To achieve the desired behavior and learn the sequence decision process, we trained ego with the help of Deep Reinforcement Learning (DRL) algorithms and compared the results with rule-based algorithms.

We proposed two neural networks as function approximators that help the ego to learn the optimum actions.
The driving environment for this problem was developed by using Simulation Urban Mobility (SUMO) as an open-source traffic simulator.
We will show our proposed solution based on the DRL outperforming the rule-based solution and demonstrate that it has a decent performance both in normal driving situations and when an emergency vehicle is approaching.


title: Safe, Efficient, and Comfortable Autonomous Driving Based on Cooperative Vehicle Infrastructure System
abstract: Traffic crashes, heavy congestion, and discomfort often occur on rough pavements due to human drivers' imperfect decision-making for vehicle control.
Autonomous vehicles (AVs) will flood onto urban roads to replace human drivers and improve driving performance in the near future.
With the development of the cooperative vehicle infrastructure system (CVIS), multi-source road and traffic information can be collected by onboard or roadside sensors and integrated into a cloud.
The information is updated and used for decision-making in real-time.
This study proposes an intelligent speed control approach for AVs in CVISs using deep reinforcement learning (DRL) to improve safety, efficiency, and ride comfort.
First, the irregular and fluctuating road profiles of rough pavements are represented by maximum comfortable speeds on segments via vertical comfort evaluation.
A DRL-based speed control model is then designed to learn safe, efficient, and comfortable car-following behavior based on road and traffic information.
Specifically, the model is trained and tested in a stochastic environment using data sampled from 1341 car-following events collected in California and 110 rough pavements detected in Shanghai.
The experimental results show that the DRL-based speed control model can improve computational efficiency, driving efficiency, longitudinal comfort, and vertical comfort in cars by 93.
47%, 26.
99%, 58.
33%, and 6.
05%, respectively, compared to a model predictive control-based adaptive cruise control.

The results indicate that the proposed intelligent speed control approach for AVs is effective on rough pavements and has excellent potential for practical application.

title: Socially Aware Crowd Navigation with Multimodal Pedestrian Trajectory Prediction for Autonomous Vehicles [arXiv]
abstract: Seamlessly operating an autonomous vehicle in a crowded pedestrian environment is a very challenging task.
This is because human movement and interactions are very hard to predict in such environments.
Recent work has demonstrated that reinforcement learning-based methods have the ability to learn to drive in crowds.
However, these methods can have very poor performance due to inaccurate predictions of the pedestrians' future state as human motion prediction has a large variance.
To overcome this problem, we propose a new method, SARL-SGAN-KCE, that combines a deep socially aware attentive value network with a human multimodal trajectory prediction model to help identify the optimal driving policy.

We also introduce a novel technique to extend the discrete action space with minimal additional computational requirements.
The kinematic constraints of the vehicle are also considered to ensure smooth and safe trajectories.
We evaluate our method against the state of art methods for crowd navigation and provide an ablation study to show that our method is safer and closer to human behaviour.

title: Learning How to Drive in Blind Intersections from Human Data
abstract: In this paper we present a method to learn how to drive in different types of blind intersections using expert driving data.
We cluster different intersections based on the velocity of how drivers approach them, and train a linear SVM classifier for each class of intersection.
Through clustering we found that there were three different classes of intersections in typical residential areas in Japan.
We used inverse reinforcement learning (IRL) to build a driving model for each type of intersection.
The models were trained from 308 trajectories traversed by 5 different drivers.
The models and policies were implemented and evaluated in a ROS simulator where the agent is provided a global path, and upon it reaching an intersection, it selects the appropriate trained policy.
By doing this, the simulated autonomous vehicle can perform proactive safe driving behaviors when approaching blind intersections.

title: Emergence of norms in interactions with complex rewards
abstract: Autonomous agents are becoming increasingly ubiquitous and are playing an increasing role in wide range of safety-critical systems, such as driverless cars, exploration robots and unmanned aerial vehicles.

These agents operate in highly dynamic and heterogeneous environments, resulting in complex behaviour and interactions.
Therefore, the need arises to model and understand more complex and nuanced agent interactions than have previously been studied.
In this paper, we propose a novel agent-based modelling approach to investigating norm emergence, in which such interactions can be investigated.
To this end, while there may be an ideal set of optimally compatible actions there are also combinations that have positive rewards and are also compatible.
Our approach provides a step towards identifying the conditions under which globally compatible norms are likely to emerge in the context of complex rewards.
Our model is illustrated using the motivating example of self-driving cars, and we present the scenario of an autonomous vehicle performing a left-turn at a T-intersection.

title: MNNP: Design and Development of Traffic Sign Identification and Recognition System to Support Smart Vehicles using Modified Neural Network Principles
abstract: In recent times, the acceleration of automotive and computer vision technologies has sparked significant interest in autonomous vehicles.
These vehicles' capacity to operate securely and effectively hinges on their adeptness at accurately identifying traffic signs.
Consequently, traffic sign recognition has emerged as a pivotal element within autonomous driving systems.
Scientists have been hard at work looking at various approaches to accomplishing this recognition, mostly drawing on machine learning and deep learning techniques.
To address this issue, we offer a Modified Neural Network approach that combines the Haar cascade algorithm with a MobileNet model classifier.
Our developed model is rigorously trained on the GTSRB dataset and then put to the test on a wide variety of traffic sign types.
The culmination of our efforts yields a remarkable testing accuracy rate of 97.25%.
This finding highlights the possible effectiveness of our technique in improving autonomously vehicle traffic sign identification, which in turn helps to improve the security and effectiveness of autonomous driving systems.


title: Leveraging autonomous vehicles in mixed-autonomy traffic networks with reinforcement learning-controlled intersections
abstract: Development of new approaches to adaptive traffic signal control has received significant attention; an example is the reinforcement learning (RL), where training and implementation of an RL agent can allow adaptive signal control in real time, considering the agent's past experiences.

Furthermore, autonomous vehicle (AV) technology has shown promise to enhancing the traffic mobility at highways and intersections.
In this paper, delayed action deep Q-learning is developed for a vehicle network with signalized intersections to control the signal phase.
A model predictive control (MPC) scheme is proposed to allow AVs to adapt their speed.
Several case studies that consider mixed autonomy are examined aiming at reducing network traffic and fuel consumption in the traffic network with multiple intersections.
Simulation studies reveal that even with a few AVs in the network, the waiting time, fuel consumption, and the number of stop-and-go movements are significantly reduced, while the travel time is increased.


title: Dynamical Driving Interactions between Human and Mentalizing-designed Autonomous Vehicle
abstract: Autonomous vehicle (AV) is progressing rapidly, but there are still many shortcomings when interacting with humans.
To address this problem, it is necessary to study the human behaviors in human-AV interactions, and build a predictive model of human decision-making in the interaction.
In turn, modelling human behavior in human-AV interaction can help us better understand human perception of AVs and human driving strategies.
In this work, we first train multi-level AV agents using reinforcement learning (RL) models to imitate three mentalizing levels (i.
e.
, level-0, level-1, and level-2), and then design a human-AV driving task that subjects interact with each level of AV agents in a two-lane merging scenario.

Both human and AV driving behaviors are recorded.
We found that conservative subjects obtain more rewards because of the randomness of the RL agents.
Our results indicate that (i) human driving strategies are flexible and changeable, which allows to quickly adjust the strategy to maximize the reward when gaming against AV; (ii) human driving strategies are related to mentalizing ability, and subjects with higher mentalizing scores drive more conservatively.

Our study shed lights on the relationship between human driving policy and mentalizing in human-AV interactions, and it can inspire the next-generation AV.

title: Motion Planning using Reinforcement Learning for Electric Vehicle Battery Optimization(EVBO)
abstract: The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning.
Motion planning is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour.
In this paper, reinforcement learning techniques are explored for the motion planning of an electric vehicle(EV) while optimizing battery consumption.
The EV travel time has also been evaluated under different reinforcement learning schemes.
A traffic simulation network is developed for a high-trunk zone of Jaipur city using Simulation for Urban Mobility(SUMO) software.
Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network.
The results show that value iteration and q-learning have shown improved battery consumption.
However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.

title: Reinforcement Learning for Autonomous Vehicle using MPC in Highway Situation
abstract: Path planning for Autonomous Vehicle(AV) is a challenging problem, as the vehicle is required to obey the traffic rules while avoiding the collision with the other vehicles.
Model Predictive Control(MPC) is one of the popular approach for proposing a feasible and stable path by reflecting vehicle dynamics in solving objective function and constraining the expected future control input.

However, one of the drawbacks with this approach is that the demanded computational power increases proportionally to the number of considered future inputs.
This paper presents a path planning algorithm using Reinforcement Learning(RL).
RL is similar to MPC in finding the optimal solution that maximizes the reward function which can be seen as intrinsic objective function.
In that respect, adequate employment of MPC path in training resulted in improved efficiency and performance.
Through the simulations, proposed method showed 98% of similarity with path of MPC and reduced computation time by 91.13% on average, thus it is qualified for real-time path planning.

title: Control of a Mixed Autonomy Signalised Urban Intersection: An Action-Delayed Reinforcement Learning Approach
abstract: We consider a mixed autonomy scenario where the traffic intersection controller decides whether the traffic light will be green or red at each lane for multiple traffic-light blocks.
The objective of the traffic intersection controller is to minimize the queue length at each lane and maximize the outflow of vehicles over each block.
We consider that the traffic intersection controller informs the autonomous vehicle (AV) whether the traffic light will be green or red for the future traffic-light block.
Thus, the AV can adapt its dynamics by solving an optimal control problem.
We model the decision process of the traffic intersection controller as a deterministic delayed Markov decision process owing to the delayed action by the traffic controller.
We propose Reinforcement Learning based model-free algorithm to obtain the optimal policy.
We show - by extensive simulations - that our algorithm converges and drastically reduces the energy costs of AVs as the traffic controller communicates with the AVs.

title: On Deep Reinforcement Learning for Target Capture Autonomous Guidance
abstract: This paper explores the prospects of motion planning of autonomous vehicles using deep reinforcement learning (DRL).
We are particularly interested in a goal-directed setting where the need is to design an optimal guidance strategy for a pursuing autonomous vehicle (the pursuer, which is also the DRL agent) to capture an adversary (the target).

To this end, we first formulate the target capture guidance problem as a Markov Decision Process (MDP) wherein the kinematics of relative motion between the vehicles constitute the MDP, and the pursuer's lateral acceleration (chosen as its steering control to account for turn constraints) is the action of DRL agent.

We show that a multifaceted reward function motivated by the collision conditions is sufficient and effective in designing the reinforcement learning action that enables the pursuer to capture the target regardless of the latter's motion.

We then empirically evaluate the performance of the trained agent in various target capture scenarios.

title: An Improved Retransmission Timeout Forecasting Algorithm for Vehicular Networks
abstract: The vehicular relay network remains a promising path of future intelligent transportation systems (ITS) that connects the autonomous vehicle with the internet for better traffic control and information sharing.

The transmission control protocol (TCP) remains the backbone of data traffic on the internet as it harbors an enormous portion of global internet traffic between end devices.
In VANET, TCP experiences drastic performance degradation during early or spurious RTO timeouts.
This paper proposes a recursive learning retransmission timeout (RL-RTO), which efficiently reduces spurious timeouts and enhances fast recovery after timeouts.
The RL-RTO forecast timer value is based on the three control parameters.
The performance of RL-RTO is validated against recent RTO approaches under multi-hop vehicular environments.
The proposed RL-RTO considerably regulates estimation errors and improves variance.

title: Real-time safety analysis using autonomous vehicle data: a Bayesian hierarchical extreme value model
abstract: This study proposes an approach for real-time road network safety analysis using autonomous vehicles (AVs) generated data.
The approach utilises a Bayesian hierarchical spatial random parameter extreme value model (BHSRP).
The model simultaneously addresses the scarcity and non-stationarity of conflict extremes and unobserved spatial heterogeneity.
Two real-time safety metrics are estimated: the risk of crash (RC) and return level (RL).
The RC and RL were applied to three months AVs data for evaluating the real-time safety level of an urban corridor in Palo Alto, California.
The indicator time to collision (TTC) was used to characterise traffic conflicts.
The conflict extreme was defined as the maxima of negated TTC in a 20-min interval (block).
The results show that RC can differentiate the block-level risk level, while RL can reflect safety levels among blocks.
For the RC, the hot (crash risk prone) segments and intersections are associated with more severe conflict frequency.

title: Reinforcement learning for the traveling salesman problem with refueling
abstract: The traveling salesman problem (TSP) is one of the best-known combinatorial optimization problems.
Many methods derived from TSP have been applied to study autonomous vehicle route planning with fuel constraints.
Nevertheless, less attention has been paid to reinforcement learning (RL) as a potential method to solve refueling problems.
This paper employs RL to solve the traveling salesman problem With refueling (TSPWR).
The technique proposes a model (actions, states, reinforcements) and RL-TSPWR algorithm.
Focus is given on the analysis of RL parameters and on the refueling influence in route learning optimization of fuel cost.
Two RL algorithms: Q-learning and SARSA are compared.
In addition, RL parameter estimation is performed by Response Surface Methodology, Analysis of Variance and Tukey Test.
The proposed method achieves the best solution in 15 out of 16 case studies.

title: Neural Circuit Policies Imposing Visual Perceptual Autonomy
abstract: We presented a hybrid deep learning architecture by combining biological brain-inspired Neural Circuit Policies with convolutional neural architecture.
This allows the agent to learn key coherent features from various environments, express generalizability and interpret dynamics.
We found that a combination of 12 inter-neurons and 8 command neurons can effectively perform high-stakes decision-making tasks such as autonomous vehicles.
Our architecture not only has 305 times fewer trainable parameters than competing approaches but shows superior performance, robustness, and noise resiliency in famous autonomous vehicle simulations i.
e.

OpenAI-CarRacing and Udacity Simulator.

title: Autonomous Vehicle Fuel Economy Optimization with Deep Reinforcement Learning
abstract: The ever-increasing number of vehicles on the road puts pressure on car manufacturers to make their car fuel-efficient.
With autonomous vehicles, we can find new strategies to optimize fuels.
We propose a reinforcement learning algorithm that trains deep neural networks to generate a fuel-efficient velocity profile for autonomous vehicles given road altitude information for the planned trip.

Using a highly accurate industry-accepted fuel economy simulation program, we train our deep neural network model.
We developed a technique for adapting the heterogeneous simulation program on top of an open-source deep learning framework, and reduced dimension of the problem output with suitable parameterization to train the neural network much faster.

The learned model combined with reinforcement learning-based strategy generation effectively generated the velocity profile so that autonomous vehicles can follow to control itself in a fuel efficient way.

We evaluate our algorithm's performance using the fuel economy simulation program for various altitude profiles.
We also demonstrate that our method can teach neural networks to generate useful strategies to increase fuel economy even on unseen roads.
Our method improved fuel economy by 8% compared to a simple grid search approach.

title: RTMBA: A Real-Time Model-Based Reinforcement Learning Architecture for Robot Control
abstract: Reinforcement Learning (RL) is a paradigm for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line.
For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time.
Existing model-based RL methods learn in relatively few samples, but typically take too much time between each action for practical on-line learning.
In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes in a novel way such that the acting process is sufficiently fast for typical robot control cycles.

We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.


title: Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning [arXiv]
abstract: For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car.
In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult.
In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments.

Application of the hierarchical structure allows the various layers of the behavior planning system to be satisfied.
Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car.

Such behavior is hard to evaluate as correct or incorrect, but for some aggressive expert human drivers handle such scenarios effectively and quickly.
On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process.
The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.

title: Intersection Crossing for Autonomous Vehicles based on Deep Reinforcement Learning
abstract: Future intersection crossings for autonomous vehicles will not be controlled by traffic signals, rather a controller will be used for communication among vehicles that need to cross an intersection.
In this work, we propose an innovative management system called Deep Reinforcement Learning-based Autonomous Intersection Management (DRLAIM) system, which is the first system to use deep reinforcement learning.

We train the system to learn a good intersection control policy by interacting with traffic environment through reinforcement learning.
The brake-safe control model is used to ensure the safety of each autonomous vehicle while crossing.
Experiment results show that after training using reinforcement learning, the throughput of intersection control model increased by 83%.
In comparison with the Fast First Service (FFS) policy, the average waiting time of DRLAIM reduced by about 1.2% to 11.4%.

title: Deep Reinforcement Learning-Based Resource Management for Flexible Mobile Edge Computing Architectures, Applications, and Research Issues
abstract: 

title: Edge Computing Assisted Autonomous Driving Using Artificial Intelligence
abstract: The emergence of new vehicles generation such as connected and autonomous vehicles led to new challenges in the vehicular networking and computing managements to provide efficient services and guarantee the quality of service.

The edge computing facility allows the decentralization of processing from the cloud to the edge of the network.
In this paper, we design and propose an end-to-end, reliable and low latency communication architecture that allows the allocation of compute-intensive autonomous driving services, in particular autopilot, to shared resources on edge computing servers and improve the level of performance for autonomous vehicles.

The reference architecture is used to design an Advanced Autonomous Driving (AAD) communication protocol between autonomous vehicles, edge computing servers, and the centralized cloud.
Then, a mathematical programming approach using Integer Linear Programming (ILP) is formulated to model the autopilot chain resources Offloading at the network edge.
Further, a deep reinforcement learning (DRL) approach is proposed to deal with dense Internet of Autonomous Vehicle (IoAV) networks.
Moreover, several scenarios are considered to quantify the behavior of the optimization approaches.
We compare their efficiency in terms of Total Edge Servers Utilization, Total Edge Servers Allocation Time, and Successfully Allocated Edge Autopilots.

title: Dueling DQN-Rollout for Collision Avoidance Path Planning with Vehicle Speed Location
abstract: The rapid progress of artificial intelligence has led to significant advancements in the field of autonomous driving, yet effective collision avoidance path planning remains a challenging task.
In response, deep reinforcement learning offers an efficient and modern alternative to traditional navigation strategies.
This paper proposes a novel approach that incorporates vehicle speed location into the deep reinforcement learning process, utilizing the Dueling DQN-Rollout framework to consider both the distance of the road and obstacles ahead.

The agent interacts with the environment to learn a policy, with a reward function that accounts for deviations from the intended path and collisions with obstacles.
The training process focuses on imparting human-like driving skills to the autonomous vehicle.
By employing the rollout algorithm, the rough Q-value is optimized to reduce training costs.
Experimental results demonstrate that this approach can successfully plan a collision-free path for autonomous driving from origin to destination on a simulation platform.

title: Trajectory Tracking Control of Autonomous Vehicles Based on Reinforcement Learning and Curvature Feedforward
abstract: This paper proposes a trajectory tracking algorithm with adaptive parameter PID (Proportional-Integral-Derivative) controller based on reinforcement learning and curvature feedforward controller.
This paper uses the deep reinforcement learning PPO algorithm (Proximal Policy Optimization) to adjust the parameters of the PID controller online, which is used for the lateral control of the autonomous vehicle.

In addition, the maximum policy entropy is introduced based on PPO algorithm to enhance the exploration of policy.
And a curvature feedforward controller is added, which will limit the speed according to the curvature of the road ahead, to avoid the problem that the vehicle is difficult to control when turning due to excessive speed.

The training results show that compared with the traditional PPO algorithm, this method can get more rewards in the later stage of training.
The simulation results show that this algorithm can reduce the average track error, reduce the overshoot and oscillation, and improve the tracking accuracy and robustness.

title: PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning
abstract: The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution.
It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks.
The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e.
harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane.
This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles.
Specifically, we propose a velocity control framework, called PATROL (sPAtial-Temporal ReinfOrcement Learning).
First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g.
velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment.
Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time.
At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB.

We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments.
Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.

title: Q-Learning for autonomous vehicle navigation
abstract: In this work, we proposed and developed a reinforcement Q-learning method to do the lane-keeping and obstacle evasion driving maneuvers.
We detail how to design a simple car simulator and how to use it to do the training.
For each problem, we define different states, actions, and reward functions to obtain a Q-table.
Next, we use it as a driving maneuver controller in a different simulation environment.
With this method, our car successfully droves on a road different to where it was training.
An important conclusion is the possibility to build, more complex controllers to do passing or behavior selectors.

title: Deep reinforcement learning based optimization of automated guided vehicle time and energy consumption in a container terminal
abstract: The energy efficiency of port container terminal equipment and the reduction of CO2 emissions are among one of the biggest challenges facing every seaport in the world.
The article pre-sents the modeling of the container transportation process in a terminal from the quay crane to the stack using battery-powered Automated Guided Vehicle (AGV) to estimate the energy consump-tion parameters.

An AGV speed control algorithm based on Deep Reinforcement Learning (DRL) is proposed to optimize the energy consumption of container transportation.
The results obtained and compared with real transportation measurements showed that the proposed DRL-based approach dynamically changing the driving speed of the AGV reduces energy consumption by 4.
6%.

The obtained results of the research provide the prerequisites for further research in order to find optimal strategies for autonomous vehicle movement including context awareness and infor-mation sharing with other vehicles in the terminal.

(c) 2022 THE AUTHORS.
Published by Elsevier BV on behalf of Faculty of Engineering, Alexandria University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/ licenses/by-nc-nd/4.0/).

title: Online Trajectory Planning with Reinforcement Learning for Pedestrian Avoidance
abstract: Planning the optimal trajectory of emergency avoidance maneuvers for highly automated vehicles is a complex task with many challenges.
The algorithm needs to decrease accident risk by reducing the severity and keeping the car in a controllable state.
Optimal trajectory generation considering all aspects of vehicle and environment dynamics is numerically complex, especially if the object to be avoided is moving.
This paper presents a hierarchical method for the avoidance of moving objects in an autonomous vehicle, where a reinforcement learning agent is responsible for local planning, while longitudinal and lateral control is performed by the low-level model-predictive controller and Stanley controllers.

In the developed architecture, the agent is responsible for the optimization.
It is trained in various scenarios to provide the necessary parameters for a polynomial-based path and a velocity profile in a neural network output.
The vehicle performs only the first step of the trajectory, which is redesigned repeatedly by the planner based on the new state.
In the training phase, the vehicle executes the entire trajectory via low-level controllers to determine the reward value, which realizes a prediction for the future.
The agent receives feedback and can further improve its performance.
Finally, the proposed framework was tested in a simulation environment and was also compared to human drivers' abilities.

title: State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections
abstract: Traversing intersections is a challenging problem for autonomous vehicles, especially when the intersections do not have traffic control.
Recently deep reinforcement learning has received massive attention due to its success in dealing with autonomous driving tasks.
In this work, we address the problem of traversing unsignalized intersections using a novel curriculum for deep reinforcement learning.
The proposed curriculum leads to: 1) A faster training process for the reinforcement learning agent, and 2) Better performance compared to an agent trained without curriculum.
Our main contribution is two-fold: 1) Presenting a unique curriculum for training deep reinforcement learning agents, and 2) demonstrating the performance improvement using the proposed curriculum in the unsignalized intersection traversal task.

The framework expects processed observations of the surroundings from the perception system of the autonomous vehicle.
We test our method in the CommonRoad motion planning simulator on T-intersections and four-way intersections.

title: Trained Model Reuse of Autonomous-Driving in Pygame with Deep Reinforcement Learning
abstract: Autonomous-Driving technology has begun to bring great convenience to daily trip, transportation, and surveying harsh environment.
Considering that deep reinforcement learning has requirements for the convergence performance of the training results, and the actual training results sometimes cannot converge steadily or fail to reach the training goals, in this paper, the trained model reuse method was proposed, which can use the trained model generates Q(S-t, A(t)) and can he used as a part of Deep Reinforcement Learning model, and this model was built based on the value function that could predict the Q value corresponding to the various actions performed in the environment state.

In the Pygame platform, a simplified traffic simulation environment was set, it is observed that the Autonomous-Driving vehicle could run smoothly without collision in a fixed-length test simulation environment, and this trained model reuse method could help autonomous vehicle accelerate the learning process, obtain better simulation results during most of the training process, save simulation time and computing resources.


title: An interpretation framework for autonomous vehicles decision-making via SHAP and RF
abstract: Decision-making for autonomous vehicles is critical to achieving safe and efficient autonomous driving.
In recent years, deep reinforcement learning (DRL) techniques have emerged as the most promising way to enable intelligent decision-making.
However, DRL with 'black box' nature is not widely understood by humans, thus hindering their social acceptance.
In this paper, we combine SHapley Additive exPlanation (SHAP) and random forest (RF) techniques to bring transparency to decision-making obtained by DRL.
Specifically, we first implement decision-making of autonomous vehicles following in discrete action space based on DRL algorithm with the goal of safety and efficiency.
Then we use the SHAP technique to simplify the feature space, which shows that relative distance, longitudinal speed of the ego vehicle, and longitudinal speed of the proceeding vehicle have a critical impact on vehicle following task.

Finally, we collect the state-action pairs generated by the DRL model and perform feature filtering, and fit the decision model with an interpretable RF model.
The simulation results show that the RF model achieves the behavioral explanation of autonomous vehicle following.

title: Lane Following Method Based on Improved DDPG Algorithm
abstract: In an autonomous vehicle, the lane following algorithm is an important component, which is a basic function of autonomous driving.
However, the existing lane following system has a few shortcomings: first, the control method it adopts requires an accurate system model, and different vehicles have different parameters, which needs a lot of parameter calibration work.

The second is that it may fail on road sections where the lateral acceleration requirements of vehicles are large, such as large curves.
Third, its decision-making system is defined based on rules, which has disadvantages: it is difficult to formulate; human subjective factors cannot guarantee objectivity; coverage is difficult to guarantee.

In recent years, the deep deterministic policy gradient (DDPG) algorithm has been widely used in the field of autonomous driving due to its strong nonlinear fitting ability and generalization performance.

However, the DDPG algorithm has overestimated state action values and large cumulative errors, low training efficiency and other issues.
Therefore, this paper improves the DDPG algorithm based on the double critic networks and priority experience replay mechanism.
Then this paper proposes a lane following method based on this algorithm.
Experiment shows that the algorithm can achieve excellent following results under various road conditions.

title: CHAMP: Integrated Logic with Reinforcement Learning for Hybrid Decision Making for Autonomous Vehicle Planning
abstract: A Cognitive Hybrid Autonomous Motion Planner (CHAMP) is developed for autonomous driving applications in challenging driving scenarios.
The proposed hybrid planner unifies a hierarchical rule-based decision-making architecture with Reinforcement Learning (RL).
For challenging intersection scenarios, RL agents are trained to replace a subset of the rules in the logical planner.
The hybrid planner is systematically tested and benchmarked to demonstrate its effectiveness in handling challenging road scenario with congested and chaotic traffic conditions.

title: Proactive caching in auto driving scene via deep reinforcement learning
abstract: The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry.
The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction.
In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network.

First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively.
Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy.
Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.

title: Decision Making for Autonomous Driving Via Multimodal Transformer and Deep Reinforcement Learning*
abstract: On the basis of environmental information processed by the sensing module, the decision module in automatic driving integrates environmental and vehicle information to make the autonomous vehicle produce safe and reasonable driving behavior.

Considering the complexity and variability of the driving environment of autonomous vehicles, researchers have begun to apply deep reinforcement learning (DRL) in the study of autonomous driving control strategies in recent years.

In this paper, we apply an algorithm framework combining multimodal transformer and DRL to solve the autonomous driving decision problem in complex scenarios.
We use ResNet and transformer to extract the features of LiDAR point cloud and image.
We use Deep Deterministic Policy Gradient (DDPG) algorithm to complete the subsequent autonomous driving decision-making task.
And we use information bottleneck to improve the sampling efficiency of RL.
We use CARLA simulator to evaluate our approach.
The results show that our approach allows the agent to learn better driving strategies.

title: Quantum-based Offloading Strategy for Intelligent Vehicle Network
abstract: Role of RSUs become essential in enhancing the service reliability rate of the end vehicle to strengthen autonomous vehicle technology.
Computation-intensive services are delaysensitive, and most existing methods attempted comprehensively to meet the application deadline but have not reached the expectations due to classical computing.

In this regard, we design a novel offloading decision-making method based on quantum theory through reinforcement learning.
Grover's algorithm is employed to select a feasible device based on cost and energy usage probability ratio.
Theoretical and mathematical validations and simulation outcomes confine the impact of novel decisionmaking methods on the statistical constraints of the heterogeneous framework.

title: Runtime Safety Assurance for Learning-enabled Control of Autonomous Driving Vehicles
abstract: Providing safety guarantees for Autonomous Vehicle (AV) systems with machine-learning based controllers remains a challenging issue.
In this work, we propose Simplex-Drive, a framework that can achieve runtime safety assurance for machine-learning enabled controllers of AVs.
The proposed Simplex-Drive consists of an unverified Deep Reinforcement Learning (DRL)-based advanced controller (AC) that achieves desirable performance in complex scenarios, a Velocity-Obstacle (VO) based baseline safe controller (BC) with provably safety guarantees, and a verified mode management unit that monitors the operation status and switches the control authority between AC and BC based on safety-related conditions.

We provide a formal correctness proof of Simplex-Drive and conduct a lane-changing case study in dense traffic scenarios.
The simulation experiment results demonstrate that Simplex-Drive can always ensure the operation safety without sacrificing control performance, even if the DRL policy may lead to deviations from the safe status.


title: Controlling an Autonomous Vehicle with Deep Reinforcement Learning [arXiv]
abstract: We present a control approach for autonomous vehicles based on deep reinforcement learning.
A neural network agent is trained to map its estimated state to acceleration and steering commands given the objective of reaching a specific target state while considering detected obstacles.
Learning is performed using state-of-the-art proximal policy optimization in combination with a simulated environment.
Training from scratch takes five to nine hours.
The resulting agent is evaluated within simulation and subsequently applied to control a full-size research vehicle.
For this, the autonomous exploration of a parking lot is considered, including turning maneuvers and obstacle avoidance.
Altogether, this work is among the first examples to successfully apply deep reinforcement learning to a real vehicle.
[doi:10.1109/IVS.2019.8814124].

title: Motion Planning using Reinforcement Learning for Electric Vehicle Battery optimization(EVBO)
abstract: The increasing demand for electric vehicle and autonomous vehicle as the alternate to the combustion-driven vehicle has motivated the research in the area of motion planning.
Motion planning is a complicated problem as it requires the consideration of multiple entities, mainly human behaviour.
In this paper, reinforcement learning techniques are explored for the motion planning of an electric vehicle(EV) while optimizing battery consumption.
The EV travel time has also been evaluated under different reinforcement learning schemes.
A traffic simulation network is developed for a high-traffic zone of Jaipur city using Simulation for Urban Mobility(SUMO) software.
Model-based and model-free method like value-iteration and q-learning are applied to the developed traffic network.
The results show that value iteration and q-learning have shown improved battery consumption.
However, value iteration gives greater efficiency in terms of travel time as well as battery consumption.

title: Belief state separated reinforcement learning for autonomous vehicle decision making under uncertainty
abstract: In autonomous driving, the ego vehicle and its surrounding traffic environments always have uncertainties like parameter and structural errors, behavior randomness of road users, etc.
Furthermore, environmental sensors are noisy or even biased.
This problem can be formulated as a partially observable Markov decision process.
Existing methods lack a good representation of historical information, making it very challenging to find an optimal policy.
This paper proposes a belief state separated reinforcement learning (RL) algorithm for decision-making of autonomous driving in uncertain environments.
We extend the separation principle from linear Gaussian systems to general nonlinear stochastic environments, where the belief state, defined as the posterior distribution of the true state, is found to be a sufficient statistic of historical information.

This belief state is estimated by action-enhanced variational inference from historical information and is proved to satisfy the Markovian property, thus allowing us to obtain the optimal policy using traditional RL algorithms for Markov decision processes.

The policy gradient of a task-specific prior model is mixed with that of the interaction data to improve learning performance.
The proposed algorithm is evaluated in a multi-lane autonomous driving task, where the surrounding vehicles are subject to behavior uncertainty and observation noise.
The simulation results show that compared with existing RL algorithms, the proposed method can achieve a higher average return with better driving performance.

title: Longitudinal Dynamic versus Kinematic Models for Car-Following Control Using Deep Reinforcement Learning
abstract: The majority of current studies on autonomous vehicle control via deep reinforcement learning (DRL) utilize point-mass kinematic models, neglecting vehicle dynamics which includes acceleration delay and acceleration command dynamics.

The acceleration delay, which results from sensing and actuation delays, results in delayed execution of the control inputs.
The acceleration command dynamics dictates that the actual vehicle acceleration does not rise up to the desired command acceleration instantaneously due to dynamics.
In this work, we investigate the feasibility of applying DRL controllers trained using vehicle kinematic models to more realistic driving control with vehicle dynamics.
We consider a particular longitudinal car-following control, i.e., Adaptive Cruise Control (ACC), problem solved via DRL using a point-mass kinematic model.
When such a controller is applied to car following with vehicle dynamics, we observe significantly degraded car-following performance.
Therefore, we redesign the DRL framework to accommodate the acceleration delay and acceleration command dynamics by adding the delayed control inputs and the actual vehicle acceleration to the reinforcement learning environment state, respectively.

The training results show that the redesigned DRL controller results in near-optimal control performance of car following with vehicle dynamics considered when compared with dynamic programming solutions.


title: A DRL-based Multiagent Cooperative Control Framework for CAV Networks: a Graphic Convolution Q Network [arXiv]
abstract: Connected Autonomous Vehicle (CAV) Network can be defined as a collection of CAVs operating at different locations on a multilane corridor, which provides a platform to facilitate the dissemination of operational information as well as control instructions.

Cooperation is crucial in CAV operating systems since it can greatly enhance operation in terms of safety and mobility, and high-level cooperation between CAVs can be expected by jointly plan and control within CAV network.

However, due to the highly dynamic and combinatory nature such as dynamic number of agents (CAVs) and exponentially growing joint action space in a multiagent driving task, achieving cooperative control is NP hard and cannot be governed by any simple rule-based methods.

In addition, existing literature contains abundant information on autonomous driving's sensing technology and control logic but relatively little guidance on how to fuse the information acquired from collaborative sensing and build decision processor on top of fused information.

In this paper, a novel Deep Reinforcement Learning (DRL) based approach combining Graphic Convolution Neural Network (GCN) and Deep Q Network (DQN), namely Graphic Convolution Q network (GCQ) is proposed as the information fusion module and decision processor.

The proposed model can aggregate the information acquired from collaborative sensing and output safe and cooperative lane changing decisions for multiple CAVs so that individual intention can be satisfied even under a highly dynamic and partially observed mixed traffic.

The proposed algorithm can be deployed on centralized control infrastructures such as road-side units (RSU) or cloud platforms to improve the CAV operation.

title: Runtime Safety Assurance for Learning-enabled Control of Autonomous Driving Vehicles
abstract: Providing safety guarantees for Autonomous Vehicle (AV) systems with machine-learning based controllers remains a challenging issue.
In this work, we propose Simplex-Drive, a framework that can achieve runtime safety assurance for machine-learning enabled controllers of AVs.
The proposed Simplex-Drive consists of an unverified Deep Reinforcement Learning (DRL)-based advanced controller (AC) that achieves desirable performance in complex scenarios, a Velocity-Obstacle (VO) based baseline safe controller (BC) with provably safety guarantees, and a verified mode management unit that monitors the operation status and switches the control authority between AC and BC based on safety-related conditions.

We provide a formal correctness proof of Simplex-Drive and conduct a lane-changing case study in dense traffic scenarios.
The simulation experiment results demonstrate that Simplex-Drive can always ensure the operation safety without sacrificing control performance, even if the DRL policy may lead to deviations from the safe status.


title: Autonomous Vehicle Cut-In Algorithm for Lane-Merging Scenarios via Policy-Based Reinforcement Learning Nested Within Finite-State Machine
abstract: Lane-merging scenarios pose highly challenging problems for autonomous vehicles due to conflicts of interest between the human-driven and cutting-in autonomous vehicles.
Such conflicts become severe when traffic increases, and cut-in algorithms suffer from a steep trade-off between safety and cut-in performance.
In this study, a reinforcement learning (RL)-based cut-in policy network nested within a finite state machine (FSM)--which is a high-level decision maker, is proposed to achieve high cut-in performance without sacrificing safety.

This FSM-RL hybrid approach is proposed to obtain 1) a strategic and adjustable algorithm, 2) optimal safety and cut-in performance, and 3) robust and consistent performance.
In the high-level decision making algorithm, the FSM provides a framework for four cut-in phases (ready for safe gap selection, gap approach, negotiation, and lane-change execution) and handles the transitions between these phases by calculating the collision risks associated with target vehicles.

For the lane-change phase, a policy-based deep-RL approach with a soft actor-critic network is employed to get optimal cut-in performance.
The results of simulations show that the proposed FSM-RL cut-in algorithm consistently achieves a high cut-in success rate without sacrificing safety.
In particular, as the traffic increases, the cut-in success rate and safety are significantly improved over existing optimized rule-based cut-in algorithms and end-to-end RL algorithm.

title: Dynamic and Interpretable State Representation for Deep Reinforcement Learning in Automated Driving
abstract: Understanding the causal relationship between an autonomous vehicle's input state and its output action is important for safety mitigation and explainable automated driving.
However, reinforcement learning approaches have the drawback of being black box models.
This work proposes an interpretable state representation that can capture state-action causalities for an automated driving agent, while also allowing for the underlying formulation to be general enough to be adapted to different driving scenarios.

It also proposes encoding temporally-extended information in the state representation for better driving performance.
We test this approach on a reinforcement learning agent in a highway simulation environment and demonstrate that the proposed state representation can capture state-action causalities in an interpretable manner.

Experimental results show that the formulation and interpretation can be used to adapt the behavior of the driving agent to achieve desired, even unseen, driving behaviors after training.
Copyright (c) 2022 The Authors.
This is an open access article under the CC BY-NC-ND license

title: Deep reinforcement learning based control for Autonomous Vehicles in CARLA
abstract: Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them.
This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle.
More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them.
The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route.
In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands.
For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world.

The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance.
DDPG perfoms trajectories very similar to classic controller as LQR.
In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m.
To conclude, some conclusions and future works are commented.

title: High-Speed Ramp Merging Behavior Decision for Autonomous Vehicles Based on Multiagent Reinforcement Learning
abstract: To improve the decision success rate of a multiagent reinforcement learning algorithm in merging high-speed ramps of autonomous vehicles, the independent proximal policy optimization (IPPO) method is presented.

The Markov decision process (MDP) model for autonomous vehicle behavioral decision making is developed.
Moreover, the state space, reward function, and action space are all designed.
An IPPO method is proposed using independent learning and parameter-sharing strategies based on the proximal policy optimization algorithm.
And further, a decision-making model for autonomous driving behavior is built.
For simulation experiments, a highway ramp scenario is set.
The experiment findings indicate that the IPPO algorithm can significantly increase the decision success rate of autonomous vehicles in the ramp merging assignment.
Also, as compared to the MAACKTR and GPPO algorithms, the IPPO algorithm can achieve a better average reward and finish the ramp merging more rapidly.

title: End-to-end Autonomous Driving in Heterogeneous Traffic Scenario Using Deep Reinforcement Learning
abstract: In this paper, we propose an end-to-end autonomous driving architecture for safe maneuvering in heterogeneous traffic using a reinforcement learning (RL) algorithm.
Using the proposed architecture we develop an RL agent that can make driving decisions directly from the sensor data.
We formulate the autonomous driving problem as a Markov Decision Process and propose different architectures using Deep Q -Networks for two types of sensor data - top view images of the autonomous vehicle (AV) and its surrounding vehicles and information on relative position and velocities of the surrounding vehicles w.
r.
t the AV.

We consider a highway scenario and analyze the performance of the RL agent using the proposed architectures using the highway-env simulator.
We compare the driving performance of the AV for both sensor types and discuss their efficacy under varying traffic densities.

title: Proceedings of 2020 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)
abstract: 

title: Decentralized Adaptive Optimal Tracking Control for Massive Autonomous Vehicle Systems With Heterogeneous Dynamics: A Stackelberg Game
abstract: In this article, a decentralized optimal tracking control problem has been studied for a large-scale autonomous vehicle system with heterogeneous system dynamics.
Due to the ultralarge number of agents, the notorious "curse of dimension" problem as well as the unrealistic assumption of the existence of reliable very large-scale communication links in uncertain environments have challenged the traditional multiagent system (MAS) algorithms for decades.

The emerging mean-field game (MFG) theory has recently been widely adopted to generate a decentralized control method that deals with those challenges by encoding the large scale MASs' information into a novel time-varying probability density functions (PDF) which can be obtained locally.

However, the traditional MFG methods assume all agents are homogeneous, which is unrealistic in practical industrial applications, e.g., Internet of Things (IoTs), and so on.
Therefore, a novel mean-field Stackelberg game (MFSG) is formulated based on the Stackelberg game, where all the agents have been classified as two different categories where one major leader's decision dominates the other minor agents.

Moreover, a hierarchical structure that treats all minor agents as a mean-field group is developed to tackle the assumption of homogeneous agents.
Then, the actor-actor-critic-critic-mass (A(2)C(2) M) algorithm with five neural networks is designed to learn the optimal policies by solving the MFSG.
The Lyapunov theory is utilized to prove the convergence of A(2)C(2) M neural networks and the closed-loop system's stability.
Finally, a series of numerical simulations are conducted to demonstrate the effectiveness of the developed method.

title: Supervised Learning Guided Reinforcement Learning for Humanized Autonomous Driving Following Decision Making
abstract: In order to improve the humanization of the autonomous vehicle following function and reduce the reinforcement learning exploration space, the adaptive following control strategy based on human experience supervised deep reinforcement learning is proposed in this paper.

First, an end-to-end human driving reference model based on a temporal LSTM network is built to learn the driver's policy function from temporal state information to action.
The model is trained by imitation learning using the driver-following data, and the action signals are output online through the input temporal state, and the output action signals are introduced into reinforcement learning through the reward function.

The reward function is designed by combining the safety and comfort of the following process.
The decision model is trained by a dual delay depth deterministic policy gradient.
The simulation results based on CARLA simulator show that the proposed control strategy is closer to the human driver's driving habits and converges faster than the traditional reinforcement learning.

title: An Optimal Vehicle Speed Planning Algorithm for Regenerative Braking at Traffic Lights Intersections based on Reinforcement Learning
abstract: For electric vehicle or hybrid electric vehicles, the regenerative braking is one of the important means to realize energy saving, for which braking ahead of a traffic light intersection is a representative scenario.

The uncertainty in driver behavior and future traffic flow, however, make it challenging to achieve optimal dynamic energy recovery through conventional braking operation by drivers.
Therefore, in this paper, an energy recovery optimization-oriented vehicle speed planning algorithm ahead of traffic lights intersection is proposed, for autonomous vehicle or driving assistance system.

First, the reward function is designed, taking the energy recovery amount, traffic efficiency and driving smoothness into consideration.
Then, the information of traffic lights at intersections is obtained in advance through V2I (vehicle to infrastructure) communication.
Finally, the q-table and neural network are trained in the framework of reinforcement learning, deriving optimal vehicle speed profile.
Simulation results on a high-fidelity model show that the amount of recovered electrical energy using q-learning algorithm is 45.08% higher than that of uniform deceleration.
The amount of electrical energy using DQN (Deep Q-network) algorithm is 2.24% higher than q-learning, showing to be a better candidate in terms of comprehensive optimality than q-learning.

title: Research on Vehicle Dispatch Problem Based on Kuhn-Munkres and Reinforcement Learning Algorithm
abstract: With the development of artificial intelligence and 5G communication technology, autonomous vehicles are gradually becoming more achievable.
Autonomous vehicles are used in urban transportation to provide taxi service, which effectively reduces labor cost and realizes intelligent transportation systems.
The vehicle system combined with 5G technology can quickly obtain traffic information, which provides a decision basis for the vehicle dispatching.
Thus, it is necessary to develop an efficient way to distribute and allocate these vehicles to maximize the potential income for the system.
This paper studies the vehicle dispatching based on the travel data from the 2016 New York City Green Taxi data and propose two dispatching methods.
First, we consider the dispatching problem as a maximum weight value matching problem.
Then a distance-based dispatching method is proposed with the goal of minimizing the waiting time of passengers by using the Kuhn and Munkres (KM) algorithm.
Finally, we formulate the decision of vehicle dispatch with a Markov Decision Process (MDP) and introduce a Reinforcement Learning (RL)-based dispatching method, which combines RL algorithm and KM algorithm to solve the dispatching problem with the goal of maximizing long-term revenue of divers.

In the experiment, KM algorithm is compared with the full permutation algorithm to prove the effectiveness of KM algorithm.
The performance of the distance-based dispatching method and RL-based dispatching method are presented in a small-scale dispatching and a large-scale dispatching.
Experiment results show that the total revenue of vehicles is improved by about 20% by using RL-based dispatching method, compared to dispatching method based on the distance.
Thus, RL-based dispatching method is more effective in a dispatching platform.
It could be used by future public autonomous vehicle companies to achieve the fulfill the need of maximizing the potential income for the system.

title: Deep reinforcement-learning-based driving policy for autonomous road vehicles
abstract: In this work, the problem of path planning for an autonomous vehicle that moves on a freeway is considered.
The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics.
On the contrary, this work proposes the development of a driving policy based on reinforcement learning.
In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required.
Driving scenarios where the road is occupied both by autonomous and manual driving vehicles are considered.
To the best of the authors' knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments.
The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established SUMO microscopic traffic flow simulator.

Finally, some initial results regarding the effect of autonomous vehicles' behaviour on the overall traffic flow are presented.

title: A Roadside Decision-Making Methodology Based on Deep Reinforcement Learning to Simultaneously Improve the Safety and Efficiency of Merging Zone
abstract: The safety and efficiency of the merging zone is particularly important for traffic networks.
Although autonomous vehicle improves the safety and efficiency from vehicle view, traffic controlling in merging zone mostly focus on improving efficiency from roadside view.
Lacking of detailed driving recommendation, it ignores the safety of merging zone where commercial vehicle pose a high collision risk in real traffic.
This paper proposes a roadside decision-making methodology to simultaneously improve the safety and efficiency of merging zone.
We have built two modules, namely assessment and decision-making.
Assessment module takes advantage of Bayesian inference to evaluate dynamic collision risk.
Decision-making module based on deep reinforcement learning recommends the actions to commercial vehicles by roadside unit.
A series of typical simulation tests show that our method increases the TTC of commercial vehicles by an average of 62.7%.
In the free flow, the overall travel time of vehicles in the merging zone is reduced by 11.68%.
Most notably, when congestion occurred, the average jam length is reduced by 59.68% on the premise of safety.
Moreover, the average accuracy of the roadside decision-making method on the evaluation metrics of TTC, travel time, and jam length are 93.73%, 91.65%, and 94.45%, respectively.
The experimental results show that the roadside decision-making methodology simultaneously improves safety and efficiency, and it dynamically adapts free and congested traffic flow.

title: PNNUAD: Perception Neural Networks Uncertainty Aware Decision-Making for Autonomous Vehicle
abstract: Most environment perception methods in autonomous vehicles rely on deep neural networks because of their impressive performance.
However, neural networks have black-box characteristics in nature, which may lead to perception uncertainty and untrustworthy autonomous vehicles.
Thus, this work proposes a decision-making method to adapt the potential perception uncertainty due to the sensor noises, fuzzy features, and unfamiliar inputs.
The whole method is named as Perception Neural Networks Uncertainty Aware Decision-Making (PNNUAD) method.
PNNUAD first uses the Monte Carlo dropout method to estimate the perception neural network uncertainty into a distribution around the original output.
Then, the perception uncertainty will be considered in a designed reinforcement learning-based planner using a distributed value function.
Finally, a backup policy will maintain the vehicle's performance to avoid disastrous perception uncertainty.
The evaluation section uses an augmented reality urban driving scenario; namely, the scenario builds in the CARLA simulator while the perception uncertainty comes from the real dataset.
This case study focuses on the object class uncertainty of a widely used neural network, i.e., YOLO-V3.
The results indicate that the proposed method can maintain AV safety even with poor perception performance.
Meanwhile, the AV has not become too conservative by defending the perception uncertainty.
This work is necessary for applying the statistics neural networks to safety-critical autonomous vehicles, and the source code will be open-source in this work.

title: RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging
abstract: A platoon refers to a group of vehicles traveling together in very close proximity using automated driving technology.
Owing to its immense capacity to improve fuel efficiency, driving safety, and driver comfort, platooning technology has garnered substantial attention from the autonomous vehicle research community.
Although highly advantageous, recent research has uncovered that an excessively small intra-platoon gap can impede traffic flow during highway on-ramp merging.
While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a challenge due to the massive computational complexity.

In this paper, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging.

The framework's state space has been meticulously designed in consultation with the transportation literature to take into account critical traffic parameters that bear direct relevance to merging efficiency.

An intra-platoon gap decision making method based on the deep deterministic policy gradient algorithm is created to incorporate the continuous action space to ensure precise and continuous adaptation of the intra-platoon gap.

An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway on-ramp merging scenarios.

title: Stable and Efficient Shapley Value-Based Reward Reallocation for Multi-Agent Reinforcement Learning of Autonomous Vehicles
abstract: With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs).

However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability.
When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process.
In this work, we propose to reallocate the system's total reward efficiently to motivate stable cooperation among autonomous vehicles.
We formally define and quantify how to reallocate the system's total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system's total reward.

We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game.
Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group.
We then propose a cooperative policy learning algorithm with Shapley value reward reallocation.
In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.

title: RL-DOVS: Reinforcement Learning for Autonomous Robot Navigation in Dynamic Environments
abstract: Autonomous navigation in dynamic environments where people move unpredictably is an essential task for service robots in real-world populated scenarios.
Recent works in reinforcement learning (RL) have been applied to autonomous vehicle driving and to navigation around pedestrians.
In this paper, we present a novel planner (reinforcement learning dynamic object velocity space, RL-DOVS) based on an RL technique for dynamic environments.
The method explicitly considers the robot kinodynamic constraints for selecting the actions in every control period.
The main contribution of our work is to use an environment model where the dynamism is represented in the robocentric velocity space as input to the learning system.
The use of this dynamic information speeds the training process with respect to other techniques that learn directly either from raw sensors (vision, lidar) or from basic information about obstacle location and kinematics.

We propose two approaches using RL and dynamic obstacle velocity (DOVS), RL-DOVS-A, which automatically learns the actions having the maximum utility, and RL-DOVS-D, in which the actions are selected by a human driver.

Simulation results and evaluation are presented using different numbers of active agents and static and moving passive agents with random motion directions and velocities in many different scenarios.
The performance of the technique is compared with other state-of-the-art techniques for solving navigation problems in environments such as ours.

title: Deep reinforcement learning based path tracking controller for autonomous vehicle
abstract: Path tracking is an essential task for autonomous vehicles (AV), for which controllers are designed to issue commands so that the AV will follow the planned path properly to ensure operational safety, comfort, and efficiency.

While solving the time-varying nonlinear vehicle dynamic problem is still challenging today, deep neural network (NN) methods, with their capability to deal with nonlinear systems, provide an alternative approach to tackle the difficulties.

This study explores the potential of using deep reinforcement learning (DRL) for vehicle control and applies it to the path tracking task.
In this study, proximal policy optimization (PPO) is selected as the DRL algorithm and is combined with the conventional pure pursuit (PP) method to structure the vehicle controller architecture.
The PP method is used to generate a baseline steering control command, and the PPO is used to derive a correction command to mitigate the inaccuracy associated with the baseline from PP.
The blend of the two controllers makes the overall operation more robust and adaptive and attains the optimality to improve tracking performance.
In this paper, the structure, settings and training process of the PPO are described.
Simulation experiments are carried out based on the proposed methodology, and the results show that the path tracking capability in a low-speed driving condition is significantly enhanced.

title: Controlling an Autonomous Vehicle with Deep Reinforcement Learning
abstract: We present a control approach for autonomous vehicles based on deep reinforcement learning.
A neural network agent is trained to map its estimated state to acceleration and steering commands given the objective of reaching a specific target state while considering detected obstacles.
Learning is performed using state-of-the-art proximal policy optimization in combination with a simulated environment.
Training from scratch takes five to nine hours.
The resulting agent is evaluated within simulation and subsequently applied to control a full-size research vehicle.
For this, the autonomous exploration of a parking lot is considered, including turning maneuvers and obstacle avoidance.
Altogether, this work is among the first examples to successfully apply deep reinforcement learning to a real vehicle.

title: Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation
abstract: In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs).
Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time.
Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm.

We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities.
Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost.

This study contributes to the growing body of knowledge on reinforcement learning applications in autonomous driving and highlights the potential of single-agent, cost-effective methods for addressing more complex driving scenarios and advancing reinforcement learning algorithms in the future.


title: FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers and Goods Transportation
abstract: The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries.
On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching.
The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems.
This article considers combining passenger transportation with goods delivery to improve vehicle-based transportation.
We propose FlexPool: a distributed model-free deep reinforcement learning algorithm that jointly serves passengers & goods workloads by learning optimal dispatch policies from its interaction with the environment.

The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method.
These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods.
The dispatching algorithm based on deep reinforcement learning is integrated with an efficient matching algorithm for passengers and goods.
Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers & goods.
FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers & goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.


title: Simulation of an Autonomous Car Drive Scenario
abstract: In everyday life, people are dependent on public or private means of transport.
Day by day, this phenomenon leads to a continuous increase in road traffic, a fact that implies the appearance of urban agglomerations, jeopardizing traffic safety, but also a major impact on the environment.

Thanks to modern information and communication technologies, these aforementioned problems will be able to be given at least one pertinent advanced solution to lessen the number of transportation related problems we face today.

These intelligent transport systems have a wide area of applicability in increasing road safety, minimizing the impact on the environment but also in improving traffic management and maximizing the benefits due to transport.

By autonomous vehicle it is understood that the system assists the driver by maintaining a constant speed according to the requirements and conditions of the road segment on which the movement is carried out, by correctly entering the lane without inadvertently leaving it, but also by maintaining a safe distances from surrounding vehicles and obstacles, all this to ensure a safe and incident-free journey.

By using a simulation environment such as Carla, we can create an autonomous driving scenario and its training model that we can later use, if it meets all the requirements, on a real vehicle.
The simulation allows us to see if the model can be validated or where it still needs improvement without endangering someone's life and without material damage.

title: Efficient falsification approach for autonomous vehicle validation using a parameter optimisation technique based on reinforcement learning [arXiv]
abstract: The widescale deployment of Autonomous Vehicles (AV) appears to be imminent despite many safety challenges that are yet to be resolved.
It is well-known that there are no universally agreed Verification and Validation (VV) methodologies guarantee absolute safety, which is crucial for the acceptance of this technology.
The uncertainties in the behaviour of the traffic participants and the dynamic world cause stochastic reactions in advanced autonomous systems.
The addition of ML algorithms and probabilistic techniques adds significant complexity to the process for real-world testing when compared to traditional methods.
Most research in this area focuses on generating challenging concrete scenarios or test cases to evaluate the system performance by looking at the frequency distribution of extracted parameters as collected from the real-world data.

These approaches generally employ Monte-Carlo simulation and importance sampling to generate critical cases.
This paper presents an efficient falsification method to evaluate the System Under Test.
The approach is based on a parameter optimisation problem to search for challenging scenarios.
The optimisation process aims at finding the challenging case that has maximum return.
The method applies policy-gradient reinforcement learning algorithm to enable the learning.
The riskiness of the scenario is measured by the well established RSS safety metric, euclidean distance, and instance of a collision.
We demonstrate that by using the proposed method, we can more efficiently search for challenging scenarios which could cause the system to fail in order to satisfy the safety requirements.

title: Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem [arXiv]
abstract: This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles.
We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies.

We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach.

We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach.
In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the approaches in stochastic environments, since the agents were mutually independent and exhibited asynchronous motion behavior.

The problems were further aggravated by providing the agents with sparse observation spaces and requiring them to sample control commands that implicitly satisfied the imposed kinodynamic as well as safety constraints.

The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training as well as deployment phases.

title: A Conceptual Multi-Layer Framework for the Detection of Nighttime Pedestrian in Autonomous Vehicles Using Deep Reinforcement Learning
abstract: The major challenge faced by autonomous vehicles today is driving through busy roads without getting into an accident, especially with a pedestrian.
To avoid collision with pedestrians, the vehicle requires the ability to communicate with a pedestrian to understand their actions.
The most challenging task in research on computer vision is to detect pedestrian activities, especially at nighttime.
The Advanced Driver-Assistance Systems (ADAS) has been developed for driving and parking support for vehicles to visualize sense, send and receive information from the environment but it lacks to detect nighttime pedestrian actions.

This article proposes a framework based on Deep Reinforcement Learning (DRL) using Scale Invariant Faster Region-based Convolutional Neural Networks (SIFRCNN) technologies to efficiently detect pedestrian operations through which the vehicle, as agents train themselves from the environment and are forced to maximize the reward.

The SIFRCNN has reduced the running time of detecting pedestrian operations from road images by incorporating Region Proposal Network (RPN) computation.
Furthermore, we have used Reinforcement Learning (RL) for optimizing the Q-values and training itself to maximize the reward after getting the state from the SIFRCNN.
In addition, the latest incarnation of SIFRCNN achieves near-real-time object detection from road images.
The proposed SIFRCNN has been tested on KAIST, City Person, and Caltech datasets.
The experimental results show an average improvement of 2.3% miss rate of pedestrian detection at nighttime compared to the other CNN-based pedestrian detectors.

title: A Selective Federated Reinforcement Learning Strategy for Autonomous Driving
abstract: Currently, the complex traffic environment challenges the fast and accurate response of a connected autonomous vehicle (CAV).
More importantly, it is difficult for different CAVs to collaborate and share knowledge.
To remedy that, this paper proposes a selective federated reinforcement learning (SFRL) strategy to achieve online knowledge aggregation strategy to improve the accuracy and environmental adaptability of the autonomous driving model.

First, we propose a federated reinforcement learning framework that allows participants to use the knowledge of other CAVs to make corresponding actions, thereby realizing online knowledge transfer and aggregation.

Second, we use reinforcement learning to train local driving models of CAVs to cope with collision avoidance tasks.
Third, considering the efficiency of federated learning (FL) and the additional communication overhead it brings, we propose a CAVs selection strategy before uploading local models.
When selecting CAVs, we consider the reputation of CAVs, the quality of local models, and time overhead, so as to select as many high-quality users as possible while considering resources and time constraints.

With above strategic processes, our framework can aggregate and reuse the knowledge learned by CAVs traveling in different environments to assist in driving decisions.
Extensive simulation results validate that our proposal can improve model accuracy and learning efficiency while reducing communication overhead.

title: A Survey on Localization for Autonomous Vehicles
abstract: Research on autonomous vehicles has made significant advances in recent years.
To operate an autonomous vehicle safely and effectively, precise localization is essential.
This study aims to present the state of the art in localization to scientists new to the area.
It presents and summarizes works from the field of localization and suggests a classification for the works.
Approaches to localization are mainly divided into three categories: conventional localization, machine-learning-based localization, and vehicle-to-everything (V2X) localization.
Conventional localization primarily depends on high-definition (HD) maps or certain marks, such as landmarks and road marks.
Machine-learning-based localization approaches include using neural networks, end-to-end approaches, as well as reinforcement learning for performing or improving localization.
Moreover, V2X localization methods localize vehicles by communicating with other vehicles (V2V) or infrastructures (V2I).
This study not only presents a bigger picture of the area of localization in autonomous driving but also presents the potentials and drawbacks of different localization methods.
At the end of the review, some research areas open for future research are also highlighted.

title: Data-Driven Lateral Fault-tolerance Control of Autonomous Vehicle System Using Reinforcement Learning
abstract: In this paper, a data driven lateral fault-tolerant control (LFTC) method is proposed for four wheels drive vehicles, which considering both vehicle velocity and trajectory tracking performance.
The novel design of the lateral control employs with zero-sum game and adaptive dynamics programming technique to solve the Riccati equation without requiring the knowledge of system, only using online data.

The LFTC consists of the data driven off-policy and adaptive control.
The adaptive parameters adjusted online to compensate the actuator faults automatically.
The tracking system is asymptotically stable with the disturbance attenuation level gamma.
Finally, simulation is provided to show the effectiveness of the proposed method.

title: Reinforcement learning based throttle and brake control for autonomous vehicle following
abstract: In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower.
A reinforcement learning based throttle and brake control approach is developed for the follower vehicle.
Near optimal control law is directly learned by trial and error with the neural dynamic programming algorithm.
According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower.
Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim.

title: Pursuit-escape Strategy for Autonomous Agents
abstract: The pursuit-escape strategy is the cutting-edge research related to autonomous vehicle.
This paper focuses on training 2 agents based on the Q-Learning algorithm model in reinforce learning, in a pursuit-escape game confrontation to solve the game strategy selection problem for a single pursuer and a single escapee in a simulation environment.

First of all, the simulation model uses the gym API library provided by the OpenAI research organization to call the pre-defined environment to build the simulation environment.
At the same time, the training model of the pursuit-escape game would be built and trained in the cloud computing platform of Baidu PaddlePaddle, and the final training effect that meets the expectation proves that the algorithm is verified.

This research also introduces the current interference in to increase the realism of the simulation and optimizes the algorithm related to obstacle judgment.
The simulation and experimental results demonstrate that the sensor model can assist in providing "rewards and punishments" in the pursuit process of an agent, which can be applied to improve the autonomous motion strategy selection of the agent.


title: Reinforcement Learning-Based Guidance of Autonomous Vehicles
abstract: Reinforcement learning (RL) has attracted significant research efforts to guide an autonomous vehicle (AV) for a collision-free path due to its advantages in investigating interactions among multiple vehicles and dynamic environments.

This study deploys a Deep Q-Network (DQN) based RL algorithm with reward shaping to control an ego AV in an environment with multiple vehicles.
Specifically, the state space of the RL algorithm depends on the desired destination, the ego vehicle's location and orientation, and the location of other vehicles in the system.
The training time of the proposed RL algorithm is much shorter than most current image-based algorithms.
The RL algorithm also provides an extendable framework to include a varying number of vehicles in the environment and can be easily adapted to different maps without changing the setup of the RL algorithm.

Three scenarios were simulated in the Cars Learn to Act (CARLA) simulator to examine the effects of the proposed RL algorithm on guiding the ego AV interacting with multiple vehicles on straight and curvy roads.

Our results showed that the ego AV could learn to reach its destination within 5000 episodes for all scenarios tested.

title: Proactive Caching in Auto Driving Scene via Deep Reinforcement Learning
abstract: The Internet of Vehicles is a product of the mobile Internet background and an innovative driving force for the industrial upgrading of the automotive industry.
The rapid and reliable completion of the vehicle to roadside unit (V2R) communication is an important research direction.
In order to improve the communication efficiency between the vehicles and the roadside units (RSUs), we propose a deep reinforcement learning approach for joint proactive caching and automatic vehicle control in the vehicle network.

First, we model the autonomous vehicle system and the proactive caching system as separate Markov decision processes, respectively.
Then, we propose a model-free algorithm based on deep Q-learning to find effective automatic vehicle control policy and proactive caching policy.
Finally, the simulation results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience in the V2R communication.

title: Deep, Consistent Behavioral Decision Making with Planning Features for Autonomous Vehicles
abstract: Autonomous driving promises to be the main trend in the future intelligent transportation systems due to its potentiality for energy saving, and traffic and safety improvements.
However, traditional autonomous vehicles' behavioral decisions face consistency issues between behavioral decision and trajectory planning and shows a strong dependence on the human experience.
In this paper, we present a planning-feature-based deep behavior decision method (PFBD) for autonomous driving in complex, dynamic traffic.
We used a deep reinforcement learning (DRL) learning framework with the twin delayed deep deterministic policy gradient algorithm (TD3) to exploit the optimal policy.
We took into account the features of topological routes in the decision making of autonomous vehicles, through which consistency between decision making and path planning layers can be guaranteed.
Specifically, the features of a route extracted from path planning space are shared as the input states for the behavioral decision.
The actor-network learns a near-optimal policy from the feasible and safe candidate emulated routes.
Simulation tests on three typical scenarios have been performed to demonstrate the performance of the learning policy, including the comparison with a traditional rule-based expert algorithm and the comparison with the policy considering partial information of a contour.

The results show that the proposed approach can achieve better decisions.
Real-time test on an HQ3 (HongQi the third ) autonomous vehicle also validated the effectiveness of PFBD.

title: A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles [arXiv]
abstract: Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems.

Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position.

Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption.

To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms.
Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies.
The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g.
the median predicted error at the beginning of the target's localisation is 17% less.
These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles.

This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios

title: Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data [arXiv]
abstract: Designing traffic-smoothing cruise controllers that can be deployed onto autonomous vehicles is a key step towards improving traffic flow, reducing congestion, and enhancing fuel efficiency in mixed autonomy traffic.

We bypass the common issue of having to carefully fine-tune a large traffic microsimulator by leveraging real-world trajectory data from the I-24 highway in Tennessee, replayed in a one-lane simulation.

Using standard deep reinforcement learning methods, we train energy-reducing wave-smoothing policies.
As an input to the agent, we observe the speed and distance of only the vehicle in front, which are local states readily available on most recent vehicles, as well as non-local observations about the downstream state of the traffic.

We show that at a low 4% autonomous vehicle penetration rate, we achieve significant fuel savings of over 15% on trajectories exhibiting many stop-and-go waves.
Finally, we analyze the smoothing effect of the controllers and demonstrate robustness to adding lane-changing into the simulation as well as the removal of downstream information.

title: Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning
abstract: Deep neural network-based systems are now state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness.
Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane.

In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates.
This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms.
The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst case deviation in input space due to possible adversaries or noise.

Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations.
The approach is demonstrated on a deep Q-network (DQN) policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios, a classic control task, and Atari Pong.

This article extends our prior work with new performance guarantees, extensions to other reinforcement learning algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.


title: End-to-end autonomous vehicle navigation control method guided by the dynamic window approach
abstract: Existing end-to-end vehicle navigation control methods based on deep reinforcement learning generally have low exploration efficiency and difficulty converging the model to the ideal state.
To address these problems, this paper proposes a hybrid reinforcement learning framework that can fuse the traditional path planning algorithm (dynamic window approach, DWA) with the deep reinforcement learning approach.

By taking advantage of DWA's ability to plan a collision-free trajectory with guaranteed vehicle dynamics constraints quickly, giving positive guidance to the DRL module at the early stage of its training, thus improving exploration efficiency while ensuring exploration breadth.

To verify the effectiveness of the algorithm, a joint CARLA and ROS simulation environment is built and simulated in a typical scenario.
The simulation results show that compared with existing deep reinforcement learning methods, the proposed method in this paper has significantly improved in terms of model convergence speed, stability, and pre-mid-term decision performance, in which the training time of TD3 decision network can be shortened by more than 85%.


title: Continuous Control in Car Simulator with Deep Reinforcement Learning
abstract: Deep reinforcement learning (DRL), which can be trained without abundant labeled data required in supervised learning, plays an important role in autonomous vehicle researches.
According to action space, DRL can be further divided into two classes: discrete domain and continuous domain.
In this work, we focus on continuous steering control since it's impossible to switch among different discrete steering values at short intervals in reality.
We first define the steering smoothness to quantify the degree of continuity.
Then we propose a new penalty in reward shaping.
We carry experiments based on Deep Deterministic Policy Gradient (DDPG) and Asynchronous Advantage Actor Critic (A3C), which are the state of the art in continuous domain.
Results show that the proposed penalty improves the steering smoothness with both algorithms.

title: Autonomous Vehicle Path Tracking Control based on Adaptive Dynamic Programming
abstract: This paper presents a path tracking control method for autonomous vehicles by utilizing Adaptive Dynamic Programming (ADP).
The primary goal is to enhance the path tracking performance under various longitudinal speeds.
The proposed approach incorporates the consideration of nonlinear tire characteristics and establishes a nonlinear vehicle system model based on the fundamental principles of vehicle dynamics.
By formulating a performance index function that captures the correlation between the vehicle's position and desired paths, the optimal control strategy is obtained using the Dual Heuristic Programming algorithm (DHP), known for its computational efficiency, within the ADP framework.

The simulation results verify the effectiveness of the proposed path tracking control scheme, which can have better control performance under different vehicle speeds.

title: A Novel Vehicle Tracking Method for Cross-Area Sensor Fusion with Reinforcement Learning Based GMM
abstract: Radars, LiDARs and cameras have been widely adopted in autonomous driving applications due to their complementary capabilities of environment perception.
However, one problem lies in how to effectively improve the cross-area tracking accuracy with massive data from multiple sensors.
This paper proposes a novel tracking solution that is composed of a reinforcement-learning-based Gaussian mixture model (GMM), submodel center realignment, and data-driven trajectory association.
Specifically, developed with a Q-learning-based cluster number, an improved GMM-EM algorithm is firstly investigated to cluster the dense short-range radar data points.
Subsequently, an innovative kinetic-energy-aware approach is presented to realign the Q-learning GMM cluster centers for position error mitigation.
In addition to Q-learning GMM clustering, a weight-scheduled method is presented to associate the data from a long-range radar and cameras for cross-area object extraction and trajectory fusion.
Eighteen experiments for training and one experiment for verification were conducted on a fully-instrumented autonomous vehicle.
Experimental results demonstrate that a better tracking performance in crossing detection areas can be achieved by the proposed method.

title: SHAIL: Safety-Aware Hierarchical Adversarial Imitation Learning for Autonomous Driving in Urban Environments
abstract: Designing a safe and human-like decision-making system for an autonomous vehicle is a challenging task.
Generative imitation learning is one possible approach for automating policy-building by leveraging both real-world and simulated decisions.
Previous work that applies generative imitation learning to autonomous driving policies focuses on learning a low-level controller for simple settings.
However, to scale to complex settings, many autonomous driving systems combine fixed, safe, optimization-based low-level controllers with high-level decision-making logic that selects the appropriate task and associated controller.

In this paper, we attempt to bridge this gap in complexity by employing Safety-Aware Hierarchical Adversarial Imitation Learning (SHAIL), a method for learning a high-level policy that selects from a set of low-level controller instances in a way that imitates low-level driving data on-policy.

We introduce an urban roundabout simulator that controls non-ego vehicles using real data from the Interaction dataset.
We then demonstrate empirically that even with simple controller options, our approach can produce better behavior than previous approaches in driver imitation that have difficulty scaling to complex environments.

Our implementation is available at https://github.com/sisl/InteractionImitation.

title: A Reinforcement Learning based Decision-making System with Aggressive Driving Behavior Consideration for Autonomous Vehicles
abstract: With the fast development of autonomous vehicle (AV) technology and possible popularity of AVs in the near future, a mixed-vehicle type driving environment where both AVs and their surrounding human-driving vehicles drive on the same road will exist and last for a long time.

An AV measures its driving environments in real time and make control decisions to ensure driving safety.
However, surrounding human-driving vehicles may conduct aggressive driving behaviors (e.
g.
, sudden deceleration, sudden acceleration, sudden left or right lane change) in practice, which requires an AV to make correct control decisions to eliminate the effect of aggressive driving behaviors on its driving safety.

In this paper, we propose a rein-inrcement learning based decision-making system (ReDS) which considers aggressive driving behaviors of surrounding human-driving vehicles during the decision making process.

In ReDS, we firstly build a mixture density network based aggressive driving behavior detection method to detect possible aggressive driving behaviors among surrounding vehicles of an AV.
We then build a reward function based on aggressive driving behavior detection results and incorporate the reward function into a reinforcement learning model to make optimal control decisions considering aggressive driving behaviors.

We use a real-world traffic dataset from the United States Department of Transportation Federal Highway Administration to evaluate optimal control decision determination performance of ReDS in comparison with the state-of-the-art methods.

The comparison results show that ReDS can improve optimal control decision success rate by 43% compared with existing methods, which demonstrates that ReDS has good optimal control decision determination performance.


title: Safe Reinforcement Learning for Autonomous Vehicle Using Monte Carlo Tree Search
abstract: Reinforcement learning has gradually demonstrated its decision-making ability in autonomous driving.
Reinforcement learning is learning how to map states to actions by interacting with environment so as to maximize the long-term reward.
Within limited interactions, the learner will get a suitable driving policy according to the designed reward function.
However there will be a lot of unsafe behaviors during training in traditional reinforcement learning.
This paper proposes a RL-based method combined with RI, agent and Monte Carlo tree search algorithm to reduce unsafe behaviors.
The proposed safe reinforcement learning framework mainly consists of two modules: risk state estimation module and safe policy search module.
Once the future state will he risky calculated by the risk state estimation module using current state information and the action outputted by the RL agent, the MCTS based safe policy search module will activate to guarantee a safer exploration by adding an additional reward for risk actions.

We test the approach in several random overtake scenarios, resulting in faster convergence and safer behaviors compared to traditional reinforcement learning.

title: Vision-based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles
abstract: Given the recent advances in computer vision, image processing and control systems, self-driving vehicles has been one of the most promising and challenging research topics nowadays.
The design of vision -based robust controllers to keep an autonomous car in the center of the lane, despite uncertainties and disturbances, is still an ongoing challenge.
This paper presents a hybrid control architecture that combines Deep Reinforcement Learning (DRL) and Robust Linear Quadratic Regulator (RLQR) for vision-based lateral control of an autonomous vehicle.

Evolutionary estimation is used to model the vehicle uncertainties.
For performance comparison, a DRL method and three other hybrid controllers are also evaluated.
The inputs for each controller are real-time semantically segmented RGB camera images which serve as the basis to calculate continuous steering actions to keep the vehicle on the center of the lane with a constant velocity.

Simulation results show that the proposed hybrid RLQR with evolutionary estimation of uncertainties architecture outperforms the other algorithms implemented.
It presents lower tracking errors, smoother steering inputs, total collision avoidance and better generalization in new urban environments.
Furthermore, it significantly decreases the required training time.

title: Hierarchical Learning for Model Predictive Collision Avoidance
abstract: Recent progress in model predictive control (MPC) has shown great potential to control complex nonlinear systems in real-time.
However, if parts of the controlled system cannot be modeled exactly by differential equations, the performance of MPC can decrease significantly.
This paper approaches this problem by combining MPC with deep reinforcement learning (DRL) to a hierarchical control system, which is applied to control the motion of an autonomous vehicle.
While the DRL algorithm is responsible for the decision-making with regard to obstacles on the street, the model predictive controller deals with the nonlinear dynamics of the vehicle.
To this end, the vehicle dynamics are modeled by differential equations and the decision-making problem is modeled as a Markov decision process (MDP).
The decisions are considered in the optimization problem of the controller, whose cost function, in turn, is considered in the reward function of the MDP.
The performance of the hierarchical vehicle control is evaluated in scenarios with static and moving obstacles.
Furthermore, it is examined whether adding information about the predicted trajectory to the state space of the MDP can increase the convergence speed.
Copyright (C) 2022 The Authors.

title: A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning
abstract: This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV).

It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling.
We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions.
We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AVdominated, and the pure AVs.
This paper is primarily focused on the latter three phases.
Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning.
While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs?
(2) How do we estimate human driver behaviors?
(3) How should the driving behavior of uncontrollable AVs be modeled in the environment?
(4) How are the interactions between human drivers and autonomous vehicles characterized?
We also provide a list of public datasets and simulation software related to AVs.
Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.


title: End-to-end Autonomous Driving in Heterogeneous Traffic Scenario Using Deep Reinforcement Learning
abstract: In this paper, we propose an end-to-end autonomous driving architecture for safe maneuvering in heterogeneous traffic using a reinforcement learning (RL) algorithm.
Using the proposed architecture we develop an RL agent that can make driving decisions directly from the sensor data.
We formulate the autonomous driving problem as a Markov Decision Process and propose different architectures using Deep Q-Networks for two types of sensor data - top view images of the autonomous vehicle (AV) and its surrounding vehicles and information on relative position and velocities of the surrounding vehicles w.
r.
t the AV.

We consider a highway scenario and analyze the performance of the RL agent using the proposed architectures using the highway-env simulator.
We compare the driving performance of the AV for both sensor types and discuss their efficacy under varying traffic densities.

title: An Improved Retransmission Timeout Forecasting Algorithm for Vehicular Networks
abstract: The vehicular relay network remains a promising path of future intelligent transportation systems ( ITS) that connects the autonomous vehicle with the internet for better traffic control and information sharing.

The transmission control protocol (TCP) remains the backbone of data traffic on the internet as it harbors an enormous portion of global internet traffic between end devices.
In VANET, TCP experiences drastic performance degradation during early or spurious RTO timeouts.
This paper proposes a recursive learning retransmission timeout (RL-RTO), which efficiently reduces spurious timeouts and enhances fast recovery after timeouts.
The RL-RTO forecast timer value is based on the three control parameters.
The performance of RL-RTO is validated against recent RTO approaches under multi-hop vehicular environments.
The proposed RL-RTO considerably regulates estimation errors and improves variance.

title: Hierarchical Reinforcement Learning for Dynamic Autonomous Vehicle Navigation at Intelligent Intersections
abstract: Recent years have witnessed the rapid development of the Cooperative Vehicle Infrastructure System (CVIS), where road infrastructures such as traffic lights (TL) and autonomous vehicles (AVs) can share information among each other and work collaboratively to provide safer and more comfortable transportation experience to human beings.

While many efforts have been made to develop efficient and sustainable CVIS solutions, existing approaches on urban intersections heavily rely on domain knowledge and physical assumptions, preventing them from being practically applied.

To this end, this paper proposes NavTL, a learning-based framework to jointly control traffic signal plans and autonomous vehicle rerouting in mixed traffic scenarios where human-driven vehicles and AVs co-exist.

The objective is to improve travel efficiency and reduce total travel time by minimizing congestion at the intersections while guiding AVs to avoid the temporally congested roads.
Specifically, we design a graph-enhanced multi-agent decentralized bi-directional hierarchical reinforcement learning framework by regarding TLs as manager agents and AVs as worker agents.
At lower temporal resolution timesteps, each manager sets a goal for the workers within its controlled region.
Simultaneously, managers learn to take the signal actions based on the observation from the environment as well as an intention information extracted from its workers.
At higher temporal resolution timesteps, each worker makes rerouting decisions along its way to the destination based on its observation from the environment, an intention-enhanced manager state representation, and a goal from its present manager.

Finally, extensive experiments on one synthetic and two real-world network-level datasets demonstrate the effectiveness of our proposed framework in terms of improving travel efficiency.

title: Reinforcement Learning Based Autonomous E-Vehicle Speed Control
abstract: Greater efficiency in both energy use and traffic flow are two benefits of autonomous self-driving cars.
Due to their superior performance, effectiveness, and lack of carbon emission, electric vehicles (EVs) have recently become popular and used in an autonomous vehicle.
As EVs are making a splash in the automotive industry, researchers are taking a greater interest in studying, modelling, and simulating them.
Controlling EV speed is not an easy task.
Modelling and Conventional Proportional Integral Derivative (PID) controller tuning for EV speed regulation are presented in this paper.
To design and simulation of EV speed control in MATLAB, the transfer function of EV is derived and considered.
PID controller is found to be easy to implement, practical, and provide superior closed-loop performance.
Two PID tuning techniques, Ziegler-Nichols (ZN) and a Reinforcement Learning (RL) method are designed to regulate the EV speed.
Characteristics of the time domain have been used to perform a comparative analysis.
Additionally, the Integral Square Error (ISE) is analysed to determine the optimal PID tuning strategy for EV speed regulation.

title: Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning*
abstract: For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car.
In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult.
In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments.

Application of the hierarchical structure [1] allows the various layers of the behavior planning system to be satisfied.
Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car.

Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly.
On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process.
The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.

title: Reinforce lent Learning based Throttle and Brake Control for Autonomous Vehicle Following
abstract: In this paper, we focus on the basic form of autonomous follow driving problem with one leader and one follower.
A reinforcement learning based throttle and brake control approach is developed for the follower vehicle.
Near optimal control law is directly learned by "trial and error" with the neural dynamic programming algorithm.
According to the timely updated following state, the learned control policy can deliver appropriate throttle and brake control commands for the follower.
Simulation tests to illustrate the effectiveness of the presented method are carried out with the highly recognized vehicle dynamic simulator CarSim.

title: Joint Optimization of Sensing, Decision-Making and Motion-Controlling for Autonomous Vehicles: A Deep Reinforcement Learning Approach
abstract: The three main modules of autonomous vehicles, i.
e.
, sensing, decision making, and motion controlling, have been studied separately in most existing works on autonomous driving, which overlook the correlations among these modules, leading to a result of unsatisfactory performance.

In this paper, we propose a novel scheme that first tactfully processes the sensing data, then jointly learns and optimizes the decision-making and motion-controlling using reinforcement learning (RL).

Specifically, the proposed scheme designs a novel state representation mechanism, where the sensing data goes through the attention layer and the convolutional neural network (CNN) layer sequentially.
The attention layer focuses on extracting the most important local information and then CNN layer takes a broad view to comprehensively consider the global information for a better representation.
Furthermore, the proposed scheme jointly learns decision-making and motion-controlling, therefore, the relevance of these two modules is implicitly considered, which helps to achieve a better autonomous driving policy.

Extensive simulation results show that the proposed scheme is better than classic control methods and some RL methods in terms of safety, velocity, etc.
We also demonstrate the respective functions of the attention layer and the CNN layer through ablation studies.
Finally, we construct a traffic scene with a real autonomous vehicle, and verified the feasibility of the proposed scheme.

title: A Decision-Making Strategy for Vehicle Autonomous Braking in Emergency via Deep Reinforcement Learning
abstract: Autonomous braking through vehicle precise decision-making and control to reduce accidents is a key issue, especially in the early diffusion phase of autonomous vehicle development.
This paper proposes a deep reinforcement learning (DRL)-based autonomous braking decision-making strategy in an emergency situation.
Three key influencing factors, including efficiency, accuracy and passengers' comfort, are fully considered and satisfied by the proposed strategy.
First, the vehicle lane-changing process and the braking process are analyzed in detail, which include the critical factors in the design of the autonomous braking strategy.
Second, we propose a DRL process that determines the optimal strategy for autonomous braking.
Particularly, a multi-objective reward function is designed, which can compromise the rewards achieved of different brake moments, the degree of the accident, and the comfort of the passenger.
Third, a typical actor-critic (AC) algorithm named deep deterministic policy gradient (DDPG) is adopted for solving the autonomous braking problem, which can improve the efficiency of the optimal strategy and be stable in continuous control tasks.

Once the strategy is well trained, the vehicle can automatically take optimal braking behavior in an emergency to improve driving safety.
Extensive simulations validate the effectiveness and efficiency of our proposal in terms of learning effectiveness, decision-making accuracy and driving safety.

title: Local Path Generation Method for Unmanned Autonomous Vehicles Using Reinforcement Learning
abstract: Path generation methods are required for safe and efficient driving in unmanned autonomous vehicles.
There are two kinds ofpaths: global and local.
A global path consists of all the way points including the source and the destination.
A local path is thetrajectory that a vehicle needs to follow from a way point to the next in the global path.
In this paper, we propose a novel methodfor local path generation through machine learning, with an effective curve function used for initializing the trajectory.
First,reinforcement learning is applied to a set of candidate paths to produce the best trajectory with maximal reward.
Then the optimalsteering angle with respect to the trajectory is determined by training an artificial neural network.
Our method outperformed existingapproaches and successfully found quality paths in various experimental settings, including the cases with obstacles.

title: A Discrete Soft Actor-Critic Decision-Making Strategy With Sample Filter for Freeway Autonomous Driving
abstract: Autonomous driving is a promising technology to reduce traffic accidents and improve driving efficiency.
Although significant progress has been achieved, existing decision-making systems of autonomous vehicle still cannot meet the safety and driving efficiency requirements in highly dynamic environments.
In this work, we design a discrete decision-making strategy based on the discrete soft actor-critic with sample filter algorithm (DSAC-SF) to improve driving efficiency and safety on freeways with dynamics traffic.

Specifically, we first propose a sample filter method for discrete soft actor-critic, which improves the sample efficiency and stability of the algorithm via enhancing the utilization of effective samples.

Subsequently, we construct the discrete decision-making strategy for autonomous driving based on the DSAC-SF algorithm, and further design the area observation method and the multi-objective reward function to improve the driving safety and efficiency.

Finally, we carry out comparison and ablation experiments on the the scalable multi-agent reinforcement learning training school (SMARTS) simulation environment.
Experimental results indicate that our strategy obtains a high success rate and a fast vehicle speed in the decision-making tasks on freeways.
Moreover, our DSAC-SF algorithm also achieves improved performance in training efficiency and stability compared to the commonly used discrete reinforcement learning algorithm.

title: Modeling Human Driving Behavior Through Generative Adversarial Imitation Learning
abstract: An open problem in autonomous vehicle safety validation is building reliable models of human driving behavior in simulation.
This work presents an approach to learn neural driving policies from real world driving demonstration data.
We model human driving as a sequential decision making problem that is characterized by non-linearity and stochasticity, and unknown underlying cost functions.
Imitation learning is an approach for generating intelligent behavior when the cost function is unknown or difficult to specify.
Building upon work in inverse reinforcement learning (IRL), Generative Adversarial Imitation Learning (GAIL) aims to provide effective imitation even for problems with large or continuous state and action spaces, such as modeling human driving.

This article describes the use of GAIL for learning-based driver modeling.
Because driver modeling is inherently a multi-agent problem, where the interaction between agents needs to be modeled, this paper describes a parameter-sharing extension of GAIL called PS-GAIL to tackle multi-agent driver modeling.

In addition, GAIL is domain agnostic, making it difficult to encode specific knowledge relevant to driving in the learning process.
This paper describes Reward Augmented Imitation Learning (RAIL), which modifies the reward signal to provide domain-specific knowledge to the agent.
Finally, human demonstrations are dependent upon latent factors that may not be captured by GAIL.
This paper describes Burn-InfoGAIL, which allows for disentanglement of latent variability in demonstrations.
Imitation learning experiments are performed using NGSIM, a real-world highway driving dataset.
Experiments show that these modifications to GAIL can successfully model highway driving behavior, accurately replicating human demonstrations and generating realistic, emergent behavior in the traffic flow arising from the interaction between driving agents.


title: Neuromodulated Patience for Robot and Self-Driving Vehicle Navigation
abstract: Robots and self-driving vehicles face a number of challenges when navigating through real environments.
Successful navigation in dynamic environments requires prioritizing subtasks and monitoring resources.
Animals are under similar constraints.
It has been shown that the neuromodulator serotonin (5-HT) regulates impulsiveness and patience in animals.
In the present paper, we take inspiration from the serotonergic system and apply it to the task of robot navigation.
In a set of outdoor experiments, we show how changing the level of patience can affect the amount of time the robot will spend searching for a desired location.
To navigate GPS compromised environments, we introduce a deep reinforcement learning paradigm in which the robot learns to follow sidewalks.
This may further regulate a tradeoff between a smooth long route and a rough shorter route.
Using patience as a parameter may be beneficial for autonomous systems under time pressure.

title: A Hierarchical Learning Approach to Autonomous Driving Using Rule Specifications
abstract: Understanding the movement of surrounding objects and controlling robot platforms (such as autonomous vehicles and social robots) in a safe way are challenging problems.
In the autonomous driving problem, autonomous vehicles must take into account the future behaviors of nearby vehicles and make appropriate controls accordingly.
This is the biggest factor that makes autonomous driving problems difficult.
In this work, this problem is tackled by combining benefits from both sequence prediction and deep reinforcement learning in a hierarchical manner.
The driver's behavior is classified according to the driving style defined by the rules selected in the autonomous driving situation.
High-level behavior represents driving style, and the vehicle's movement model is trained to condition itself to high-level behavior.
Instead of directly finding low-level controls, we focus on finding high-level behaviors to increase efficiency.
For example, if an autonomous vehicle needs to change lanes in certain situations, the high-level behavior "change lanes" is first selected and the corresponding vehicle movement model is used to find the appropriate low-level controls.

Reinforcement learning is used to help select the best high-level behavior, and future behaviors of nearby vehicles are jointly reasoned to lead to better understanding of the current situation.
The feasibility of the proposed approach is tested in publicly available datasets.
The proposed method shows better efficiency and performance compared to existing learning-based control algorithms.

title: Navigating Autonomous Vehicle at the Road Intersection Simulator with Reinforcement Learning
abstract: In this paper, we consider the problem of controlling an agent that simulates the behavior of an self-driving car when passing a road intersection together with other vehicles.
We consider the case of using smart city systems, which allow the agent to get full information about what is happening at the intersection in the form of video frames from surveillance cameras.
The paper proposes the implementation of a control system based on a trainable behavior generation module.
The agent's model is implemented using reinforcement learning (RL) methods.
In our work, we analyze various RL methods (PPO, Rainbow, TD3), and variants of the computer vision subsystem of the agent.
Also, we present our results of the best implementation of the agent when driving together with other participants in compliance with traffic rules.

title: Autonomous Driving Technology Trend and Future Outlook: Powered by Artificial Intelligence
abstract: Autonomous driving is not a new concept, and relevant technology has been developed for a long time.
However, in recent years, autonomous driving technology has been leaping forward, fueled by the advance of AI-based technologies.
In particular, the essential components of autonomous driving, such as perception, prediction, and planning, deliver entirely different performances from those of the pre-AI era.
In this study, the trends and development of autonomous driving technology will be analyzed by decomposing it into element technologies ranging from perception, prediction, and planning, focusing on AI-based research.

For the perception part, LiDAR and camera-based research and sensor fusion technologies will be examined.
For the prediction part, we will look into various prediction paradigms such as interaction-aware and map-based prediction.
The planning part will cover maneuver decisions, motion planning, and reinforcement learning-based methods.

title: REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES
abstract: Reinforcement learning (RL) is one of the most emerging domains of artificial intelligence.
It is widely used in almost all sort of applications, including medical field, stock market, forecasting, and engineering field.
One of the most effective uses of RL is in the control engineering domain owing to its learning by trial, for example, in building autonomous system like autonomous vehicle and robotics.
In this paper, we have focused on the applications of RL in various control engineering problems.
Stability of controller (or agent) using RL paradigm is a very crucial task due to exploration-exploitation policy used by any RL.
Also, the unavailability of exact model of system or environment may lead to unsafe behaviour of the agent.
Therefore, this paper focuses mainly on the stability aspect in RL-based controller.
Many concepts are used to study and analyse the stability of a system viz Lyapunov method and Barrier function.
This paper surveys the detailed application of these well-established stability certifier methods in various model-free and model-based RL framework.

title: Comparative Study of Cooperative Platoon Merging Control Based on Reinforcement Learning
abstract: The time that a vehicle merges in a lane reduction can significantly affect passengers' safety, comfort, and energy consumption, which can, in turn, affect the global adoption of autonomous electric vehicles.

In this regard, this paper analyzes how connected and automated vehicles should cooperatively drive to reduce energy consumption and improve traffic flow.
Specifically, a model-free deep reinforcement learning approach is used to find the optimal driving behavior in the scenario in which two platoons are merging into one.
Several metrics are analyzed, including the time of the merge, energy consumption, and jerk, etc.
Numerical simulation results show that the proposed framework can reduce the energy consumed by up to 76.
7%, and the average jerk can be decreased by up to 50%, all by only changing the cooperative merge behavior.

The present findings are essential since reducing the jerk can decrease the longitudinal acceleration oscillations, enhance comfort and drivability, and improve the general acceptance of autonomous vehicle platooning as a new technology.


title: Belief state separated reinforcement learning for autonomous vehicle decision making under uncertainty
abstract: In autonomous driving, the ego vehicle and its surrounding traffic environments always have uncertainties like parameter and structural errors, behavior randomness of road users, etc.
Furthermore, environmental sensors are noisy or even biased.
This problem can be formulated as a partially observable Markov decision process.
Existing methods lack a good representation of historical information, making it very challenging to find an optimal policy.
This paper proposes a belief state separated reinforcement learning (RL) algorithm for decision-making of autonomous driving in uncertain environments.
We extend the separation principle from linear Gaussian systems to general nonlinear stochastic environments, where the belief state, defined as the posterior distribution of the true state, is found to be a sufficient statistic of historical information.

This belief state is estimated by action-enhanced variational inference from historical information and is proved to satisfy the Markovian property, thus allowing us to obtain the optimal policy using traditional RL algorithms for Markov decision processes.

The policy gradient of a task-specific prior model is mixed with that of the interaction data to improve learning performance.
The proposed algorithm is evaluated in a multi-lane autonomous driving task, where the surrounding vehicles are subject to behavior uncertainty and observation noise.
The simulation results show that compared with existing RL algorithms, the proposed method can achieve a higher average return with better driving performance.

title: Exploring the use of AI in marine acoustic sensor management
abstract: Underwater passive acoustic source detection and tracking is important for various marine applications, including marine mammal monitoring and naval surveillance.
The performance in these applications is dependent on the placement and operation of sensing assets, such as autonomous underwater vehicles.
Conventionally, these decisions have been made by human operators aided by acoustic propagation modelling tools, situational and environmental data, and experience.
However, this is time-consuming and computationally expensive.
We consider a 'toy problem' of a single autonomous vehicle (agent) in search of a stationary source of low frequency within a reinforcement learning (RL) architecture.
We initially choose the observation space to be the agent's current position.
The agent is allowed to explore the environment with a limited action space, taking equal distance steps in one of $n$ directions.
Rewards are received for positive detections of the source.
Using OpenAI's PPO algorithm an increase in median episode reward of approximately 20 points in the RL environment developed is seen when the agent is given a history of it's previous moves and signal-to-noise ratio compared to the simple state.

The future expansion of the RL framework is discussed in terms of the observation and action spaces, reward function and RL architecture.
[The copyright for the referenced work is owned by Acoustical Society of America.
Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]

title: An Agent-Based Modelling Framework for Driving Policy Learning in Connected and Autonomous Vehicles
abstract: Due to the complexity of the natural world, a programmer cannot foresee all possible situations, a connected and autonomous vehicle (CAV) will face during its operation, and hence, CAVs will need to learn to make decisions autonomously.

Due to the sensing of its surroundings and information exchanged with other vehicles and road infrastructure, a CAV will have access to large amounts of useful data.
While different control algorithms have been proposed for CAVs, the benefits brought about by connectedness of autonomous vehicles to other vehicles and to the infrastructure, and its implications on policy learning has not been investigated in literature.

This paper investigates a data driven driving policy learning framework through an agent-based modelling approaches.
The contributions of the paper are two-fold.
A dynamic programming framework is proposed for in-vehicle policy learning with and without connectivity to neighboring vehicles.
The simulation results indicate that while a CAV can learn to make autonomous decisions, vehicle-to-vehicle (V2V) communication of information improves this capability.
Furthermore, to overcome the limitations of sensing in a CAV, the paper proposes a novel concept for infrastructure-led policy learning and communication with autonomous vehicles.
In infrastructure-led policy learning, road-side infrastructure senses and captures successful vehicle maneuvers and learns an optimal policy from those temporal sequences, and when a vehicle approaches the road-side unit, the policy is communicated to the CAV.

Deep-imitation learning methodology is proposed to develop such an infrastructure-led policy learning framework.

title: Joint Antenna Selection and Beamforming in Integrated Automotive Radar Sensing-Communications with Quantized Double Phase Shifters
abstract: We consider an integrated sensing-communication system operating in a dynamic environment, such as an autonomous vehicle scenario.
We propose a novel, low-cost, low power consumption and low-computation approach for designing a beam that can simultaneously reach the radar target of interest and the desired communication destination.

The transmitter is a uniform linear array, equipped with quantized double phase shifters, which enables a flexible beam design while using analog only processing.
Only a small number of antennas are selected to transmit in each channel use, in order to save system power and reduce antenna coupling.
We propose a deep reinforcement learning approach to adaptively adjust the double phase shifters and select the active antennas in order to optimize the transmit beamforming, through a transmission and feedback trail.

The actor-critic network strategy together with the Wolpertinger policy is adopted to obtain the optimal solutions efficiently and effectively.
Numerical results demonstrate the feasibility of the proposed method.

title: Two-step dynamic obstacle avoidance [arXiv]
abstract: Dynamic obstacle avoidance (DOA) is a fundamental challenge for any autonomous vehicle, independent of whether it operates in sea, air, or land.
This paper proposes a two-step architecture for handling DOA tasks by combining supervised and reinforcement learning (RL).
In the first step, we introduce a data-driven approach to estimate the collision risk of an obstacle using a recurrent neural network, which is trained in a supervised fashion and offers robustness to non-linear obstacle movements.

In the second step, we include these collision risk estimates into the observation space of an RL agent to increase its situational awareness.
~We illustrate the power of our two-step approach by training different RL agents in a challenging environment that requires to navigate amid multiple obstacles.

The non-linear movements of obstacles are exemplarily modeled based on stochastic processes and periodic patterns, although our architecture is suitable for any obstacle dynamics.
The experiments reveal that integrating our collision risk metrics into the observation space doubles the performance in terms of reward, which is equivalent to halving the number of collisions in the considered environment.

Furthermore, we show that the architecture's performance improvement is independent of the applied RL algorithm.

title: Decision making for autonomous vehicles in highway scenarios using Harmonic SK Deep SARSA
abstract: The complexity of taking decisions for an autonomous vehicle (AV) to avoid road accident fatalities, provide safety, comfort, and reduce traffic raises the need for improvements in the field of decision making.

To solve these challenges, many algorithms and techniques were applied, and the most common ones were reinforcement learning (RL) algorithms combined with deep learning techniques.
Therefore, in this paper we proposed a novel extension of the popular "SARSA" (State-Action-Reward-State-Action) RL technique called "Harmonic SK Deep SARSA" that takes advantage of the stability which SARSA algorithm provides and uses the notion of similar and cumulative states saved in an alternative memory to enhance the stability of the algorithm and achieve remarkable performance that SARSA could not accomplish due to its on policy nature.

Through the investigation of our novel extension the adaptability of the algorithm to unexpected situations during learning and to unforeseen changes in the environment was proved while reducing the computational load in the learning process and increasing the convergence rate that plays a key role in upgrading decision making application that require numerous real time consecutive decisions, including autonomous vehicles, industrial robots, gaming, aerial navigation.
.
.

The novel algorithm was tested in a gym environment simulator called "Highway-env" with multiple highway situations (multiple lanes configurations, highway with dynamic number of lanes (from 4-lane to 2-lane, from 4-lane to 6-lane), merge) with numerous dynamic obstacles.

For the purpose of comparison, we used a benchmark of cutting edge algorithms known for their prominent performance.
The experimental results showed that the proposed algorithm outperformed the comparison algorithms in learning stability and performance that were validated by the following metrics: average loss value per episode, average accuracy per episode, maximum speed value reached per episode, average speed per episode, and the total reward per episode.


title: TEXPLORE: real-time sample-efficient reinforcement learning for robots
abstract: The use of robots in society could be expanded by using reinforcement learning (RL) to allow robots to learn and adapt to new situations online.
RL is a paradigm for learning sequential decision making tasks, usually formulated as a Markov Decision Process (MDP).
For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time.
In addition, the algorithm must learn efficiently in the face of noise, sensor/actuator delays, and continuous state features.
In this article, we present texplore, the first algorithm to address all of these challenges together.
texplore is a model-based RL method that learns a random forest model of the domain which generalizes dynamics to unseen states.
The agent explores states that are promising for the final policy, while ignoring states that do not appear promising.
With sample-based planning and a novel parallel architecture, texplore can select actions continually in real-time whenever necessary.
We empirically evaluate the importance of each component of texplore in isolation and then demonstrate the complete algorithm learning to control the velocity of an autonomous vehicle in real-time.

title: Augmented Random Search based Autonomous Driving System
abstract: The development of autonomous driving technology led to a rise in the popularity of self-driving automobiles.
CARLA is an open-source simulator for autonomous driving research and is used in these autonomous driving systems.
From the beginning, CARLA has supported the creation, instruction, and testing of autonomous driving systems.
Along with this, CARLA offers free to use open digital assets (such as city plans, structures, and vehicles).
To perform addition and deletion of random sounds from the weight and tracking the entire rewards received as a result of this weight modification, the Augmented Random Search (ARS) approach trains a supervised learning on input data.

The Augmented Random Generator is then used to measure the weights depending on these incentives in a predetermined number of episodes at a predestined learning rate.
The Algorithm for Robotic Search (ARS) will be used to train self-driving cars.
Automobiles based on data received from each car's front cameras.
As a result of this research, a framework for training self-driving cars has been developed.
Carla's policy employing ARS will be created with it as the core algorithm, giving a more realistic picture of how things work.
Because of its novelty, ARS is an extremely light tool for analyzing difficult control tasks, and the authors of the study discovered that ARS had at least 15 times the computing efficiency of the fastest competitive learning approaches.

CARLA has been used to provide more consistent results from autonomous vehicle training, and this research has proved successful considering how many unique circumstances there are, in opening up the majority of the chances for additional study on the same issue.


title: Quadratic Q-network for Learning Continuous Control for Autonomous Vehicles [arXiv]
abstract: Reinforcement Learning algorithms have recently been proposed to learn time-sequential control policies in the field of autonomous driving.
Direct applications of Reinforcement Learning algorithms with discrete action space will yield unsatisfactory results at the operational level of driving where continuous control actions are actually required.

In addition, the design of neural networks often fails to incorporate the domain knowledge of the targeting problem such as the classical control theories in our case.
In this paper, we propose a hybrid model by combining Q-learning and classic PID (Proportion Integration Differentiation) controller for handling continuous vehicle control problems under dynamic driving environment.

Particularly, instead of using a big neural network as Q-function approximation, we design a Quadratic Q-function over actions with multiple simple neural networks for finding optimal values within a continuous space.

We also build an action network based on the domain knowledge of the control mechanism of a PID controller to guide the agent to explore optimal actions more efficiently.
We test our proposed approach in simulation under two common but challenging driving situations, the lane change scenario and ramp merge scenario.

Results show that the autonomous vehicle agent can successfully learn a smooth and efficient driving behavior in both situations.

title: Communication-efficient decision-making of digital twin assisted Internet of vehicles: A hierarchical multi-agent reinforcement learning approach
abstract: The connected autonomous vehicle is considered an effective way to improve transport safety and efficiency.
To overcome the limited sensing and computing capabilities of individual vehicles, we design a digital twin assisted decision-making framework for Internet of Vehicles, by leveraging the integration of communication, sensing and computing.

In this framework, the digital twin entities residing on edge can effectively communicate and cooperate with each other to plan sub-targets for their respective vehicles, while the vehicles only need to achieve the sub-targets by generating a sequence of atomic actions.

Furthermore, we propose a hierarchical multiagent reinforcement learning approach to implement the framework, which can be trained in an end-to-end way.
In the proposed approach, the communication interval of digital twin entities could adapt to time-varying environment.
Extensive experiments on driving decision-making have been performed in traffic junction scenarios of different difficulties.
The experimental results show that the proposed approach can largely improve collaboration efficiency while reducing communication overhead.

title: Optimization of On-Demand Shared Autonomous Vehicle Deployments Utilizing Reinforcement Learning
abstract: Ride-hailed shared autonomous vehicles (SAV) have emerged recently as an economically feasible way of introducing autonomous driving technologies while serving the mobility needs of under-served communities.

There has also been corresponding research work on optimization of the operation of these SAVs.
However, the current state-of-the-art research in this area treats very simple networks, neglecting the effect of a realistic other traffic representation, and is not useful for planning deployments of SAV service.

In contrast, this paper utilizes a recent autonomous shuttle deployment site in Columbus, Ohio, as a basis for mobility studies and the optimization of SAV fleet deployment.
Furthermore, this paper creates an SAV dispatcher based on reinforcement learning (RL) to minimize passenger wait time and to maximize the number of passengers served.
The created taxi-dispatcher is then simulated in a realistic scenario while avoiding generalization or over-fitting to the area.
It is found that an RL-aided taxi dispatcher algorithm can greatly improve the performance of a deployment of SAVs by increasing the overall number of trips completed and passengers served while decreasing the wait time for passengers.


title: A driving profile recommender system for autonomous driving using sensor data and reinforcement learning
abstract: The design of algorithms for autonomous vehicles includes a wide range of machine learning tasks including scene perception by the visual input from cameras and other sensors, monitoring and prediction of the driver and passengers' state, and others.

The aim of the present work is to study the task of personalizing the driving experience in an autonomous vehicle, taking into account the particularities and differences of each person in how he/she perceives the vehicle's velocity.

For this purpose, we employ the Actor-Critic Reinforcement Learning technique in order to automatically select the best driving mode during driving.
The input to the actor-critic model comprises the driver's stress and excitement, which are affected by the route conditions, and the vehicle velocity and angular velocity.
The output at each step is the best mode for each driver, which better balances stress, excitement, and route completion time.
The whole setup is simulated and tested within the Carla open-source simulator for autonomous driving research.

title: Device Placement for Autonomous Vehicles using Reinforcement Learning
abstract: Autonomous driving is a complex function consisting of multiple parallel AI tasks running at the same time for information sensing, fusion, and decision making.
To process the complex computing tasks, an autonomous vehicle is typically equipped with different processing units at the same time, such as CPU, GPU, FPGA, of the different computing capabilities.
As the AI tasks have different requirements on the computing resources, a fundamental issue is how to optimally allocate the real-time computation tasks to different processing units (as known as device placement) on board towards maximum utility for autonomous driving.

Towards the issue, this paper develops a reinforcement learning algorithm which is based on the proximal policy optimization (PPO) specifically for finding the optimal device placement for running a neural network model.

A sequence-to-sequence model is proposed to allocate the operations of a neural network model on appropriate computing units in the vehicle.
The execution time and energy consumption of the placement solution is used as a reward signal to further optimize the parameters.
We implement our algorithm in different benchmarks, and compare it with different baselines.
Experiments have demonstrated that our algorithm can find the optimal device placement position, and its performance is better than previous state-of-the-art RL algorithm as well as traditional methods.


title: Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions [arXiv]
abstract: Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems.
However, for AVs to effectively navigate the road, they must possess the capability to predict the future behavior of nearby traffic participants, similar to the predictive driving abilities of human drivers.

Building upon existing literature is crucial to advance the field and develop a comprehensive understanding of trajectory prediction methods in the context of automated driving.
To address this need, we have undertaken a comprehensive review that focuses on trajectory prediction methods for AVs, with a particular emphasis on machine learning techniques including deep learning and reinforcement learning-based approaches.

We have extensively examined over two hundred studies related to trajectory prediction in the context of AVs.
The paper begins with an introduction to the general problem of predicting vehicle trajectories and provides an overview of the key concepts and terminology used throughout.
After providing a brief overview of conventional methods, this review conducts a comprehensive evaluation of several deep learning-based techniques.
Each method is summarized briefly, accompanied by a detailed analysis of its strengths and weaknesses.
The discussion further extends to reinforcement learning-based methods.
This article also examines the various datasets and evaluation metrics that are commonly used in trajectory prediction tasks.
Encouraging an unbiased and objective discussion, we compare two major learning processes, considering specific functional features.
By identifying challenges in the existing literature and outlining potential research directions, this review significantly contributes to the advancement of knowledge in the domain of AV trajectory prediction.


title: A Context-Aware Path Forecasting Method for Connected Autonomous Vehicles
abstract: Forecasting the future paths of surrounding vehicles of a Connected Autonomous Vehicle (CAV) can enhance connectivity and efficiency of vehicular networks, and accurate motion forecasting of nearby vulnerable road users can advance road safety and urban mobility.

This task needs a high-level situational awareness for the CAV.
Early methods rely solely on vehicle kinematics and overlook the uncertainty within agents behavior and the effects of surrounding context on the behavior of nearby agents, resulting in lower performance or infeasible predictions.

In the current work, we introduce a novel context-aware forecasting approach for CAVs that benefits from inverse reinforcement learning (IRL) to condition the future motions of nearby agents on scene-based state sequences defined using a Markov Decision Process.

More precisely, we map the images of the surrounding context and the behavior history of agents into rewards and learn optimal expert behaviors using IRL.
We validate the path forecasting efficiency of our model using two large motion prediction benchmarks with different scenes and achieve state-of-the-art results in terms of FDE and ADE metrics.

title: Robust reinforcement learning with UUB guarantee for safe motion control of autonomous robots
abstract: This paper addresses the issue of safety in reinforcement learning (RL) with disturbances and its application in the safety-constrained motion control of autonomous robots.
To tackle this problem, a robust Lyapunov value function (rLVF) is proposed.
The rLVF is obtained by introducing a data-based LVF under the worst-case disturbance of the observed state.
Using the rLVF, a uniformly ultimate boundedness criterion is established.
This criterion is desired to ensure that the cost function, which serves as a safety criterion, ultimately converges to a range via the policy to be designed.
Moreover, to mitigate the drastic variation of the rLVF caused by differences in states, a smoothing regularization of the rLVF is introduced.
To train policies with safety guarantees under the worst disturbances of the observed states, an off-policy robust RL algorithm is proposed.
The proposed algorithm is applied to motion control tasks of an autonomous vehicle and a cartpole, which involve external disturbances and variations of the model parameters, respectively.
The experimental results demonstrate the effectiveness of the theoretical findings and the advantages of the proposed algorithm in terms of robustness and safety.

title: IoV and blockchain-enabled driving guidance strategy in complex traffic environment
abstract: Diversified traffic participants and complex traffic environment (e.
g.
, roadblocks or road damage exist) challenge the decision-making accuracy of a single connected and autonomous vehicle (CAV) due to its limited sensing and computing capabilities.

Using Internet of Vehicles (IoV) to share driving rules between CAVs can break limitations of a single CAV, but at the same time may cause privacy and safety issues.
To tackle this problem, this paper proposes to combine IoV and blockchain technologies to form an efficient and accurate autonomous guidance strategy.
Specifically, we first use reinforcement learning for driving decision learning, and give the corresponding driving rule extraction method.
Then, an architecture combining IoV and blockchain is designed to ensure secure driving rule sharing.
Finally, the shared rules will form an effective autonomous driving guidance strategy through driving rules selection and action selection.
Extensive simulation proves that the proposed strategy performs well in complex traffic environment, mainly in terms of accuracy, safety, and robustness.

title: Adaptive Stress Testing of Trajectory Planning Systems
abstract: Trajectory planners play a critical role in many autonomous vehicle systems, such as unmanned aircraft and driverless cars.
However, analyzing the safety of these systems is very challenging due to their complexity.
Existing validation approaches are generally focused on the correctness of the plans in isolation, while ignoring that the planner may not be able to generate a valid plan at all due to unanticipated operational conditions.

While large scale simulation testing can find some failures, they are extremely unlikely to uncover complex failure modes due to the complexity of the system and the rarity of failures.
This paper proposes to use adaptive stress testing to efficiently search for failure scenarios.
Adaptive stress testing is a black box testing method that uses reinforcement learning to sample paths from a simulator of the system under test and its operating environment.
The algorithm adversarially optimizes the stochastic elements in the system's environment to find the most likely sequence of states that results in a failure event.
We provide a detailed discussion of sources of uncertainty and failure modes in trajectory planning systems.
We apply our approach to stress test a prototype trajectory planner and show that adaptive stress testing can find a variety of operational failures including collisions with obstacles and various planning failures.


title: An efficient planning method based on deep reinforcement learning with hybrid actions for autonomous driving on highway
abstract: Due to the complexity and uncertainty of the traffic, planning for autonomous driving (AD) on highway is challenging.
Traditional planning algorithms have the problems of low and unstable efficiency, which reduces the real-time performance of the autonomous vehicle (AV).
Deep reinforcement learning (DRL) is an emerging and promising method that has achieved amazing performance in many fields.
In this paper, we propose a novel planning approach based on soft actor critic (SAC) with hybrid actions.
The algorithm takes the structured information of the ego vehicle and the surroundings as input, and generates a termination state on the Frenet space for ego vehicle, then a feasible and continuous spatiotemporal trajectory will be output by a polynomial planner based on the intermediate state.

Different from other sampling-based planning methods, only single polynomial planning is required, which improves planning efficiency significantly.
Experiments show that DRL agent with hybrid actions is more secure than the agents with only continuous or discrete actions.
Compared with other planning methods, the proposed algorithm has the least and most robust time for planning in different scenarios.

title: A DQN-based Autonomous Car-following Framework Using RGB-D Frames
abstract: Modeling car-following behavior has recently garnered much attention due to the wide variety of applications it may be utilized in, such as accident analysis, driver assessment, and support systems.
Some of the latest approaches investigate scenario-based autonomous driving algorithms.
In this paper, we propose an end-to-end car-following framework that, based on high dimensional RGB-D features only, it ensures autonomous driving by following the actions of a leader car while taking into account other environmental factors (e.
g.

pedestrians, sidewalk crashing, etc.)
To this end, a reinforcement learning (RL) algorithm, precisely an improved Deep Q-Network algorithm, is designed to avoid crashes with the leader car and its detection loss while effectively driving on road.

The model is trained and tested using the CARLA simulator in different environments.
Our preliminary tests show promising results for enhancing the driving capabilities of autonomous vehicles in many situations such as highways, one-way roads, and no-overtaking roads.

title: Decision-Making Based on Reinforcement Learning and Model Predictive Control Considering Space Generation for Highway On-Ramp Merging
abstract: Reducing traffic accidents pertaining to autonomous vehicles has garnered attention.
Merging on a highway is one of the most challenging problems that must be addressed for the realization of autonomous vehicles.
It is difficult because an agent must decide where to merge in a complex and everchanging environment.
Merging with congested highway traffic involves significant interaction with vehicles in the main lane.
If there is no space for the autonomous vehicle to merge, it needs to work on vehicles in the main lane to create space and subsequently decide to merge or not.
Reinforcement learning (RL) is a promising method for solving decision -making problems.
However, it is difficult to guarantee the safety of the controller obtained using RL.
Therefore, we propose a combined method in which decision -making is performed by RL and vehicle control by model predictive control (MPC) to ensure safety.
The performance of the proposed system is tested by simulations.
The proposed system made appropriate decisions according to the situation, and by controlling the vehicle in consideration of collision avoidance constraints, it showed a high merge success rate even in a crowded situation.

Copyright 2022 The Authors.
This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)

title: A deep reinforcement-learning-based driving policy for autonomous road vehicles [arXiv]
abstract: In this work we consider the problem of path planning for an autonomous vehicle that moves on a freeway.
The most common approaches that are used to address this problem are based on optimal control methods, which make assumptions about the model of the environment and the system dynamics.
On the contrary, we propose the development of a driving policy based on reinforcement learning.
In this way, the proposed driving policy makes minimal or no assumptions about the environment, since a priori knowledge about the system dynamics is not required.
We consider driving scenarios where the road is occupied both by autonomous and manual driving vehicles.
To the best of our knowledge, this is one of the first approaches that propose a reinforcement learning driving policy for mixed driving environments.
The derived reinforcement learning policy, firstly, is compared against an optimal policy derived via dynamic programming, and, secondly, its efficiency is evaluated under realistic scenarios generated by the established SUMO microscopic traffic flow simulator.

Finally, we present some initial results regarding the effect of autonomous vehicles' behavior on the overall traffic flow.

title: Optimal synchronization in pulse-coupled oscillator networks using reinforcement learning
abstract: Spontaneous synchronization is ubiquitous in natural and man-made systems.
It underlies emergent behaviors such as neuronal response modulation and is fundamental to the coordination of robot swarms and autonomous vehicle fleets.
Due to its simplicity and physical interpretability, pulse-coupled oscillators has emerged as one of the standard models for synchronization.
However, existing analytical results for this model assume ideal conditions, including homogeneous oscillator frequencies and negligible coupling delays, as well as strict requirements on the initial phase distribution and the network topology.

Using reinforcement learning, we obtain an optimal pulse-interaction mechanism (encoded in phase response function) that optimizes the probability of synchronization even in the presence of nonideal conditions.

For small oscillator heterogeneities and propagation delays, we propose a heuristic formula for highly effective phase response functions that can be applied to general networks and unrestricted initial phase distributions.

This allows us to bypass the need to relearn the phase response function for every new network.

title: Lane Changing Trajectory Planning of Autonomous Vehicle Based on Driving Characteristic Learning
abstract: Large deviation between vehicle planning trajectory and driver decision trajectory exists in the process of lane change for autonomous vehicles.
To solve this problem,a lane change trajectory planning algorithm is developed based on learning trajectory feature.
Based on the sampling and cost optimization combination of trajectory planning,the algorithm collects the driver's lane changing trajectory function characteristics.
By means of the maximum entropy inverse reinforcement learning,cost function weight is updated iteratively.
According to the achieved cost function,the alternative sampling paths are designated to generate lane changing trajectory of autonomous vehicles which reflect the characteristics of drivers' trajectories.
The experimental results show that the lane changing trajectory after learning of drivers' characteristics are incorporated in the lane changing trajectory area of the driver.
The trajectory's features are more similar to the real lane changing trajectory's features of the driver,and can reflect driver's subjective feeling.


title: An Autonomous Vehicle-Following Technique for Self-Driving Cars Based on the Semantic Segmentation Technique
abstract: Self-driving cars are an increasingly concerned and researched popular field.
In this paper, we utilize the CARLA (Car Learning to Act) self-driving car simulator to build a custom simulation environment.
The adopted environment consists of a figure-8-shaped driveway with two lanes, where the training car will follow a lead reference car with a varying speed on the inner lane and multiple reference cars will intermittently cut into the inner lane from the outer lane so as to interfere with the training car.

This setup aims to more accurately emulate real-world traffic conditions.
Specifically, we employ the Convolutional Neural Network (CNN) combined with the Deep Reinforcement Learning (DRL) technology to achieve autonomous control of the self-driving car.
A semantic segmentation camera is installed beside the rearview mirror of the training car to capture the observation image of the road ahead while the car is moving, which is then fed into the DRL models for training and decision making, along with the car speed.

Additionally, we design an appropriate reward mechanism for our models according to the current situation of the self-driving car to improve driving safety and comfort.
We adopt the Deep Deterministic Policy Gradient (DDPG) and time-cycling Recurrent Deterministic Policy Gradient (RDPG) learning algorithms in DRL to train the self-driving car, aiming to achieve the goal of autonomously determining safe and comfortable driving paths while following other vehicles.


title: Integration of Robust Control with Reinforcement Learning for Safe Autonomous Vehicle Motion
abstract: This paper presents a control design framework for the integration of robust control and reinforcement learning-based (RL) control agent.
The proposed integration method is applied for motion control of autonomous road vehicles, providing safe motion.
In the integrated motion control, longitudinal and lateral dynamics are incorporated.
The high-performance motion of the vehicle, e.g., high-velocity motion, path following, and reduction of lateral acceleration, through the RL-based control agent is achieved.
The training through Proximal Policy Optimization during episodes is performed.
Safe motion with guaranteed performances, i.e., keeping limits on lateral error, through the robust control and the supervisor is achieved.
The robust control is designed through the H method, and in the supervisor, a constrained quadratic programming task is performed.
As a result, lateral and longitudinal control inputs of the vehicle are calculated by the integrated control system.
The effectiveness of the proposed control method using simulation scenarios and test scenarios on a small-scaled test vehicle is illustrated.
All rights reserved Elsevier.

title: Online Generation of Trajectories for Autonomous Vehicles using a Multi-Agent System
abstract: Autonomous vehicles are frequently deployed in environments where only certain trajectories are feasible.
Classical trajectory generation methods attempt to find a feasible trajectory that satisfies a set of constraints.
In some cases the optimal trajectory may be known, but it is hidden from the autonomous vehicle.
Under such circumstance the vehicle must discover a feasible trajectory.
This paper describes a multi-agent system that uses a combination of reinforcement learning and differential evolution to generate a trajectory that is epsilon-close to a target trajectory that is hidden.


title: Autonomous Driving Control Based on the Technique of Semantic Segmentation
abstract: Advanced Driver Assistance Systems (ADAS) are only applied to relatively simple scenarios, such as highways.
If there is an emergency while driving, the driver should take control of the car to deal properly with the situation at any time.
Obviously, this incurs the uncertainty of safety.
Recently, in the literature, several studies have been proposed for the above-mentioned issue via Artificial Intelligence (AI).
The achievement is exactly the aim that we look forward to, i.e., the autonomous vehicle.
In this paper, we realize the autonomous driving control via Deep Reinforcement Learning (DRL) based on the CARLA (Car Learning to Act) simulator.
Specifically, we use the ordinary Red-Green-Blue (RGB) camera and semantic segmentation camera to observe the view in front of the vehicle while driving.
Then, the captured information is utilized as the input for different DRL models so as to evaluate the performance, where the DRL models include DDPG (Deep Deterministic Policy Gradient) and RDPG (Recurrent Deterministic Policy Gradient).

Moreover, we also design an appropriate reward mechanism for these DRL models to realize efficient autonomous driving control.
According to the results, only the RDPG strategies can finish the driving mission with the scenario that does not appear/include in the training scenario, and with the help of the semantic segmentation camera, the RDPG control strategy can further improve its efficiency.


title: Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems
abstract: Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams.
Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers.
However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks.
In this work, we explore the backdooring/trojanning of DRL-based AV controllers.
We develop a trigger design methodology that is based on well-established principles of traffic physics.
The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack).

We test our attack on single-lane and two-lane circuits.
Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%.
Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.

title: Zero-Shot Autonomous Vehicle Policy Transfer: From Simulation to Real-World via Adversarial Learning
abstract: In this article, we demonstrate a zero-shot transfer of an autonomous driving policy from simulation to University of Delaware's scaled smart city with adversarial multi-agent reinforcement learning, in which an adversary attempts to decrease the net reward by perturbing both the inputs and outputs of the autonomous vehicles during training.

We train the autonomous vehicles to coordinate with each other while crossing a roundabout in the presence of an adversary in simulation.
The adversarial policy successfully reproduces the simulated behavior and incidentally outperforms, in terms of travel time, both a human-driving baseline and adversary-free trained policies.
Finally, we demonstrate that the addition of adversarial training considerably improves the performance of the policies after transfer to the real world compared to Gaussian noise injection.

title: A Comparative Analysis of Reinforcement Learning Activation Functions for Parking of Autonomous Vehicles
abstract: Autonomous vehicles, which can dramatically solve the lack of parking spaces, are making great progress through deep reinforcement learning.
Activation functions are used for deep reinforcement learning, and various activation functions have been proposed, but their performance deviations were large depending on the application environment.

Therefore, finding the optimal activation function depending on the environment is important for effective learning.
This paper analyzes 12 functions mainly used in reinforcement learning to compare and evaluate which activation function is most effective when autonomous vehicles use deep reinforcement learning to learn parking.

To this end, a performance evaluation environment was established, and the average reward of each activation function was compared with the success rate, episode length, and vehicle speed.
As a result, the highest reward was the case of using GELU, and the ELU was the lowest.
The reward difference between the two activation functions was 35.2%.

title: Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions Using Reinforcement Learning
abstract: The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure.
In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed.
Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand.

Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions.
We implement a centralised learning paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations.
Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians.
Furthermore, the distributive learning algorithm outperforms its centralised counterpart regarding computational cost (49.
55%), benchmark rewards (25.
35%), best cumulative rewards (24.
58%), optimal actions (13.
49%) and rate of convergence.

This novel road management technique could potentially contribute to the flow-adaptive and active mobility-friendly streets in the AVs era.

title: Car-following strategy of intelligent connected vehicle using extended disturbance observer adjusted by reinforcement learning
abstract: Disturbance observer-based control method has achieved good results in the car-following scenario of intelligent and connected vehicle (ICV).
However, the gain of conventional extended disturbance observer (EDO)-based control method is usually set manually rather than adjusted adaptively according to real time traffic conditions, thus declining the car-following performance.

To solve this problem, a car-following strategy of ICV using EDO adjusted by reinforcement learning is proposed.
Different from the conventional method, the gain of proposed strategy can be adjusted by reinforcement learning to improve its estimation accuracy.
Since the "equivalent disturbance" can be compensated by EDO to a great extent, the disturbance rejection ability of the car-following method will be improved significantly.
Both Lyapunov approach and numerical simulations are carried out to verify the effectiveness of the proposed method.

title: Video representation learning for decoupled deep reinforcement learning applied to autonomous driving
abstract: This work focuses on using Deep Reinforcement Learning (DRL) to control an autonomous vehicle in the hyper-realistic urban simulation LGSVL.
Classical control systems such as MPC maneuver vehicles based on a given trajectory, current velocity, position, distances, and more.
Our approach does not pass this information to the DRL agent but only images provided by the camera.
Current DRL efforts also exploit similar approaches for autonomous driving, but they are only suitable for small, simple tasks using simple simulations.
Our approach consists of two differently trained neural networks (NN), a perceptual NN for representation learning and an actor NN for selecting the correct action.
The perception NN will be trained via representation and self-supervised learning to strengthen our DRL agent's understanding of the scene.
It can recognize temporal information and the dynamics of a complex environment.
This work shows the importance of decoupling the perception and decision (actor) model for autonomous driving.
All in all, we could drive autonomously in a hyper-realistic urban simulation using our modular DRL framework.
Moreover, our approach also provides a solution for other similar tasks in the field of robotics based on images.

title: Review of Autonomous Driving Decision-Making Research Based on Reinforcement Learning
abstract: Decision-making technology of autonomous vehicle is promoted by the development of reinforcement learning, and intelligent decision-making technology has become a key issue of high concern in the field of autonomous driving.

Taking the development of reinforcement learning algorithm as the main line in this paper, the indepth application of this algorithm in the field of single-car autonomous driving decision-making is summarized.

Traditional reinforcement learning algorithms, classic algorithms and frontier algorithms are summarized and compared from the aspect of basic principles and theoretical modeling methods.
According to the classification of autonomous driving decision-making methods in different scenarios, the impact of environmental state observability on modeling is analyzed, and the application technology routes of typical reinforcement learning algorithms at different levels are emphasized.

The research prospects for the autonomous driving decision-making method are proposed in order to provide a useful reference for the research of autonomous driving decision-making.

title: Scalable Model-based Policy Optimization for Decentralized Networked Systems
abstract: Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks.
Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly, requiring communications or shifting or resources.
This work aims to improve data efficiency of multi-agent control by model-based learning.
We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO).
In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts.
To alleviate the bias of model-generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation.
To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients.
We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traffic signal control (ATSC).
Empirical results show that our method achieves superior data efficiency and matches the performance of model-free methods using true models.
The source code of our algorithm and baselines can be found at https://github.
com/PKU-MARL/Model-Based-MARL.


title: Mobility-Aware Partial Computation Offloading in Vehicular Networks: A Deep Reinforcement Learning Based Scheme
abstract: Encouraged by next-generation networks and autonomous vehicle systems, vehicular networks must employ advanced technologies to guarantee personal safety, reduce traffic accidents and ease traffic jams.

By leveraging the computing ability at the network edge, multi-access edge computing (MEC) is a promising technique to tackle such challenges.
Compared to traditional full offloading, partial offloading offers more flexibility in the perspective of application as well as deployment of such systems.
Hence, in this paper, we investigate the application of partial computing offloading in-vehicle networks.
In particular, by analyzing the structure of many emerging applications, e.g., AR and online games, we convert the application structure into a sequential multi-component model.
Focusing on shortening the application execution delay, we extend the optimization problem from the single-vehicle computing offloading (SVCOP) scenario to the multi-vehicle computing offloading (MVCOP) by taking multiple constraints into account.

A deep reinforcement learning (DRL) based algorithm is proposed as a solution to this problem.
Various performance evaluation results have shown that the proposed algorithm achieves superior performance as compared to existing offloading mechanisms in deducing application execution delay.

title: Vision-Based Autonomous Driving: A Model Learning Approach
abstract: We present an integrated approach for perception and control for an autonomous vehicle and demonstrate this approach in a high-fidelity urban driving simulator.
Our approach first builds a model for the environment, then trains a policy exploiting the learned model to identify the action to take at each time-step.
To build a model for the environment, we leverage several deep learning algorithms.
To that end, first we train a variational autoencoder to encode the input image into an abstract latent representation.
We then utilize a recurrent neural network to predict the latent representation of the next frame and handle temporal information.
Finally, we utilize an evolutionary-based reinforcement learning algorithm to train a controller based on these latent representations to identify the action to take.
We evaluate our approach in CARLA, a high-fidelity urban driving simulator, and conduct an extensive generalization study.
Our results demonstrate that our approach outperforms several previously reported approaches in terms of the percentage of successfully completed episodes for a lane keeping task.

title: Safe Reinforcement Learning-based Driving Policy Design for Autonomous Vehicles on Highways
abstract: Safe decision-making strategy of autonomous vehicles (AVs) plays a critical role in avoiding accidents.
This study develops a safe reinforcement learning (safe-RL)-based driving policy for AVs on highways.
The hierarchical framework is considered for the proposed safe-RL, where an upper layer executes a safe exploration-exploitation by modifying the exploring process of the epsilon-greedy algorithm, and a lower layer utilizes a finite state machine (FSM) approach to establish the safe conditions for state transitions.

The proposed safe-RL-based driving policy improves the vehicle's safe driving ability using a Q-table that stores the values corresponding to each action state.
Moreover, owing to the trade-off between the epsilon-greedy values and safe distance threshold, the simulation results demonstrate the superior performance of the proposed approach compared to other alternative RL approaches, such as the epsilon-greedy Q-learning (GQL) and decaying epsilon-greedy Q-learning (DGQL), in an uncertain traffic environment.

This study's contributions are twofold: it improves the autonomous vehicle's exploration-exploitation and safe driving ability while utilizing the advantages of FSM when surrounding cars are inside safe-driving zones, and it analyzes the impact of safe-RL parameters in exploring the environment safely.


title: Adaptive Stochastic Nonlinear Model Predictive Control with Look-ahead Deep Reinforcement Learning for Autonomous Vehicle Motion Control [arXiv]
abstract: In this paper, we present a Deep Reinforcement Learning (RL)-driven Adaptive Stochastic Nonlinear Model Predictive Control (SNMPC) to optimize uncertainty handling, constraints robustification, feasibility, and closed-loop performance.

To this end, we conceive an RL agent to proactively anticipate upcoming control tasks and to dynamically determine the most suitable combination of key SNMPC parameters - foremost the robustification factor $\kappa$ and the Uncertainty Propagation Horizon (UPH) $T_u$.

We analyze the trained RL agent's decision-making process and highlight its ability to learn context-dependent optimal parameters.
One key finding is that adapting the constraints robustification factor with the learned policy reduces conservatism and improves closed-loop performance while adapting UPH renders previously infeasible SNMPC problems feasible when faced with severe disturbances.

We showcase the enhanced robustness and feasibility of our Adaptive SNMPC (aSNMPC) through the real-time motion control task of an autonomous passenger vehicle to follow an optimal race line when confronted with significant time-variant disturbances.

Experimental findings demonstrate that our look-ahead RL-driven aSNMPC outperforms its Static SNMPC (sSNMPC) counterpart in minimizing the lateral deviation both with accurate and inaccurate disturbance assumptions and even when driving in previously unexplored environments.


title: Trajectory Planning for Autonomous Vehicles Using Hierarchical Reinforcement Learning [arXiv]
abstract: Planning safe trajectories under uncertain and dynamic conditions makes the autonomous driving problem significantly complex.
Current sampling-based methods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this problem because of the high computational cost.
Supervised learning methods such as Imitation Learning lack generalization and safety guarantees.
To address these problems and in order to ensure a robust framework, we propose a Hierarchical Reinforcement Learning (HRL) structure combined with a Proportional-Integral-Derivative (PID) controller for trajectory planning.

HRL helps divide the task of autonomous vehicle driving into sub-goals and supports the network to learn policies for both high-level options and low-level trajectory planner choices.
The introduction of sub-goals decreases convergence time and enables the policies learned to be reused for other scenarios.
In addition, the proposed planner is made robust by guaranteeing smooth trajectories and by handling the noisy perception system of the ego-car.
The PID controller is used for tracking the waypoints, which ensures smooth trajectories and reduces jerk.
The problem of incomplete observations is handled by using a Long-Short-Term-Memory (LSTM) layer in the network.
Results from the high-fidelity CARLA simulator indicate that the proposed method reduces convergence time, generates smoother trajectories, and is able to handle dynamic surroundings and noisy observations.


title: Decision-Making in Fallback Scenarios for Autonomous Vehicles: Deep Reinforcement Learning Approach
abstract: This paper proposes a decision-making algorithm based on deep reinforcement learning to support fallback techniques in autonomous vehicles.
The fallback technique attempts to mitigate or escape risky driving conditions by responding to appropriate avoidance maneuvers essential for achieving a Level 4+ autonomous driving system.
However, developing a fallback technique is difficult because of the innumerable fallback situations to address and eligible optimal decision-making among multiple maneuvers.
We employed a decision-making algorithm utilizing a scenario-based learning approach to address these issues.
First, we crafted a specific fallback scenario encompassing the challenges to be addressed and matched the anticipated optimal maneuvers as determined by heuristic methods.
In this scenario, the ego vehicle learns through trial and error to determine the most effective maneuver.
We conducted 100 independent training sessions to evaluate the proposed algorithm and compared the results with those of heuristic-derived maneuvers.
The results were promising; 38% of the training sessions resulted in the vehicle learning lane-change maneuvers, whereas 9% mastered slow following.
Thus, the proposed algorithm successfully learned human-equivalent fallback capabilities from scratch within the provided scenario.

title: Intelligent Resource Allocation Based on Reinforcement Learning for NOMA Vehicular Cooperative Communication Networks
abstract: There exits difficulty in environment sensing for autonomous vehicles in Internet of vehicles (IoV) networks.
The autonomous vehicle can obtain a full sense of the environment from connected vehicles by vehicle-to-vehicle (V2V) communications and make better decisions for driving safety.
However, limited spectrum resources cannot adapt to the increasing number of communication links and the traditional frequency multiplexing will lead to serious co-frequency interference and fail to connect more vehicles.

Therefore, we develop a resource allocation scheme through jointly optimizing sub-band selection and power control based on V2X Network.
Meanwhile, the non-orthogonal multiple access (NOMA) technology is adopted for multi-V2V communication that shares the spectrum allocated to vehicle-to-infrastructure (V2I) links.
Considering time-varying channel conditions caused by vehicle movement and power control over a continuous range in a vehicular environment, we investigate a deep deterministic policy gradient algorithm to maximize the sum transmission rate of the V2I links and V2V links while ensuring the quality of service of V2V links.

Simulation results demonstrate the proposed solution can effectively optimize the total throughput.

title: Reinforcement Learning with Non-uniform State Representations for Adaptive Search
abstract: Efficient spatial exploration is a key aspect of search and rescue.
In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher.
This should allow an autonomous vehicle find one or more lost targets as rapidly as possible.
We do this by performing non-uniform sampling of the search region.
The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning.
We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target.
Key features of our search algorithm are the ability to employ a very general nondeterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions.

One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics.
We compare the paths generated by our algorithm with other accepted spatial coverage techniques such as distribution independent boustrophedonic coverage and model dependent spiral search.
We present a proof showing that rewarding for clearing probability mass instead of locating the target does not bias the objective function.
The experiments show that the learned policy outperforms several well-known baselines even in scenarios different from the one it has been trained on.

title: ReLearner: a reinforcement learning-based self driving car model using gym environment
abstract: In the recent past, Artificial intelligence and its sister technology such as Machine Learning, Deep Learning, and Reinforcement learning have grown rapidly in several applications.
The self-driving car is one of the applications, which is the need of the hour.
In this paper, we describe the trends in autonomous vehicle technology for the self-driving car.
There are many different approaches to mathematically formulate a design for the self-driving car such as deep Q-learning, Q-learning, and machine learning.
However, in this paper, we propose a very basic and less compute-intensive simplistic self-driving car model called "ReLearner" using the Gym environment.
To simulate the self-driving car model, we preferred to create a simple small environment OpenAi gym which is a deterministic environment.
The OpenAi gym provides the virtual simulation environment and parameter tuning to train and test the model.
We have focused on two methods to test our model.
The basic approach is to compare the performance of the car when tested using Q-Learning and another using a random action agent, i.e., No reinforcement learning.
We have derived a theoretical model and analyzed how to use Q-learning to train cars to drive.
We have carried out a simulation and on evaluating the performance and found that Q-learning is a more optimal approach to solve the issue of a self-driving car.

title: Robust Deep Reinforcement Learning for Security and Safety in Autonomous Vehicle Systems
abstract: The dependence of autonomous vehicles (AVs) on sensors and communication links exposes them to cyber-physical (CP) attacks by adversaries that seek to take control of the AVs by manipulating their data.

In this paper, the state estimation process for monitoring AV dynamics, in presence of CP attacks, is analyzed and a novel adversarial deep reinforcement learning (RL) algorithm is proposed to maximize the robustness of AV dynamics control to CP attacks.

The attacker's action and the AV's reaction to CP attacks are studied in a game-theoretic framework.
In the formulated game, the attacker seeks to inject faulty data to AV sensor readings so as to manipulate the inter-vehicle optimal safe spacing and potentially increase the risk of AV accidents or reduce the vehicle flow on the roads.

Meanwhile, the AV, acting as a defender, seeks to minimize the deviations of spacing so as to ensure robustness to the attacker's actions.
Since the AV has no information about the attacker's action and due to the infinite possibilities for data value manipulations, each player uses long short term memory (LSTM) blocks to learn the expected spacing deviation resulting from its own action and feeds this deviation to a reinforcement learning (RL) algorithm.

Then, the attacker's RL algorithm chooses the action which maximizes the spacing deviation, while the AV's RL algorithm seeks to find the optimal action that minimizes such deviation.
Simulation results show that the proposed adversarial deep RL algorithm can improve the robustness of the AV dynamics control as it minimizes the intra-AV spacing deviation.

title: Hierarchical Driving Strategy for Connected and Autonomous Vehicles Making a Protected Left Turn at Signalized Intersections
abstract: Left-turn execution in autonomous driving at urban intersections is often complex and characterized by unpredicted events, such as vehicles speeding and running a red light.
Despite these hazards, autonomous vehicles must drive through intersections safely and efficiently.
To solve this problem, a new hierarchical driving strategy (HDS) is proposed for connected and autonomous vehicles making a protected left turn at signalized intersections, which combines the rule-based method and deep reinforcement learning (DRL).

The high level of the HDS is the rule-based decision-making module, and the low level is the driving skill, which is dependent on the status.
Specifically, the vehicle status when turning left is divided into safe and alert statuses.
Further, DRL is used to train the driving skill of the vehicle for each status.
The HDS design effectively combines the advantages of end-to-end driving.
Moreover, the algorithm was experimentally evaluated in multiple protected left-turn scenarios.
Compared with the pure rule-based method, the HDS achieved a 34% reduction in failure rate in the test scenario, and the driving behavior of the autonomous vehicle employing HDS was more intelligent.
Moreover, the HDS is highly robust to complex scenarios.

title: A Reinforcement Learning Approach to Autonomous Decision Making of Intelligent Vehicles on Highways
abstract: Autonomous decision making is a critical and difficult task for intelligent vehicles in dynamic transportation environments.
In this paper, a reinforcement learning approach with value function approximation and feature learning is proposed for autonomous decision making of intelligent vehicles on highways.
In the proposed approach, the sequential decision making problem for lane changing and overtaking is modeled as a Markov decision process with multiple goals, including safety, speediness, smoothness, etc.

In order to learn optimized policies for autonomous decision-making, a multiobjective approximate policy iteration (MO-API) algorithm is presented.
The features for value function approximation are learned in a data-driven way, where sparse kernel-based features or manifold-based features can be constructed based on data samples.
Compared with previous RL algorithms such as multiobjective Q-learning, the MO-API approach uses data-driven feature representation for value and policy approximation so that better learning efficiency can be achieved.

A highway simulation environment using a 14 degree-of-freedom vehicle dynamics model was established to generate training data and test the performance of different decision-making methods for intelligent vehicles on highways.

The results illustrate the advantages of the proposed MO-API method under different traffic conditions.
Furthermore, we also tested the learned decision policy on a real autonomous vehicle to implement overtaking decision and control under normal traffic on highways.
The experimental results also demonstrate the effectiveness of the proposed method.

title: DeepCollision: Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions
abstract: With the aim to test autonomous driving systems, we propose a novel reinforcement learning (RL)-based approach named DeepCollision to learn operating environment configurations of autonomous vehicles, including formalizing environment configuration learning as an MDP and adopting DQN algorithm as the RL solution;DeepCollisionlearns environment configurations to maximize collisions of an Autonomous Vehicle Under Test (AVUT).

This dataset contains: algorithms- The algorithm of DeepCollision, which includes the network architecture and the DQN hyperparameter settings; pilot-study- All the raw data and plots for the pilot study; formal-experiment- A dataset contains all the raw data for analysis and the scenarios with detailed demand values; rest-api- The REST API endpoints for environment configuration and oneexampleto show the usage of the APIs.

Copyright: Creative Commons Attribution 4.0 International Open Access

title: Simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles [arXiv]
abstract: Using deep reinforcement learning, we train control policies for autonomous vehicles leading a platoon of vehicles onto a roundabout.
Using Flow, a library for deep reinforcement learning in micro-simulators, we train two policies, one policy with noise injected into the state and action space and one without any injected noise.
In simulation, the autonomous vehicle learns an emergent metering behavior for both policies in which it slows to allow for smoother merging.
We then directly transfer this policy without any tuning to the University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for connected and automated vehicles.
We characterize the performance of both policies on the scaled city.
We show that the noise-free policy winds up crashing and only occasionally metering.
However, the noise-injected policy consistently performs the metering behavior and remains collision-free, suggesting that the noise helps with the zero-shot policy transfer.
Additionally, the transferred, noise-injected policy leads to a 5% reduction of average travel time and a reduction of 22% in maximum travel time in the UDSSC.
Videos of the controllers can be found at https://sites.google.com/view/iccps-policy-transfer.

title: Lane Change Decision Control of Autonomous Vehicle Based on A3C Algorithm
abstract: Coordinating the lateral and longitudinal control of vehicles during lane changing, while considering the vehicle's operational state and the surrounding environment, poses a highly challenging task.
In recent years, deep reinforcement learning (DRL) technology has experienced rapid development and has found widespread applications in the traditional automatic control industry.
With the improvement of driving safety requirements, DRL technology provides a new research direction and an effective way for the development of vehicle autonomous lane change.
This paper investigates automated decision control for lane changing in autonomous vehicles using the Asynchronous Advantage Actor-Critic (A3C) algorithm, while also proposing reasonable multi-objective performance evaluation metrics.

Nevertheless, the traditional A3C algorithm frequently encounters convergence oscillation or degradation issues, hindering agents from attaining the highest reward.
To address the mentioned issues, an improved parameter updating method based on a weighted average of advantage value is proposed.
The simulation results on the highway simulation platform demonstrate that the enhanced A3C algorithm offers increased stability in comparison to the traditional A3C algorithm.
Moreover, in comparison to the Deep Q-Network (DQN) algorithm and the Deep Deterministic Policy Gradient (DDPG) algorithm, the enhanced A3C algorithm showcases faster convergence speed and a higher success rate, hence confirming the superiority of the proposed improvement.


title: Machine Learning for Cooperative Driving in a Multi-Lane Highway Environment
abstract: Most of the research in automated driving currently involves using the on-board sensors on the vehicle to collect information regarding surrounding vehicles to maneuver around them.
In this paper we discuss how information communicated through vehicular networking can be used for controlling an autonomous vehicle in a multi-lane highway environment.
A driving algorithm is designed using deep Q learning, a type of reinforcement learning.
In order to train and test driving algorithms, we deploy a simulated traffic system, using SUMO (Simulation of Urban Mobility).
The performance of the driving algorithm is tested for perfect knowledge regarding surrounding vehicles.
Furthermore, the impact of limited communication range and random packet loss is investigated.
Currently the performance of the driving algorithm is far from ideal with the collision ratios being quite high.
We propose directions for additional research to improve the performance of the algorithm.

title: Model-based deep reinforcement learning for CACC in mixed-autonomy vehicle platoons
abstract: This paper proposes a model-based deep reinforcement learning (DRL) algorithm for cooperative adaptive cruise control (CACC) of connected vehicles.
Differing from most existing CACC works, we consider a platoon consisting of both human-driven and autonomous vehicles.
The human-driven vehicles are heterogeneous and connected via vehicle-to-vehicle (V2V) communication and the autonomous vehicles are controlled by a cloud-based centralized DRL controller via vehicle-to-cloud (V2C) communication.

To overcome the safety and robustness issues of RL, the algorithm informs lower-level controllers of desired headway signals instead of directly controlling vehicle accelerations.
The lower-level behavior is modeled according to the optimal velocity model (OVM), which determines vehicle acceleration according to a headway input.
Numerical experiments show that the model-based DRL algorithm outperforms its model-free version in both safety and stability of CACC.
Furthermore, we study the impact of different penetration ratios of autonomous vehicles on the safety, stability, and optimality of the CACC policy.

title: Video Representation Learning for Decoupled Deep Reinforcement Learning Applied to Autonomous Driving
abstract: This work focuses on using Deep Reinforcement Learning (DRL) to control an autonomous vehicle in the hyper-realistic urban simulation LGSVL.
Classical control systems such as MPC maneuver vehicles based on a given trajectory, current velocity, position, distances, and more.
Our approach does not pass this information to the DRL agent but only images provided by the camera.
Current DRL efforts also exploit similar approaches for autonomous driving, but they are only suitable for small, simple tasks using simple simulations.
Our approach consists of two differently trained neural networks (NN), a perceptual NN for representation learning and an actor NN for selecting the correct action.
The perception NN will be trained via representation and self-supervised learning to strengthen our DRL agent's understanding of the scene.
It can recognize temporal information and the dynamics of a complex environment.
This work shows the importance of decoupling the perception and decision (actor) model for autonomous driving.
All in all, we could drive autonomously in a hyperrealistic urban simulation using our modular DRL framework.
Moreover, our approach also provides a solution for other similar tasks in the field of robotics based on images.

title: Defining metrics for scenario-based evaluation of autonomous vehicle models
abstract: The paper deals with the evaluation of autonomous vehicles along with the quantification of their behavior and maneuvers.
The article outlines the positive aspects of autonomy and lists several arguments in their favor, e.g.
convenience and efficiency considerations.
Furthermore, it also addresses the associated difficulties including the feasibility of road testing and the establishment of appropriate simulations.
The current work aims to define methods providing objective indicators to compare algorithms solving the complex tasks of road transport.
Rule-based, supervised and reinforcement learning control models, test environments, accelerated test methods and assessment indicators of the corresponding literature are reviewed and evaluated.
After investigating the different metrics, we formulate an evaluation framework that can be applied in the development and assessment process of new artificial intelligence controlled models.
As an outcome of this work, we aim to aid a missing sector in the field of autonomous driving function development by collecting and defining metrics that intend to help qualitatively evaluate and compare algorithms.

The key aspect during the definition of the suggested method was to ensure its extensive applicability by selecting only metrics that can be obtained from the already installed sensors of the vehicles.

Additionally, we also assess multiple agents to observe how their behavior compares and whether the proposed metrics reflect the expected behavior.

title: Zero-Sum Game (ZSG) based Integral Reinforcement Learning for Trajectory Tracking Control of Autonomous Smart Car
abstract: The ultimate aim of our research study is the development, practical implementation, and benchmarking of continuous-time, online reinforcement learning (RL) schemes for the trajectory tracking control (TTC) of fully autonomous vehicles (AVs) in real-world scenarios.

The adaptive optimality and model-free nature offered by RL has a stronger promise against its model-based counterparts, such as MPC, against uncertainties related to the vehicle, road, tire-terrain and environmental dynamics.

The existing studies on RL based AV control are mostly theoretical, often dealing with high-level TTC, and perform evaluations in simulations considering simplified or linear models with no disturbance and slip effects.

The literature also demonstrates the lack of practical implementations in overall RL based autonomous vehicle control.
Our ultimate goal is to fill these theoretical and practical gaps by designing and practically evaluating novel RL strategies that will improve the performance of TTC against uncertainties at all levels.

This paper presents the simulation results of our preliminary studies in the online, longitudinal tracking control of a realistic AV (with uncertain nonlinear dynamics, as well as disturbance, and slip effects), which we treat as a Zero-Sum Game (ZSG) problem using an Integral Reinforcement Learning (IRL) approach with synchronous actor and critic updates (SyncIRL).

The results are promising and motivate the practical implementation of the approach for combined longitudinal and lateral control of AV.

title: An approximate dynamic programming approach for solving an air combat maneuvering problem
abstract: Within visual range air combat involves execution of highly complex and dynamic activities, requiring rapid, sequential decision-making to achieve success.
Fighter pilots spend years perfecting tactics and maneuvers for these types of combat engagements, yet the ongoing emergence of unmanned, autonomous vehicle technologies elicits a natural question - can an autonomous unmanned combat aerial vehicle (AUCAV) be imbued with the necessary artificial intelligence to perform challenging air combat maneuvering tasks independently?
We formulate and solve the air combat maneuvering problem (ACMP) to examine this important question, developing a Markov decision process (MDP) model to control a defending AUCAV seeking to destroy an attacking adversarial vehicle.

The MDP model includes a 5-degree-of-freedom, point-mass aircraft state transition model to accurately represent both kinematics and energy while maneuvering.
An approximate dynamic programming (ADP) approach is proposed wherein we develop and test an approximate policy iteration algorithm that implements value function approximation via neural network regression to attain high-quality maneuver policies for the AUCAV.

A representative intercept scenario is specified for testing purposes wherein the AUCAV must engage and destroy an adversary aircraft attempting to penetrate the defended airspace.
Several designed experiments are conducted to determine how aircraft velocity and adversary maneuvering tactics impact the efficacy of the proposed ADP solution approach and to enable efficient algorithm parameter tuning.

ADP-generated policies are compared to two benchmark maneuver policies constructed from two reward shaping functions found in the ACMP literature, attaining improved mean probabilities of kill for 24 of 36 air combat situations considered

title: Proactive Longitudinal Control to Manage Disruptive Lane Changes of Human-Driven Vehicles in Mixed-Flow Traffic
abstract: Connected and autonomous vehicles (CAVs) can be leveraged to enable cooperative platooning control to alleviate traffic oscillations.
However, in the near future, CAVs and human-driven vehicles (HDV5) will coexist on roads, creating a mixed-flow traffic environment.
In mixed-flow traffic, CAV platoons would inevitably encounter lane changes by HDVs in adjacent lanes.
These lane changes can generate disturbances and oscillations upstream, jeopardizing the performance of platooning control.
Hence, it is necessary to explore the interactions between CAVs and HDVs in the lane -change process, to analyze how CAVs can be used to manage disruptive lane changes of HDVs in mixed-flow traffic.
This study proposes deep reinforcement learning-based proactive longitudinal control for CAVs to counteract disruptive HDV lane -change behaviors that can induce disturbances, such that the smoothness of traffic flow can be preserved in the platooning control process.

Results from numerical experiments suggest that CAVs controlled by the proposed control strategy can effectively reduce the occurrence of disruptive lane change maneuvers of HDVs to improve string stability performance in mixed-flow traffic.

Further, the reliability of the proposed control strategy for different HDV driver types is illustrated.
Copyright (c) 2021 The Authors.

title: PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning
abstract: The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution.
It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks.
The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e.
harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane.
This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles.
Specifically, we propose a velocity control framework, called PATROL (sPAtial-temporal ReinfOrcement Learning).
First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g.
velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment.
Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time.
At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB.

We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments.
Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.

title: Reinforcement-Learning-Based Cooperative Adaptive Cruise Control of Buses in the Lincoln Tunnel Corridor with Time-Varying Topology
abstract: The exclusive bus lane (XBL) is one of the most popular bus transit systems in the U.S.
The Lincoln Tunnel utilizes an XBL through the tunnel in the AM peak period.
This paper proposes a novel data-driven cooperative adaptive cruise control (CACC) algorithm that aims to minimize a cost function for connected and autonomous buses along the XBL.
Different from existing model-based CACC algorithms, the proposed approach employs the idea of reinforcement learning, which does not rely on accurate knowledge of bus dynamics.
Considering a time-varying topology, where each autonomous vehicle can only receive information from preceding vehicles that are within its communication range, a distributed controller is learned real-time by online headway, velocity, and acceleration data collected from the system trajectories.

The convergence of the proposed algorithm and the stability of the closed-loop system are rigorously analyzed.
The effectiveness of the proposed approach is demonstrated using a well-calibrated Paramics microscopic traffic simulation model of the XBL corridor.
The simulation results show that the travel time in the autonomous version of the XBL are close to the present day travel time even when the bus volume is increased by 30%.

title: Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing
abstract: World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms.
While learning world models for high-dimensional observations (e.
g.
, pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored.

In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail.
In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity.
In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization.

Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model.
We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.

title: Reinforcement Learning-Based Control of Signalized Intersections Having Platoons
abstract: Smart transportation cities are based on intelligent systems and data sharing, whereas human drivers generally have limited capabilities and imperfect traffic observations.
The perception of Connected and Autonomous Vehicle (CAV) utilizes data sharing through Vehicle-To-Vehicle (V2V) and Vehicle-To-Infrastructure (V2I) communications to improve driving behaviors and reduce traffic delays and fuel consumption.

This paper proposes a Double Agent (DA) intelligent traffic signal module based on the Reinforcement Learning (RL) method, where the first agent, the Velocity Agent (VA) aims to minimize the fuel consumption by controlling the speed of platoons and single CAVs crossing a signalized intersection, while the second agent, the Signal Agent (SA) proceeds to efficiently reduce traffic delays through signal sequencing and phasing.

Several simulation studies have been conducted for a signalized intersection with different traffic flows and the performance of the single-agent with only VA, DA with both VA and SA, and Intelligent Driver Model (IDM) are compared.

It is shown that the proposed DA solution improves the average delay by 47.3% and the fuel efficiency by 13.6% compared to the Intelligent Driver Model (IDM).

title: Jointly Learning V2X Communication and Platoon Control with Deep Reinforcement Learning
abstract: In autonomous vehicle platooning, Vehicle-to-Everything (V2X) communications are leveraged in cooperative adaptive cruise control (CACC) to improve control performance.
Since exchanging information at all times incurs significant communication overhead in vehicular networks, it is important to determine when V2X communication is necessary.
To solve this problem, we propose a Deep Reinforcement Learning (DRL)-based algorithm named Attention-DDPG, which learns platoon control with Deep Deterministic Policy Gradient (DDPG), and learns when to communicate with an attention network.

Specifically, each preceding vehicle is equipped with a deep neural network (DNN), which takes as input its local state and platoon control action and determines whether to transmit its acceleration or not to the following vehicle at each time step.

The attention network of a preceding vehicle is trained using the feedback from the following vehicle on the value of V2X information in the form of an advantage function.
In order to evaluate Attention-DDPG, simulations are performed using real driving data, and performance is compared with those of two baselines that communicate and do not communicate at all times, respectively.

The results demonstrate that Attention-DDPG strikes a competitive tradeoff between control performance and communication overhead while ensuring platoon string stability.

title: Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing
abstract: World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms.
While learning world models for high-dimensional observations (e.
g.
, pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored.

In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail.
In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity.
In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization.

Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model.
We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.

title: Trajectory Optimization of CAVs in Freeway Work Zone considering Car-Following Behaviors Using Online Multiagent Reinforcement Learning
abstract: Work zone areas are frequent congested sections considered as the freeway bottleneck.
Connected and autonomous vehicle (CAV) trajectory optimization can improve the operating efficiency in bottleneck areas by harmonizing vehicles' manipulations.
This study presents a joint trajectory optimization of cooperative lane changing, merging, and car-following actions for CAV control at a local merging point together with upstream points.
The multiagent reinforcement learning (MARL) method is applied in this system, with one agent providing a merging advisory service at the merging point and controlling the inner-lane vehicles' headway for smooth outer-lane vehicle merging, while other agents provide lane-changing advisory services at advance lane-changing points to control how vehicles make lane changes in advance and perform corresponding headway adjustment, similar to and jointly with the merging advisory service.

Uniting all agents, the coordination graph (CG) method is applied to seek the global optimum, overcoming the exponential growth problem in MARL.
Using MATLAB and the VISSIM COM interface, an online simulation platform is established.
The simulation results show that MARL is effective for online computation with in-timing response.
More importantly, comparisons of the results obtained in various scenarios demonstrate that the proposed system obtained smoother vehicle trajectories in all controlled sections, rather than only in the merging area, indicating that it can achieve better traffic conditions in freeway work zone areas.


title: Differentially Private Reward Functions for Multi-Agent Markov Decision Processes [arXiv]
abstract: Reward functions encode desired behavior in multi-agent Markov decision processes, but onlookers may learn reward functions by observing agents, which can reveal sensitive information.
Therefore, in this paper we introduce and compare two methods for privatizing reward functions in policy synthesis for multi-agent Markov decision processes.
Reward functions are privatized using differential privacy, a statistical framework for protecting sensitive data.
Both methods we develop rely on the Gaussian mechanism, which is a method of randomization we use to perturb (i) each agent's individual reward function or (ii) the joint reward function shared by all agents.

We prove that both of these methods are differentially private and compare the abilities of each to provide accurate reward values for policy synthesis.
We then develop an algorithm for the numerical computation of the performance loss due to privacy on a case-by-case basis.
We also exactly compute the computational complexity of this algorithm in terms of system parameters and show that it is inherently tractable.
Numerical simulations are performed on a gridworld example and in waypoint guidance of an autonomous vehicle, and both examples show that privacy induces only negligible performance losses in practice.


title: Data-Driven Adaptive Optimal Control of Connected Vehicles
abstract: In this paper, a data-driven non-model-based approach is proposed for the adaptive optimal control of a class of connected vehicles that is composed of n human-driven vehicles only transmitting motional data and an autonomous vehicle in the tail receiving the broadcasted data from preceding vehicles by wireless vehicle-to-vehicle (V2V) communication devices.

Considering the cases of range-limited V2V communication and input saturation, several optimal control problems are formulated to minimize the errors of distance and velocity and to optimize the fuel usage.

By employing an adaptive dynamic programming technique, the optimal controllers are obtained without relying on the knowledge of system dynamics.
The effectiveness of the proposed approaches is demonstrated via the online learning control of the connected vehicles in Paramics' traffic microsimulation.

title: RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging [arXiv]
abstract: A platoon refers to a group of vehicles traveling together in very close proximity.
It has received significant attention from the autonomous vehicle research community due to its strong potential to significantly enhance fuel efficiency, driving safety, and driver comfort.
Despite these advantages, recent research has revealed a detrimental effect of the extremely small intra-platoon gap on traffic flow for highway on-ramp merging.
While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a significant challenge due to the massive computational complexity.

To this end, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging.

The state space of the framework is carefully designed in consultation with the transportation literature to incorporate critical traffic parameters relevant to merging efficiency.
A deep deterministic policy gradient algorithm is adopted to account for the continuous action space to ensure precise and continuous adjustment of the intra-platoon gap.
An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway merging scenarios.

title: Multi-task safe reinforcement learning for navigating intersections in dense traffic
abstract: Multi-task intersection navigation, which includes unprotected turning left, turning right, and going straight in heavy traffic, remains a difficult task for autonomous vehicles.
For the human driver, negotiation skills with other interactive vehicles are the key to guaranteeing safety and efficiency.
However, it is hard to balance the safety and efficiency of the autonomous vehicle for multi-task intersection navigation.
In this paper, we formulate a multi-task safe reinforcement learning framework with social attention to improve the safety and efficiency when interacting with other traffic participants.
Specifically, the social attention module is used to focus on the states of negotiation vehicles.
In addition, a safety layer is added to the multi-task reinforcement learning framework to guarantee safe negotiation.
We deploy experiments in the simulators SUMO, which has abundant traffic flows, and CARLA, which has high-fidelity vehicle models.
Both show that the proposed algorithm improves safety while maintaining stable traffic efficiency for the multi-task intersection navigation problem.
More details and demonstrations are available at https:// github.com/ liuyuqi123/ SAT.
(c) 2022 The Franklin Institute.
Published by Elsevier Ltd. All rights reserved.

title: Monte Carlo Tree Search With Reinforcement Learning for Motion Planning
abstract: Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic.
In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way.
In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries.
In this work, we propose a motion planning system addressing these challenges.
We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic.
We learn a fast evaluation function from accurate, but non real-time models.
While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions.
We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control.
We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.

title: Autonomous Vehicle Drift With a Soft Actor-critic Reinforcement Learning Agent
abstract: Self-driving vehicles have become a more and more important field in recent years.
Supported by the techniques of Artificial Intelligence (AI), the current tendency of positive results in applications is making it a promising area to focus further research.
Additionally, Reinforcement Learning (RL) is already proved to be an efficient approach for complex problems, e.g.
robots, industrial systems and also games (like chess, Go), etc.
Drifting is a driving technique at handling limits where the driver intentionally oversteers, with loss of traction, while driving the vehicle through the entirety of a corner.

It is a very challenging control task and often results in an accident when it occurs on public roads, consequently, the efficient control of this motion is especially important in the safety of autonomous vehicles.
The paper reports novel research results whose main goal is to develop a self-driving agent for drift motion control based on vehicle simulation by Matlab/Simulink.

Longitudinal and lateral velocity together with the yaw rate formed the state representation of the vehicle.
The agent action space consists of two continuous actuator values: pedal ratio and roadwheel angle.
The goal of the agent is twofold: first, it has to jump into a drifting state, second, it has to keep the vehicle in drift.
The simulation results show that the proposed Soft Actor-Critic (SAC) RL agent is capable of learning to approach a pre-determined drift equilibrium from cornering and staying in this drift situation as well.

For the training, the solution excluded using any kind of prior data, it only works with information gained from the simulation model, which is a remarkable difference from the actual state-of-the-art RL-based solutions.


title: Delayed-onset immune-related adverse events involving the thyroid gland by immune checkpoint inhibitors in combination with chemotherapy: a case report and retrospective cohort study
abstract: The connected autonomous vehicle is considered an effective way to improve transport safety and efficiency.
To overcome the limited sensing and computing capabilities of individual vehicles, we design a digital twin assisted decision-making framework for Internet of Vehicles, by leveraging the integration of communication, sensing and computing.

In this framework, the digital twin entities residing on edge can effectively communicate and cooperate with each other to plan sub-targets for their respective vehicles, while the vehicles only need to achieve the sub-targets by generating a sequence of atomic actions.

Furthermore, we propose a hierarchical multi-agent reinforcement learning approach to implement the framework, which can be trained in an end-to-end way.
In the proposed approach, the communication interval of digital twin entities could adapt to time-varying environment.
Extensive experiments on driving decision-making have been performed in traffic junction scenarios of different difficulties.
The experimental results show that the proposed approach can largely improve collaboration efficiency while reducing communication overhead.

title: 2010 International Conference on Autonomous and Intelligent Systems (AIS 2010)
abstract: The following topics are dealt with: stroke patient rehabilitation; augmented reality; autonomous vehicle control; human-robot interaction; mobile surveillance; epileptic seizure onset detection; tutorial dialogues; wireless sensor network routing; fuzzy quadtree; mobile robots; landscape visualization; fuzzy control; induction motor drives; handwriting classification; reinforcement learning; power system operations; nonlinear state estimation; dynamic programming; fingerprint image segmentation; sensorless control; and speech recognition.


title: Deep Reinforcement Learning for Safe Local Planning of a Ground Vehicle in Unknown Rough Terrain
abstract: Safe unmanned ground vehicle navigation in unknown rough terrain is crucial for various tasks such as exploration, search and rescue and agriculture.
Offline global planning is often not possible when operating in harsh, unknown environments, and therefore, online local planning must be used.
Most online rough terrain local planners require heavy computational resources, used for optimal trajectory searching and estimating vehicle orientation in positions within the range of the sensors.
In this work, we present a deep reinforcement learning approach for local planning in unknown rough terrain with zero-range to local-range sensing, achieving superior results compared to potential fields or local motion planning search spaces methods.

Our approach includes reward shaping which provides a dense reward signal.
We incorporate self-attention modules into our deep reinforcement learning architecture in order to increase the explainability of the learnt policy.
The attention modules provide insight regarding the relative importance of sensed inputs during training and planning.
We extend and validate our approach in a dynamic simulation, demonstrating successful safe local planning in environments with a continuous terrain and a variety of discrete obstacles.
By adding the geometric transformation between two successive timesteps and the corresponding action as inputs, our architecture is able to navigate on surfaces with different levels of friction.
Reinforcement learning, autonomous vehicle navigation, motion and path planning.

title: Social behavior for autonomous vehicles
abstract: Deployment of autonomous vehicles on public roads promises increased efficiency and safety.
It requires understanding the intent of human drivers and adapting to their driving styles.
Autonomous vehicles must also behave in safe and predictable ways without requiring explicit communication.
We integrate tools from social psychology into autonomous-vehicle decision making to quantify and predict the social behavior of other drivers and to behave in a socially compliant way.
A key component is Social Value Orientation (SVO), which quantifies the degree of an agent's selfishness or altruism, allowing us to better predict how the agent will interact and cooperate with others.

We model interactions between agents as a best-response game wherein each agent negotiates to maximize their own utility.
We solve the dynamic game by finding the Nash equilibrium, yielding an online method of predicting multiagent interactions given their SVOs.
This approach allows autonomous vehicles to observe human drivers, estimate their SVOs, and generate an autonomous control policy in real time.
We demonstrate the capabilities and performance of our algorithm in challenging traffic scenarios: merging lanes and unprotected left turns.
We validate our results in simulation and on human driving data from the NGSIM dataset.
Our results illustrate how the algorithm's behavior adapts to social preferences of other drivers.
By incorporating SVO, we improve autonomous performance and reduce errors in human trajectory predictions by 25%.

title: Energy-Efficient Autonomous Vehicle Control Using Reinforcement Learning and Interactive Traffic Simulations
abstract: Connected and autonomous vehicles are expected to improve mobility and transportation, as well as to provide energy efficiency benefits.
The integration of safety and energy efficiency aspects is challenging as there are certain trade-offs between them, and also because the assessment of these attributes requires different time horizons.

This paper illustrates the development of a controller for highway driving that, through reinforcement learning, can simultaneously address requirements of safety, comfort, performance and energy efficiency for battery electric vehicles.

The training process of the decision policy exploits traffic simulations that are capable of representing the interactive behavior of vehicles in traffic based on game theory.
Results indicate the potential for improved energy efficiency by adding powertrain-related states in the decision policy and by suitably defining the reward function.

title: Jamming and Eavesdropping Defense Scheme Based on Deep Reinforcement Learning in Autonomous Vehicle Networks
abstract: As a legacy from conventional wireless services, illegal eavesdropping is regarded as one of the critical security challenges in Connected and Autonomous Vehicles (CAVs) network.
Our work considers the use of Distributed Kalman Filtering (DKF) and Deep Reinforcement Learning (DRL) techniques to improve anti-eavesdropping communication capacity and mitigate jamming interference.

Aiming to improve the security performance against smart eavesdropper and jammer, we first develop a DKF algorithm that is capable of tracking the attacker more accurately by sharing state estimates among adjacent nodes.

Then, a design problem for controlling transmission power and selecting communication channel is established while ensuring communication quality requirements of the authorized vehicular user.
Since the eavesdropping and jamming model is uncertain and dynamic, a hierarchical Deep Q-Network (DQN)-based architecture is developed to design the anti-eavesdropping power control and possibly channel selection policy.

Specifically, the optimal power control scheme without prior information of the eavesdropping behavior can be quickly achieved first.
Based on the system secrecy rate assessment, the channel selection process is then performed when necessary.
Simulation results confirm that our jamming and eavesdropping defense technique enhances the secrecy rate as well as achievable communication rate compared with currently available techniques.

title: A Reinforcement Learning based Path Guidance Scheme for Long-range Autonomous Valet Parking in Smart Cities
abstract: Finding a parking slot in the city centre has always been a great challenge.
In many cases, drivers spend a lot of time roaming around looking for an empty and suitable parking slot.
The emerging machine learning technologies in intelligent transport system has made it more flexible for Electric Autonomous Vehicle (EAV) to find a parking slot and get parked.
The Long-range Autonomous Valet Parking (LAVP) allows an EAV to drop user at a suitable drop-off spot and select an economical parking slot.
With the evolution of battery operated vehicles, the primary concern is efficient use of battery resources.
This can be done either by maximizing battery capacity or by smartly using battery with existing capacity.
During the parking process, most of the energy is consumed by finding an optimal path to parking slot.
The work proposed in this paper guides EAV from a random starting point to nearest drop-off spot and CP.
A Reinforcement Learning based Autonomous Valet Parking technique (RL-LAVP) has been designed to guide EAV to drop-off spot, CP and minimize the total distance covered during this process.
The RL-LAVP results show a significant improvement towards minimizing covered distance and consumed energy when compared with RaNdom (RN) parking and LAVP parking techniques.

title: An open framework for human-like autonomous driving using Inverse Reinforcement Learning
abstract: Research on autonomous car driving and advanced driving assistance systems has come to occupy a very significant place in robotics research.
On the other hand, there are significant entry barriers (eg cost, legislation, logistics) that make it very difficult for small research groups and individual researchers to have access to a real autonomous vehicle for their experiments.

This paper proposes to leverage an existing driving simulator (Torcs) by developing a ROS communication bridge for it.
We use is as the basis for an experimental framework for the development and evaluation of Human-like autonomous driving based on Inverse Reinforce Learning (IRL).
Based on an extensible and open architecture, this framework provides efficient GPU-based implementations of state-of the art IRL algorithms, as well as two challenging test environments and a set of evaluation metrics as a first step toward a benchmark.


title: A multi-agent reinforcement learning-based longitudinal and lateral control of CAVs to improve traffic efficiency in a mandatory lane change scenario
abstract: Bottleneck areas are prone to severe traffic congestion due to the sudden drop in capacity.
To improve traffic efficiency in the bottleneck area, this paper proposes a multi-agent deep reinforcement learning framework integrating collision avoidance strategies to improve traffic efficiency in a mandatory lane change scenario.

The proposed method considers distance-keeping and lane-changing coordination in a connected autonomous vehicle (CAV) environment, by controlling vehicles' longitudinal and lateral movement to effectively reduce traffic congestion in a mandatory lane change scenario.

This framework was trained and tested in a simulation environment that is the same as the natural driving environment.
Compared with real-world data and the benchmark model (a Dueling Double Deep Q-Network-based model), the proposed model shows better performance in terms of average speed, travel time, throughput, and safety in the bottleneck area.

The results show that the proposed model can effectively reduce traffic congestion and improve traffic efficiency in a mandatory lane change scenario.

title: Monte Carlo Tree Search With Reinforcement Learning for Motion Planning
abstract: Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic.
In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way.
In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries.
In this work, we propose a motion planning system addressing these challenges.
We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic.
We learn a fast evaluation function from accurate, but non real-time models.
While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions.
We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A* search, deep learning, and Model Predictive Control.
We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.

title: A Survey of Deep Learning Applications to Autonomous Vehicle Control
abstract: Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment.

However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios.

For these reasons, the use of deep learning for vehicle control is becoming increasingly popular.
Although important advancements have been achieved in this field, these works have not been fully summarised.
This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods.
Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection.

The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety.

Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.

title: Operational policies and performance analysis for overhead robotic compact warehousing systems with bin reshuffling
abstract: This paper studies a novel robotic warehousing system called the overhead robotic compact storage and retrieval system, which can free up the floor space occupation at a low cost.
Bins, as basic storage containers, are stacked on top of each other to form a bin stack.
Along overhead tracks, bin-picking robots transport bins between storage/retrieval positions and workstations with the aid of track-changing robots.
Little research has been done to study operational policies and performance analysis for this new robotic compact warehousing system.
We propose a nested queuing network model that considers two transportation resources and performs reinforcement learning using real data to improve the reshuffling efficiency.
We find that reinforcement learning based reshuffling policy greatly reduces the reshuffling distance and saves computation time compared to existing policies.
We find that the storage policy of stacks affects the optimal width/length ratio regardless of the system height.
Interestingly, we obtain the number of robots that can stabilise the system to avoid an explosion of the order queue; two more robots than that number will produce relatively low throughput times.
Compared to an AutoStore system, using our system reduces cost by 30% with a slight increase in throughput time.

title: Intelligent Control of a Swarm of Unmanned Aerial Vehicles in Turbulent Environments Using Clustering-PPO Algorithm
abstract: 

title: Adaptive Stress Testing without Domain Heuristics using Go-Explore
abstract: Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems.
During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible.
Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures.
For example, some approaches give rewards for taking more likely actions, in order to to find more likely failures.
However, the agent may then learn to only take likely actions, and may not be able to find a failure at all.
Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration.
A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field.
We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most likely failure scenario.
We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road.
We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve.
Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.

title: A Novel Mountain Driving Unity Simulated Environment for Autonomous Vehicles
abstract: The simulated driving environment provides a low cost and time-saving platform to test the performance of the autonomous vehicle by linkage with existing machine learning approaches.
However, most of existing simulated driving environments focus on building flat roads in urban areas.
Still, they neglected to endeavour the tough steep, curvy hill roads, such as mountain paths around suburban areas.
In this study, by deploying in Unity engine, we developed the first complex mountain driving simulated environment with characterizing continuous curves and up/downhill.
Then, two state-of-art reinforcement learning (RL) algorithms are used to train a vehicle agent and test the performance of autonomous vehicles in our developed simulated environment.
Also, we set 5 different levels of vehicle's speeds and observe the cumulative rewards during the vehicle agent training.
Our demonstration presents the developed environment supports for complex mountain scenario configurations and RL-based autonomous vehicles, and our findings show that the vehicle agent could achieve high cumulative rewards during the training stage, suggesting that our work is a potential new simulation environment for autonomous vehicles research.

The demonstration video can be viewed via the link.
https://youtu.be/0wSqGeCn-NU.

title: Learning from Suboptimal Demonstration via Trajectory-Ranked Adversarial Imitation
abstract: Robots trained by Imitation Learning(IL) are used in many tasks(e.g., autonomous vehicle manipulation).
Generative Adversarial Imitation Learning (GAIL) assumes that the demonstration set used for training is of high quality.
However, such demonstrations are difficult and expensive to obtain.
GAIL-related methods fail to learn effective strategies if non-high quality demonstrations are used because the performance of agents trained by this method is limited by the demonstrator's operations.

Our idea is to enable the agent to learn strategy with better performance than the demonstrator from a suboptimal demonstration set, which contains non-high quality demonstrations that are easier to obtain.

Inspired by this, we propose the Trajectory-Ranked Adversarial Imitation Learning (TRAIL) method.
First, for demonstration set processing, we introduce a ranking process and define the concept of Performance Relative Advantage of suboptimal demonstrations to specify the ranking order.
Second, for model training, we reconstruct the objective function of GAIL and use an experience replay buffer, enabling the agent to learn implicit features and ranking information from the ranked suboptimal demonstration set and possess the ability to outperform the demonstrator.

Experiments show that in Mujoco's tasks, our method can learn from a suboptimal demonstration set and can achieve better performance than baseline methods.

title: How to train a self-driving vehicle: On the added value (or lack thereof) of curriculum learning and replay buffers
abstract: Learning from only real-world collected data can be unrealistic and time consuming in many scenario.
One alternative is to use synthetic data as learning environments to learn rare situations and replay buffers to speed up the learning.
In this work, we examine the hypothesis of how the creation of the environment affects the training of reinforcement learning agent through auto-generated environment mechanisms.
We take the autonomous vehicle as an application.
We compare the effect of two approaches to generate training data for artificial cognitive agents.
We consider the added value of curriculum learning-just as in human learning-as a way to structure novel training data that the agent has not seen before as well as that of using a replay buffer to train further on data the agent has seen before.

In other words, the focus of this paper is on characteristics of the training data rather than on learning algorithms.
We therefore use two tasks that are commonly trained early on in autonomous vehicle research: lane keeping and pedestrian avoidance.
Our main results show that curriculum learning indeed offers an additional benefit over a vanilla reinforcement learning approach (using Deep-Q Learning), but the replay buffer actually has a detrimental effect in most (but not all) combinations of data generation approaches we considered here.

The benefit of curriculum learning does depend on the existence of a well-defined difficulty metric with which various training scenarios can be ordered.
In the lane-keeping task, we can define it as a function of the curvature of the road, in which the steeper and more occurring curves on the road, the more difficult it gets.
Defining such a difficulty metric in other scenarios is not always trivial.
In general, the results of this paper emphasize both the importance of considering data characterization, such as curriculum learning, and the importance of defining an appropriate metric for the task.


title: Cooperative Adaptive Cruise Control: A Reinforcement Learning Approach
abstract: Recently, improvements in sensing, communicating, and computing technologies have led to the development of driver-assistance systems (DASs).
Such systems aim at helping drivers by either providing a warning to reduce crashes or doing some of the control tasks to relieve a driver from repetitive and boring tasks.
Thus, for example, adaptive cruise control (ACC) aims at relieving a driver from manually adjusting his/her speed to maintain a constant speed or a safe distance from the vehicle in front of him/her.
Currently, ACC can be improved through vehicle-to-vehicle communication, where the current speed and acceleration of a vehicle can be transmitted to the following vehicles by intervehicle communication.

This way, vehicle-to-vehicle communication with ACC can be combined in one single system called cooperative adaptive cruise control (CACC).
This paper investigates CACC by proposing a novel approach for the design of autonomous vehicle controllers based on modern machine-learning techniques.
More specifically, this paper shows how a reinforcement-learning approach can be used to develop controllers for the secure longitudinal following of a front vehicle.
This approach uses function approximation techniques along with gradient-descent learning algorithms as a means of directly modifying a control policy to optimize its performance.
The experimental results, through simulation, show that this design approach can result in efficient behavior for CACC.

title: Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning*
abstract: Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort.
The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle.

The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled.

The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios.
Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance.

This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action.
In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity.
All rights reserved Elsevier.

title: A Reinforcement Learning based Decision-making System with Aggressive Driving Behavior Consideration for Autonomous Vehicles
abstract: With the fast development of autonomous vehicle (AV) technology and possible popularity of AVs in the near future, a mixed-vehicle type driving environment where both AVs and their surrounding human-driving vehicles drive on the same road will exist and last for a long time.

An AV measures its driving environments in real time and make control decisions to ensure driving safety.
However, surrounding human-driving vehicles may conduct aggressive driving behaviors (e.
g.
, sudden deceleration, sudden acceleration, sudden left or right lane change) in practice, which requires an AV to make correct control decisions to eliminate the effect of aggressive driving behaviors on its driving safety.

In this paper, we propose a reinforcement learning based decision-making system (ReDS) which considers aggressive driving behaviors of surrounding human-driving vehicles during the decision making process.

In ReDS, we firstly build a mixture density network based aggressive driving behavior detection method to detect possible aggressive driving behaviors among surrounding vehicles of an AV.
We then build a reward function based on aggressive driving behavior detection results and incorporate the reward function into a reinforcement learning model to make optimal control decisions considering aggressive driving behaviors.

We use a real-world traffic dataset from the United States Department of Transportation Federal Highway Administration to evaluate optimal control decision determination performance of ReDS in comparison with the state-of-the-art methods.

The comparison results show that ReDS can improve optimal control decision success rate by 43% compared with existing methods, which demonstrates that ReDS has good optimal control decision determination performance.


title: Integrating Deep Reinforcement Learning with Optimal Trajectory Planner for Automated Driving
abstract: Trajectory planning in the intersection is a challenging problem due to the strong uncertain intentions of surrounding agents.
Conventional methods may fail in some corner cases when the ad-hoc parameters or predictions do not match the real traffic.
This paper proposes a trajectory planning method, adaptive to the uncertain interactions, called Value-Estimation-Guild (VEG) trajectory planner.
The method builds on the Frenet frame trajectory planner, in the meantime, uses the deep reinforcement learning to deal with the high uncertainty.
The deep reinforcement learning learns from past failures and adjusts the sample direction of the optimal planner under the Frenet frame.
In this way, the generated trajectory can be partially optimal and adapt to the stochastic as well.
This method drives the automated vehicle through intersections and completes the unprotected left turn mission.
During the testing, traffic density, surrounding vehicles' types, and intentions are all generated randomly.
The statistics results show that the proposed trajectory planner works well under high uncertainty.
It helps the automatic vehicles to finish the unprotected left turn with a success rate of 94.4 %, compared with the baseline method of 90%.

title: Next-Generation Edge Computing Assisted Autonomous Driving Based Artificial Intelligence Algorithms
abstract: Edge Computing and Network Function Virtualization (NFV) concepts can improve network processing and multi-resources allocation when intelligent optimization algorithms are deployed.
Multiservice offloading and allocation approaches pose interesting challenges in the current and next-generation vehicle networks.
The state-of-the-art optimization approaches still formulate exact algorithms, and tune approximation methods to get sufficient solutions.
These approaches are data-centric that aim to use heterogeneous data inputs to find the near optimal solutions.
In the context of connected and autonomous vehicles (CAVs), these techniques show an exponential computational time and deal only with small and medium scale networks.
Therefore, we are motivated by using recent Deep Reinforcement Learning (DRL) techniques to learn the behavior of exact optimization algorithms while enhancing the Quality of Service (QoS) of network operators and satisfying the requirements of the next-generation Autonomous Vehicles (AVs).

DRL algorithms can improve AVs service offloading and optimize edge resources.
An Optimal Virtual Edge Autopilot Placement (OVEAP) algorithm is proposed using Integer Linear Programming (ILP).
Moreover, an autopilot placement protocol is presented to support the algorithm.
Optimal allocation and Virtual Network Function (VNF) placement and chaining of the autopilot, based on several new constraints such as computing and networking loads, network edge infrastructure, and placement cost, are designed.

Further, a DRL approach is formulated to deal with dense Internet of Autonomous Vehicle (IoAV) networks.
Extensive simulations and evaluations are carried out.
Results show that the proposed allocation strategies outperform the state-of-the-art solutions and give better performance in terms of Total Edge Servers Utilization, Total Edge Servers Allocation Time, and Successfully Allocated autopilots.


title: Learning-Based Optimal Control of Connected and Autonomous Vehicles
abstract: 

title: FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers & Goods Transportation [arXiv]
abstract: The growth in online goods delivery is causing a dramatic surge in urban vehicle traffic from last-mile deliveries.
On the other hand, ride-sharing has been on the rise with the success of ride-sharing platforms and increased research on using autonomous vehicle technologies for routing and matching.
The future of urban mobility for passengers and goods relies on leveraging new methods that minimize operational costs and environmental footprints of transportation systems.
This paper considers combining passenger transportation with goods delivery to improve vehicle-based transportation.
Even though the problem has been studied with a defined dynamics model of the transportation system environment, this paper considers a model-free approach that has been demonstrated to be adaptable to new or erratic environment dynamics.

We propose FlexPool, a distributed model-free deep reinforcement learning algorithm that jointly serves passengers & goods workloads by learning optimal dispatch policies from its interaction with the environment.

The proposed algorithm pools passengers for a ride-sharing service and delivers goods using a multi-hop transit method.
These flexibilities decrease the fleet's operational cost and environmental footprint while maintaining service levels for passengers and goods.
Through simulations on a realistic multi-agent urban mobility platform, we demonstrate that FlexPool outperforms other model-free settings in serving the demands from passengers & goods.
FlexPool achieves 30% higher fleet utilization and 35% higher fuel efficiency in comparison to (i) model-free approaches where vehicles transport a combination of passengers & goods without the use of multi-hop transit, and (ii) model-free approaches where vehicles exclusively transport either passengers or goods.


title: SRL-TR<SUP>2</SUP>: A <i>S</i>afe <i>Re</i>inforcement <i>L</i>earning Based <i>TR</i>ajectory <i>TR</i>acker Framework
abstract: This paper aims to solve the trajectory tracking con-trol problem for an autonomous vehicle based on reinforcement learning methods.
Existing reinforcement learning approaches have found limited successful applications on safety-critical tasks in the real world mainly due to two challenges: 1) sim-to-real transfer; 2) closed-loop stability and safety concern.

In this paper, we propose an actor-critic-style framework SRL-TR2, in which the RL-based TRajectory TRackers are trained under the safety constraints and then deployed to a full-size vehicle as the lateral controller.

To improve the generalization ability, we adopt a light-weight adapter State and Action Space Alignment (SASA) to establish mapping relations between the simulation and reality.
To address the safety concern, we leverage an expert strategy to take over the control when the safety constraints are not satisfied.
Hence, we conduct safe explorations during the training process and improve the stability of the policy.
The experiments show that our agents can achieve one-shot transfer across simulation scenarios and unseen realistic scenarios, finishing the field tests with average running time less than 10 ms/step and average lateral error less than 0.
1 m under the speed ranging from 12 km/h to 18 km/h.

A video of the field tests is available at https://youtu.be/pjWcN_fV24g.

title: DeepRacer Model Training for autonomous vehicles on AWS EC2
abstract: Autonomous vehicle (AV) is the future of public transportation to reduce the road congestion and accidents.
However, fully self-driving car is still a challenge for carmaker, and they must ensure the maximum driving security to avoid the ethical issue.
To develop a mature model to AV, reinforcement learning becomes a solution which can explore many possibilities and choose the best possible action choice facing different road conditions.
AWS DeepRacer is a comprehensive platform for researchers to start reinforcement learning for AV.
With using DeepRacer Console, all the parameters including action spaces, reward functions and hyper parameters can be edited online and trained remotely.
However, the price for training a converged model on DeepRacer Console is quite expensive for beginners.
This paper present a DeepRacer simulation process built on EC2 instance which demand no hardware and cost significantly less than regular DeepRacer Console.
Based on the default models from AWS, the authors experimentally adjusted hyper parameters and added reward functions to achieve higher speed and smoother driving actions.
Even though the computing resources still limited the agent performance and stopped the model from convergence, there are 2-3m/s speed increases when adding low speed penalty and progress penalty on reward function.


title: Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning
abstract: For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car.
In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult.
In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments.

Application of the hierarchical structure Ill allows the various layers of the behavior planning system to be satisfied.
Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car.

Such behavior is hard to evaluate as correct or incorrect, but some aggressive expert human drivers handle such scenarios effectively and quickly.
On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process.
The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.

title: Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations
abstract: Reinforcement learning has shown remarkable success in various applications, and in some cases, even out-performs human performance.
However, despite the potential of reinforcement learning, numerous challenges still exist.
In this paper, we introduce a novel approach that exploits the synergies between hierarchical reinforcement learning and distributional reinforcement learning to address complex sparse-reward tasks, where noisy state observations or non-stationary exogenous perturbations are present.

Our proposed method has a hierarchical policy structure, where random rewards are modeled as random variables that follow a value distribution.
This approach enables the handling of complex tasks and increases robustness to uncertainties arising from measurement noise or exogenous perturbations, such as wind.
To achieve this, we extend the distributional soft Bellman operator and temporal difference error to include the hierarchical structure, and we use quantile regression to approximate the reward distribution.

We evaluate our method using a bipedal robot in the OpenAI Gym environment and an electric autonomous vehicle in the SUMO traffic simulator.
The results demonstrate the effectiveness of our approach in solving complex tasks with the aforementioned uncertainties when compared to state-of-the-art methods.
Our approach demonstrates promising results in handling uncertainties caused by noise and perturbations for challenging sparse-reward tasks, and could potentially pave the way for the development of more robust and effective reinforcement learning algorithms in real physical systems.


title: Autonomous vehicle platoons in urban road networks: a joint distributed reinforcement learning and model predictive control approach
abstract: In this paper, platoons of autonomous vehicles operating in urban road networks are considered.
From a methodological point of view, the problem of interest consists of formally characterizing vehicle state trajectory tubes by means of routing decisions complying with traffic congestion criteria.

To this end, a novel distributed control architecture is conceived by taking advantage of two methodologies: deep reinforcement learning and model predictive control.
On one hand, the routing decisions are obtained by using a distributed reinforcement learning algorithm that exploits available traffic data at each road junction.
On the other hand, a bank of model predictive controllers is in charge of computing the more adequate control action for each involved vehicle.
Such tasks are here combined into a single framework: the deep reinforcement learning output (action) is translated into a set-point to be tracked by the model predictive controller; conversely, the current vehicle position, resulting from the application of the control move, is exploited by the deep reinforcement learning unit for improving its reliability.

The main novelty of the proposed solution lies in its hybrid nature: on one hand it fully exploits deep reinforcement learning capabilities for decision-making purposes; on the other hand, time-varying hard constraints are always satisfied during the dynamical platoon evolution imposed by the computed routing decisions.

To efficiently evaluate the performance of the proposed control architecture, a co-design procedure, involving the SUMO and MATLAB platforms, is implemented so that complex operating environments can be used, and the information coming from road maps (links, junctions, obstacles, semaphores, etc.
)
and vehicle state trajectories can be shared and exchanged.
Finally by considering as operating scenario a real entire city block and a platoon of eleven vehicles described by double-integrator models, several simulations have been performed with the aim to put in light the main features of the proposed approach.

Moreover, it is important to underline that in different operating scenarios the proposed reinforcement learning scheme is capable of significantly reducing traffic congestion phenomena when compared with well-reputed competitors.


title: Stability Analysis in Mixed-Autonomous Traffic With Deep Reinforcement Learning
abstract: The emergence of autonomous driving vehicles on roads has increased the importance of research on autonomous driving in mixed-autonomous traffic.
In mixed-autonomous traffic scenarios, it is necessary to comprehend the instability of autonomous vehicles and traffic flow corresponding to the uncertainty level in human-driven behaviors.
However, studies of stability analysis in deep reinforcement learning are limited.
This study focuses on the impact of deep reinforcement learning based autonomous vehicles in mixed-autonomous traffic from the stability perspective.
We define the policy instability and traffic flow instabilities using the entropy of the velocity distributions to quantitatively measure the instability of an autonomous vehicle.
Subsequently, we provide mathematical analyses to explain logarithmic growth patterns of instability.
Moreover, we propose a novel deep reinforcement learning approach that jointly determines discrete and continuous actions under partial observation.
To verify the proposed solution, we perform extensive simulations of various traffic scenarios (e.
g.
, increasing traffic volumes, increasing the number of autonomous vehicles on the road, and setting the multiple uncertainty levels for human-driven behaviors) with ablation studies on reward function.

Moreover, we analyze instabilities when human-driven vehicles are modeled using the human-like noisy controller and a policy that imitates actual human-driving data based on imitation learning.
The simulation results support the theoretical analysis and confirm that the proposed method is stabler compared to a conventional control-theoretic approach.

title: Zero-Sum Game (ZSG) based Integral Reinforcement Learning for Trajectory Tracking Control of Autonomous Smart Car
abstract: The ultimate aim of our research study is the development, practical implementation, and benchmarking of continuous-time, online reinforcement learning (RL) schemes for the trajectory tracking control (TTC) of fully autonomous vehicles (AVs) in real-world scenarios.

The adaptive optimality and model-free nature offered by RL has a stronger promise against its model-based counterparts, such as MPC, against uncertainties related to the vehicle, road, tire-terrain and environmental dynamics.

The existing studies on RL based AV control are mostly theoretical, often dealing with high-level TTC, and perform evaluations in simulations considering simplified or linear models with no disturbance and slip effects.

The literature also demonstrates the lack of practical implementations in overall RL based autonomous vehicle control.
Our ultimate goal is to fill these theoretical and practical gaps by designing and practically evaluating novel RL strategies that will improve the performance of TTC against uncertainties at all levels.

This paper presents the simulation results of our preliminary studies in the online, longitudinal tracking control of a realistic AV (with uncertain nonlinear dynamics, as well as disturbance, and slip effects), which we treat as a Zero-Sum Game (ZSG) problem using an Integral Reinforcement Learning (IRL) approach with synchronous actor and critic updates (SyncIRL).

The results are promising and motivate the practical implementation of the approach for combined longitudinal and lateral control of AV.

title: Lane keeping decision-making method of autonomous vehicles based on improved TD3
abstract: This paper proposes a new end-to-end decision-making scheme for lane keeping based on the improved TD3 algorithm.
First, a multi-data fusion TD3 algorithm framework is constructed to perceive the kinematics data information and visual image information of autonomous vehicle to enhance the stability of the algorithm.

Second, combined with the concept of attention mechanism, image features are refined so that the algorithm pays attention to the key road information, which enhances the interpretability of the algorithm.

Then, a guidance-based reward function is designed with comprehensive consideration of driving safety, comfort and efficiency to guide the intelligent agent to learn a more human-like driving strategy.

Finally, a classification and high-value prioritized experience replay method is applied to improve the sample utilization and accelerate the algorithm convergence.
With the aid of TORCS simulation platform, multiple sets of comparative experiments are designed to verify the effectiveness and feasibility of the proposed method.
Furthermore, through simulation tests in multiple scenarios, it is verified that the overall performance of the proposed improved TD3 algorithm is better than that of the TD3 algorithm.

title: Development of a Simulator for Prototyping Reinforcement Learning-Based Autonomous Cars
abstract: Autonomous driving is a research field that has received attention in recent years, with increasing applications of reinforcement learning (RL) algorithms.
It is impractical to train an autonomous vehicle thoroughly in the physical space, i.
e.
, the so-called 'real world'; therefore, simulators are used in almost all training of autonomous driving algorithms.

There are numerous autonomous driving simulators, very few of which are specifically targeted at RL.
RL-based cars are challenging due to the variety of reward functions available.
There is a lack of simulators addressing many central RL research tasks within autonomous driving, such as scene understanding, localization and mapping, planning and driving policies, and control, which have diverse requirements and goals.

It is, therefore, challenging to prototype new RL projects with different simulators, especially when there is a need to examine several reward functions at once.
This paper introduces a modified simulator based on the Udacity simulator, made for autonomous cars using RL.
It creates reward functions, along with sensors to create a baseline implementation for RL-based vehicles.
The modified simulator also resets the vehicle when it gets stuck or is in a non-terminating loop, making it more reliable.
Overall, the paper seeks to make the prototyping of new systems simple, with the testing of different RL-based systems.

title: Predictive trajectory planning for autonomous vehicles at intersections using reinforcement learning
abstract: In this work we put forward a predictive trajectory planning framework to help autonomous vehicles plan future trajectories.
We develop a partially observable Markov decision process (POMDP) to model this sequential decision making problem, and a deep reinforcement learning solution methodology to learn high-quality policies.

The POMDP model utilizes driving scenar-ios, condensed into graphs, as inputs.
More specifically, an input graph contains information on the history trajectory of the subject vehicle, predicted trajectories of other agents in the scene (e.
g.
, other vehicles, pedestrians, and cyclists), as well as predicted risk levels posed by surrounding vehicles to devise safe, comfortable, and energy-efficient trajectories for the subject vehicle to follow.

In order to obtain sufficient driving scenarios to use as training data, we propose a simulation framework to generate socially acceptable driving scenarios using a real world autonomous vehicle dataset.

The simulation framework utilizes Bayesian Gaussian mixture models to learn trajectory patterns of different agent types, and Gibbs sampling to ensure that the distribution of simulated scenarios matches that of the real-world dataset collected by an autonomous fleet.

We evaluate the proposed work in two complex urban driving environments: a non-signalized T-junction and a non-signalized lane merge intersection.
Both environments provide vastly more complex driving scenarios compared to a highway driving environment, which has been mostly the focus of previous studies.
The framework demonstrates promising performance for planning horizons as long as five seconds.
We compare safety, comfort, and energy efficiency of the planned trajectories against human-driven trajectories in both experimental driving environments, and demonstrate that it outperforms human-driven trajectories in a statistically significant fashion in all aspects.


title: Informative Trajectory Planning Using Reinforcement Learning for Minimum-Time Exploration of Spatiotemporal Fields
abstract: This article studies the informative trajectory planning problem of an autonomous vehicle for field exploration.
In contrast to existing works concerned with maximizing the amount of information about spatial fields, this work considers efficient exploration of spatiotemporal fields with unknown distributions and seeks minimum-time trajectories of the vehicle while respecting a cumulative information constraint.

In this work, upon adopting the observability constant as an information measure for expressing the cumulative information constraint, the existence of a minimum-time trajectory is proven under mild conditions.

Given the spatiotemporal nature, the problem is modeled as a Markov decision process (MDP), for which a reinforcement learning (RL) algorithm is proposed to learn a continuous planning policy.
To accelerate the policy learning, we design a new reward function by leveraging field approximations, which is demonstrated to yield dense rewards.
Simulations show that the learned policy can steer the vehicle to achieve an efficient exploration, and it outperforms the commonly-used coverage planning method in terms of exploration time for sufficient cumulative information.


title: Design and Implement an Enhanced Simulator for Autonomous Delivery Robot
abstract: As autonomous driving technology is getting more and more mature today, autonomous delivery companies like Starship, Marble, and Nuro has been making progress in the tests of their autonomous delivery robots.

While simulations and simulators are very important for the final product landing of the autonomous delivery robots since the autonomous delivery robots need to navigate on the sidewalk, campus, and other urban scenarios, where the simulations can avoid real damage to pedestrians and properties in the real world caused by any algorithm failures and programming errors and thus accelerate the whole developing procedure and cut down the cost.

In this case, this study proposes an open-source simulator based on our autonomous delivery robot ZebraT to accelerate the research on autonomous delivery.
The simulator developing procedure is illustrated step by step.
What is more, the applications on the simulator that we are working on are also introduced, which includes autonomous navigation in the simulated urban environment, cooperation between an autonomous vehicle and an autonomous delivery robot, and reinforcement learning practice on the task training in the simulator.

We have published the proposed simulator in Github.

title: Jointly Learning V2X Communication and Platoon Control with Deep Reinforcement Learning
abstract: In autonomous vehicle platooning, Vehicle-to-Everything (V2X) communications are leveraged in cooperative adaptive cruise control (CACC) to improve control performance.
Since exchanging information at all times incurs significant communication overhead in vehicular networks, it is important to determine when V2X communication is necessary.
To solve this problem, we propose a Deep Reinforcement Learning (DRL)-based algorithm named Attention-DDPG, which learns platoon control with Deep Deterministic Policy Gradient (DDPG), and learns when to communicate with an attention network.

Specifically, each preceding vehicle is equipped with a deep neural network (DNN), which takes as input its local state and platoon control action and determines whether to transmit its acceleration or not to the following vehicle at each time step.

The attention network of a preceding vehicle is trained using the feedback from the following vehicle on the value of V2X information in the form of an advantage function.
In order to evaluate Attention-DDPG, simulations are performed using real driving data, and performance is compared with those of two baselines that communicate and do not communicate at all times, respectively.

The results demonstrate that Attention-DDPG strikes a competitive trade-off between control performance and communication overhead while ensuring platoon string stability.

title: Optimization of autonomous vehicle speed control mechanisms using hybrid DDPG-SHAP-DRL-stochastic algorithm
abstract: Autonomous Vehicles (AV) are the future milestones of the automobile industry, which functions without the intervention of human being.
Numerous researches have been stimulated by leading automobile sectors of the world, to address the anticipated challenges in implementing the autonomous vehicles in a practical scenario.
The speed control mechanism is the predominant challenge which acts in the basis of Machine Learning mechanism is the major thrust area associated with autonomous vehicles.
Reinforcement Learning (RL) is the effective algorithm to solve the challenges associated with the autonomous driving of vehicles and its decision on complex scenarios.
A simulative environment is advantageous for training and validation of an RL algorithm because it reduces risk and saves resources.
This research work introduces a novel hybrid algorithm composed of Deep Deterministic Policy Gradient (DDPG) -SHapley Additive exPlanations (SHAP) - Deep Reinforcement Learning (DRL)-stochastic algorithm.

The primary objective of this research work is to introduce an RL envi-ronment for optimizing longitudinal control.

title: Cooperative Learning for Smart Charging of Shared Autonomous Vehicle Fleets
abstract: We study the operational problem of shared autonomous electric vehicles that cooperate in providing on-demand mobility services while maximizing fleet profit and service quality.
Therefore, we model the fleet operator and vehicles as interactive agents enriched with advanced decision-making aids.
Our focus is on learning smart charging policies (when and where to charge vehicles) in anticipation of uncertain future demands to accommodate long charging times, restricted charging infrastructure, and time-varying electricity prices.

We propose a distributed approach and formulate the problem as a semiMarkov decision process to capture its stochastic and dynamic nature.
We use cooperative multiagent reinforcement learning with reshaped reward functions.
The effectiveness and scalability of the proposed model are upgraded through deep learning.
A mean-field approximation deals with environment instabilities, and hierarchical learning distinguishes high-level and low-level decisions.
We evaluate our model using various numerical examples based on real data from ShareNow in Berlin, Germany.
We show that the policies learned using our decentralized and dynamic approach outperform central static charging strategies.
Finally, we conduct a sensitivity analysis for different fleet characteristics to demonstrate the proposed model's robustness and provide managerial insights into the impacts of strategic decisions on fleet performance and derived charging policies.


title: Optimal Path Tracking Control Based on Online Modeling for Autonomous Vehicle With Completely Unknown Parameters
abstract: Reliable path tracking control (PTC) method is essential for autonomous driving.
However, existing PTC methods count on prior vehicle parameters to achieve good performance.
This paper presents an optimal PTC method without requiring any prior vehicle parameters based on online modeling with strict parameter convergence ability.
First, we build a virtual optimal control problem using adaptive dynamic programming (ADP) scheme to guide the data collection and solve two characteristic matrices containing parameter information.
Then, the model construction method is derived using the solved matrices and the optimal PTC method is constructed using the constructed model.
Finally, a fault-tolerant control scheme is further designed using the constructed model and the online modeling ability of the proposed method.
The effectiveness of the proposed method is validated through co-simulation between Matlab/Simulink and high-fidelity vehicle dynamic simulation software CarSim (R) under both fault-free and fault-tolerant situations.


title: A Control Method with Reinforcement Learning for Urban Un-Signalized Intersection in Hybrid Traffic Environment
abstract: To control autonomous vehicles (AVs) in urban unsignalized intersections is a challenging problem, especially in a hybrid traffic environment where self-driving vehicles coexist with human driving vehicles.

In this study, a coordinated control method with proximal policy optimization (PPO) in Vehicle-Road-Cloud Integration System (VRCIS) is proposed, where this control problem is formulated as a reinforcement learning (RL) problem.

In this system, vehicles and everything (V2X) was used to keep communication between vehicles, and vehicle wireless technology can detect vehicles that use vehicles and infrastructure (V2I) wireless communication, thereby achieving a cost-efficient method.

Then, the connected and autonomous vehicle (CAV) defined in the VRCIS learned a policy to adapt to human driving vehicles (HDVs) across the intersection safely by reinforcement learning (RL).
We have developed a valid, scalable RL framework, which can communicate topologies that may be dynamic traffic.
Then, state, action and reward of RL are designed according to urban unsignalized intersection problem.
Finally, how to deploy within the RL framework was described, and several experiments with this framework were undertaken to verify the effectiveness of the proposed method.

title: Vehicle Trajectory Prediction Model Based on Attention Mechanism and Inverse Reinforcement Learning
abstract: Predicting the future trajectory of a vehicle in a dynamic scene is not a simple problem because the future trajectory of a vehicle is not only influenced by its historical trajectory but also by other vehicles.

To solve this problem, we propose a vehicle trajectory prediction model based on attention mechanism and inverse reinforcement learning.
The model uses the LSTM encoder-decoder framework as an infrastructure to efficiently extract the temporal features of vehicle trajectories.
A social attention module is proposed to model the degree of inter-vehicle influence based on the distance between vehicles.
The module generates feature vectors and serves as the weight values of the attention mechanism, enabling the prediction network to focus more on the surrounding vehicles with a greater degree of influence.

An inverse reinforcement learning framework is introduced to regularize the encoder network using a reward function.
The reward function effectively evaluates the gap between the predicted and true positions of the encoder output and enables the predicted positions to be closer to the true positions by training the network parameters.

Based on the experimental results of public datasets SDD and NGSIM, our model can predict the future trajectory of vehicles more accurately than other models.

title: Model-Free Output Consensus Control for Partially Observable Heterogeneous Multivehicle Systems
abstract: Internet of Vehicles (IoV) is a typical application of Internet-of-Things (IoT) technology in the field of intelligent transportation systems.
In the actual IoV, such as the autonomous vehicle fleet, there exists the problem of heterogeneous multivehicle coordination based on IoT communication.
How to ensure the synchronization of multiple vehicles is a hot issue.
In particular, when the system can only obtain a partial state of the vehicle, and does not know the dynamic model, including the vehicle itself and the companion model.
To overcome these deficiencies, this article deals with the model-free output consensus control problem for a class of partially observable heterogeneous multivehicle systems (MVSs).
Using measurable input/output data without any system knowledge, this article develops a Q-function-based adaptive dynamic programming (ADP).
First, an adaptive distributed observer is designed to estimate the output of the leader.
The augmented state representation is built using historical measurable input/output data instead of the unmeasurable inner system state.
Then, a Q-function-based ADP method using measurable input/output data was introduced.
The method is used to solve this distributed tracking control problem without the requirement for the MVSs dynamics.
The convergence analysis of the proposed method is also given.
To facilitate the implementation of the proposed method, an actor-critic framework is adopted to approximate the optimal Q-functions and the optimal control policies.
It shows that the approximated control policies achieve the distributed optimal tracking control.
Finally, the simulation results verify the effectiveness of the developed method for solving multivehicle formation control problems.

title: Integrated Longitudinal Speed Decision-Making and Energy Efficiency Control for Connected Electrified Vehicles [arXiv]
abstract: To improve the driving mobility and energy efficiency of connected autonomous electrified vehicles, this paper presents an integrated longitudinal speed decision-making and energy efficiency control strategy.

The proposed approach is a hierarchical control architecture, which is assumed to consist of higher-level and lower-level controls.
As the core of this study, model predictive control and reinforcement learning are combined to improve the powertrain mobility and fuel economy for a group of automated vehicles.
The higher-level exploits the signal phase and timing and state information of connected autonomous vehicles via vehicle to infrastructure and vehicle to vehicle communication to reduce stopping at red lights.

The higher-level outputs the optimal vehicle velocity using model predictive control technique and receives the power split control from the lower-level con-troller.
These two levels communicate with each other via a controller area network in the real vehicle.
The lower-level utilizes a model-free reinforcement learning method to improve the fuel economy for each connected autonomous vehicle.
Numerical tests illustrate that vehicle mobility can be noticeably improved (traveling time reduced by 30%) by reducing red-light idling.
The effectiveness and performance of the proposed method are validated via comparison analysis among different energy efficiency controls (fuel economy promoted by 13%).

title: Learn Zero-Constraint-Violation Safe Policy in Model-Free Constrained Reinforcement Learning.
abstract: We focus on learning the zero-constraint-violation safe policy in model-free reinforcement learning (RL).
Existing model-free RL studies mostly use the posterior penalty to penalize dangerous actions, which means they must experience the danger to learn from the danger.
Therefore, they cannot learn a zero-violation safe policy even after convergence.
To handle this problem, we leverage the safety-oriented energy functions to learn zero-constraint-violation safe policies and propose the safe set actor-critic (SSAC) algorithm.
The energy function is designed to increase rapidly for potentially dangerous actions, locating the safe set on the action space.
Therefore, we can identify the dangerous actions prior to taking them and achieve zero-constraint violation.
Our major contributions are twofold.
First, we use the data-driven methods to learn the energy function, which releases the requirement of known dynamics.
Second, we formulate a constrained RL problem to solve the zero-violation policies.
We prove that our Lagrangian-based constrained RL solutions converge to the constrained optimal zero-violation policies theoretically.
The proposed algorithm is evaluated on the complex simulation environments and a hardware-in-loop (HIL) experiment with a real autonomous vehicle controller.
Experimental results suggest that the converged policies in all environments achieve zero-constraint violation and comparable performance with model-based baseline.

title: Decision-Making of an Autonomous Vehicle when Approached by an Emergency Vehicle using Deep Reinforcement Learning
abstract: Autonomous Vehicles (AVs) are the future of road transportation which can increase safety, efficiency, and productivity.
Decision-making of AVs in a highway environment with different goals like overtaking, staying in a lane, and merging have been the focus of many studies.
In this study, we want to address a new edge case in autonomous driving when the AV (ego) needs to make the best lateral and longitudinal decisions when approached by an emergency vehicle (emg).
To achieve the desired behavior and learn the sequence decision process, we trained ego with the help of Deep Reinforcement Learning (DRL) algorithms and compared the results with rule-based algorithms.

We proposed two neural networks as function approximators that help the ego to learn the optimum actions.
The driving environment for this problem was developed by using Simulation Urban Mobility (SUMO) as an open-source traffic simulator.
We will show our proposed solution based on the DRL outperforming the rule-based solution and demonstrate that it has a decent performance both in normal driving situations and when an emergency vehicle is approaching.


title: Explainable Reinforcement Learning for Longitudinal Control
abstract: Deep Reinforcement Learning (DRL) has the potential to surpass the existing state of the art in various practical applications.
However, as long as learned strategies and performed decisions are difficult to interpret, DRL will not find its way into safety-relevant fields of application.
SHAP values are an approach to overcome this problem.
It is expected that the addition of these values to DRL provides an improved understanding of the learned action-selection policy.
In this paper, the application of a SHAP method for DRL is demonstrated by means of the OpenAI Gym LongiControl Environment.
In this problem, the agent drives an autonomous vehicle under consideration of speed limits in a single lane route.
The controls learned with a DDPG algorithm are interpreted by a novel approach combining learned actions and SHAP values.
The proposed RL-SHAP representation makes it possible to observe in every time step which features have a positive or negative effect on the selected action and which influences are negligible.
The results show that RL-SHAP values are a suitable approach to interpret the decisions of the agent.

title: Autonomous driving in the uncertain traffica deep reinforcement learning approach
abstract: Driving in the complex traffic safely and efficiently is a difficult task for autonomous vehicle because of the stochastic characteristics of engaged human drivers.
Deep reinforcement learning (DRL),which combines the abstract representation capability of deep learning (DL) and the optimal decision making and control capability of reinforcement learning (RL),is a good approach to address this problem.

Traffic environment is built up by combining intelligent driver model (IDM) and lane-change model as behavioral model for vehicles.
To increase the stochastic of the established traffic environment,tricks such as defining a speed distribution with cutoff for traffic cars and using various politeness factors to represent distinguished lane-change style,are taken.

For training an artificial agent to achieve successful strategies that lead to the greatest long-term rewards and sophisticated maneuver,deep deterministic policy gradient (DDPG) algorithm is deployed for learning.

Reward function is designed to get a trade-off between the vehicle speed,stability and driving safety.
Results show that the proposed approach can achieve good autonomous maneuvering in a scenario of complex traffic behavior through interaction with the environment.

title: Effective Communications: A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning Over Noisy Channels
abstract: We propose a novel formulation of the "effectiveness problem" in communications, put forth by Shannon and Weaver in their seminal work "The Mathematical Theory of Communication", by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework.

Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment, can also communicate with each other over a noisy communication channel.

The noisy communication channel is considered explicitly as part of the dynamics of the environment, and the message each agent sends is part of the action that the agent can take.
As a result, the agents learn not only to collaborate with each other but also to communicate "effectively" over a noisy channel.
This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the "learning to communicate" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free.

We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP.
This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems.


title: ON THE DEVELOPMENT OF AUTONOMOUS AGENTS USING DEEP REINFORCEMENT LEARNING
abstract: This paper presents a study on the general concept of autonomous agents, with an accent on the development of such agents using deep reinforcement learning.
This is combined with the domain of autonomous vehicles, as illustrated by a practical application: having a vehicle agent learn how to navigate and park by itself on a designated spot, in a virtual parking lot environment created in Unity.

The reinforcement learning method Deep Q-Learning is implemented, with the addition of a few improvements such as Double Deep Q-Learning and Experience Replay.

title: A reinforcement learning approach to predictive control design: Autonomous vehicle applications
abstract: 

title: Deep Reinforcement Learning Based Decision-Making Strategy of Autonomous Vehicle in Highway Uncertain Driving Environments
abstract: Uncertain environment on multi-lane highway, e.g., the stochastic lane-change maneuver of surrounding vehicles, is a big challenge for achieving safe automated highway driving.
To improve the driving safety, a heuristic reinforcement learning decision-making framework with integrated risk assessment is proposed.
First, the framework includes a long short-term memory model to predict the trajectory of surrounding vehicles and a future integrated risk assessment model to estimate the possible driving risk.
Second, a heuristic decaying state entropy deep reinforcement learning algorithm is introduced to address the exploration and exploitation dilemma of reinforcement learning.
Finally, the framework also includes a rule-based vehicle decision model for interaction decision problems with surrounding vehicles.
The proposed framework is validated in both low-density and high-density traffic scenarios.
The results show that the traffic efficiency and vehicle safety are both improved compared to the common dueling double deep Q-Network method and rule-based method.

title: Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios
abstract: Autonomous vehicles must be comprehensively evaluated before deployed in cities and highways.
However, most existing evaluation approaches for autonomous vehicles are static and lack adaptability, so they are usually inefficient in generating challenging scenarios for tested vehicles.
In this paper, we propose an adaptive evaluation framework to efficiently evaluate autonomous vehicles in adversarial environments generated by deep reinforcement learning.
Considering the multimodal nature of dangerous scenarios, we use ensemble models to represent different local optimums for diversity.
We then utilize a nonparametric Bayesian method to cluster the adversarial policies.
The proposed method is validated in a typical lane-change scenario that involves frequent interactions between the ego vehicle and the surrounding vehicles.
Results show that the adversarial scenarios generated by our method significantly degrade the performance of the tested vehicles.
We also illustrate different patterns of generated adversarial environments, which can be used to infer the weaknesses of the tested vehicles.

title: Proactive Car-Following Using Deep-Reinforcement Learning
abstract: Car-following is a fundamental operation for vehicle control for both ADAS on modern vehicles and vehilce control on autonomous vehicles.
Most existing car following mechanisms react to the observations of nearby vehicles in real-time.
Unfortunately, lack of capability of taking into account multiple constraints and objectives, these mechanisms lead to poor efficiency, discomfort, and unsafe operations.
In this paper, we design and implement a proactive car-following model to take into account safety regulation, efficiency, and comfort using deep reinforcement learning.
The evaluation results show that the proactive model not only reduces the number of inefficient and unsafe headway but also eliminates the traffic jerk, compared to human drivers.
The model outperformed 79% human drivers in public data set and the road efficiency is only 2% less than the optimal bound.
Compared to ACC model, the DDPG model allows 4.1% more vehicles to finish the simulation than ACC model does, and increases the average speed for 28.4%.

title: Vehicle Safety Planning Control Method Based on Variable Gauss Safety Field
abstract: The existing intelligent vehicle trajectory-planning methods have limitations in terms of efficiency and safety.
To overcome these limitations, this paper proposes an automatic driving trajectory-planning method based on a variable Gaussian safety field.
Firstly, the time series bird's-eye view is used as the input state quantity of the network, which improves the effectiveness of the trajectory planning policy network in extracting the features of the surrounding traffic environment.

Then, the policy gradient algorithm is used to generate the planned trajectory of the autonomous vehicle, which improves the planning efficiency.
The variable Gaussian safety field is used as the reward function of the trajectory planning part and the evaluation index of the control part, which improves the safety of the reinforcement learning vehicle tracking algorithm.

The proposed algorithm is verified using the simulator.
The obtained results show that the proposed algorithm has excellent trajectory planning ability in the highway scene and can achieve high safety and high precision tracking control.

title: Cybertwin-Driven Federated Learning Based Personalized Service Provision for 6G-V2X
abstract: The rapid growth of Autonomous Vehicle (AV) technology and the integration of edge computing grasp new challenges along with the ever-increasing mobile internet traffic and services.
Tackling such challenges through customized edge computing services is the critical research in 6G Vehicle-to-Everything (6G-V2X) communication.
V2X contributes detailed information about the current navigation of vehicles, automatic payments for toll roads, parking fees and other services.
With the countless, unique, and personalized service requirements of AVs over computation-intensive applications, exploring the edge resources for the excellent Quality of Service (QoS) provision is the greatest concern.

This paper proposes a Federated Learning and edge Cache-assisted Cybertwin (FLCC) framework for personalized service provision in 6G-V2X.
Integration of cybertwin in 6G enables the connectivity of the physical system to the digital realm, allowing for adequate instantaneous wireless access.
The FLCC jointly considers the edge cooperation and optimizations through the proposed Federated Multi-agent Deep Reinforcement Learning based (FM-DRL) algorithm.
The FM-DRL algorithm balances the FLCC's learning accuracy.
It minimizes the time and cost by taking the factors such as cybertwin association, training data batch size, and bandwidth.
Finally, caching is performed using the Federated Reinforcement Learning-based Edge Caching (FREC) algorithm to obtain the desired datasets required that train the model for providing personalized 6G-V2X services for the AVs.

Numerical studies and simulation results reveal that the proposed system outperforms the baseline learning approaches by 17.6%.

title: Towards Safe Autonomous Driving: Decision Making with Observation-Robust Reinforcement Learning
abstract: Most real-world situations involve unavoidable measurement noises or perception errors which result in unsafe decision making or even casualty in autonomous driving.
To address these issues and further improve safety, automated driving is required to be capable of handling perception uncertainties.
Here, this paper presents an observation-robust reinforcement learning against observational uncertainties to realize safe decision making for autonomous vehicles.
Specifically, an adversarial agent is trained online to generate optimal adversarial attacks on observations, which attempts to amplify the average variation distance on perturbed policies.
In addition, an observation-robust actor-critic approach is developed to enable the agent to learn the optimal policies and ensure that the changes of the policies perturbed by optimal adversarial attacks remain within a certain bound.

Lastly, the safe decision making scheme is evaluated on a lane change task under complex highway traffic scenarios.
The results show that the developed approach can ensure autonomous driving performance, as well as the policy robustness against adversarial attacks on observations.

title: Decentralized Multi-Robot Collision Avoidance in Complex Scenarios With Selective Communication
abstract: Deep reinforcement learning has been demonstrated to be an effective solution to the multi-robot collision avoidance problem.
However, with existing methods, robots typically generate actions only based on local observations, sometimes augmented with global communication.
Their performance deteriorates in limited bandwidth environments and complex scenarios with various obstacles and high robot density.
We propose SelComm, a selective communication framework to generate cooperative and collision-free actions for robots in multi-robot navigation tasks.
Specifically, we develop a decentralized message selector, enabling each robot to calculate relations with other robots using both agent-level information and sensor-level information, and select the most valuable messages to meet the bandwidth limitation.

Then we introduce the attentional communication channel for efficient communication.
Our experimental evaluations based on various scenarios demonstrate that SelComm learns more cooperative behaviors and outperforms state-of-the-art methods in limited bandwidth environments and complex scenarios.


title: Velocity control in car-following behavior with autonomous vehicles using reinforcement learning
abstract: Car-following behavior is a common driving behavior.
It is necessary to consider the following vehicle in the car following model of autonomous vehicle (AV) under the background of the vehicle-to-vehicle transportation system.
In this study, a safe velocity control method for AV based on reinforcement learning with considering the following vehicle is proposed.
First, the mixed driving environment of AVs and human-driven vehicles is constructed, and the trajectories of the leading and following vehicles are extracted from the naturalistic High D driving dataset.

Next, the soft actor-critic (SAC) algorithm is used as the velocity control algorithm, in which the agent is AV, the action is acceleration, and the state is the relative distance and relative speed between the AV and the leading and following vehicles.

Then, a reward function based on state and corresponding action is designed to guide AV to choose acceleration without collision between the leading and following vehicles.
Furthermore, AVs are gradually able to learn to avoid collisions between the leading and following vehicles after training the model.
The test result of the trained model shows that the SAC agent can achieve complete collision avoidance, resulting in zero collision.
Finally, the driving performance of the SAC agent and that of human driving are compared and analyzed for safety and efficiency.
The results of this study are expected to improve the safety of the car-following process..

title: Game theoretic decision making based on real sensor data for autonomous vehicles' maneuvers in high traffic
abstract: This paper presents an approach for implementing game theoretic decision making in combination with realistic sensory data input so as to allow an autonomous vehicle to perform maneuvers, such as lane change or merge in high traffic scenarios.

The main novelty of this work, is the use of realistic sensory data input to obtain the observations as input of an iterative multi-player game in a realistic simulator.
The game model allows to anticipate reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication.
Moreover, direct information from the simulator, such as position or speed of the vehicles is also avoided.
The solution of the game is based on cognitive hierarchy reasoning and it uses Monte Carlo reinforcement learning in order to obtain a near-optimal policy towards a specific goal.

Moreover, the game proposed is capable of solving different situations using a single policy.
The system has been successfully tested and compared with previous techniques using a realistic hybrid simulator, where the ego-vehicle and its sensors are simulated on a 3D simulator and the additional vehicles' behavior is obtained from a traffic simulator.


title: Reinforcement Learning based Negotiation-aware Motion Planning of Autonomous Vehicles
abstract: For autonomous vehicles integrating onto road-ways with human traffic participants, it requires understanding and adapting to the participants' intention by responding in predictable ways.
This paper proposes a reinforcement learning based negotiation-aware motion planning framework, which adopts RL to adjust the driving style of the planner by dynamically modifying the prediction horizon length of the motion planner in real time adaptively.

The framework models the interaction between the autonomous vehicle and other traffic participants as a Markov Decision Process.
A temporal sequence of occupancy grid maps are taken as inputs for RL module to embed an implicit intention reasoning.
Curriculum learning is employed to enhance the training efficiency and the robustness of the algorithm.
We applied our method to narrow lane navigation in both simulation and real world to demonstrate that the proposed method outperforms the common alternative due to its advantage in alleviating the social dilemma problem with proper negotiation skills.


title: A DECISION-MAKING METHOD FOR AUTONOMOUS VEHICLES BASED ON SIMULATION AND REINFORCEMENT LEARNING
abstract: There are still some problems need to be solved though there are a lot of achievements in the field of automatic driving.
One of those problems is the difficulty of designing a decision-making system for complex traffic conditions.
In recent years, reinforcement learning (RL) shows the potential in solving sequential decision optimization problems, which can be modeled as Markov decision processes (MDPs).
In this paper, we establish a 14-DOF dynamic model of an autonomous vehicle and use RL to build a decision-making system for autonomous driving based on simulation.
The decision-making process of the vehicle is modeled as an MDP, and the performance of the MDP is improved using an approximate RL.
At last, we show the efficiency of the proposed method by simulation in a highway environment.

title: A Reinforcement Learning Approach for Enacting Cautious Behaviours in Autonomous Driving System: Safe Speed Choice in the Interaction With Distracted Pedestrians
abstract: Driving requires the ability to handle unpredictable situations.
Since it is not always possible to predict an impending danger, a good driver should preventively assess whether a situation has risks and adopt a safe behavior.
Considering, in particular, the possibility of a pedestrian suddenly crossing the road, a prudent driver should limit the traveling speed.
We present a work exploiting reinforcement learning to learn a function that specifies the safe speed limit for a given artificial driver agent.
The safe speed function acts as a behavioral directive for the agent, thus extending its cognitive abilities.
We consider scenarios where the vehicle interacts with a distracted pedestrian that might cross the road in hard-to-predict ways and propose a neural network mapping the pedestrian's context onto the appropriate traveling speed so that the autonomous vehicle can successfully perform emergency braking maneuvers.

We discuss the advantages of developing a specialized neural network extension on top of an already functioning autonomous driving system, removing the burden of learning to drive from scratch while focusing on learning safe behavior at a highlevel.

We demonstrate how the safe speed function can be learned in simulation and then transferred into a real vehicle.
We include a statistical analysis of the network's improvements compared to the original autonomous driving system.
The code implementing the presented network is available at https://githuh.com/tonegas/safe-speed-neural-network with MIT license and at https://zenodo.org/communities/dreams4cars.

title: Human-like Autonomous Vehicle Speed Control by Deep Reinforcement Learning with Double Q-Learning
abstract: Autonomous driving has become a popular research project.
How to control vehicle speed is a core problem in autonomous driving.
Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to control the vehicle speed.
However, the popular Q-learning algorithm is unstable in some games in the Atari 2600 domain.
In this paper, a reinforcement learning approach called Double Q-learning is used to control a vehicle's speed based on the environment constructed by naturalistic driving data.
Depending on the concept of the direct perception approach, we propose a new method called integrated perception approach to construct the environment.
The input of the model is made up of high dimensional data including road information processed from the video data and the low dimensional data processed from the sensors.
During experiment, compared with deep Q-learning algorithm, double deep Q-learning has improvements both in terms of value accuracy and policy quality.
Our model's score is 271.73% times that of deep Q-learning.

title: Online optimum velocity calculation under V2X for smart new energy vehicles
abstract: In this paper, a vector data net solver is proposed, which can reduce bivariate discrete-time dynamic programming (DP) computation time by 98.0% without losing accuracy.
Therefore, for the first time, bivariate discrete-time DP can operate under model predictive control rolling optimization to calculate future optimum vehicle velocity in real time considering future road altitude and instant traffic information.

Simulation results indicate that with the solution presented in this paper, front vehicles and the proper windows to pass through front intersections can be constantly considered.
Meanwhile, the calculated optimum vehicle velocity almost remains the same as the global optimum solutions.
Simulation results are validated by real-car tests, and the test new energy vehicle (NEV) electricity consumption is reduced by up to 48.6%.
A comparison experiment is performed between the solution presented in this paper and commonly used adaptive dynamic programming (ADP), and the results indicate that the former has better performance and stability.

This paper describes a novel solution for online optimum velocity calculation under vehicle to everything (V2X) environment and can be used by all smart NEVs with autonomous driving or active cruise control functions for lower electricity consumption and better riding comfort.


title: Model-Based Reinforcement Learning for Time-Optimal Velocity Control
abstract: Autonomous navigation has recently gained great interest in the field of reinforcement learning.
However, little attention was given to the time-optimal velocity control problem, i.e.
controlling a vehicle such that it travels at the maximal speed without becoming dynamically unstable (roll-over or sliding).
Time optimal velocity control can be solved numerically using existing methods that are based on optimal control and vehicle dynamics.
In this letter, we develop a model-based deep reinforcement learning to generate the time-optimal velocity control.
Moreover, we introduce a method that uses a numerical solution that predicts whether the vehicle may become unstable and intervenes if needed.
We show that our combined model outperforms several baselines as it achieves higher velocities (with only one minute of training) and does not encounter any failures during the training process.

title: Reliable and Efficient Lane Changing Behaviour for Connected Autonomous Vehicle through Deep Reinforcement Learning
abstract: The establishment of future intelligent transport systems is dependable on the reliable and seamless function of Connected and Autonomous Vehicles (CAV).
Reinforcement learning (RL), which allows autonomous vehicles (AVs) to learn an ideal driving strategy through constant contact with the environment, plays a significant part in the decision-making process of autonomous driving (AD).

The networking of CAV is advantageous since it allows for the transmission of traffic-related data to vehicles via Vehicle-to-External (V2X) communication.
Recognition and anticipation of driving behaviour are critical for avoiding collisions because they can provide useful information to other drivers and vehicles.
The fundamental challenge in developing CAV is the construction of an autonomous controller that can effectively perform close real-time control selections, such as a fast acceleration while merging onto a highway and rapid speed adjustments in stop-and-go traffic congestion.

CAV driving behaviours can be considerably improved by utilizing shared information, resulting in more accountable, intelligent, and efficient driving.
In the present work, a deep reinforcement learning approach is proposed that integrates the information gathered through connectivity capabilities and sensing from neighbour automobiles in the vicinity of CAV.

The fused information is used for providing safe and cooperative lane-changing behaviour.
The deployment of an algorithm in CAV is expected to improve the transportation safety of CAV driving behaviours.
All rights reserved Elsevier.

title: COOR-PLT: A hierarchical control model for coordinating adaptive platoons of connected and autonomous vehicles at signal-free intersections based on deep reinforcement learning
abstract: Platooning and coordination are two implementation strategies that are frequently proposed for traffic control of connected and autonomous vehicles (CAVs) at signal-free intersections instead of using conventional traffic signals.

However, few studies have attempted to integrate both strategies to better facilitate the CAV control at signal-free intersections.
To this end, this study proposes a hierarchical control model, named COOR-PLT, to coordinate adaptive CAV platoons at a signal-free intersection based on deep reinforcement learning (DRL).
COOR-PLT has a two-layer framework.
The first layer uses a centralized control strategy to form adaptive platoons.
The optimal size of each platoon is determined by considering multiple objectives (i.e., efficiency, fairness and energy saving).
The second layer employs a decentralized control strategy to coordinate multiple platoons passing through the intersection.
Each platoon is labeled with coordinated status or independent status, upon which its passing priority is determined.
As an efficient DRL algorithm, Deep Q-network (DQN) is adopted to determine platoon sizes and passing priorities respectively in the two layers.
The model is validated and examined on the simulator Simulation of Urban Mobility (SUMO).
The simulation results demonstrate that the model is able to: (1) achieve satisfactory convergence performances; (2) adaptively determine platoon size in response to varying traffic conditions; and (3) completely avoid deadlocks at the intersection.

By comparison with other control methods, the model manifests its superiority of adopting adaptive platooning and DRL-based coordination strategies.
Also, the model outperforms several state-of-the-art methods on reducing travel time and fuel consumption in different traffic conditions.

title: Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning
abstract: Autonomous driving in an urban environment with surrounding agents remains challenging.
One of the key challenges is to accurately predict the traversability map that probabilistically represents future trajectories considering multiple contexts: inertial, environmental, and social.
To address this, various approaches have been proposed; however, they mainly focus on considering the individual context.
In addition, most studies utilize expensive prior information (such as HD maps) of the driving environment, which is not a scalable approach.
In this study, we extend a deep inverse reinforcement learning-based approach that can predict the traversability map while incorporating multiple contexts for autonomous driving in a dynamic environment.

Instead of using expensive prior information of the driving scene, we propose a novel deep neural network to extract contextual cues from sensing data and effectively incorporate them in the output, i.
e.
, the reward map.

Based on the reward map, our method predicts the ego-centric traversability map that represents the probability distribution of the plausible and socially acceptable future trajectories.
The proposed method is qualitatively and quantitatively evaluated in real-world traffic scenarios with various baselines.
The experimental results show that our method improves the prediction accuracy compared to other baseline methods and can predict future trajectories similar to those followed by a human driver.

title: Leveraging the Capabilities of Connected and Autonomous Vehicles and Multi-Agent Reinforcement Learning to Mitigate Highway Bottleneck Congestion [arXiv]
abstract: Active Traffic Management strategies are often adopted in real-time to address such sudden flow breakdowns.
When queuing is imminent, Speed Harmonization (SH), which adjusts speeds in upstream traffic to mitigate traffic showckwaves downstream, can be applied.
However, because SH depends on driver awareness and compliance, it may not always be effective in mitigating congestion.
The use of multiagent reinforcement learning for collaborative learning, is a promising solution to this challenge.
By incorporating this technique in the control algorithms of connected and autonomous vehicle (CAV), it may be possible to train the CAVs to make joint decisions that can mitigate highway bottleneck congestion without human driver compliance to altered speed limits.

In this regard, we present an RL-based multi-agent CAV control model to operate in mixed traffic (both CAVs and human-driven vehicles (HDVs)).
The results suggest that even at CAV percent share of corridor traffic as low as 10%, CAVs can significantly mitigate bottlenecks in highway traffic.
Another objective was to assess the efficacy of the RL-based controller vis-\`a-vis that of the rule-based controller.
In addressing this objective, we duly recognize that one of the main challenges of RL-based CAV controllers is the variety and complexity of inputs that exist in the real world, such as the information provided to the CAV by other connected entities and sensed information.

These translate as dynamic length inputs which are difficult to process and learn from.
For this reason, we propose the use of Graphical Convolution Networks (GCN), a specific RL technique, to preserve information network topology and corresponding dynamic length inputs.
We then use this, combined with Deep Deterministic Policy Gradient (DDPG), to carry out multi-agent training for congestion mitigation using the CAV controllers.

title: <i>CoverNav</i>: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning
abstract: Autonomous navigation in off-road environments has been extensively studied in the robotics field.
However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an under-explored area.
In this paper, we propose CoverNav, a novel Deep Reinforcement Learning (DRL) based algorithm, for identifying covert and navigable trajectories with minimal cost in off-road terrains and jungle environments in the presence of observers.

CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination.
Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low-cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information.

If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, trees, etc.)
and use them as shelters to hide behind.
We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL-based navigation algorithm.

Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects.
We observe competitive performance comparable to state-of-the-art (SOTA) methods without compromising accuracy.

title: Intersection crossing for autonomous vehicles based on deep reinforcement learning
abstract: Future intersection crossings for autonomous vehicles will not be controlled by traffic signals, rather a controller will be used for communication among vehicles that need to cross an intersection.
In this work, we propose an innovative management system called Deep Reinforcement Learning-based Autonomous Intersection Management (DRLAIM) system, which is the first system to use deep reinforcement learning.

We train the system to learn a good intersection control policy by interacting with traffic environment through reinforcement learning.
The brake-safe control model is used to ensure the safety of each autonomous vehicle while crossing.
Experiment results show that after training using reinforcement learning, the throughput of intersection control model increased by 83%.
In comparison with the Fast First Service (FFS) policy, the average waiting time of DRLAIM reduced by about 1.2% to 11.4%.

title: Connected autonomous vehicles for improving mixed traffic efficiency in unsignalized intersections with deep reinforcement learning
abstract: Human driven vehicles (HDVs) with selfish objectives cause low traffic efficiency in an un-signalized intersection.
On the other hand, autonomous vehicles can overcome this inefficiency through perfect coordination.
In this paper, we propose an intermediate solution, where we use vehicular communication and a small number of autonomous vehicles to improve the transportation system efficiency in such intersections.

In our solution, two connected autonomous vehicles (CAVs) lead multiple HDVs in a double-lane intersection in order to avoid congestion in front of the intersection.
The CAVs are able to communicate and coordinate their behavior, which is controlled by a deep reinforcement learning (DRL) agent.
We design an altruistic reward function which enables CAVs to adjust their velocities flexibly in order to avoid queuing in front of the intersection.
The proximal policy optimization (PPO) algorithm is applied to train the policy and the generalized advantage estimation (GAE) is used to estimate state values.
Training results show that two CAVs are able to achieve significantly better traffic efficiency compared to similar scenarios without and with one altruistic autonomous vehicle.

title: CNN based Reinforcement Learning for Driving Behavior of Simulated Self-Driving Car
abstract: This paper proposes a self-learning method for autonomous vehicle driving behavior using reinforcement learning without considering the dynamic model of the vehicle.
In order to make decision needed for determine the optimal driving behavior (steering, throttle, brake) to achieve a given driving purpose in each state by using state information of the vehicle, such as vehicle movement speed, direction, degree of deviation from the center of the track, and distance to the edge of the track, we propose a method of applying the reinforcement learning by the DDPG structure and further using the driving image to improve driving performance.

In this paper, we propose structures of an action decision network(Actor) and an action value evaluation network(Critic) to implement the DDPG learning model.
We also propose a prediction model for predict the next state driving image based on the current driving image to improve driving performance in the corner path and a corner classifier for classifying the driving track type.

The method proposed in this paper was implemented in a TORCS simulator environment, and the performance of the target driving behavior was evaluated through applying the learning model to driving agent.


title: Trajectory Optimization for Connected and Autonomous Vehicle Platooning and Split Operations: Modeling and Experiments
abstract: 

title: Wireless Communications Meets Artificial Intelligence: An Illustration by Autonomous Vehicles on Manhattan Streets
abstract: Interactions of multiple smart agents serve a fundamental aspect of Internet of Things (IoT), or known as social IoT.
Such smart agents are equipped with sophisticated machine learning for mobile operation, such as autonomous vehicles and robots.
Although wireless networking is intuitively important in such scenarios, there lacks investigations to provide a holistic and in-depth understanding on wireless networked multi-agent systems.
In this paper, we disruptively use reinforcement learning to model each agent of artificial intelligence, and explore the interplay between wireless communications and multi-agent systems.
Autonomous vehicles navigating over Manhattan streets serve the illustrating system.
The first new finding is the need to modify reinforcement learning of policy exchange due to getting information from other agents through wireless communication.
The advantage of applying wireless communication is clearly observed.
We also demonstrate the impacts of communication errors to result in penalty in system performance.
Multi-agent systems equipped with direct vehicleto-vehicle communication and vehicle-to-infrastructure communication are compared to initially conclude favorable using infrastructure of small cells.
Finally, we explore multiple access communication over multi-agent systems by employing realtime ALOHA.
Different from traditional thinking on reliable delivery of packets using re-transmit after collisions, real-time ALOHA discards re-transmission mechanism to ensure in-time contributions from wireless communication on the learning algorithm of a multi-agent system with satisfactory performance.


title: Spatial-Temporal Flows-Adaptive Street Layout Control Using Reinforcement Learning
abstract: Complete streets scheme makes seminal contributions to securing the basic public right-of-way (ROW), improving road safety, and maintaining high traffic efficiency for all modes of commute.
However, such a popular street design paradigm also faces endogenous pressures like the appeal to a more balanced ROW for non-vehicular users.
In addition, the deployment of Autonomous Vehicle (AV) mobility is likely to challenge the conventional use of the street space as well as this scheme.
Previous studies have invented automated control techniques for specific road management issues, such as traffic light control and lane management.
Whereas models and algorithms that dynamically calibrate the ROW of road space corresponding to travel demands and place-making requirements still represent a research gap.
This study proposes a novel optimal control method that decides the ROW of road space assigned to driveways and sidewalks in real-time.
To solve this optimal control task, a reinforcement learning method is introduced that employs a microscopic traffic simulator, namely SUMO, as its environment.
The model was trained for 150 episodes using a four-legged intersection and joint AVs-pedestrian travel demands of a day.
Results evidenced the effectiveness of the model in both symmetric and asymmetric road settings.
After being trained by 150 episodes, our proposed model significantly increased its comprehensive reward of both pedestrians and vehicular traffic efficiency and sidewalk ratio by 10.39%.
Decisions on the balanced ROW are optimised as 90.16% of the edges decrease the driveways supply and raise sidewalk shares by approximately 9%.
Moreover, during 18.22% of the tested time slots, a lane-width equivalent space is shifted from driveways to sidewalks, minimising the travel costs for both an AV fleet and pedestrians.
Our study primarily contributes to the modelling architecture and algorithms concerning centralised and real-time ROW management.
Prospective applications out of this method are likely to facilitate AV mobility-oriented road management and pedestrian-friendly street space design in the near future.

title: Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle
abstract: Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem.
It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI).
Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem.

The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process.

The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles.

The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs.

Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.

title: Deep Reinforcement Learning Reward Function Design for Autonomous Driving in Lane-Free Traffic
abstract: Lane-free traffic is a novel research domain, in which vehicles no longer adhere to the notion of lanes, and consider the whole lateral space within the road boundaries.
This constitutes an entirely different problem domain for autonomous driving compared to lane-based traffic, as there is no leader vehicle or lane-changing operation.
Therefore, the observations of the vehicles need to properly accommodate the lane-free environment without carrying over bias from lane-based approaches.
The recent successes of deep reinforcement learning (DRL) for lane-based approaches, along with emerging work for lane-free traffic environments, render DRL for lane-free traffic an interesting endeavor to investigate.

In this paper, we provide an extensive look at the DRL formulation, focusing on the reward function of a lane-free autonomous driving agent.
Our main interest is designing an effective reward function, as the reward model is crucial in determining the overall efficiency of the resulting policy.
Specifically, we construct different components of reward functions tied to the environment at various levels of information.
Then, we combine and collate the aforementioned components, and focus on attaining a reward function that results in a policy that manages to both reduce the collisions among vehicles and address their requirement of maintaining a desired speed.

Additionally, we employ two popular DRL algorithms-namely, deep Q-networks (enhanced with some commonly used extensions), and deep deterministic policy gradient (DDPG), which results in better policies.

Our experiments provide a thorough investigative study on the effectiveness of different combinations among the various reward components we propose, and confirm that our DRL-employing autonomous vehicle is able to gradually learn effective policies in environments with varying levels of difficulty, especially when all of the proposed rewards components are properly combined.


title: UNICON: Uncertainty-Conditioned Policy for Robust Behavior in Unfamiliar Scenarios
abstract: Deep reinforcement learning has been used to solve complex tasks in various fields, particularly in robotics control.
However, agents trained using deep reinforcement learning have a problem of taking overconfident actions, even when the input state is far from the learned state distribution.
This restricts deep reinforcement learning from being applied to real-world environments as overconfident actions in unlearned situations can result in catastrophic events; such as the collision of an autonomous vehicle.

To address this, the agents should know "what they do not know" and choose an action by considering not only the state but also its uncertainty.
In this study, we propose a novel uncertainty-conditioned policy (UNICON) inspired by the human behavior of changing policies according to uncertainty, e.
g.
, slowing a car on a narrow road that has never been visited before.

Our experimental results demonstrate that the proposed method is robust to unfamiliar scenarios that are not seen during training.

title: Sequential game solution for lane-merging conflict between autonomous vehicles <i>A multi-agent reinforcement learning approach</i>
abstract: Lane-merging conflict between Autonomous Vehicles (AV) calls for coordinated solution to allocate right-of-way.
Related studies resort to centralized decision-making optimization models such as "right-of-way reservation/auction", which are suitable only for the scenarios with a centralized intersection agent (acting as arbiter or auctioneer); and involved fiat currency spent on bidding may trigger controversial issues concerning law or taxation.

This paper: (i) establishes a prototype of 2-player complete-information 3-stage sequential game architecture within distributed decision-making paradigm, to formalize the interactions between 2 AVs trapped in the lane-merging conflict, so as to suit for scenarios with or without a centralized decision-maker, i.
e.

intersection or road segment; (ii) designs the dynamic rewards of AV's game-playing (velocity-adjusting) actions which result from the space-time status of AV; (iii) based on the proposed rewards, uses Multi-agent Reinforcement Learning to obtain the optimal (in Nash equilibrium sense) strategies of action-sequence for both AVs after 3-stage game-theoretic negotiations, promisingly avoiding the potential right-of-way deadlock in a lane-merging conflict.


title: Path Planning for Autonomous Vehicles in Unknown Dynamic Environment Based on Deep Reinforcement Learning
abstract: Autonomous vehicles can reduce labor power during cargo transportation, and then improve transportation efficiency, for example, the automated guided vehicle (AGV) in the warehouse can improve the operation efficiency.

To overcome the limitations of traditional path planning algorithms in unknown environments, such as reliance on high-precision maps, lack of generalization ability, and obstacle avoidance capability, this study focuses on investigating the Deep Q-Network and its derivative algorithm to enhance network and algorithm structures.

A new algorithm named APF-D3QNPER is proposed, which combines the action output method of artificial potential field (APF) with the Dueling Double Deep Q Network algorithm, and experience sample rewards are considered in the experience playback portion of the traditional Deep Reinforcement Learning (DRL) algorithm, which enhances the convergence ability of the traditional DRL algorithm.

A long short-term memory (LSTM) network is added to the state feature extraction network part to improve its adaptability in unknown environments and enhance its spatiotemporal sensitivity to the environment.

The APF-D3QNPER algorithm is compared with mainstream deep reinforcement learning algorithms and traditional path planning algorithms using a robot operating system and the Gazebo simulation platform by conducting experiments.

The results demonstrate that the APF-D3QNPER algorithm exhibits excellent generalization abilities in the simulation environment, and the convergence speed, the loss value, the path planning time, and the path planning length of the APF-D3QNPER algorithm are all less than for other algorithms in diverse scenarios.


title: Adaptive Stress Testing without Domain Heuristics using Go-Explore
abstract: Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems.
During execution, the RL agents often rely on some domains-pecific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible.
Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures.
For example, some approaches give rewards for taking more likely actions, in order to to find more likely failures.
However, the agent may then learn to only take likely actions, and may not be able to find a failure at all.
Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration.
A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field.
We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most likely failure scenario.
We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road.
We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve.
Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.

title: Stable and Efficient Shapley Value-Based Reward Reallocation for Multi-Agent Reinforcement Learning of Autonomous Vehicles
abstract: With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs).

However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability.
When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process.
In this work, we propose to reallocate the system's total reward efficiently to motivate stable cooperation among autonomous vehicles.
We formally define and quantify how to reallocate the system's total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system's total reward.

We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game.
Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group.
We then propose a cooperative policy learning algorithm with Shapley value reward reallocation.
In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.

title: Stabilization of vertical motion of a vehicle on bumpy terrain using deep reinforcement learning
abstract: Stabilizing vertical dynamics for on-road and off-road vehicles is an important research area that has been looked at mostly from the point of view of ride comfort.
The advent of autonomous vehicles now shifts the focus more towards developing stabilizing techniques from the point of view of onboard proprioceptive and exteroceptive sensors whose real-time measurements influence the performance of an autonomous vehicle.

The current solutions to this problem of managing the vertical oscillations usually limit themselves to the realm of active suspension systems without much consideration to modulating the vehicle velocity, which plays an important role by the virtue of the fact that vertical and longitudinal dynamics of a ground vehicle are coupled.

The task of stabilizing vertical oscillations for military ground vehicles becomes even more challenging due lack of structured environments, like city roads or highways, in offroad scenarios.
Moreover, changes in structural parameters of the vehicle, such as mass (due to changes in vehicle loading), suspension stiffness and damping values can have significant effect on the controller's performance.

This demands the need for developing deep learning based control policies, that can take into account an extremely large number of input features and approximate a near optimal control action.
In this work, these problems are addressed by training a deep reinforcement learning agent to minimize the vertical acceleration of a scaled vehicle travelling over bumps by controlling its velocity.
Copyright (c) 2022 The Authors.
This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)

title: Safe, Efficient, and Comfortable Reinforcement-Learning-Based Car-Following for AVs with an Analytic Safety Guarantee and Dynamic Target Speed
abstract: Over the last decade, there has been rising interest in automated driving systems and adaptive cruise control (ACC).
Controllers based on reinforcement learning (RL) are particularly promising for autonomous driving, being able to optimize a combination of criteria such as efficiency, stability, and comfort.
However, RL-based controllers typically offer no safety guarantees.
In this paper, we propose SECRM (the Safe, Efficient, and Comfortable RL-based car-following Model) for autonomous car-following that balances traffic efficiency maximization and jerk minimization, subject to a hard analytic safety constraint on acceleration.

The acceleration constraint is derived from the criterion that the follower vehicle must have sufficient headway to be able to avoid a crash if the leader vehicle brakes suddenly.
We critique safety criteria based on the time-to-collision (TTC) threshold (commonly used for RL controllers), and confirm in simulator experiments that a representative previous TTC-threshold-based RL autonomous-vehicle controller may crash (in both training and testing).

In contrast, we verify that our controller SECRM is safe, in training scenarios with a wide range of leader behaviors, and in both regular-driving and emergency-braking test scenarios.
We find that SECRM compares favorably in efficiency, comfort, and speed-following to both classical (non-learned) car-following controllers (intelligent driver model, Shladover, Gipps) and a representative RL-based car-following controller.


title: Graph Reinforcement Learning-Based Decision-Making Technology for Connected and Autonomous Vehicles: Framework, Review, and Future Trends
abstract: The proper functioning of connected and autonomous vehicles (CAVs) is crucial for thesafety and efficiency of future intelligent transport systems.
Meanwhile, transitioning to fully autonomousdriving requires a long period of mixed autonomy traffic, including both CAVs andhuman-driven vehicles.
Thus, collaborative decision-making technology for CAVs is essential togenerate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomytraffic.
In recent years, deep reinforcement learning (DRL) methods have become an efficient way insolving decision-making problems.
However, with the development of computing technology, graphreinforcement learning (GRL) methods have gradually demonstrated the large potential to furtherimprove the decision-making performance of CAVs, especially in the area of accurately representingthe mutual effects of vehicles and modeling dynamic traffic environments.

To facilitate the developmentof GRL-based methods for autonomous driving, this paper proposes a review of GRL-basedmethods for the decision-making technologies of CAVs.
Firstly, a generic GRL framework is proposedin the beginning to gain an overall understanding of the decision-making technology.
Then, theGRL-based decision-making technologies are reviewed from the perspective of the constructionmethods of mixed autonomy traffic, methods for graph representation of the driving environment,and related works about graph neural networks (GNN) and DRL in the field of decision-makingfor autonomous driving.

Moreover, validation methods are summarized to provide an efficientway to verify the performance of decision-making methods.
Finally, challenges and future researchdirections of GRL-based decision-making methods are summarized.

title: Edge-enhanced Graph Attention Network for driving decision-making of autonomous vehicles via Deep Reinforcement Learning
abstract: Despite the rapid advancement in the field of autonomous driving vehicles, developing a safe and sensible decision-making system remains a challenging problem.
The driving decision-making module is one of the most essential sections of the entire autonomous driving system, and the decision generated from it can significantly impinge the lives and property of passengers.

Complicated interactions among traffic participants have the most profound impact on the decision-making process, yet the interactions are often simplified or overlooked due to their complexity and implicit nature.

To address this issue, this work proposes an Edge-Enhanced Graph Attention Reinforcement Learning (EGARL) framework that aims to make rational driving decisions by comprehensively modeling the interactions among agents.

EGARL comprises three core components: a graphical representation of the traffic scenario that covers both topological and interactive information; an Edge-enhanced Graph Attention Network (E-GAT) that utilizes the graphical representation to extract interactive features by comprehensively considering nodes and edges of the graph; and a deep reinforcement learning method that generates driving decisions based on the current state and features extracted from E-GAT.

Experimental results demonstrate the satisfying performance of EGARL.
Our proposed framework can contribute to the development of intelligent transportation systems, enhancing the safety and efficiency of driving.

title: Modeling Car Following Behavior of Autonomous Driving Vehicles Based on Deep Reinforcement Learning
abstract: In order to enhance the performance of car following behavior of autonomous vehicles and mitigate the negative effects of traffic oscillations, a deep reinforcement learning-based car following model for automated driving is investigated.

The existing reward function is improved by incorporating energy consumption, and the related terms for representing energy consumption are established based on the VT-Micro model.
In addition, the method of using the time gap between vehicles to establish the reward function related to driving efficiency is improved by adding virtual speed to the time gap, in order to avoid computation overflow and unrealistic short following distance in the traffic oscillation scenario.

To overcome the limitations of training on closed-loop simulated roads and simulated vehicle trajectories, human driver behavior extracted from the NGSIM trajectory data during traffic oscillation are used to develop the training environment.

By applying the twin delayed deep deterministic policy gradient algorithm (TD3), a multi-objective car following model is then developed.
A system for evaluating model performance is established to compare the performance of the TD3 model with traditional models in car following and traffic oscillations scenarios.
Study results of car following scenarios show that the TD3 model and the traditional adaptive cruise control (ACC) model perform similarly in terms of comfort and driving efficiency, but both outperform the human drivers.

In terms of safety, the TD3 model reduces safety hazards by 53.65% compared to the traditional ACC model, and 36.24% compared to the human drivers.
Regarding energy consumption, the TD3 model reduces the energy consumption of the conventional ACC model and human drivers by 6.73% and 15.65%, respectively.
Study results show that the TD3 model can reduce the negative impacts of traffic oscillations.
In the scenario with a 100% TD3 model penetration rate, driving discomfort decreases by 55.
95%, driving efficiency increases by 8.
82%, crash risks reduce by 73.
21%, and fuel consumption drops by 5.
97%, compared to a 100% human-driven environment.


title: Application of SAC-Based Autonomous Vehicle Control Method
abstract: In order to improve the problem of slow network convergence and unstable training process caused by equal probability sampling of SAC (soft actor critic) algorithm samples and random initialization of the network, an improved algorithm PE-SAC (priority playback soft actor) is proposed that combines priority playback and expert data.

The algorithm classifies the sample pool according to the sample value, uses expert data to pre-train the network, reduces the invalid exploration space of unmanned vehicles, reduces the number of trials and errors, and effectively improves the learning efficiency of the algorithm.

At the same time, a reward function for multiple obstacles is designed to enhance the applicability of the algorithm.
Simulation experiments are carried out on the CARLA platform, and the results show that the proposed method can better control the safe driving of unmanned vehicles in the environment, and the reward value and convergence speed obtained under the same training times are better than TD3 (twin delayed deep deterministic policy gradient algorithm) and SAC algorithm.

Finally, combined with the radar point cloud map and the PID (proportional integral derivative) control method, the difference between the simulation environment and the real scene is reduced, and the training model is transplanted to the low-speed unmanned vehicle in the park to verify the generality of the algorithm.


title: AdaptiveON: Adaptive Outdoor Local Navigation Method for Stable and Reliable Actions
abstract: We present a novel outdoor navigation algorithm to generate stable and efficient actions to navigate a robot to reach a goal.
We use a multi-stage training pipeline and show that our approach produces policies that result in stable and reliable robot navigation on complex terrains.
Based on the Proximal Policy Optimization (PPO) algorithm, we developed a novel method to achieve multiple capabilities for outdoor local navigation tasks, namely alleviating the robot's drifting, keeping the robot stable on bumpy terrains, avoiding climbing on hills with steep elevation changes, and avoiding collisions.

Our training process mitigates the reality (sim-to-real) gap by introducing generalized environmental and robotic parameters and training with rich features captured from light detection and ranging (Lidar) sensor in a high-fidelity Unity simulator.

We evaluate our method in both simulation and real-world environments using Clearpath Husky and Jackal robots.
Further, we compare our method against the state-of-the-art approaches and observe that, in the real world, our method improves stability by at least 30.
7% on uneven terrains, reduces drifting by 8.
08%, and decreases the elevation changes by 14.
75%.


title: A Multi-Agent Reinforcement Learning Approach for Safe and Efficient Behavior Planning of Connected Autonomous Vehicles
abstract: The recent advancements in wireless technology enable connected autonomous vehicles (CAVs) to gather information about their environment by vehicle-to-vehicle (V2V) communication.
In this work, we design an information-sharing based multi-agent reinforcement learning (MARL) framework for CAVs, to take advantage of the extra information when making decisions to improve traffic efficiency and safety.

The safe actor-critic algorithm we propose has two new techniques: the truncated Q-function and safe action mapping.
The truncated Q-function utilizes the shared information from neighboring CAVs such that the joint state and action spaces of the Q-function do not grow in our algorithm for a large-scale CAV system.
We prove the bound of the approximation error between the truncated -Q and global Q-functions.
The safe action mapping provides a provable safety guarantee for both the training and execution based on control barrier functions.
Using the CARLA simulator for experiments, we show that our approach improves the CAV system's efficiency in terms of average velocity and comfort under different CAV ratios and different traffic densities.

We also show that our approach avoids the execution of unsafe actions and always maintains a safe distance from other vehicles.
We construct an obstacle-at-corner scenario to show that the shared vision can help CAVs to observe obstacles earlier and take action to avoid traffic jams.
The experiment video is on https://songyanghan.github.io/cavmarl/.

title: Reinforcement Learning with Non-uniform State Representations for Adaptive Search
abstract: Efficient spatial exploration is a key aspect of search and rescue.
In this paper, we present a search algorithm that generates efficient trajectories that optimize the rate at which probability mass is covered by a searcher.
This should allow an autonomous vehicle find one or more lost targets as rapidly as possible.
We do this by performing non-uniform sampling of the search region.
The path generated minimizes the expected time to locate the missing target by visiting high probability regions using non-myopic path generation based on reinforcement learning.
We model the target probability distribution using a classic mixture of Gaussians model with means and mixture coefficients tuned according to the location and time of sightings of the lost target.
Key features of our search algorithm are the ability to employ a very general nondeterministic action model and the ability to generate action plans for any new probability distribution using the parameters learned on other similar looking distributions.

One of the key contributions of this paper is the use of non-uniform state aggregation for policy search in the context of robotics.
We compare the paths generated by our algorithm with other accepted spatial coverage techniques such as distribution independent boustrophedonic coverage and model dependent spiral search.
We present a proof showing that rewarding for clearing probability mass instead of locating the target does not bias the objective function.

The experiments show that the learned policy outperforms several well-known baselines even in scenarios different from the one it has been trained on.

title: Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions using Reinforcement Learning [arXiv]
abstract: The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure.
In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed.
Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand.

Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions.
We implement a centralised paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations.
Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians.
Furthermore, the distributive learning algorithm outperforms its centralised counterpart regarding computational cost (49.
55\%), benchmark rewards (25.
35\%), best cumulative rewards (24.
58\%), optimal actions (13.
49\%) and rate of convergence.

This novel road management technique could potentially contribute to the flow-adaptive and active mobility-friendly streets in the AVs era.

title: Cognitive Risk Control for Anti-Jamming V2V Communications in Autonomous Vehicle Networks
abstract: The future of intelligent transportation system (ITS) is expected to be composed of connected and autonomous vehicles (CAVs), the development of which will have great impact on peoples everyday life.
Unfortunately, this progress will be accompanied by all kinds of potential threats and attacks rising in CAV network.
As a legacy from traditional wireless networks, jamming attack is still one of the major and serious threats to vehicle-to-vehicle (V2V) communications.
In this paper, we investigate the anti-jamming V2V communication in CAV networks through power control in conjunction with channel selection.
Bringing into play a brain-inspired research tool called cognitive dynamic system (CDS), the general structure of cognitive risk control (CRC) is well-tailored to analyze and address the jamming problem.

Specifically, power control is carried out first using reinforcement learning, the result of which is then examined by a module called task-switch control.
Based on the risk assessment, a multi-armed bandit (MAB) problem is formulated to perform the channel-selection process when necessary.
Through continuous perception-action cycles (PACs), the feature of predictive adaptation is realized for the legitimate vehicle in its behavioral interactions with the jammer.
Simulation results have shown that the proposed method has desirable performance in terms of several evaluation metrics.

title: Adaptive Nonlinear Model Predictive Horizon Using Deep Reinforcement Learning for Optimal Trajectory Planning
abstract: This paper presents an adaptive trajectory planning approach for nonlinear dynamical systems based on deep reinforcement learning (DRL).
This methodology is applied to the authors' recently published optimization-based trajectory planning approach named nonlinear model predictive horizon (NMPH).
The resulting design, which we call 'adaptive NMPH', generates optimal trajectories for an autonomous vehicle based on the system's states and its environment.
This is done by tuning the NMPH's parameters online using two different actor-critic DRL-based algorithms, deep deterministic policy gradient (DDPG) and soft actor-critic (SAC).
Both adaptive NMPH variants are trained and evaluated on an aerial drone inside a high-fidelity simulation environment.
The results demonstrate the learning curves, sample complexity, and stability of the DRL-based adaptation scheme and show the superior performance of adaptive NMPH relative to our earlier designs.

title: Robot Navigation in Crowds by Graph Convolutional Networks With Attention Learned From Human Gaze
abstract: Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task.
Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies.
However, their performance deteriorates when the crowd size grows.
We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation.
We propose a novel network utilizing a graph representation to learn the policy.
We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd as they perform a navigation task based on a top down view of the environment.

We incorporate the learned attention into a graph-based reinforcement learning architecture.
The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability.
Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods, increasing task completion rate by 18.
4% and decreasing navigation time by 16.
4%.


title: Merging guidance of exclusive lanes for connected and autonomous vehicles based on deep reinforcement learning
abstract: Exclusive lanes for connected and autonomous vehicles(CAVs)will emerge in order to ensure the safety and efficiency requirements in the process of traffic flow mixed with human-driving vehicles and CAVs.

When the inner lane of the expressway is set as the exclusive lane for CAVs, it has important theoretical significance and practical value to study the strategy of guiding CAVs to merge from the ordinary lane to the exclusive lane.

Firstly, the entrance area of exclusive lane was designed and vehicle control rules were proposed.
Secondly, with the goal of making more CAVs change lanes to the exclusive lane, the strategy of selecting lane-changing signal actions was proposed based on deep reinforcement learning.
Finally, the numerical simulation was carried out with Python language compilation.
The results show that the proposed algorithm can converge very quickly under 9 scenarios constructed by different factors, such as the CAV penetration rates and the proportion of CAVs arriving at the exclusive lane; it can effectively guide CAVs to merge into exclusive lanes and ensure traffic efficiency; congestion in the second lane can be significantly reduced compared to the unsignalized control when the penetration rate changes from 20% to 40%; the proportion of CAVs changing to the exclusive lane is significantly higher under the two exclusive lane entrances scenario than that under the one entrance scenario.

It shows that the proposed strategy has good applicability and can provide reference for engineering construction.

title: Adaptive stress testing without domain heuristics using go-explore [arXiv]
abstract: Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems.
During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible.
Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures.
For example, some approaches give rewards for taking more-likely actions, because we want to find more-likely failures.
However, the agent may then learn to only take likely actions, and may not be able to find a failure at all.
Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration.
A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field.
We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most-likely failure scenario.
We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road.
We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve.
Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.

title: Learning When to Use Adaptive Adversarial Image Perturbations Against Autonomous Vehicles
abstract: Deep neural network (DNN) models are widely used in autonomous vehicles for object detection using camera images.
However, these models are vulnerable to adversarial image perturbations.
Existing methods for generating these perturbations use the image frame as the decision variable, resulting in a computationally expensive optimization process that starts over for each new image.
Few approaches have been developed for attacking online image streams while considering the physical dynamics of autonomous vehicles, their mission, and the environment.
To address these challenges, we propose a multi-level stochastic optimization framework that monitors the attacker's capability to generate adversarial perturbations.
Our framework introduces a binary decision attack/not attack based on the attacker's capability level to enhance its effectiveness.
We evaluate our proposed framework using simulations for vision-guided autonomous vehicles and actual tests with a small indoor drone in an office environment.
Our results demonstrate that our method is capable of generating real-time image attacks while monitoring the attacker's proficiency given state estimates.

title: Agile Tyre Mobility: Observation and Control in Severe Terrain Environments
abstract: This research study develops fundamentals for a new ground vehicle technology to radically improve and protect off-road vehicle mobility by providing agile (fast, exact and pre-emptive) responses and advanced mobility controls in severe terrain conditions.

The current framework of terrain vehicle mobility that estimates a vehicle capability to go through or not to go through the given terrain conditions cannot provide an analytical basis for novel system design solutions.

Indeed, modern traction control and other mobility related electronic control systems possess control response time within the range of 100-120 milliseconds and greater.
With this response time, the actual control occurs after the vehicle has reached a critical motion situation, e.g., a wheel(s) is/are spinning and the vehicle is already losing its mobility.
In this study, the developed methods allowed for estimating tyre mobility and controlling tyre motion before the tyre starts spinning.
As shown in the conducted analysis, the response time, which occurs within the longitudinal tyre relaxation time constant of 40-60 ms, is sufficient for a tyre to avoid spinning and to maintain its required mobility.

Most common traditional approaches to observation of data supplied by virtual sensors were simulated and improved by means of machine learning algorithms.
Computational simulations of an one-wheel-locomotion module driven by an electric driveline system demonstrated a sufficient performance of the proposed observation method to estimate mobility margins of the module in real time.
A hybrid intelligent control algorithm was designed, in which reinforcement learning was used to fine-tune the parameters of a fuzzy logic controller.

A new wheel mobility index was utilized as a cost function to guarantee a designed behavior of the locomotion module.
A fuzzy corrector was additionally designed to take into account both the dynamic state of the system and the dynamics of the tyre-terrain interaction.
The fuzzy corrector supports upper level controls of autonomous vehicle dynamics by decreasing tyre slippage on severe terrains.
Computer simulations testified both stability of the controller (due to utilization of fuzzy logic polynomial control) and its desired performance (due to application of reinforcement learning).

The fine-tuned controller requires minimal online computations.This paper provides an extended summary of the above-listed research studies.
Further details can be found in publications referenced in the paper.

title: Deep-Reinforcement-Learning-Based Latency Minimization in Edge Intelligence Over Vehicular Networks
abstract: A novel paradigm that combines federated learning with blockchain to empower edge intelligence over vehicular networks (FBVN) can enable latency-sensitive deep neural network-based applications to be executed in a distributed pattern.

However, the complex environments in FBVN make the system latency much harder to minimize by traditional methods.
In this article, we model the training and transmission latency of each autonomous vehicle (AV) and consensus latency of the blockchain in-edge side in FBVN.
Considering the dynamic and time-varying wireless channel conditions, unpredictable packet error rate, and unstable data sets quality, we adopt duel deep Q-learning (DDQL) as the solving approach.
We propose a federated DDQL algorithm, in which the learning agent is deployed on each AV side, and the sensing states on each AV do not need to be shared so that it increases scalability and flexibility for practical implementation.

Simulation results show that the proposed algorithm has better performance in reducing system latency compared with the other schemes.

title: Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing
abstract: Motion planning for autonomous racing is a challenging task due to the safety requirement while driving aggressively.
Most previous solutions utilize the prior information or depend on complex dynamics modeling.
Classical model-free reinforcement learning methods are based on random sampling, which severely increases the training consumption and undermines the exploration efficiency.
In this letter, we propose an efficient residual policy learning method for high-speed autonomous racing named ResRace, which leverages only the real-time raw observation of LiDAR and IMU for low-latency obstacle avoiding and navigation.

We first design a controller based on the modified artificial potential field (MAPF) to generate a policy for navigation.
Besides, we utilize the deep reinforcement learning (DRL) algorithm to generate a residual policy as a supplement to obtain the optimal policy.
Concurrently, the MAPF policy effectively guides the exploration and increases the update efficiency.
This complementary property contributes to the fast convergence and few required resources of our method.
We also provide extensive experiments to illustrate our method outperforms the leading algorithms and reaches the comparable level of professional human players on the five F1Tenth tracks.

title: Control of a Mixed Autonomy Signalised Urban Intersection: An Action-Delayed Reinforcement Learning Approach
abstract: We consider a mixed autonomy scenario where the traffic intersection controller decides whether the traffic light will be green or red at each lane for multiple traffic-light blocks.
The objective of the traffic intersection controller is to minimize the queue length at each lane and maximize the outflow of vehicles over each block.
We consider that the traffic intersection controller informs the autonomous vehicle (AV) whether the traffic light will be green or red for the future traffic-light block.
Thus, the AV can adapt its dynamics by solving an optimal control problem.
We model the decision process of the traffic intersection controller as a deterministic delayed Markov decision process owing to the delayed action by the traffic controller.
We propose Reinforcement Learning based model-free algorithm to obtain the optimal policy.
We show - by extensive simulations - that our algorithm converges and drastically reduces the energy costs of AVs as the traffic controller communicates with the AVs.

title: eSense 2.0: modeling multi-agent biomimetic predation with multi-layered reinforcement learning
abstract: Learning in multi-agent systems, especially with adversarial behavior being exhibited, is difficult and challenging.
The learning within these complicated environments is often muddied by the multitudinous conflicting or poorly correlated data coming from the multiple agents and their diverse goals.
This should not be compared against well-known flocking-type behaviors where each agent has the same policy; rather, in our scenario each agent may have their own policy, sets of behaviors, or overall group strategy.

Most learning algorithms will observe the actions of the agents and inform their algorithm which seeks to form the models.
When these actions are consistent a reasonable model can be formed; however, eSense was designed to work even when observing complicated and highly-interactive must-agent behavior.
eSense provides a powerful yet simplistic reinforcement learning algorithm that employs model-based behavior across multiple learning layers.
These independent layers split the learning objectives across multiple layers, avoiding the learning-confusion common in many multi-agent systems.
We examine a multi-agent predator-prey biomimetic sensing environment that simulates such coordinated and adversarial behaviors across multiple goals.
This work could also be applied to theater wide autonomous vehicle coordination, such as that of the hierarchical command and control of autonomous drones and ground vehicles.

title: Robust Multiagent Reinforcement Learning toward Coordinated Decision-Making of Automated Vehicles
abstract: Automated driving is essential for developing and deploying intelligent transportation systems.
However, unavoidable sensor noises or perception errors may cause an automated vehicle to adopt suboptimal driving policies or even lead to catastrophic failures.
Additionally, the automated driving longitudinal and lateral decision-making behaviors (e.
g.
, driving speed and lane changing decisions) are coupled, that is, when one of them is perturbed by unknown external disturbances, it causes changes or even performance degradation in the other.

The presence of both challenges significantly curtails the potential of automated driving.
Here, to coordinate the longitudinal and lateral driving decisions of an automated vehicle while ensuring policy robustness against observational uncertain-ties, we propose a novel robust coordinated decision-making technique via robust multiagent reinforcement learning.

Specifically, the automated driving longitudinal and lateral decisions under observational perturbations are modeled as a constrained robust multiagent Markov decision process.
Meanwhile, a nonlinear constraint setting with Kullback-Leibler divergence is developed to keep variation of the driving policy perturbed by stochastic perturbations within bounds.
Additionally, robust multiagent policy optimization approach is proposed to approximate the optimal robust coordinated driving policy.
Finally, we evaluate the proposed robust coordinated decision-making method in three highway scenarios with different traffic densities.
Quantitatively, in the absence noises, the proposed method achieves an approximate average enhancement of 25.
58% in traffic efficiency and 91.
31% in safety compared to all baselines across the three scenarios.

In the presence of noises, our technique improves traffic efficiency and safety by an approximate average of 30.81% and 81.02% compared to all baselines in the three scenarios, respectively.
The results demonstrate that the proposed approach is capable of improving automated driving performance and ensuring policy robustness against observational uncertainties.

title: High-Speed Highway Scene Prediction Based on Driver Models Learned From Demonstrations
abstract: One of the key factors to ensure the safe operation of autonomous and semi-autonomous vehicles in dynamic environments is the ability to accurately predict the motion of the dynamic obstacles in the scene.

In this work, we show how to use a realistic driver model learned from demonstrations via Inverse Reinforcement Learning to predict the long-term evolution of highway traffic scenes.
We model each traffic participant as a Markov Decision Process in which the cost function is a linear combination of static and dynamic features.
In particular, the static features capture the preferences of the driver while the dynamic features, which change over time depending on the actions of the other traffic participants, capture the driver's risk-aversive behavior.

Using such a model for prediction enables us to explicitly consider the interactions between traffic participants while keeping the computational complexity quadratic in the number of vehicles in the scene.

Preliminary experiments in simulated and real scenarios show the capability of our approach to produce reliable, human-like scene predictions.

title: Certified Adversarial Robustness for Deep Reinforcement Learning [arXiv]
abstract: Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness.

Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane.

In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates.
This work leverages research on certified adversarial robustness to develop an online certified defense for deep reinforcement learning algorithms.
The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise.

The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task.
This work extends our previous paper with new performance guarantees, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.


title: Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal Fluid Processes [arXiv]
abstract: Persistent monitoring of a spatiotemporal fluid process requires data sampling and predictive modeling of the process being monitored.
In this paper we present PASST algorithm: Predictive-model based Adaptive Sampling of a Spatio-Temporal process.
PASST is an adaptive robotic sampling algorithm that leverages predictive models to efficiently and persistently monitor a fluid process in a given region of interest.
Our algorithm makes use of the predictions from a learned prediction model to plan a path for an autonomous vehicle to adaptively and efficiently survey the region of interest.
In turn, the sampled data is used to obtain better predictions by giving an updated initial state to the predictive model.
For predictive model, we use Knowledged-based Neural Ordinary Differential Equations to train models of fluid processes.
These models are orders of magnitude smaller in size and run much faster than fluid data obtained from direct numerical simulations of the partial differential equations that describe the fluid processes or other comparable computational fluids models.

For path planning, we use reinforcement learning based planning algorithms that use the field predictions as reward functions.
We evaluate our adaptive sampling path planning algorithm on both numerically simulated fluid data and real-world nowcast ocean flow data to show that we can sample the spatiotemporal field in the given region of interest for long time horizons.

We also evaluate PASST algorithm's generalization ability to sample from fluid processes that are not in the training repertoire of the learned models.

title: Traffic Agent Trajectory Prediction Using Social Convolution and Attention Mechanism
abstract: The trajectory prediction is significant for the decision-making of autonomous driving vehicles.
In this paper, we propose a model to predict the trajectories of target agents around an autonomous vehicle.
The main idea of our method is considering the history trajectories of the target agent and the influence of surrounding agents on the target agent.
To this end, we encode the target agent history trajectories as an attention mask and construct a social map to encode the interactive relationship between the target agent and its surrounding agents.
Given a trajectory sequence, the LSTM networks are firstly utilized to extract the features for all agents, based on which the attention mask and social map are formed.
Then, the attention mask and social map are fused to get the fusion feature map, which is processed by the social convolution to obtain a fusion feature representation.
Finally, this fusion feature is taken as the input of a variable-length LSTM to predict the trajectory of the target agent.
We note that the variable-length LSTM enables our model to handle the case that the number of agents in the sensing scope is highly dynamic in traffic scenes.
To verify the effectiveness of our method, we widely compare with several methods on a public dataset, achieving a 20% error decrease.
In addition, the model satisfies the real-time requirement with the 32 fps.

title: Optimal design of a driver assistance controller based on surrounding vehicle?s social behavior game model
abstract: Driver assistance control in the cut-in scenarios is challenging, since the controller needs to ensure driving safety and avoid unnecessary intervention, while considering the inter-action with surrounding vehicles.

This paper proposes an optimal driver assistance con-troller considering the social behaviors of the surrounding vehicles to assist the drivers in the cut-in scenarios.
To model the social behavior of the surrounding vehicle, we first for-mulate the interaction between the semi-autonomous vehicle and the surrounding vehicle as a misinformation game, which is achieved by assuming the surrounding vehicle is in-teracting with a hypothetical vehicle under the framework of non-cooperative game.

Then, adaptive dynamic programming theory is utilized to find the Nash equilibrium, which is represented by deep neural networks and solved iteratively.
Based on the established so-cial behavior model and the nonlinear driver-vehicle dynamic model, an affine input non-linear system model is obtained for the design of driver assistance controller, and the op-timal assistance control strategy is also derived under the structure of adaptive dynamic programming.

Several numerical simulations and driver-in-the-loop simulator experiments are conducted for validation.
Results show that the proposed strategy can assist the driver in the cut-in scenario while addressing different social interactions with the surround-ing vehicles.
Importantly, by taking into account the surrounding vehicle's social behavior through our established social behavior model, our proposed strategy significantly outper-forms the no-interaction strategy both in terms of driving safety and intervention degree, validating its effectiveness and superiority.

(c) 2022 Published by Elsevier Inc.

title: Parallel Planning: A New Motion Planning Framework for Autonomous Driving
abstract: Motion planning is one of the most significant technologies for autonomous driving.
To make motion planning models able to learn from the environment and to deal with emergency situations, a new motion planning framework called as "parallel planning" is proposed in this paper.
In order to generate sufficient and various training samples, artificial traffic scenes are firstly constructed based on the knowledge from the reality.
A deep planning model which combines a convolutional neural network (CNN) with the Long Short-Term Memory module (LSTM) is developed to make planning decisions in an end-toend mode.
This model can learn from both real and artificial traffic scenes and imitate the driving style of human drivers.
Moreover, a parallel deep reinforcement learning approach is also presented to improve the robustness of planning model and reduce the error rate.
To handle emergency situations, a hybrid generative model including a variational auto-encoder (VAE) and a generative adversarial network (GAN) is utilized to learn from virtual emergencies generated in artificial traffic scenes.

While an autonomous vehicle is moving, the hybrid generative model generates multiple video clips in parallel, which correspond to different potential emergency scenarios.
Simultaneously, the deep planning model makes planning decisions for both virtual and current real scenes.
The final planning decision is determined by analysis of real observations.
Leveraging the parallel planning approach, the planner is able to make rational decisions without heavy calculation burden when an emergency occurs.

title: Improve generalization of driving policy at signalized intersections with adversarial learning
abstract: Intersections are quite challenging among various driving scenes wherein the interaction of signal lights and distinct traffic actors poses great difficulty to learn a wise and robust driving policy.
Current research rarely considers the diversity of intersections and stochastic behaviors of traffic participants.
For practical applications, the randomness usually leads to some devastating events, which should be the focus of autonomous driving.
This paper introduces an adversarial learning paradigm to boost the intelligence and robustness of driving policy for signalized intersections with dense traffic flow.
Firstly, we design a static path planner which is capable of generating trackable candidate paths for multiple intersections with diversified topology.
Next, a constrained optimal control problem (COCP) is built based on these candidate paths wherein the bounded uncertainty of dynamic models is considered to capture the randomness of driving environment.

We propose adversarial policy gradient (APG) to solve the COCP wherein the adversarial policy is introduced to provide disturbances by seeking the most severe uncertainty while the driving policy learns to handle this situation by competition.

Finally, a comprehensive system is established to conduct training and testing wherein the perception module is introduced and the human experience is incorporated to solve the yellow light dilemma.
Simulation results indicate that the trained policy can handle the signal lights flexibly meanwhile realizing smooth and efficient passing with a humanoid paradigm.
Besides, APG enables a large-margin improvement of the resistance to the abnormal behaviors and thus ensures a high safety level for the autonomous vehicle.

title: Learning predictive representations in autonomous driving to improve deep reinforcement learning [arXiv]
abstract: Reinforcement learning using a novel predictive representation is applied to autonomous driving to accomplish the task of driving between lane markings where substantial benefits in performance and generalization are observed on unseen test roads in both simulation and on a real Jackal robot.

The novel predictive representation is learned by general value functions (GVFs) to provide out-of-policy, or counter-factual, predictions of future lane centeredness and road angle that form a compact representation of the state of the agent improving learning in both online and offline reinforcement learning to learn to drive an autonomous vehicle with methods that generalizes well to roads not in the training data.

Experiments in both simulation and the real-world demonstrate that predictive representations in reinforcement learning improve learning efficiency, smoothness of control and generalization to roads that the agent was never shown during training, including damaged lane markings.

It was found that learning a predictive representation that consists of several predictions over different time scales, or discount factors, improves the performance and smoothness of the control substantially.

The Jackal robot was trained in a two step process where the predictive representation is learned first followed by a batch reinforcement learning algorithm (BCQ) from data collected through both automated and human-guided exploration in the environment.

We conclude that out-of-policy predictive representations with GVFs offer reinforcement learning many benefits in real-world problems.

title: Driving policies of V2X autonomous vehicles based on reinforcement learning methods
abstract: Autonomous driving has been achieving great progress since last several years.
However, the autonomous vehicles always ignore the important traffic information on the road because of the uncertainties of driving environment and the limitations of onboard sensors.
This might cause serious safety problem in autonomous driving.
This study argues that the connected vehicles could share much more environmental information with each other.
Therefore, a decision-making method based on reinforcement learning is proposed for V2X autonomous vehicles.
First, the V2X autonomous driving architecture with three subsystems is designed.
By V2V communication, an autonomous vehicle could obtain much more environmental information.
Second, a reinforcement learning based model is applied to learn from the V2V observation data.
A simulation environment is setup based on OpenAI reinforcement learning framework.
The experimental results demonstrate the effectiveness of the V2X in autonomous driving.

title: Sampling-based Inverse Reinforcement Learning Algorithms with Safety Constraints
abstract: Planning for robotic systems is frequently formulated as an optimization problem.
Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL).
Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective.
The consideration of safety constraints is of paramount importance for human-robot collaboration.
For this reason, our work addresses maximum entropy IRL in constrained environments.
Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs).

(2) We transfer maximum entropy IRL to CMDPs based on CSRL.
(3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching.
In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.

title: Route Planning for Autonomous Transmission of Large Sport Utility Vehicle
abstract: The autonomous driving aims at ensuring the vehicle to effectively sense the environment and use proper strategies to navigate the vehicle without the interventions of humans.
Hence, there exist a prediction of the background scenes and that leads to discontinuity between the predicted and planned outputs.
An optimal prediction engine is required that suitably reads the background objects and make optimal decisions.
In this paper, the author(s) develop an autonomous model for vehicle driving using ensemble model for large Sport Utility Vehicles (SUVs) that uses three different modules involving (a) recognition mod-el, (b) planning model and (c) prediction model.

The study develops a direct realization method for an autonomous vehicle driving.
The direct realization method is designed as a behavioral model that incorporates three different modules to ensure optimal autonomous driving.
The behavioral model includes recognition, planning and prediction modules that regulates the input trajectory processing of input video datasets.
A deep learning algorithm is used in the proposed approach that helps in the classification of known or unknown objects along the line of sight.
This model is compared with conventional deep learning classifiers in terms of recall rate and root mean square error (RMSE) to estimate its efficacy.
Simulation results on different traffic environment shows that the Ensemble Convolutional Network Reinforcement Learning (E-CNN-RL) offers increased accuracy of 95.
45%, reduced RMSE and increased recall rate than existing Ensemble Convolutional Neural Networks (CNN) and Ensemble Stacked CNN.


title: HAZARD DETECTION AI SYSTEM FOR AUTONOMOUS VEHICLE
abstract: Perform literature review.Develop a Perception AI system based on deep learning using publicly available data (e.g.
KAIST Multi-Spectral Day/Night Data Set (Choi et al., 2018)) to: (a) Predict the intent for different road users e.g.
pedestrians, cyclists etc.
(b) Detect static road features e.g.
road works, road signs etc.
and (c) Detect weather condition e.g.
fog, rain etc.Investigate the deep learning framework e.g.
TensorFlow or PyTorch.
Build, train, test and validate the neural network.
Determine the performance of the AI in an unconstrained environment.
Investigate reinforcement learning for continuous learning of the developed AI system.
Develop a Hazard Detection AI system for AV using deep learning based on the output from the Perception AI system.
Build, train, test and validate the neural network.
Determine the performance of the Hazard Detection AI system system in an unconstrained environment.
Investigate reinforcement learning for continuous learning of the developed Hazard Detection AI system.


title: The Real-Time Signal Control System Using Reinforcement Learning Considering Priority Signaling for Emergency Vehicle
abstract: Recently, with the development of autonomous vehicles and V2X (Vehicle-to-Everyting) technology, researches are being conducted to reduce vehicle delay time and unnecessary stop-and-go phenomenon at the urban signalized intersection.

In particular, current urban networks, in which the fixed-time traffic signal control system is dominant, can not reflect the real-time traffic situation in the signaling system.
Even if the autonomous vehicle is commercialized, it could not exhibit its performance.
At this time, if the artificial intelligence traffic signal control system based on accurate real-time vehicle information obtained through V2I technology is developed, more efficient traffic signal control will be possible and it will affect not only traffic flow but also the performance of the V2X technology-equipped vehicles.

Especially, when there is unnecessary waiting time for the emergency vehicle in the urban intersection, it may affect the travel time to reach the destination.
In case of driving while leaving the lane for the purpose of shortening the travel time, it can threaten the safety of a patient and surrounding vehicles.
At this point, if the traffic signal control system that detects the entrance of the emergency vehicle to an intersection through the V2I technology and gives the priority signal to the emergency vehicle is developed, the safer emergency vehicle operation could be guaranteed.

In this paper, the change of traffic flow according to the change of traffic signal control system is analyzed based on micro traffic information at single signalized intersection with V2I technology commercialization.

A reinforcement learning model that expresses the optimum signal in real-time by learning the traffic information at the single signalized intersection was constructed based on the deep learning through the tensor flow.

The performance of the developed traffic signal control system was verified through Vissim.
The developed reinforcement learning model expresses a specific signal phase through real-time traffic information and expresses appropriate signal display in the changed network situation.
The traffic signal control system developed in this paper is expected to contribute to achieve system optimization in a complex road network and to contribute to the analysis of traffic flow.
In addition, it will contribute to building a smart city when autonomous vehicles are operated in the future V2X environment.

title: Model-based versus Model-free Deep Reinforcement Learning for Autonomous Racing Cars [arXiv]
abstract: Despite the rich theoretical foundation of model-based deep reinforcement learning (RL) agents, their effectiveness in real-world robotics-applications is less studied and understood.
In this paper, we, therefore, investigate how such agents generalize to real-world autonomous-vehicle control-tasks, where advanced model-free deep RL algorithms fail.
In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional LiDAR sensors, on a set of test tracks with a gradual increase in their complexity.
In this continuous-control setting, we show that model-based agents capable of learning in imagination, substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization.

Moreover, we show that the generalization ability of model-based agents strongly depends on the observation-model choice.
Finally, we provide extensive empirical evidence for the effectiveness of model-based agents provided with long enough memory horizons in sim2real tasks.

title: Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles
abstract: A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment.
The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements.
Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations.
Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control.

The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques.
Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources.

Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor.

In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor.
We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp.
For purposes of comparison, the proposed model is evaluated vis-a-vis traditional rule-based and long short-term memory-based fusion models.
The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility.

That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams.
The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations.
The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities.

title: Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems
abstract: This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments.
To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL).
In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud.
Then, effective transfer learning methods in LFRL are introduced.
LFRL is consistent with human cognitive science and fits well in cloud robotic systems.
Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation.
The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge.
In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.

title: Deep Ensemble Reinforcement Learning with Multiple Deep Deterministic Policy Gradient Algorithm
abstract: Deep deterministic policy gradient algorithm operating over continuous space of actions has attracted great attention for reinforcement learning.
However, the exploration strategy through dynamic programming within the Bayesian belief state space is rather inefficient even for simple systems.
Another problem is the sequential and iterative training data with autonomous vehicles subject to the law of causality, which is against the i.i.d.
(independent identically distributed) data assumption of the training samples.
This usually results in failure of the standard bootstrap when learning an optimal policy.
In this paper, we propose a framework of m-out-of-n bootstrapped and aggregated multiple deep deterministic policy gradient to accelerate the training process and increase the performance.
Experiment results on the 2D robot arm game show that the reward gained by the aggregated policy is 10%-50% better than those gained by subpolicies.
Experiment results on the open racing car simulator (TORCS) demonstrate that the new algorithm can learn successful control policies with less training time by 56.7%.
Analysis on convergence is also given from the perspective of probability and statistics.
These results verify that the proposed method outperforms the existing algorithms in both efficiency and performance.

title: Flexpool: A Distributed Model-Free Deep Reinforcement Learning Algorithm for Joint Passengers & Goods Transportation
abstract: 

title: Deep-Q-Network hybridization with extended Kalman filter for accelerate learning in autonomous navigation with auxiliary security module
abstract: This article proposes an algorithm for autonomous navigation of mobile robots that mixes reinforcement learning with extended Kalman filter (EKF) as a localization technique, namely EKF-DQN, aiming to accelerate the maximization of the learning curve and improve the reward values obtained in the learning process.

More specifically, Deep-Q-Networks (DQN) are used to control the trajectory of an autonomous robot in an environment with many obstacles.
To improve navigation capability in this environment, we also propose a fusion of visual and nonvisual sensors.
Due to the ability of EKF to predict states, this algorithm is used as a learning accelerator for the DQN network, predicting future states and inserting this information into the memory replay.
Aiming to increase the safety of the navigation process, a visual safety system is also proposed to avoid collisions between the mobile robot and people circulating in the environment.
The efficiency of the proposed control system is verified through computational simulations using the CoppeliaSIM simulator with code insertion in Python.
The simulation results show that the EKF-DQN algorithm accelerates the maximization of rewards obtained and provides a higher success rate in fulfilling the mission assigned to the robot when compared to other value-based and policy-based algorithms.

A demo video of the navigation system can be seen at: .
This article proposes an algorithm for autonomous navigation of mobile robots that merges reinforcement learning with extended Kalman filter (EKF) as a localization technique, namely, EKF-DQN, aiming to accelerate learning and improve the reward values obtained in the process of apprenticeship.

More specifically, deep neural networks (DQN-Deep-Q-Networks) are used to control the trajectory of an autonomous vehicle in an indoor environment.
Due to the ability of EKF to predict states, this algorithm is proposed to be used as a learning accelerator of the DQN network, predicting states ahead and inserting this information in the memory replay.

Aiming at the enhancing safety of the navigation process, it is also proposed a visual safety system that avoids collisions of the mobile vehicle with people moving in the environment.
image

title: Understand-Before-Talk (UBT): A Semantic Communication Approach to 6G Networks
abstract: In Shannon theory, semantic aspects of communication were identified but considered irrelevant to the technical communication problems.
Semantic communication (SC) techniques have recently attracted renewed research interests in 6(th) generation (6G) wireless, because they have the capability to support an efficient interpretation of the significance and meaning intended by a sender (or accomplishment of the goal) when dealing with multi-modal data such as videos, images, audio, text messages, and so on, which would be the case for various applications such as intelligent transportation systems where each autonomous vehicle needs to deal with real-time videos and data from a number of sensors including radars.

To this end, most of the emerging SC works focus on specific data types and employ sophisticated machine learning models including deep learning and neural networks.
However, they could be impractical for multi-modal data possibly within a real-time constraint, relative to the purpose of the communication.
A notable difficulty of existing SC frameworks lies in handling the discrete constraints imposed on the pursued semantic coding and its interaction with the independent knowledge-base, which makes reliable semantic extraction extremely challenging.

Therefore, we develop a new hashing-based semantic extraction approach to SC framework, where our learning objective is to generate one time signatures (hash codes) using supervised learning for low latency, secure and efficient management of the SC dynamics.

We first evaluate the proposed semantic extraction framework over large image data sets, extend it with domain adaptive hashing and then demonstrate the effectiveness of "semantics signature" in bulk transmission and multi-modal data.


title: Prediction-Aware and Reinforcement Learning-Based Altruistic Cooperative Driving
abstract: Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs.
In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes.
Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents' behaviors and use that to forecast what might happen in the future.

Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness.

In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction.
We formulate the AV's decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework.

We also propose a Hybrid Predictive Network (HPN) that anticipates future observations.
The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN).
Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy.

We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.

title: Reinforcement learning for autonomous vehicle movements in wireless multimedia applications
abstract: We develop a Deep Reinforcement Learning (DeepRL)-based, multi-agent algorithm to efficiently control autonomous vehicles that are typically used within the context of Wireless Sensor Networks (WSNs), in order to boost application performance.

As an application example, we consider wireless acoustic sensor networks where a group of speakers move inside a room.
In a traditional setup, microphones cannot move autonomously and are, e.g., located at fixed positions.
We claim that autonomously moving microphones improve the application performance.
To control these movements, we compare simple greedy heuristics against a DeepRL solution and show that the latter achieves best application performance.
As the range of audio applications is broad and each has its own (subjective) per-formance metric, we replace those application metrics by two immediately observable ones: First, quality of information (QoI), which is used to measure the quality of sensed data (e.
g.
, audio signal strength).

Second, quality of service (QoS), which is used to measure the network's performance when forwarding data (e.g., delay).
In this context, we propose two multi-agent solutions (where one agent controls one microphone) and show that they perform similarly to a single-agent solution (where one agent controls all microphones and has a global knowledge).

Moreover, we show via simulations and theoretical analysis how other parameters such as the number of microphones and their speed impacts performance.
(c) 2023 Elsevier B.V. All rights reserved.

title: Distributed Control and Learning of Connected and Autonomous Vehicles Approaching and Departing Signalized Intersections
abstract: 

title: A Method for High-Value Driving Demonstration Data Generation Based on One-Dimensional Deep Convolutional Generative Adversarial Networks
abstract: As a promising sequential decision-making algorithm, deep reinforcement learning (RL) has been applied in many fields.
However, the related methods often demand a large amount of time before they can achieve acceptable performance.
While learning from demonstration has greatly improved reinforcement learning efficiency, it poses some challenges.
In the past, it has required collecting demonstration data from controllers (either human or controller).
However, demonstration data are not always available in some sparse reward tasks.
Most importantly, there exist unknown differences between agents and human experts in observing the environment.
This means that not all of the human expert's demonstration data conform to a Markov decision process (MDP).
In this paper, a method of reinforcement learning from generated data (RLfGD) is presented, and consists of a generative model and a learning model.
The generative model introduces a method to generate the demonstration data with a one-dimensional deep convolutional generative adversarial network.
The learning model applies the demonstration data to the reinforcement learning process to greatly improve the effectiveness of training.
Two complex traffic scenarios were tested to evaluate the proposed algorithm.
The experimental results demonstrate that RLfGD is capable of obtaining higher scores more quickly than DDQN in both of two complex traffic scenarios.
The performance of reinforcement learning algorithms can be greatly improved with this approach to sparse reward problems.

title: Affordance-based Reinforcement Learning for Urban Driving [arXiv]
abstract: Traditional autonomous vehicle pipelines that follow a modular approach have been very successful in the past both in academia and industry, which has led to autonomy deployed on road.
Though this approach provides ease of interpretation, its generalizability to unseen environments is limited and hand-engineering of numerous parameters is required, especially in the prediction and planning systems.

Recently, deep reinforcement learning has been shown to learn complex strategic games and perform challenging robotic tasks, which provides an appealing framework for learning to drive.
In this work, we propose a deep reinforcement learning framework to learn optimal control policy using waypoints and low-dimensional visual representations, also known as affordances.
We demonstrate that our agents when trained from scratch learn the tasks of lane-following, driving around inter-sections as well as stopping in front of other actors or traffic lights even in the dense traffic setting.

We note that our method achieves comparable or better performance than the baseline methods on the original and NoCrash benchmarks on the CARLA simulator.

title: CRII: CPS: RUI: Cognizant Learning for Autonomous Cyber-Physical Systems
abstract: This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).
&lt;br/&gt;&lt;br/&gt;The objective of this Computer and Information Science and Engineering (CISE) Research Initiation Initiative (CRII) proposal is to develop a cognizant learning framework for cyber-physical systems (CPS) that incorporates risk-sensitive and irrational decision making.

The necessity for such a framework is exemplified by two observations.
First, CPS such as self-driving cars will share an environment with other CPS and human users.
Human drivers demonstrate a heightened sensitivity to changes in speed and can easily adapt to changes in the environment and road conditions, which makes it essential for a CPS to have an ability to recognize non-rational behaviors.

Second, large amounts of data generated during their operation and limited access to models of their environments can make a CPS reliant on machine learning algorithms for decision making to meet performance requirements such as reachability and safety.

Our research will be grounded on improving behaviors of autonomous vehicles in realistic traffic situations.
Outcomes from this effort will contribute to the development of a research paradigm unifying control, learning, and behavioral economics.
Students at a Primarily Undergraduate Institution will benefit by being directly involved in all aspects of the research process.
Research tasks will involve a team of undergraduate students in a vertically integrated manner where more experienced students will mentor newer team members.
&lt;br/&gt;&lt;br/&gt;The proposed effort comprises two thrusts.
Thrust 1 will construct utilities to encode CPS performance objectives consistent with practical models of risk-sensitive and irrational decision making.
Strategies will be learned by formulating and solving a reinforcement learning problem to maximize this utility.
Methods to enable learned strategies to adequately consider delays between evaluation and execution of actions arising from the physical components of the CPS will be developed.
Thrust 2 will design algorithms to learn decentralized cognizant strategies when multiple CPS operate in the same environment.
To improve reliability in uncertain environments, or when feedback is sparse, techniques to identify contributions of each CPS to a shared utility will be identified.
Solution methodologies will be evaluated empirically through extensive experiments and theoretically by determining probabilistic performance guarantees.
The PI will develop a research agenda and new undergraduate curriculum in CPS and machine learning at Western Washington University (WWU).
Research and educational goals of the project will be integrated through the CARLA simulator for autonomous vehicle research and the F1/10 Autonomous Vehicle platform.
The multidisciplinary scope of the project will be emphasized in outreach efforts through Student Outreach Services and STEM Clubs at WWU to encourage and broaden participation from traditionally underrepresented student groups.
&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


title: Provably Safe Learning-Based Robot Control Via Anomaly Detection and Generalization Theory
abstract: 

title: Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios
abstract: When learning to behave in a stochastic environment where safety is critical, such as driving a vehicle in traffic, it is natural for human drivers to plan fallback strategies as a backup to use if ever there is an unexpected change in the environment.

Knowing to expect the unexpected, and planning for such outcomes, increases our capability for being robust to unseen scenarios and may help prevent catastrophic failures.
Control of Autonomous Vehicles (AVs) has a particular interest in knowing when and how to use fallback strategies in the interest of safety.
Due to imperfect information available to an AV about its environment, it is important to have alternate strategies at the ready which might not have been deduced from the original training data distribution.
In this paper we present a principled approach for a model-free Reinforcement Learning (RL) agent to capture multiple modes of behaviour in an environment.

We introduce an extra pseudo-reward term to the reward model, to encourage exploration to areas of state-space different from areas privileged by the optimal policy.
We base this reward term on a distance metric between the trajectories of agents, in order to force policies to focus on different areas of state-space than the initial exploring agent.
Throughout the paper, we refer to this particular training paradigm as learning fallback strategies.
We apply this method to an autonomous driving scenario and show that we are able to learn useful policies that would have otherwise been missed out on during training, and would have been unavailable to the agent when executing the control algorithm.


title: Spatiotemporal Costmap Inference for MPC Via Deep Inverse Reinforcement Learning
abstract: It can he difficult to autonomously produce driver behavior so that it appears natural to other traffic participants.
Through Inverse Reinforcement Learning (IRL), we can automate this process by learning the underlying reward function from human demonstrations.
We propose a new IRL algorithm that learns a goal-conditioned spatio-temporal reward function.
The resulting costmap is used by Model Predictive Controllers (MPCs) to perform a task without any hand-designing or hand-tuning of the cost function.
We evaluate our proposed Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL framework together with MPC in the CARLA simulator for autonomous driving, lane keeping, and lane changing tasks in a challenging dense traffic highway scenario.

Our proposed methods show higher success rates compared to other baseline methods including behavior cloning, state-of-the-art RL policies, and MPC with a learning-based behavior prediction model.

title: Decision-Making in Autonomous Driving Using Reinforcement Learning
abstract: 

title: Multi-Kernel Online Reinforcement Learning for Path Tracking Control of Intelligent Vehicles
abstract: Path tracking control of intelligent vehicles has to deal with the difficulties of model uncertainties and nonlinearities.
As a class of adaptive optimal control methods, reinforcement learning (RL) has received increasing attention in solving difficult control problems.
However, feature representation and online learning ability are two major problems to be solved for learning control of uncertain dynamic systems.
In this article, we propose a multi-kernel online RL approach for path tracking control of intelligent vehicles.
In the proposed approach, a multiple kernel feature learning framework is designed for online learning control based on dual heuristic programming (DHP) and the new online learning control algorithm is called multi-kernel DHP (MKDHP).

In MKDHP, instead of the expert knowledge for selecting and fine-tuning of a suitable kernel function, only a set of basic kernel functions is required to be predefined and the multi-kernel features can be learned for value function approximation in the critic.

The simulation studies on path tracking control for intelligent vehicles have been conducted under S-curve and urban road conditions.
The results demonstrated that compared with other typical path tracking controllers for intelligent vehicles, such as the linear quadratic regulator (LQR), the pure pursuit controller and the ribbon-based controller, the proposed multi-kernel learning controller can achieve better performance in terms of tracking precision and smoothness.


title: Self-evolution Scenarios for Simulation Tests of Autonomous Vehicles Based on Different Models of Driving Styles
abstract: To verify the safety of the decision-making results of autonomous vehicles (AVs), a method for generating driving models with autonomous decision-making and interaction capabilities was proposed, and the driving models were as background vehicles (BVs) and used to build a self-evolution simulation scenario to test the continuous decision-making capability of AVs.

First, based on reinforcement learning and a combination of inheritance and evolution ideas, different driving styles with autonomous decision-making and interaction capabilities were designed in this study.

Second, in the model-building stage, three styles of driving models, namely, conservative, general, and aggressive, were generated and trained.
The simulation training parameters for the general-style driving model were derived from the parameter distribution of a naturalistic driving dataset named highD to ensure fidelity.
Finally, based on this, an aggressive-style driving model with significant aggressive features was designed and trained to enhance the complexity and testing effect of the self-evolution scenario.
The results show that the distributions of parameters such as the car-following speed, distance headway, and lane-change moment time-to-collision obtained by using the highD dataset are in agreement with real data.

An average similarity of 88% is observed between the general-style driving model generated and the corresponding real data, which is an improvement of 20.
3% on the results obtained from the rule-based intelligent driver model (IDM).

The proposed self-evolution scenario is seven times more testable than the baseline scenario composed of IDMs for the different driving models generated, as confirmed by the number of collisions in which the system under test is primarily responsible.

Thus, self-evolution scenarios composed of the driving models designed and generated in this study can effectively support simulation tests for aiding the decision-making system in AVs.

title: Robust ASV Navigation Through Ground to Water Cross-Domain Deep Reinforcement Learning
abstract: This paper presents a framework to alleviate the Deep Reinforcement Learning (DRL) training data sparsity problem that is present in challenging domains by creating a DRL agent training and vehicle integration methodology.

The methodology leverages accessible domains to train an agent to solve navigational problems such as obstacle avoidance and allows the agent to generalize to challenging and inaccessible domains such as those present in marine environments with minimal further training.

This is done by integrating a DRL agent at a high level of vehicle control and leveraging existing path planning and proven low-level control methodologies that are utilized in multiple domains.
An autonomy package with a tertiary multilevel controller is developed to enable the DRL agent to interface at the prescribed high control level and thus be separated from vehicle dynamics and environmental constraints.

An example Deep Q Network (DQN) employing this methodology for obstacle avoidance is trained in a simulated ground environment, and then its ability to generalize across domains is experimentally validated.

Experimental validation utilized a simulated water surface environment and real-world deployment of ground and water robotic platforms.
This methodology, when used, shows that it is possible to leverage accessible and data rich domains, such as ground, to effectively develop marine DRL agents for use on Autonomous Surface Vehicle (ASV) navigation.

This will allow rapid and iterative agent development without the risk of ASV loss, the cost and logistic overhead of marine deployment, and allow landlocked institutions to develop agents for marine applications.


title: Deep Reinforcement Learning Approach for V2X Managed Intersections of Connected Vehicles
abstract: Intersections are major bottlenecks for road traffic, as well as the origin of many accidents.
Efficient management of traffic at intersections is required to ensure both safety and efficiency.
Yet, the traditional solutions (static signs, traffic lights) are limited in their efficiency as they consider the flow of vehicles and not the vehicles at the microscopic level.
By using inter-vehicular communication of connected vehicles, recent works have shown the possibility to have a great increase in the number of evacuated vehicles thanks to the possibility to give an individual right-of-way directly to each vehicle.

In this context of intersections of cooperative vehicles, the scheduling of this right-of-way in order to maximize the throughput of the intersection is still a challenging task, with regard to the hybrid and dynamic aspects of the problem.

In this paper, we propose an approach based on Deep Reinforcement Learning (DRL) to efficiently distribute the right-of-way to each vehicle.
A Markov Decision Process model of intersections of cooperative vehicles, enabling the application of DRL, is proposed.
The performance of the DRL-based scheduling is then compared with classic traffic lights, and with two state-of-the-art cooperative scheduling policies, showing the benefits of the approach (increase of the flow, reduction of CO2 emissions).


title: Learning-Based End-to-End Navigation for Planetary Rovers Considering Non-Geometric Hazards
abstract: Autonomous navigation plays an increasingly crucial role in rover-based planetary missions.
End-to-end navigation approaches developed upon deep reinforcement learning have enabled great adaptability in complex environments.
However, most existing works focus on geometric obstacle avoidance thus have limited capability to cope with ubiquitous non-geometric hazards, such as sinkage and slippage.
Autonomous navigation in unstructured harsh environments remains a great challenge requiring further in-depth study.
In this letter, a DRL-based navigation method is proposed to autonomously guide a planetary rover towards goals via hazard-free paths with low wheel slip ratios.
We introduce an end-to-end network architecture, in which the visual perception and the wheel-terrain interaction are fused to learn the representation of terrain mechanical properties implicitly and further facilitate policy learning for non-geometric hazard avoidance.

Our approach outperforms baseline methods in simulation evaluation with superior avoidance capabilities against geometric obstacles and non-geometric hazards.
Experiments conducted at a Mars emulation site suggest the successful deployment of our approach on a planetary rover prototype and the capacity of dealing with locomotion risks in real-world navigation tasks.


title: A Joint Learning and Communication Framework for Multi-Agent Reinforcement Learning over Noisy Channels [arXiv]
abstract: We propose a novel formulation of the "effectiveness problem" in communications, put forth by Shannon and Weaver in their seminal work [2], by considering multiple agents communicating over a noisy channel in order to achieve better coordination and cooperation in a multi-agent reinforcement learning (MARL) framework.

Specifically, we consider a multi-agent partially observable Markov decision process (MA-POMDP), in which the agents, in addition to interacting with the environment can also communicate with each other over a noisy communication channel.

The noisy communication channel is considered explicitly as part of the dynamics of the environment and the message each agent sends is part of the action that the agent can take.
As a result, the agents learn not only to collaborate with each other but also to communicate "effectively" over a noisy channel.
This framework generalizes both the traditional communication problem, where the main goal is to convey a message reliably over a noisy channel, and the "learning to communicate" framework that has received recent attention in the MARL literature, where the underlying communication channels are assumed to be error-free.

We show via examples that the joint policy learned using the proposed framework is superior to that where the communication is considered separately from the underlying MA-POMDP.
This is a very powerful framework, which has many real world applications, from autonomous vehicle planning to drone swarm control, and opens up the rich toolbox of deep reinforcement learning for the design of multi-user communication systems.


title: Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm
abstract: Validating the safety of autonomous systems generally requires the use of high-fidelity simulators that adequately capture the variability of real-world scenarios.
However, it is generally not feasible to exhaustively search the space of simulation scenarios for failures.
Adaptive stress testing (AST) is a method that uses reinforcement learning to find the most likely failure of a system.
AST with a deep reinforcement learning solver has been shown to be effective in finding failures across a range of different systems.
This approach generally involves running many simulations, which can be very expensive when using a high-fidelity simulator.
To improve efficiency, we present a method that first finds failures in a low-fidelity simulator.
It then uses the backward algorithm, which trains a deep neural network policy using a single expert demonstration, to adapt the low-fidelity failures to high-fidelity.
We have created a series of autonomous vehicle validation case studies that represent some of the ways low-fidelity and high-fidelity simulators can differ, such as time discretization.
We demonstrate in a variety of case studies that this new AST approach is able to find failures with significantly fewer high-fidelity simulation steps than are needed when just running AST directly in high-fidelity.

As a proof of concept, we also demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art high-fidelity simulator for finding failures in autonomous vehicles.

title: Personalized Adaptive Cruise Control and Impacts on Mixed Traffic
abstract: This paper presents a personalized adaptive cruise control (PACC) design that can learn human driver behavior and adaptively control the semi-autonomous vehicle (SAV) in the car-following scenario, and investigates its impacts on mixed traffic.

In mixed traffic where the SAV and human-driven vehicles share the road, the SAV's driver can choose a PACC tuning that better fits the driver's preferred driving strategies.
The individual driver's preferences are learned through the inverse reinforcement learning (IRL) approach by recovering a unique cost function from the driver's demonstrated driving data that best explains the observed driving style.

The proposed PACC design plans the motion of the SAV by minimizing the learned unique cost function considering the short preview information of the preceding human-driven vehicle.
The results reveal that the learned driver model can identify and replicate the personalized driving behaviors accurately and consistently when following the preceding vehicle in a variety of traffic conditions.

Furthermore, we investigated the impacts of the PACC with different drivers on mixed traffic by considering time headway, gap distance, and fuel economy assessments.
A statistical investigation shows that the impacts of the PACC on mixed traffic vary among tested drivers due to their intrinsic driving preferences.

title: Event-Triggered Model Predictive Adaptive Dynamic Programming for Road Intersection Path Planning of Unmanned Ground Vehicle
abstract: Autonomous driving of unmanned ground vehicle (UGV) at road intersection is a challenging task due to the complicated traffic conditions.
In this paper, an event-triggered model predictive adaptive dynamic programming (MPADP) algorithm is proposed for path planning of UGV at road intersection.
Following the critic-actor scheme of adaptive dynamic programming (ADP), cost function approximation and control policy generation are combined to formulate MPADP.
The infinite horizon cost function of ADP is stacked over predictive horizon of model predictive control (MPC), and then the infinite horizon cost function is converted to the finite horizon-stacked cost function in MPADP.

By minimizing the approximation error within predictive horizon, the approximation accuracy is enhanced.
Considering the limitation of energy consumption, the event-triggered mechanism is designed based on the mismatch of cost function approximation.
Three triggering conditions are designed, and the corresponding boundedness of approximation error is proved.
Simulation results illustrate the effectiveness, efficiency and feasibility in application of the event-triggered MPADP method for path planning at road intersection.

title: RMRL: Robot Navigation in Crowd Environments With Risk Map-Based Deep Reinforcement Learning
abstract: Achieving safe and effective navigation in crowds is a crucial yet challenging problem.
Recent work has mainly encoded the pedestrian-robot state pairs, which cannot fully capture the interactions among humans.
Besides, existing work attempts to achieve "hard" collision avoidance, which may leave no feasible path to the robot in human-rich scenarios.
We suppose that this can be addressed by introducing the local risk map and thus incorporate the risk map into the deep reinforcement learning architecture.
The proposed map structure contains the crowd interaction states and geometric information.
Meanwhile, a "soft" risk mapping of pedestrians is proposed to promote the robot to generate more humanlike motion patterns, and the riskaware dynamic window is designed to enhance the robot's obstacle avoidance ability.

Experiments show that our method outperforms the baseline in terms of navigation performance and social attributes.
Furthermore, we successfully validate the proposed policy through real-world environments.

title: Data-Driven Shared Steering Control of Semi-Autonomous Vehicles
abstract: This paper presents a cooperative/shared framework of the driver and his/her semi-autonomous vehicle in order to achieve desired steering performance.
In particular, a copilot controller and the driver together operate and control the vehicle.
Exploiting the classical small-gain theory, our proposed shared steering controller is developed independent of the unmeasurable internal states of the human driver, and only relies on his/her steering torque.

Furthermore, by adopting data-driven adaptive dynamic programming and an iterative learning scheme, the shared steering controller is studied from the measurable data of the driver and the vehicle.
Meanwhile, the accurate knowledge of the driver and the vehicle dynamics is unnecessary, which settles the problem of their potential parametric variations/uncertainties in practice.
The effectiveness of the proposed method is validated by rigorous analysis and demonstrated by numerical simulations.

title: Space-weighted information fusion using deep reinforcement learning: The context of tactical control of lane-changing autonomous vehicles and connectivity range assessment
abstract: The connectivity aspect of connected autonomous vehicles (CAV) is beneficial because it facilitates dissemination of traffic-related information to vehicles through Vehicle-to-External (V2X) communication.

Onboard sensing equipment including LiDAR and camera can reasonably characterize the traffic environment in the immediate locality of the CAV.
However, their performance is limited by their sensor range (SR).
On the other hand, longer-range information is helpful for characterizing imminent conditions downstream.
By contemporaneously coalescing the shortand long-range information, the CAV can construct comprehensively its surrounding environment and thereby facilitate informed, safe, and effective movement planning in the shortterm (local decisions including lane change) and long-term (route choice).

Current literature provides useful information on CAV control approaches that use only local information sensed from the proximate traffic environment but relatively little guidance on how to fuse this information with that obtained from downstream sources and from different time stamps, and how to use the fused information to enhance CAV movements.

In this paper, we describe a Deep Reinforcement Learning based approach that integrates the data collected through sensing and connectivity capabilities from other vehicles located in the proximity of the CAV and from those located further downstream, and we use the fused data to guide lane changing, a specific context of CAV operations.

In addition, recognizing the importance of the connectivity range (CR) to the performance of not only the algorithm but also of the vehicle in the actual driving environment, the study carried out a case study.

The case study demonstrates the application of the proposed algorithm and duly identifies the appropriate CR for each level of prevailing traffic density.
It is expected that implementation of the algorithm in CAVs can enhance the safety and mobility associated with CAV driving operations.
From a general perspective, its implementation can provide guidance to connectivity equipment manufacturers and CAV operators, regarding the default CR settings for CAVs or the recommended CR setting in a given traffic environment.


title: Adversarial scenario curriculums for navigation and scene understanding
abstract: Brief description of the context of the research including potential impact:Autonomous vehicles are increasingly reliant on deep-learned solutions for many tasks within scene understanding and navigation.

While broadly improving effectiveness, these solutions often cannot guarantee robustness, whichcontrasts with the safety-critical nature of these applications.
Many rare challenging scenarios are likely to be absent from training sets but could lead to unexpected behaviours and disastrous outcomes when encountered in practice.
The ability to generate synthetic examples of such rare and adversarial scenarios for a given system during learning will help assess its robustness and improve it.
Aims and Objectives:This work will create a framework to automatically discover challenging scenarios that have not been observed in an autonomous vehicle system's training set.

The framework will be able to understand how to synthesise an autonomous vehicle scenario along several &quot;axes&quot; of difficulty - which we propose to include the appearance of the surroundings and the configuration of a scene.
Moreover, we propose a curriculum-based method to incorporate these synthesised scenarios into the training of the studied systems to improve their performance and robustness before deployment as compared to standard neural network training techniques.
Novelty of the research methodology:While many research areas relevant to our project are well explored, they often consider specific components of a challenging scenario (e.
g.

adversarial attacks for image classification).
However, the framework we envision will contain many interacting aspects, such as modelling distributions, synthesising data, efficiently searching for adversarial scenarios, or augmenting training.
To develop it, we will combine ideas from generative modelling, active learning, simulation, reinforcement learning, adversarial attacks, and curriculum learning.

Our task will include devising how existing methods can interact and combining ideas from different fields into a new framework.
Alignment to EPSRC's strategies and research areas:This project relates to the EPSRC's research areas: robotics, artificial intelligence technologies,engineering design, software engineering, and verification and correctness.
Companies or collaborators involved:We will conduct the project in collaboration with industry-partner Oxbotica

title: Online longitudinal trajectory planning for connected and autonomous vehicles in mixed traffic flow with deep reinforcement learning approach
abstract: This manuscript presents an Adam optimization-based Deep Reinforcement Learning model for Mixed Traffic Flow control (ADRL-MTF), so as to guide Connected and Autonomous vehicle's (CAV) longitudinal trajectory on a typical urban roadway with signal-controlled intersections.

Two improvements are made when compared with prior literatures.
First, the common assumptions to simplify the problem solving, such as dividing a vehicle trajectory into several segments with constant acceleration/deceleration, are avoided, to improve the modeling realism.

Second, built on the efficient Adam Optimization and Deep Q-Learning, the proposed mod& avoids the enumeration of states and actions, and is computational efficient and suitable for real time applications.

The mixed traffic flow dynamic is firstly formulated as a finite Markov decision process (MDP) model.
Due to the discretization of time, space and speed, this MDP model becomes high-dimensional in state, and is very challenging to solve.
We then propose a temporal difference-based deep reinforcement learning approach, with E-greedy for exploration-exploitation balance.
Two neural networks are developed to replace the traditional Q function and generate the targets in the Q-learning update.
These two neural networks are trained by the Adam optimization algorithm, which extends stochastic gradient descent and considers the second moments of the gradients, and is thus highly computational efficient and has lower memory requirements.

The proposed model is shown to reduce fuel consumption by 7.8%, which outperforms a prior benchmark model based on Monte Carlo Tree Search.
The model's runtime efficiency and stability are tested, and the sensitivity analysis is also performed.

title: A reinforcement learning path planning approach for range-only underwater target localization with autonomous vehicles
abstract: Underwater target localization using range-only and single-beacon (ROSB) techniques with autonomous vehicles has been used recently to improve the limitations of more complex methods, such as long baseline and ultra-short baseline systems.

Nonetheless, in ROSB target localization methods, the trajectory of the tracking vehicle near the localized target plays an important role in obtaining the best accuracy of the predicted target position.

Here, we investigate a Reinforcement Learning (RL) approach to find the optimal path that an autonomous vehicle should follow in order to increase and optimize the overall accuracy of the predicted target localization, while reducing time and power consumption.

To accomplish this objective, different experimental tests have been designed using state-of-the-art deep RL algorithms.
Our study also compares the results obtained with the analytical Fisher information matrix approach used in previous studies.
The results revealed that the policy learned by the RL agent outperforms trajectories based on these analytical solutions, e.g.
the median predicted error at the beginning of the target's localisation is 17% less.
These findings suggest that using deep RL for localizing acoustic targets could be successfully applied to in-water applications that include tracking of acoustically tagged marine animals by autonomous underwater vehicles.

This is envisioned as a first necessary step to validate the use of RL to tackle such problems, which could be used later on in a more complex scenarios.

title: An Interactive Lane Change Decision Making Model With Deep Reinforcement Learning
abstract: By considering lane change maneuver as primarily a Partial Observed Markov Decision Process (POMDP) and motion planning problem, this paper presents an interactive model with a Recurrent Neural Network (RNN) approach to determine the adversarial or cooperative intention probability of following vehicle in target lane.

To make proper and efficient lane change decision, Deep Q-value network (DQN) is applied to solve POMDP with expected global maximum reward.
Then quintic polynomials-based motion planning algorithm is used to obtain both optimal lateral and longitudinal trajectory for autonomous vehicle to pursuit.
Experimental results demonstrate the capability of the proposed model to execute lane change maneuver with comfortable and safety reference trajectory at an appropriate time instance and traffic gap in various highway traffic scenarios.


title: Joint resource allocation and security redundancy for autonomous driving based on deep reinforcement learning algorithm
abstract: Autonomous vehicles navigating urban roads require technology that combines low latency with high computing power.
The limited resources of the vehicle itself compel it to offload task requirements to edge server (ES) for processing assistance.
However, as the number of vehicles continues to increase, how edge servers reasonably allocate limited resources to autonomous vehicles becomes critical to the success of urban intelligent transportation services.

This paper establishes an urban road scenario with multiple autonomous vehicles and an edge computing server and considers two main driving behaviour transition resource requests, namely car-following behaviour requests and lane-changing behaviour requests.

Simultaneously, acknowledging that vehicles may encounter unforeseen traffic hazards when switching driving behaviours, a safety redundancy setting strategy is employed to allocate additional resources to the vehicle to ensure safety and model the vehicle resource allocation problem in the autonomous driving system.

Double-deep Q-network (DDQN) is then used to solve this model and maximize the total system utility by comprehensively considering resource costs, system revenue, and autonomous vehicle safety.
Finally, results from the simulation experiment indicate that the proposed dynamic resource allocation scheme, based on deep reinforcement learning for autonomous vehicles under edge computing, not only greatly improves the system's benefits and reduces processing delays compared to traditional greedy algorithms and value iteration, but also effectively ensures security.
A dynamic resource allocation scheme is proposed for vehicular edge computing networks in autonomous driving systems using deep reinforcement learning.

This approach, leveraging double-deep Q-network, aims to maximize system revenue while balancing resource costs, system income, and security considerations.
Simulation results demonstrate significant improvements in system rewards and reduced processing delay, guaranteeing system security compared to traditional greedy algorithms and value iteration methods.

image

title: Signalized Intersection Control in Mixed Autonomous and Regular Vehicles Traffic Environment-A Critical Review Focusing on Future Control
abstract: The recent advancement in industrial technology has offered new opportunities to overcome different problems of stochastic driving behavior of humans through effective implementation of autonomous vehicles (AVs).

Optimum utilization of driving behavior and advanced capabilities of the AVs has enabled researchers to propose autonomous cooperative-based methods for signalized intersection control under an AV traffic environment.

In the future, AVs will share road networks with regular vehicles (RVs), representing a dynamic mixed traffic environment of two groups of vehicles with different characteristics.
Without compromising the safety and level of service, traffic operation and control of such a complex environment is a challenging task.
The current study includes a comprehensive review focused on the signalized intersection control methods under a mixed traffic environment.
The different proposed methods in the literature are based on certain assumptions, requirements, and constraints mainly associated with traffic composition, connectivity, road infrastructures, intersection, and functional network design.

Therefore, these methods should be evaluated with appropriate consideration of the underlying assumptions and limitations.
This study concludes that the application of adaptive traffic signal control can effectively optimize traffic signal plans for variations of AV traffic environments.
However, artificial intelligence approaches primarily focusing on reinforcement learning should be considered to better utilization of the improved AV characteristics.

title: Highway Exiting Planner for Automated Vehicles Using Reinforcement Learning
abstract: Exiting from highways in crowded dynamic traffic is an important path planning task for autonomous vehicles (AVs).
This task can be challenging because of the uncertain motion of surrounding vehicles and limited sensing/observing window.
Conventional path planning methods usually compute a mandatory lane change (MLC) command, but the lane change behavior (e.
g.
, vehicle speed and gap acceptance) should also adapt to traffic conditions and the urgency for exiting.

In this paper, we propose a reinforcement learning-enhanced highway-exit planner.
The learning-based strategy learns from past failures and adjusts the vehicle motion when the AV fails to exit.
The reinforcement learning is based on the Monte Carlo tree search (MCTS) approach.
The proposed learning-enhanced highway-exit planner is tested 6000 times in stochastic simulations.
The results indicate that the proposed planner achieves a higher probability of successful highway exiting than a benchmark MLC planner.

title: Guided Hierarchical Reinforcement Learning for Safe Urban Driving
abstract: Designing a safe decision-making system for end-to-end urban driving is still challenging.
Numerous contributions based on Deep Reinforcement Learning (DRL) were developed.
However, they all suffer from the cold start issue and require extensive convergence training.
Recent solutions for urban driving have emerged based on both Hierarchical Reinforcement Learning (HRL) and imitation learning to overcome these limitations.
Nevertheless, they do not guarantee a safe exploration for an autonomous vehicle.
In the literature, rule-based systems played a pivotal role in ensuring the safety of self-driving cars, but they require manual rule encoding.
This paper introduces GHRL, a decision-making framework for vision-based urban driving that benefits from HRL, and a rule-based system for safe urban driving.
The HRL algorithm learns the high-level policies, whereas the low-level policies are guided by the expert demonstration rules modeled with the Answer Set Programming (ASP) formalism.
When a critical situation occurs, the system will shift to rely on ASP rules.
The state of each policy includes visual features extracted by a convolutional neural network from a monocular camera, information on localization, and waypoints.
GHRL is evaluated on the Carla NoCrash benchmark.
The results show that by incorporating logical rules, GHRL achieved better performance over state-of-the-art HRL algorithms.

title: Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness
abstract: Driving safety is the most important element that needs to be considered for autonomous vehicles (AVs).
To ensure driving safety, we proposed a lane change decision-making framework based on deep reinforcement learning to find a risk-aware driving decision strategy with the minimum expected risk for autonomous driving.

Firstly, a probabilistic-model based risk assessment method was proposed to assess the driving risk using position uncertainty and distance-based safety metrics.
Then, a risk aware decision making algorithm was proposed to find a strategy with the minimum expected risk using deep reinforcement learning.
Finally, our proposed methods were evaluated in CARLA in two scenarios (one with static obstacles and one with dynamically moving vehicles).
The results show that our proposed methods can generate robust safe driving strategies and achieve better driving performances than previous methods.

title: Transaction selection policy in tier-to-tier SBSRS by using Deep <i>Q</i>-Learning
abstract: This paper studies a Deep Q-Learning (DQL) method for transaction sequencing problems in an automated warehousing system, Shuttle-based Storage and Retrieval System (SBSRS), in which shuttles can move between tiers flexibly.

Here, the system is referred to as tier-to-tier SBSRS (t-SBSRS), developed as an alternative design to tier-captive SBSRS (c-SBSRS).
By the flexible travel of shuttles between tiers in t-SBSRS, the number of shuttles in the system may be reduced compared to its simulant c-SBSRS design.
The flexible travel of shuttles makes the operation decisions more complex in that system, motivating us to explore whether integration of a machine learning approach would help to improve the system performance.

We apply the DQL method for the transaction selection of shuttles in the system to attain process time advantage.
The outcomes of the DQN are confronted with the well-applied heuristic approaches: first-come-first-serve (FIFO) and shortest process time (SPT) rules under different racking and numbers of shuttles scenarios.

The results show that DQL outperforms the FIFO and SPT rules promising for the future of smart industry applications.
Especially, compared to the well-applied SPT rule in industries, DQL improves the average cycle time per transaction by roughly 43% on average.

title: Safe-State Enhancement Method for Autonomous Driving via Direct Hierarchical Reinforcement Learning
abstract: Reinforcement learning (RL) has shown excellent performance in the sequential decision-making problem, where safety in the form of state constraints is of great significance in the design and application of RL.

Simple constrained end-to-end RL methods might lead to significant failure in a complex system like autonomous vehicles.
In contrast, some hierarchical RL (HRL) methods generate driving goals directly, which could be closely combined with motion planning.
With safety requirements, some safe-enhanced RL methods add post-processing modules to avoid unsafe goals or achieve expectation-based safety, which accepts the existence of unsafe states and allows some violations of safe constraints.

However, ensuring state safety is vital for autonomous vehicles.
Therefore, this paper proposes a state-based safety enhancement method for autonomous driving via direct hierarchical reinforcement learning.
Finally, we design a constrained reinforcement learner based on the State-based Constrained Markov Decision Process (SCMDP), where a learnable safety module could adjust the constraint strength adaptively.

We integrate a dynamic module in the policy training and generate future goals considering safety, temporal-spatial continuity, and dynamic feasibility, which could eliminate dependence on the prior model.

Simulations in the typical highway scenes with uncertainties show that the proposed method has better training performance, higher driving safety in interactive scenes, more decision intelligence in traffic congestions, and better economic driving ability on roads with changing slopes.


title: Multi-Agent Deep Reinforcement Learning to Manage Connected Autonomous Vehicles at Tomorrow's Intersections
abstract: In recent years, the growing development of Connected Autonomous Vehicles (CAV), Intelligent Transport Systems (ITS), and 5G communication networks have led to the advent of Autonomous Intersection Management (AIM) systems.

AIMs present a new paradigm for CAV control in future cities, taking control of CAVs in scenarios where cooperation is necessary and allowing safe and efficient traffic flows, eliminating traffic signals.

So far, the development of AIM algorithms has been based on basic control algorithms, without the ability to adapt or keep learning new situations.
To solve this, in this paper we present a new advanced AIM approach based on end-to-end Multi-Agent Deep Reinforcement Learning (MADRL) and trained using Curriculum through Self-Play, called advanced Reinforced AIM (adv.
RAIM).

adv.RAIM enables the control of CAVs at intersections in a collaborative way, autonomously learning complex real-life traffic dynamics.
In addition, adv.RAIM provides a new way to build smarter AIMs capable of proactively controlling CAVs in other highly complex scenarios.
Results show remarkable improvements when compared to traffic light control techniques (reducing travel time by 59% or reducing time lost due to congestion by 95%), as well as outperforming other recently proposed AIMs (reducing waiting time by 56%), highlighting the advantages of using MADRL.


title: Intelligent motivation framework based on Q-network for multiple agents in Internet of Things simulations
abstract: Internet of Things simulations play significant roles in the diverse kinds of activities in our daily lives and have been extensively researched.
Creating and controlling virtual agents in three-dimensional Internet of Things simulations is a key technology for achieving realism in three-dimensional simulations.
Given that traditional virtual agent-based approaches have limitations for realism, it is necessary to improve the realism of three-dimensional Internet of Things simulations.
This article proposes a Q-Network-based motivation framework that applies a Q-Network to select motivations from desires and hierarchical task network planning to execute actions based on goals of the selected motivations.

The desires are to be identified and calculated based on states.
Selected motivations will be chosen to determine the goals that agents must achieve.
In the experiments, the proposed framework achieved an average accuracy of up to 85.5% when the Q-Network-based motivation model was trained.
To verify the Q-Network-based motivation framework, a traditional Q-learning is also applied in the three-dimensional virtual environment.
Comparing the results of the two frameworks, the Q-Network-based motivation framework shows better results than those of traditional Q-learning, as the accuracy of the Q-Network-based motivation is higher by 15.
58%.

The proposed framework can be applied to the diverse kinds of Internet of Things systems such as a training autonomous vehicle.
Moreover, the proposed framework can generate big data on animal behaviors for other training systems.

title: Deep reinforcement learning algorithms for multi-agent systems - a solution for modeling epidemics
abstract: Multi-agent reinforcement learning (MARL) consists of large number of artificial intelligence-based agents interacting with each other in the same environment, often collaborating towards a common end goal.

In single-agent reinforcement learning system the change in the environment is only due to the actions of a particular agent.
In contrast, a multi-agent environment is subject to the actions of all the agents involved.
Multiagent systems can be deployed in various applications like stock trading to maximize profits in stock market, control and coordination of a swarm of robots, modeling of epidemics, autonomous vehicle and traffic control, smart grids and self-healing networks.

It is not possible to solve these complex tasks with a pre-programmed single agent.
Instead, the many agents should be trained to automatically search for a solution through reinforcement learning (RL) based algorithms.
In general, arriving at a decision in a multi-agent system is almost close to impossible due to exponential increase of problem size with an increase in the number of agents.
In this paper, multi-agent systems using Deep Reinforcement Learning (DRL) is explored with a possible application in modeling of epidemics.
Different stochastic environments are considered, and various multi-agent policies are implemented using DRL.
The performance of various MARL algorithms was evaluated against single agent RL algorithms under different environments.
MARL agents were able to learn much faster compared to single RL agents with a more stable training phase.
Mean Field Q-Learning was able to scale and perform much better even in the situation of hundreds of agents in the environment and is a sure candidate to model and predict the epidemics, in the existing frightening dangerous situation of corona pandemic.


title: A deep reinforcement learning-based approach for autonomous driving in highway on-ramp merge
abstract: In this paper, we focus on the problem of highway merge via parallel-type on-ramp for autonomous vehicles (AVs) in a decentralized non-cooperative way.
This problem is challenging because of the highly dynamic and complex road environments.
A deep reinforcement learning-based approach is proposed.
The kernel of this approach is a Deep Q-Network (DQN) that takes dynamic traffic state as input and outputs actions including longitudinal acceleration (or deceleration) and lane merge.
The total reward for this on-ramp merge problem consists of three parts, which are the merge success reward, the merge safety reward, and the merge efficiency reward.
For model training and testing, we construct a highway on-ramp merging simulation experiments with realistic driving parameters.
The experimental results show that the proposed approach can make reasonable merging decisions based on the observation of the traffic environment.
We also compare our approach with a state-of-the-art approach and the superior performance of our approach is demonstrated by making challenging merging decisions in complex highway parallel-type on-ramp merging scenarios.


title: Social Trajectory Planning for Urban Autonomous Surface Vessels
abstract: In this article, we propose a trajectory planning algorithm that enables autonomous surface vessels to perform socially compliant navigation in a city's canal.
The key idea behind the proposed algorithm is to adopt an optimal control formulation in which the deviation of movements of the autonomous vessel from nominal movements of human-operated vessels is penalized.

Consequently, given a pair of origin and destination points, it finds vessel trajectories that resemble those of human-operated vessels.
To formulate this, we adopt kernel density estimation (KDE) to build a nominal movement model of human-operated vessels from a prerecorded trajectory dataset, and use a Kullback-Leibler control cost to measure the deviation of the autonomous vessel's movements from the model.

We establish an analogy between our trajectory planning approach and the maximum entropy inverse reinforcement learning (MaxEntIRL) approach to explain how our approach can learn the navigation behavior of human-operated vessels.

On the other hand, we distinguish our approach from the MaxEntIRL approach in that it does not require well-defined bases, often referred to as features, to construct its cost function as required in many of inverse reinforcement learning approaches in the trajectory planning context.

Through experiments using a dataset of vessel trajectories collected from the automatic identification system, we demonstrate that the trajectories generated by our approach resemble those of human-operated vessels and that using them for canal navigation is beneficial in reducing head-on encounters between vessels and improving navigation safety.


title: iRDRC: An Intelligent Real-Time Dual-Functional Radar-Communication System for Automotive Vehicles
abstract: This letter introduces an intelligent Real-time Dual-functional Radar-Communication (iRDRC) system for autonomous vehicles (AVs).
This system enables an AV to perform both radar and data communications functions to maximize bandwidth utilization as well as significantly enhance safety.
In particular, the data communications function allows the AV to transmit data, e.
g.
, of current traffic, to edge computing systems and the radar function is used to enhance the reliability and reduce the collision risks of the AV, e.
g.
, under bad weather conditions.

The problem of the iRDRC is to decide when to use the communication mode or the radar mode to maximize the data throughput while minimizing the miss detection probability of unexpected events given the uncertainty of surrounding environment.

To solve the problem, we develop a deep reinforcement learning algorithm that allows the AV to quickly obtain the optimal policy without requiring any prior information about the environment.
Simulation results show that the proposed scheme outperforms baseline schemes in terms of data throughput, miss detection probability, and convergence rate.

title: Impacts of Connected and Autonomous Vehicles on Deep Reinforcement Learning Controlled Intersection Systems
abstract: 

title: The Scanner of Heterogeneous Traffic Flow in Smart Cities by an Updating Model of Connected and Automated Vehicles
abstract: The problems of traditional traffic flow detection and calculation methods include limited traffic scenes, high system costs, and lower efficiency over detecting and calculating.
Therefore, in this paper, we presented the updating Connected and Automated Vehicles (CAVs) model as the scanner of heterogeneous traffic flow, which uses various sensors to detect the characteristics of traffic flow in several traffic scenes on the roads.

The model contains the hardware platform, software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project, where the driving of vehicles is mainly controlled by Reinforcement Learning (RL).

Finally, the effectiveness of the proposed model and the corresponding swarm intelligence strategy is evaluated through simulation experiments.
The results showed that the traffic flow scanning, tracking, and data recording performed continuously by CAVs are effective.
The increase in the penetration rate of CAVs in the overall traffic flow has a significant effect on vehicle detection and identification.
In addition, the vehicle occlusion rate is independent of the CAV lane position in all cases.
The complete street scanner is a new technology that realizes the perception of the human settlement environment with the help of the Internet of Vehicles based on 5G communications and sensors.
Although there are some shortcomings in the experiment, it still provides an experimental reference for the development of smart vehicles.

title: Task Offloading of Deep Learning Services for Autonomous Driving in Mobile Edge Computing
abstract: As the utilization of complex and heavy applications increases in autonomous driving, research on using mobile edge computing and task offloading for autonomous driving is being actively conducted.
Recently, researchers have been studying task offloading algorithms using artificial intelligence, such as reinforcement learning or partial offloading.
However, these methods require a lot of training data and critical deadlines and are weakly adaptive to complex and dynamically changing environments.
To overcome this weakness, in this paper, we propose a novel task offloading algorithm based on Lyapunov optimization to maintain the system stability and minimize task processing delay.
First, a real-time monitoring system is built to utilize distributed computing resources in an autonomous driving environment efficiently.
Second, the computational complexity and memory access rate are analyzed to reflect the characteristics of the deep learning applications to the task offloading algorithm.
Third, Lyapunov and Lagrange optimization solves the trade-off issues between system stability and user requirements.
The experimental results show that the system queue backlog remains stable, and the tasks are completed within an average of 0.
4231 s, 0.
7095 s, and 0.
9017 s for object detection, driver profiling, and image recognition, respectively.

Therefore, we ensure that the proposed task offloading algorithm enables the deep learning application to be processed within the deadline and keeps the system stable.

title: Dynamic Pricing and Management for Electric Autonomous Mobility on Demand Systems Using Reinforcement Learning [arXiv]
abstract: The proliferation of ride sharing systems is a major drive in the advancement of autonomous and electric vehicle technologies.
This paper considers the joint routing, battery charging, and pricing problem faced by a profit-maximizing transportation service provider that operates a fleet of autonomous electric vehicles.
We define the dynamic system model that captures the time dependent and stochastic features of an electric autonomous-mobility-on-demand system.
To accommodate for the time-varying nature of trip demands, renewable energy availability, and electricity prices and to further optimally manage the autonomous fleet, a dynamic policy is required.
In order to develop a dynamic control policy, we first formulate the dynamic progression of the system as a Markov decision process.
We argue that it is intractable to exactly solve for the optimal policy using exact dynamic programming methods and therefore apply deep reinforcement learning to develop a near-optimal control policy.

Furthermore, we establish the static planning problem by considering time-invariant system parameters.
We define the capacity region and determine the optimal static policy to serve as a baseline for comparison with our dynamic policy.
While the static policy provides important insights on optimal pricing and fleet management, we show that in a real dynamic setting, it is inefficient to utilize a static policy.
The two case studies we conducted in Manhattan and San Francisco demonstrate the efficacy of our dynamic policy in terms of network stability and profits, while keeping the queue lengths up to 200 times less than the static policy.


title: Learning Automated Driving in Complex Intersection Scenarios Based on Camera Sensors: A Deep Reinforcement Learning Approach
abstract: Making proper decisions at intersections that are one of the most dangerous and sophisticated driving scenarios is full of challenges, especially for autonomous vehicles (AVs).
The existing decision-making approaches for AVs at intersections are limited as they only consider driving safety in simple intersection scenarios while sacrificing travel efficiency and driving comfort.

To solve this issue, a decision-making structure motivated by deep reinforcement learning was proposed for autonomous driving at complex intersection scenarios based on long short-term memory (LSTM).
The mapping relationship between traffic images collected from camera sensors and AVs' actions was established by constructing convolutional-recurrent neural networks in a decision-making framework.
Traffic images collected from camera sensors at two different timesteps were used to understand the relative motion information between AVs and other vehicles.
To model the interaction between the AV and other vehicles, Markov decision process was used.
The deep Q-network (DQN) algorithm was applied to generate the optimal driving policy that could comprehensively consider driving safety, travel efficiency and driving comfort.
Three crash-prone complex intersection scenarios were reconstructed in CARLA (car learning to act) to evaluate the performance of our proposed method.
The results indicate that our method can make AV drive through intersections safely and efficiently with desirable driving comfort in all the examined scenarios.

title: XTENTH-CAR: A Proportionally Scaled Experimental Vehicle Platform for Connected Autonomy and All-Terrain Research [arXiv]
abstract: Connected Autonomous Vehicles (CAVs) are key components of the Intelligent Transportation System (ITS), and all-terrain Autonomous Ground Vehicles (AGVs) are indispensable tools for a wide range of applications such as disaster response, automated mining, agriculture, military operations, search and rescue missions, and planetary exploration.

Experimental validation is a requisite for CAV and AGV research, but requires a large, safe experimental environment when using full-size vehicles which is time-consuming and expensive.
To address these challenges, we developed XTENTH-CAR (eXperimental one-TENTH scaled vehicle platform for Connected autonomy and All-terrain Research), an open-source, cost-effective proportionally one-tenth scaled experimental vehicle platform governed by the same physics as a full-size on-road vehicle.

XTENTH-CAR is equipped with the best-in-class NVIDIA Jetson AGX Orin System on Module (SOM), stereo camera, 2D LiDAR and open-source Electronic Speed Controller (ESC) with drivers written in the new Robot Operating System (ROS 2) to facilitate experimental CAV and AGV perception, motion planning and control research, that incorporate state-of-the-art computationally expensive algorithms such as Deep Reinforcement Learning (DRL).

XTENTH-CAR is designed for compact experimental environments, and aims to increase the accessibility of experimental CAV and AGV research with low upfront costs, and complete Autonomous Vehicle (AV) hardware and software architectures similar to the full-sized X-CAR experimental vehicle platform, enabling efficient cross-platform development between small-scale and full-scale vehicles.


title: A study of multiple reward function performances for vehicle collision avoidance systems applying the DQN algorithm in reinforcement learning
abstract: Reinforcement Learning (RL) is an area of Machine Learning (ML) that intends to improve the acts of agents learning from environmental interconnection.
The significant concern in RL is to achieve the promising potential of the training process in the model.
However, network convergence speed is often sluggish in RL and converges quickly to local optimal solutions.
Reward function has been used to deal with these problems as a useful tool to speed up the agent's learning speed.
Even though RL convergence properties have been comprehensively explored, there are no specific rules for choosing the reward function.
Therefore, searching for efficient potential reward function is still an exciting field of study.
This paper discusses the reward function, execute some analysis, and provides the learning agent with the extracted information to increase the speed of learning for collision avoidance task.
We provide an experimental study for selecting one reward function in a simulated collision-avoidance environment of an autonomous vehicle by applying the DQN algorithm.
It has been conducted on online environments, which is using the CARLA simulator.
This experimental study consists of three cases with a various exploration of reward values.
Case 1 consists of the range of the penalty value larger than the reward function by 200 times.
Case 2 is similar, but with the small range number of the penalty is applied, case 3, which is the reward function and penalty value, is in the same range value.
The result shows that case 3 performances outperform case 1 and case 2 with 94% average accuracy; meanwhile, case 1 obtains 70%, and case 2 achieves 85% accuracy.
It is may due to the monumental size of the collision penalty in comparison to all else.
Hence, the findings obtained show the efficacy of the exploration of the reward function.

title: Two-Sided Deep Reinforcement Learning for Dynamic Mobility-on-Demand Management with Mixed Autonomy
abstract: Autonomous vehicles (AVs) are expected to operate on mobility-on-demand (MoD) platforms because AV technology enables flexible self-relocation and systemoptimal coordination.
Unlike the existing studies, which focus on MoD with pure AV fleet or conventional vehicles (CVs) fleet, we aim to optimize the real-time fleet management of an MoD system with a mixed autonomy of CVs and AVs.

We consider a realistic case that heterogeneous boundedly rational drivers may determine and learn their relocation strategies to improve their own compensation.
In contrast, AVs are fully compliant with the platform's operational decisions.
To achieve a high level of service provided by a mixed fleet, we propose that the platform prioritizes human drivers in the matching decisions when on-demand requests arrive and dynamically determines the AV relocation tasks and the optimal commission fee to influence drivers' behavior.

However, it is challenging to make efficient real-time fleet management decisions when spatiotemporal uncertainty in demand and complex interactions among human drivers and operators are anticipated and considered in the operator's decision making.

To tackle the challenges, we develop a two-sided multiagent deep reinforcement learning (DRL) approach in which the operator acts as a supervisor agent on one side and makes centralized decisions on the mixed fleet, and each CV driver acts as an individual agent on the other side and learns to make decentralized decisions noncooperatively.

We establish a two-sided multiagent advantage actor-critic algorithm to simultaneously train different agents on the two sides.
For the first time, a scalable algorithm is developed here for mixed fleet management.
Furthermore, we formulate a two-head policy network to enable the supervisor agent to efficiently make multitask decisions based on one policy network, which greatly reduces the computational time.
The two-sided multiagent DRL approach is demonstrated using a case study in New York City using real taxi trip data.
Results show that our algorithm can make high-quality decisions quickly and outperform benchmark policies.
The efficiency of the two-head policy network is demonstrated by comparing it with the case using two separate policy networks.
Our fleet management strategy makes both the platform and the drivers better off, especially in scenarios with high demand volume.

title: Safe Reinforcement Learning for Model-Reference Trajectory Tracking of Uncertain Autonomous Vehicles With Model-Based Acceleration
abstract: Applying reinforcement learning (RL) algorithms to control systems design remains a challenging task due to the potential unsafe exploration and the low sample efficiency.
In this paper, we propose a novel safe model-based RL algorithm to solve the collision-free model-reference trajectory tracking problem of uncertain autonomous vehicles (AVs).
Firstly, a new type of robust control barrier function (CBF) condition for collision-avoidance is derived for the uncertain AVs by incorporating the estimation of the system uncertainty with Gaussian process (GP) regression.

Then, a robust CBF-based RL control structure is proposed, where the nominal control input is composed of the RL policy and a model-based reference control policy.
The actual control input obtained from the quadratic programming problem can satisfy the constraints of collision-avoidance, input saturation and velocity boundedness simultaneously with a relatively high probability.

Finally, within this control structure, a Dyna-style safe model-based RL algorithm is proposed, where the safe exploration is achieved through executing the robust CBF-based actions and the sample efficiency is improved by leveraging the GP models.

The superior learning performance of the proposed RL control structure is demonstrated through simulation experiments.

title: Dynamic robotic tracking of underwater targets using reinforcement learning
abstract: To realize the potential of autonomous underwater robots that scale up our observational capacity in the ocean, new approaches and techniques are needed.
Fleets of autonomous robots could be used to study complex marine systems and animals with either new imaging configurations or by tracking tagged animals to study their behavior.
These activities can then inform and create new policies for community conservation.
The role of animal connectivity via active movement of animals represents a major knowledge gap related to the distribution of deep ocean populations.
Tracking underwater targets represents a major challenge for observing biological processes in situ, and methods to robustly respond to a changing environment during monitoring missions are needed.
Analytical techniques for optimal sensor placement and path planning to locate underwater targets are not straightforward in such cases.
The aim of this study is to investigate the use of deep reinforcement learning as a tool for range-only underwater target tracking optimization, whose promising capabilities have been demonstrated in terrestrial scenarios.

To evaluate its usefulness, a reinforcement learning method was implemented as a path planning system for an autonomous surface vehicle while tracking an underwater mobile target.
A complete description of an open-source model, performance metrics in simulated environments, and evaluated algorithms based on more than 15 hours of at-sea field experiments are presented.
These efforts demonstrate that deep reinforcement learning is a powerful approach that enhances the abilities of autonomous robots in the ocean and encourages the deployment of algorithms like these for monitoring marine biological systems in the future.

Deep Reinforcement Learning methods for Underwater Target Tracking This is a set of tools developed to train an agent (and multiple agents) to find the optimal path to localize and track a target (and multiple targets).

The deep Reinforcement Learning (RL) algorithms implemented are: Deep Deterministic Policy Gradient (DDPG) Twin-Delayed DDPG (TD3) Soft Actor-Critic (SAC) The environment to train the agents is based on theOpenAI Particle.

The main objective is to find the optimal path that an autonomous vehicle (e.g.
autonomous underwater vehicles (AUV) or autonomous surface vehicles (ASV)) should follow in order to localize and track an underwater target usingrange-only and single-beacon algorithms.
The target estimation algorithms implemented are based on: Least Squares (LS) Particle Filter (PF) More information at this Github repository:https://github.
com/imasmitja/RLforUTracking Copyright: CC0 1.
0 Universal (CC0 1.
0) Public Domain Dedication

title: A Multiple-Goal Reinforcement Learning Method for Complex Vehicle Overtaking Maneuvers
abstract: In this paper, we present a learning method to solve the vehicle overtaking problem, which demands a multitude of abilities from the agent to tackle multiple criteria.
To handle this problem, we propose to adopt a multiple-goal reinforcement learning (MGRL) framework as the basis of our solution.
By considering seven different goals, either Q-learning (QL) or double-action QL is employed to determine action decisions based on whether the other vehicles interact with the agent for that particular goal.

Furthermore, a fusion function is proposed according to the importance of each goal before arriving to an overall but consistent action decision.
This offers a powerful approach for dealing with demanding situations such as overtaking, particularly when a number of other vehicles are within the proximity of the agent and are traveling at different and varying speeds.

A large number of overtaking cases have been simulated to demonstrate its effectiveness.
From the results, it can be concluded that the proposed method is capable of the following: 1) making correct action decisions for overtaking; 2) avoiding collisions with other vehicles; 3) reaching the target at reasonable time; 4) keeping almost steady speed; and 5) maintaining almost steady heading angle.

In addition, it should also be noted that the proposed method performs lane keeping well when not overtaking and lane changing effectively when overtaking is in progress.

title: Reinforcement Learning-Based Path following Control with Dynamics Randomization for Parametric Uncertainties in Autonomous Driving
abstract: Reinforcement learning-based controllers for safety-critical applications, such as autonomous driving, are typically trained in simulation, where a vehicle model is provided during the learning process.

However, an inaccurate parameterization of the vehicle model used for training heavily influences the performance of the reinforcement learning agent during execution.
This inaccuracy is either caused by changes due to environmental influences or by falsely estimated vehicle parameters.
In this work, we present our approach of combining dynamics randomization with reinforcement learning to overcome this issue for a path-following control task of an autonomous and over-actuated robotic vehicle.

We train three independent agents, where each agent experiences randomization for a different vehicle dynamics parameter, i.e., the mass, the yaw inertia, and the road-tire friction.
We randomize the parameters uniformly within predefined ranges to enable the agents to learn an equally robust control behavior for all possible parameter values.
Finally, in a simulation study, we compare the performance of the agents trained with dynamics randomization to the performance of an agent trained with the nominal parameter values.
Simulation results demonstrate that the former agents obtain a higher level of robustness against model uncertainties and varying environmental conditions than the latter agent trained with nominal vehicle parameter values.


title: Explainable navigation system using fuzzy reinforcement learning
abstract: Explainable outcomes in autonomous navigation have become crucial for drivers, other vehicles, as well as for pedestrians.
Creating trustworthy strategies is mandatory for the integration of self-driving cars into quotidian environments.
This paper presents the successful implementation of an explainable Fuzzy Deep Reinforcement Learning approach for autonomous vehicles based on the AWS DeepRacer (TM) platform.
A model of the environment is created by transforming crisp values into linguistic variables.
A fuzzy inference system is used to define the reward of the vehicle depending on its current state.
Guidelines to define the actions and to improve performance of the reinforcement learning agent are given based on the characteristics of the existing hardware.
The performance of the models is tested on tracks with distinctive properties using agents with different policies and action spaces, and shows explainable and successful navigation of the agent on diverse scenarios.

[GRAPHICS].

title: Deep Reinforcement Learning Methods for Autonomous Driving Safety and Interactivity
abstract: 

title: Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey [arXiv]
abstract: The emergence of new services and applications in emerging wireless networks (e.
g.
, beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT).

However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications.

Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches.
This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks.
In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference.

Then, we provide an extensive review of distributed learning for critical IoT services (e.
g.
, data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.
g.
, smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry).

From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.

title: Real-time self-adaptive Q-learning controller for energy management of conventional autonomous vehicles
abstract: Reducing emissions and energy consumption of autonomous vehicles is critical in the modern era.
This paper presents an intelligent energy management system based on Reinforcement Learning (RL) for conventional autonomous vehicles.
Furthermore, in order to improve the efficiency, a new exploration strategy is proposed to replace the traditional decayed epsilon-greedy strategy in the Q-learning algorithm associated with RL.
Unlike tradi-tional Q-learning algorithms, the proposed self-adaptive Q-learning (SAQ-learning) can be applied in real-time.
The learning capability of the controllers can help the vehicle deal with unknown situations in real-time.
Nu-merical simulations show that compared to other controllers, Q-learning and SAQ-learning controllers can generate the desired engine torque based on the vehicle road power demand and control the air/fuel ratio by changing the throttle angle efficiently in real-time.

Also, the proposed real-time SAQ-learning is shown to improve the operational time by 23% compared to standard Q-learning.
Our simulations reveal the effectiveness of the proposed control system compared to other methods, namely dynamic programming and fuzzy logic methods.

title: Modes of Deliberation in Machine Ethics
abstract: 

title: Trajectory Based Traffic Analysis and Control Utilizing Connected Autonomous Vehicles
abstract: 

title: Video description: A comprehensive survey of deep learning approaches
abstract: Video description refers to understanding visual content and transforming that acquired understanding into automatic textual narration.
It bridges the key AI fields of computer vision and natural language processing in conjunction with real-time and practical applications.
Deep learning-based approaches employed for video description have demonstrated enhanced results compared to conventional approaches.
The current literature lacks a thorough interpretation of the recently developed and employed sequence to sequence techniques for video description.
This paper fills that gap by focusing mainly on deep learning-enabled approaches to automatic caption generation.
Sequence to sequence models follow an Encoder-Decoder architecture employing a specific composition of CNN, RNN, or the variants LSTM or GRU as an encoder and decoder block.
This standard-architecture can be fused with an attention mechanism to focus on a specific distinctiveness, achieving high quality results.
Reinforcement learning employed within the Encoder-Decoder structure can progressively deliver state-of-the-art captions by following exploration and exploitation strategies.
The transformer mechanism is a modern and efficient transductive architecture for robust output.
Free from recurrence, and solely based on self-attention, it allows parallelization along with training on a massive amount of data.
It can fully utilize the available GPUs for most NLP tasks.
Recently, with the emergence of several versions of transformers, long term dependency handling is not an issue anymore for researchers engaged in video processing for summarization and description, or for autonomous-vehicle, surveillance, and instructional purposes.

They can get auspicious directions from this research.

title: On Using Real-Time Reachability for the Safety Assurance of Machine Learning Controllers
abstract: Over the last decade, advances in machine learning and sensing technology have paved the way for the belief that safe, accessible, and convenient autonomous vehicles may be realized in the near future.

Despite the prolific competencies of machine learning models for learning the nuances of sensing, actuation, and control, they are notoriously difficult to assure.
The challenge here is that some models, such as neural networks, are "black box" in nature, making verification and validation difficult, and sometimes infeasible.
Moreover, these models are often tasked with operating in uncertain and dynamic environments where design time assurance may only be partially transferable.
Thus, it is critical to monitor these components at runtime.
One approach for providing runtime assurance of systems with unverified components is the simplex architecture, where an unverified component is wrapped with a safety controller and a switching logic designed to prevent dangerous behavior.

In this paper, we propose the use of a real-time reachability algorithm for the implementation of such an architecture for the safety assurance of a 1/10 scale open source autonomous vehicle platform known as F1/10.

The reachability algorithm (a) provides provable guarantees of safety, and (b) is used to detect potentially unsafe scenarios.
In our approach, the need to analyze the underlying controller is abstracted away, instead focusing on the effects of the controller's decisions on the system's future states.
We demonstrate the efficacy of our architecture through experiments conducted both in simulation and on an embedded hardware platform.

title: Hardware-Level Vulnerabilities and Support for Secure and Safe Cyber-Physical Systems
abstract: 

title: Toward personalized decision making for autonomous vehicles: A constrained multi-objective reinforcement learning technique
abstract: Reinforcement learning promises to provide a state-of-the-art solution to the decision making problem of autonomous driving.
Nonetheless, numerous real-world decision making problems involve balancing multiple conflicting or competing objectives.
In addition, passengers may typically prefer to explore diversified driving modes through their specific preferences (i.e., relative importance of different objectives).
Taking into account these demands, traditional reinforcement learning algorithms with applications in personalized self-driving vehicles remain challenging.
Consequently, here we present a novel constrained multi-objective reinforcement learning technique for personalized decision making in autonomous driving, with the goal of learning a single model for Pareto optimal policies across the space of all possible user preferences.

Specifically, a nonlinear constraint incorporating a user-specified preference and a vectorized action-value function is introduced to ensure both diversity in learned decision behaviors and efficient alignment between the user-specified preference and the corresponding optimal policy.

Additionally, a constrained multi-objective actor-critic approach is advanced to approximate the Pareto optimal policies for any user-specified preferences while adhering to the nonlinear constraint.
Finally, the proposed personalized decision making scheme for autonomous driving is assessed in a highway on-ramp merging scenario with dynamic traffic flows.
The results demonstrate the effectiveness of our method by comparing it with classical and state-of-the-art baselines.

title: Reinforcement learning-based collision avoidance: impact of reward function and knowledge transfer
abstract: Collision avoidance for robots and vehicles in unpredictable environments is a challenging task.
Various control strategies have been developed for the agent (i.
e.
, robots or vehicles) to sense the environment, assess the situation, and select the optimal actions to avoid collision and accomplish its mission.

In our research on autonomous ships, we take a machine learning approach to collision avoidance.
The lack of available ship steering data of human ship masters has made it necessary to acquire collision avoidance knowledge through reinforcement learning (RL).
Given that the learned neural network tends to be a black box, it is desirable that a method is available which can be used to design an agent's behavior so that the desired knowledge can be captured.
Furthermore, RL with complex tasks can be either time consuming or unfeasible.
A multi-stage learning method is needed in which agents can learn from simple tasks and then transfer their learned knowledge to closely related but more complex tasks.
In this paper, we explore the ways of designing agent behaviors through tuning reward functions and devise a transfer RL method for multi-stage knowledge acquisition.
The computer simulation-based agent training results have shown that it is important to understand the roles of each component in a reward function and the various design parameters in transfer RL.
The settings of these parameters are all dependent on the complexity of the tasks and the similarities between them.

title: Autonomous Navigation System in Pedestrian Scenarios Using a Dreamer-Based Motion Planner
abstract: Navigation among pedestrians is a crucial capability of service robots; however, it is a challenge to manage time-varying environments stably.
Recent deep reinforcement learning (DRL)-based approaches to crowd navigation have yielded numerous promising applications.
However, they rely heavily on initial imitation learning and colossal positive datasets.
Moreover, the difficulties in accurately localizing robots, detecting and tracking humans, representing and generalizing reciprocal human relationships restrict their deployment in real-world problems.

We propose a Dreamer-based motion planner for collision-free navigation in diverse pedestrian scenarios.
Our RL framework can completely learn from zero experience via a model-based DRL.
The robot and humans are first projected onto a map, which is subsequently decoded into low-dimensional latent state.
A predictive dynamic model in the latent space is jointly created to efficiently optimize the navigation policy.
Additionally, we leverage the techniques of system identification, domain randomization, clustering and LiDAR SLAM for practical deployment.
Simulation ablations and real implementations demonstrate that our motion planner outperforms state-of-the-art methods, and that the navigation system can be physically implemented in the real world.

title: Design of a Reinforcement Learning-Based Lane Keeping Planning Agent for Automated Vehicles
abstract: Featured ApplicationThe presented method can be used as a real-time trajectory following algorithm for autonomous vehicles using prediction based on lookahead information.
Reinforcement learning-based approaches are widely studied in the literature for solving different control tasks for Connected and Autonomous Vehicles, from which this paper deals with the problem of lateral control of a dynamic nonlinear vehicle model, performing the task of lane-keeping.

In this area, the appropriate formulation of the goals and environment information is crucial, for which the research outlines the importance of lookahead information, enabling to accomplish maneuvers with complex trajectories.

Another critical part is the real-time manner of the problem.
On the one hand, optimization or search based methods, such as the presented Monte Carlo Tree Search method, can solve the problem with the trade-off of high numerical complexity.
On the other hand, single Reinforcement Learning agents struggle to learn these tasks with high performance, though they have the advantage that after the training process, they can operate in a real-time manner.

Two planning agent structures are proposed in the paper to resolve this duality, where the machine learning agents aid the tree search algorithm.
As a result, the combined solution provides high performance and low computational needs.

title: Explanation-Aware Experience Replay in Rule-Dense Environments
abstract: Human environments are often regulated by explicit and complex rulesets.
Integrating Reinforcement Learning (RL) agents into such environments motivates the development of learning mechanisms that perform well in rule-dense and exception-ridden environments such as autonomous driving on regulated roads.

In this letter, we propose a method for organising experience by means of partitioning the experience buffer into clusters labelled on a per-explanation basis.
We present discrete and continuous navigation environments compatible with modular rulesets and 9 learning tasks.
For environments with explainable rulesets, we convert rule-based explanations into case-based explanations by allocating state-transitions into clusters labelled with explanations.
This allows us to sample experiences in a curricular and task-oriented manner, focusing on the rarity, importance, and meaning of events.
We label this concept Explanation-Awareness (XA).
We perform XA experience replay (XAER) with intra and inter-cluster prioritisation, and introduce XA-compatible versions of DQN, TD3, and SAC.
Performance is consistently superior with XA versions of those algorithms, compared to traditional Prioritised Experience Replay baselines, indicating that explanation engineering can he used in lieu of reward engineering for environments with explainable features.


title: US-German Collaboration: Strategy Change in Cognitive Biological and Technical Systems
abstract: The ability for strategy change, i.
e.
, the change in action selection and action planning while an overarching goal is maintained is a fundamental, but still barely understood capability of cognitive systems.

Sudden transitions are well documented in neurophysiological and cognitive experimental data, but application of the underlying theory of the spatio-temporal neurodynamics is yet to be done.
Current physiological and theoretical frameworks of learning focus on incremental learning (as exemplified the reinforcement learning).
This project aims at improved understanding of the nature and functional role of abrupt, large-scale state transitions in complex neuronal systems as the basis of cognitive strategy change.
We exploit our experimental and theoretical understanding of a particular rodent learning model to simulate the neuronal mechanisms of instantaneous strategy change.
The investigators will develop an algorithmic formulation of the neurocomputational principles, and apply it in the engineering example of autonomous vehicle control.
This interdisciplinary project is based on the complementary and synergistic expertise of the team members in optimization theory and both theoretical and experimental neuroscience.
<br/><br/>This project arises from our deep understanding of the rapid biological and cognitive processes displayed by strategy changes in coping with changing environments.

This research on decision making in human and animal brains provides a platform for developing robust decision support systems that operate in dynamically changing scenarios in the style of brains.
Detailed analysis of the mechanisms underlying rapid strategy change in brains will allow both this research team and other groups to equip various man-made systems with the fundamental property of insightful cognition.

This work addresses important societal needs by creating the foundations of cognitive engineering systems supporting emergency response to natural disasters and cyber security threats by adversaries, as well as optimized control of autonomous vehicles under complex operating conditions.
<br/><br/>This award is being co-funded by NSF's Office of International Science and Engineering.

A companion project is being funded by the German Ministry of Education and Research (BMBF).

title: From Sim-to-Real: Learning and Deploying Autonomous Vehicle Controllers That Improve Transportation Metrics
abstract: 

title: Fast, Real-Time Robot Navigation in Initially Unknown Environments via Cross-Domain Transfer Learning of Options
abstract: 

title: Game theoretic decision making for autonomous vehicles' merge manoeuvre in high traffic scenarios
abstract: This paper presents a game theoretic decision making process for autonomous vehicles.
Its goal is to provide a solution for a very challenging task: the merge manoeuvre in high traffic scenarios.
Unlike previous approaches, the proposed solution does not rely on vehicle-to-vehicle communication or any specific coordination, moreover, it is capable of anticipating both the actions of other players and their reactions to the autonomous vehicle's movements.

\The game used is an iterative, multi-player level-k model, which uses cognitive hierarchy reasoning for decision making and has been proved to correctly model human decisions in uncertain situations.
This model uses reinforcement learning to obtain a near-optimal policy, and since it is an iterative model, it is possible to define a goal state so that the policy tries to reach it.
To test the decision making process, a kinematic simulation was implemented.

The resulting policy was compared with a rule-based approach.
The experiments show that the decision making system is capable of correctly performing the merge manoeuvre, by taking actions that require reactions of the other players to be successfully completed.

title: Design and Synthesis of Controllers for Societal-Scale Cyber-Physical Systems
abstract: 

title: ADAS-RL: Safety learning approach for stable autonomous driving
abstract: Stability is the most significant component of an autonomous driving system, affecting both the lives of drivers and pedestrians and traffic flow.
Reinforcement learning (RL) is a representative technology used in autonomous driving, but it has challenges because it is based on trial and error.
In this letter, we propose an efficient learning approach for stable autonomous driving.
The proposed deep reinforcement learning based approach can address the partially observable scenario in mixed traffic which includes both autonomous vehicles and human-driven vehicles.
Simulation results show that the proposed model outperforms the control-theoretic and vanilla RL approaches.
Furthermore, we confirm the effect of the sync-penalty, which teaches the agent about unsafe decisions without experiencing the accidents.
(C) 2022 The Author(s).
Published by Elsevier B.V. on behalf of The Korean Institute of Communications and Information Sciences.

title: Segmented Encoding for Sim2Real of RL-based End-to-End Autonomous Driving
abstract: Among the challenges in the recent research of end-to-end (E2E) driving, interpretability and distribution shift in the simulation-to-real (Sim2Real) have drawn considerable attention.
Because of low interpretability, we cannot clearly explain the causal relationship between the input image and the control actions by the network.
Moreover, the distribution shift problem in Sim2Real degrades the driving performance of the policy in the realworld deployment.
In this paper, we propose a segmentation-based classwise disentangled latent encoding algorithm to cope with the two challenges.
In the proposed algorithm, multi-class segmentation transfers RGB images in both simulation and real environments to the same domain, while preserving the necessary information of objects of primary classes, such as pedestrian, road, and cars, for driving decisions.

Besides, in the class-wise disentangled latent encoding, segmented images are encoded to a latent vector, which improves the interpretability significantly, since the state input has a structured format.

The interpretability improvement is testified by the t-stochastic neighbor embedding, image reconstruction and the causal relationship between the real images and the control actions.
We deploy the driving policy trained in the simulation directly to an autonomous vehicle platform and show, to the best of our knowledge, the first demonstration of the RL-based E2E autonomous in various real environments.


title: Noncooperative-Game-Based Intelligent Vehicle Decision ethod fo Lane-Changing Interactive Behavior
abstract: In the future transportation, it is a foreseeable trend that human-driven vehicles and autonomous vehicles will coexist and interact on roads for a long time.
how to integrate autonomous vehicles into human drivers' traffic ecology and make behavioral decisions and predictions in the complex environment considering vehicle-vehicle interaction is worthy of deep consideration.

Therefore, the proposed game theoretic method is applied to automatic lane-changing scenarios.
Then, the safety risk, ride comfort and traffic efficiency of vehicles in game are quantitatively evaluated as cost functions.
Specifically, the driving maneuvers of interactive agents could be predicted by reasoning, and the optimal strategy is obtained by calculating Nash equilibrium.
Finally, simulations are conducted using Commonroad under typical driving conditions.
The results illustrate that our model could help autonomous vehicles make reasonable and explainable lane-changing decision when interacting with heterogeneous agents, demonstrating the feasibility and reliability of our proposed approach.


title: Control of Rough Terrain Vehicles Using Deep Reinforcement Learning
abstract: We explore the potential to control terrain vehicles using deep reinforcement in scenarios where human operators and traditional control methods are inadequate.
This letter presents a controller that perceives, plans, and successfully controls a 16-tonne forestry vehicle with two frame articulation joints, six wheels, and their actively articulated suspensions to traverse rough terrain.

The carefully shaped reward signal promotes safe, environmental, and efficient driving, which leads to the emergence of unprecedented driving skills.
We test learned skills in a virtual environment, including terrains reconstructed from high-density laser scans of forest sites.
The controller displays the ability to handle obstructing obstacles, slopes up to 27 degrees, and a variety of natural terrains, all with limited wheel slip, smooth, and upright traversal with intelligent use of the active suspensions.

The results confirm that deep reinforcement learning has the potential to enhance control of vehicles with complex dynamics and high-dimensional observation data compared to human operators or traditional control methods, especially in rough terrain.


title: A Predictive-Prescriptive Safety Framework at Intersections in a Connected Vehicle Environment
abstract: 

title: Autonomous Planning and Control for Intelligent Vehicles in Traffic
abstract: This paper addresses the trajectory planning problem for autonomous vehicles in traffic.
We build a stochastic Markov decision process (MDP) model to represent the behaviors of the vehicles.
This MDP model takes into account the road geometry and is able to reproduce more diverse driving styles.
We introduce a new concept, namely, the "dynamic cell," to dynamically modify the state of the traffic according to different vehicle velocities, driver intents (signals), and the sizes of the surrounding vehicles (i.
e.
, truck, sedan, and so on).

We then use Bezier curves to plan smooth paths for lane switching.
The maximum curvature of the path is enforced via certain design parameters.
By designing suitable reward functions, different desired driving styles of the intelligent vehicle can be achieved by solving a reinforcement learning problem.
The desired driving behaviors (i.e., autonomous highway overtaking) are demonstrated with an in-house developed traffic simulator.

title: TD3 Algorithm Improving and Lane-merging Strategy Learning for Autonomous Vehicles
abstract: To enhance the comprehensive performance of automotive lane-merging, the Q-value estimation method of twin delayed deep deterministic policy gradient(TD3) algorithm and the reward function are improved.

The automotive lane-merging model is formalized as the Markov decision process, and the influences of Q-value underestimated by TD3 algorithm on lane-merging strategy are analyzed.
A Q-value estimation method based on weighted average of sample variance is proposed to enhance the Q-value estimation accuracy, when two Q-value estimation samples are obtained by performing Monte Carlo dropout on the dual target critic network.

With giving priority to the completion of the lane-merging, a more perfect reward function is designed considering the safety, comfort and traffic efficiency.
Based on the improved TD3 algorithm and the reward function, a lane-merging strategy of autonomous vehicles is learned and verified with BARK simulator.
The results show that the improved TD3 algorithm significantly enhances the accuracy of Q-value estimation.
Combined with the established reward function, the safety and ride comfort of lane-merging are improved while ensuring traffic efficiency.
Â© 2023 Journal of Mechanical Engineering.

title: Coordination Between Connected Automated Vehicles and Pedestrians to Improve Traffic Safety and Efficiency at Industrial Sites
abstract: Transportation in controlled industrial sites provides a conducive environment for technologies of Connected Automated Vehicles (CAV).
Recent studies show that safe and efficient road sharing between CAVs and pedestrians is challenging.
Besides safety issues, a significant loss of time occurs when pedestrians cross a stream of CAVs.
Currently, many techniques have been employed to improve the coordination between CAVs and pedestrians.
They focus on pedestrian detection, display of the intention of CAVs, and cooperative collision avoidance.
However, one of the most significant sources of information that the pedestrian uses for her/his decision-making is the speed profile of CAVs.
This paper aims to provide a safe and efficient pedestrian crossing at industrial sites through communicative crossing behavior.
To this end, a suitable speed profile of the CAV is designed by assuming that pedestrians and CAV play a cooperative game to move as close as possible to their desired speed.
First, a system analysis is proposed to derive the optimal decision and trajectory for each agent.
Then, Deep Reinforcement Learning (DRL) is used to control the longitudinal speed of the CAVs.
Compared with Model Predictive Control approach, DRL allows coping with unforeseeable pedestrian behaviors (e.g.
long reaction time, varying ideal speed, stop in the middle, etc.).
Simulations and experiments with real human testers based on immersive hamlet are performed.
Results show that the proposed speed profile outperforms significantly the collision avoidance approach.

title: Autonomous driving at the handling limit using residual reinforcement learning
abstract: While driving a vehicle safely at its handling limit is essential in autonomous vehicles in Level 5 autonomy, it is a very challenging task for current conventional methods.
Therefore, this study proposes a novel controller of trajectory planning and motion control for autonomous driving through manifold corners at the handling limit to improve the speed and shorten the lap time of the vehicle.

The proposed controller innovatively combines the advantages of conventional model-based control algorithm, model-free reinforcement learning algorithm, and prior expert knowledge, to improve the training efficiency for autonomous driving in extreme conditions.

The reward shaping of this algorithm refers to the procedure and experience of race training of professional drivers in real time.
After training on track maps that exhibit different levels of difficulty, the proposed controller implemented a superior strategy compared to the original reference trajectory, and can to other tougher maps based on the basic driving knowledge learned from the simpler map, which verifies its superiority and exten-sibility.

We believe this technology can be further applied to daily life to expand the application scenarios and maneuvering envelopes of autonomous vehicles.

title: B-GAP: Behavior-Rich Simulation and Navigation for Autonomous Driving
abstract: We address the problem of ego-vehicle navigation in dense simulated traffic environments populated by road agents with varying driver behaviors.
Navigation in such environments is challenging due to unpredictability in agents' actions caused by their heterogeneous behaviors.
We present a new simulation technique consisting of enriching existing traffic simulators with behavior-rich trajectories corresponding to varying levels of aggressiveness.
We generate these trajectories with the help of a driver behavior modeling algorithm.
We then use the enriched simulator to train a deep reinforcement learning (DRL) policy that consists of a set of high-level vehicle control commands and use this policy at test time to perform local navigation in dense traffic.

Our policy implicitly models the interactions between traffic agents and computes safe trajectories for the ego-vehicle accounting for aggressive driver maneuvers such as overtaking, over-speeding, weaving, a nd sudden lane changes.

Our enhanced behavior-rich simulator can be used for generating datasets that consist of trajectories corresponding to diverse driver behaviors and traffic densities, and our behavior-based navigation scheme can be combined with state-of-the-art navigation algorithms.


title: A Human Factors Approach to Validating Driver Models for Interaction-aware Automated Vehicles
abstract: A major challenge for autonomous vehicles is interacting with other traffic participants safely and smoothly.
A promising approach to handle such traffic interactions is equipping autonomous vehicles with interaction-aware controllers (IACs).
These controllers predict how surrounding human drivers will respond to the autonomous vehicle's actions, based on a driver model.
However, the predictive validity of driver models used in IACs is rarely validated, which can limit the interactive capabilities of IACs outside the simple simulated environments in which they are demonstrated.

In this article, we argue that besides evaluating the interactive capabilities of IACs, their underlying driver models should be validated on natural human driving behavior.
We propose a workflow for this validation that includes scenario-based data extraction and a two-stage (tactical/operational) evaluation procedure based on human factors literature.
We demonstrate this workflow in a case study on an inverse-reinforcement-learning-based driver model replicated from an existing IAC.
This model only showed the correct tactical behavior in 40% of the predictions.
The model's operational behavior was inconsistent with observed human behavior.
The case study illustrates that a principled evaluation workflow is useful and needed.
We believe that our workflow will support the development of appropriate driver models for future automated vehicles.

title: A Risk-Averse Preview-Based <i>Q</i> -Learning Algorithm: Application to Highway Driving of Autonomous Vehicles
abstract: A risk-averse preview-based Q-learning planner is presented for navigation of autonomous vehicles (AVs).
To this end, the multilane road ahead of a vehicle is represented by a finite-state nonstationary Markov decision process (MDP).
A risk assessment unit module is then presented, which leverages the preview information provided by sensors along with a stochastic reachability module to assign reward values to the MDP states and update them as scenarios develop.

A sampling-based risk averse preview-based Q-learning algorithm is finally developed, which generates samples using the preview information and reward function to learn risk-averse optimal planning strategies without actual interaction with the environment.

The risk factor is imposed on the objective function to avoid fluctuation of the Q values, which can jeopardize the vehicle's safety and/or performance.
The overall hybrid automaton model of the system is leveraged to develop a feasibility check unit module that detects unfeasible plans and enables the planner system to react proactively to the changes of the environment.

Finally, to verify the efficiency of the presented algorithm, its implementation on two highway driving scenarios of an AV in a varying traffic density is considered.

title: Studies on Complex and Connected Vehicle Traffic Networks
abstract: 

title: Trustworthy safety improvement for autonomous driving using reinforcement learning
abstract: Reinforcement learning (RL) can learn from past failures and has the potential to provide selfimprovement ability and higher-level intelligence.
However, the current RL algorithms still suffer from challenges in reliability, especially compared to the rule/model-based algorithms that are pre-engineered, human-input intensive, but widely used in autonomous vehicles.

To take advantages of both the RL and rule-based algorithms, this work aims to design a decision-making framework that leverages RL and use an existing rule-based policy as its performance lower bound.

In this way, the final policy remains the potential of self-learning, while guaranteeing a better system performance compared with the integrated rule-based policy.
Such a decision making framework is called trustworthy improvement RL (TiRL).
The basic idea is to make the RL policy iteration process synchronously estimate the given rule-based policy's value function.
AV will then use the RL policy to drive only in the cases where the RL has learned a better policy, i.e., a higher policy value.
This work takes highway safe driving as the case study.
The results are obtained through more than 42,000 km driving in stochastic simulated traffic, and calibrated by naturalistic driving data.
The TiRL planner is given two typical rule-based highway-driving policies for comparison.
The results show that the TiRL can outperform the given arbitrary rule based driving policy.
In summary, the proposed TiRL can leverage the learning-based method in stochastic and emergent scenarios, while having a trustworthy safety improvement from the existing rule-based policies.

title: Research progress of automatic driving control technology based on reinforcement learning
abstract: Research on fully automatic driving has been largely spurred by some important international challenges and competitions,such as the well-known Defense Advanced Research Projects Agency Grand Challenge held in 2005.

Selfdriving cars and autonomous vehicles have migrated from laboratory development and testing conditions to driving on public roads.
Self-driving cars are autonomous decision-making systems that process streams of observations coming from different on-board sources,such as cameras,radars,lidars,ultrasonic sensors,global positioning system units,and/or inertial sensors.

The development of autonomous vehicles offers a decrease in road accidents and traffic congestions.
Most driving scenarios can be simply solved with classical perception,path planning,and motion control methods.
However,the remaining unsolved scenarios are corner cases where traditional methods fail.
In the past decade,advances in the field of artificial intelligence (AI) and machine learning (ML) have greatly promoted the development of autonomous driving.
Autonomous driving is a challenging application domain for ML .
ML methods can be divided into supervised learning,unsupervised learning,and reinforcement learning (RL) .
RL is a family of algorithms that allow agents to learn how to act in different situations.
In other words,a map or a policy is established from situations (states) to actions to maximize a numerical reward signal.
Most autonomous vehicles have a modular hierarchical structure and can be divided into four components or four layers,namely,perception,decision making,control,and actuator.
RL is suitable for decision making and control in complex traffic scenarios to improve the safety and comfort of autonomous driving.
Traditional controllers utilize an a priori model composed of fixed parameters.
When robots or other autonomous systems are used in complex environments,such as driving,traditional controllers cannot foresee every possible situation that the system has to cope with.
An RL controller is a learning controller and uses training information to learn their models over time.
With every gathered batch of training data,the approximation of the true system model becomes accurate.
Deep neural networks have been applied as function approximators for RL agents,thereby allowing agents to generalize knowledge to new unseen situations,along with new algorithms for problems with continuous state and action spaces.

This paper mainly introduces the current status and progress of the application of RL methods in autonomous driving control.
This paper consists of five sections.
The first section introduces the background of autonomous driving and some basic knowledge about ML and RL.
The second section briefly describes the architecture of autonomous driving framework.
The control layer is an important part of an autonomous vehicle and has always been a key area of autonomous driving technology research.
The control system of autonomous driving mainly includes lateral control and longitudinal control,namely,steering control and velocity control.
Lateral control deals with the path tracking problem,and longitudinal control deals with the problem of tracking the reference speed and keeping a safe distance from the preceding vehicle.
The third section introduces the basic principles of RL methods and focuses on the current research status of RL in autonomous driving control.
RL algorithms are based on Markov decision process and aim to learn mapping from situations to actions to maximize a scalar reward or reinforcement signal.
RL is a new and extremely old topic in AI.
It gradually became an active and identifiable area of ML in 1980 s. Q-learning is a widely used RL algorithm.

title: A Deep-Reinforcement-Learning-Based Computation Offloading With Mobile Vehicles in Vehicular Edge Computing
abstract: Vehicular edge networks involve edge servers that are close to mobile devices to provide extra computation resource to complete the computation tasks of mobile devices with low latency and high reliability.

Considerable efforts on computation offloading in vehicular edge networks have been developed to reduce the energy consumption and computation latency, in which roadside units (RSUs) are usually considered as the fixed edge servers (FESs).

Nonetheless, the computation offloading with considering mobile vehicles as mobile edge servers (MESs) in vehicular edge networks still needs to be further investigated.
To this end, in this article, we propose a Deep-Reinforcement-Learning-based computation offloading with mobile vehicles in vehicular edge computing, namely, Deep-Reinforcement-Learning-based computation offloading scheme (DRL-COMV), in which some vehicles (such as autonomous vehicle) are deployed and considered as the MESs that move in vehicular edge networks and cooperate with FESs to provide extra computation resource for mobile devices, in order to assist in completing the computation tasks of these mobile devices with great Quality of Experience (QoE) (i.
e.
, low latency) for mobile devices.

Particularly, the computation offloading model with considering both mobile and FESs is conducted to achieve the computation tasks offloading through vehicle-to-vehicle (V2V) communications, and a collaborative route planning is considered for these MESs to move in vehicular edge networks with objective of improving efficiency of computation offloading.

Then, a Deep-Reinforcement-Learning approach with designing rational reward function is proposed to determine the effective computation offloading strategies for multiple mobile devices and multiple edge servers with objective of maximizing both QoE (i.
e.
, low latency) for mobile devices.

Through performance evaluations, our results show that our proposed DRL-COMV scheme can achieve a great convergence and stability.
Additionally, our results also demonstrate that our DRL-COMV scheme also can achieve better both QoE and task offloading requests hit ratio for mobile devices in comparison with existing approaches (i.
e.
, DDPG, IMOPSOQ, and GABDOS).


title: Obstacle avoidance planning of autonomous vehicles using deep reinforcement learning
abstract: Obstacle avoidance path planning in a dynamic circumstance is one of the fundamental problems of autonomous vehicles, counting optional maneuvers: emergency braking and active steering.
This paper proposes emergency obstacle avoidance planning based on deep reinforcement learning (DRL), considering safety and comfort.
Firstly, the vehicle emergency braking and lane change processes are analyzed in detail.
A graded hazard index is defined to indicate the degree of the potential risk of the current vehicle movement.
The longitudinal distance and lateral waypoint models are established, including the comfort deceleration and stability coefficient considerations.
Simultaneously, a fuzzy PID controller is installed to track to satisfy the stability and feasibility of the path.
Then, this paper proposes a DRL process to determine the obstacle avoidance plan.
Mainly, multi-reward functions are designed for different collisions, corresponding penalties for longitudinal rear-end collisions, and lane-changing side collisions based on the safety distance, comfort reward, and safety reward.

Apply a special DRL method-DQN to release the planning program.
The difference is that the long and short-term memory (LSTM) layer is utilized to solve incomplete observations and improve the efficiency and stability of the algorithm in a dynamic environment.
Once the policy is practiced, the vehicle can automatically perform the best obstacle avoidance maneuver in an emergency, improving driving safety.
Finally, this paper builds a simulated environment in CARLA and is trained to evaluate the effectiveness of the proposed algorithm.
The collision rate, safety distance difference, and total reward index indicate that the collision avoidance path is generated safely, and the lateral acceleration and longitudinal velocity satisfy the comfort requirements.

Besides, the method proposed in this paper is compared with traditional DRL, which proves the beneficial performance in safety and efficiency.

title: A Reinforcement Learning Framework for Video Frame-Based Autonomous Car-Following
abstract: Car-following theory has received considerable attention as a core component of Intelligent Transportation Systems.
However, its application to the emerging autonomous vehicles (AVs) remains an unexplored research area.
AVs are designed to provide convenient and safe driving by avoiding accidents caused by human errors.
They require advanced levels of recognition of other drivers' driving-style.
With car-following models, AVs can use their built-in technology to understand the environment surrounding them and make real-time decisions to follow other vehicles.
In this paper, we design an end-to-end car-following framework for AVs using automated object detection and navigation decision modules.
The objective is to allow an AV to follow another vehicle based on Red Green Blue Depth (RGB-D) frames.
We propose to employ a joint solution involving the You Look Once version 3 (YOLOv3) object detector to identify the leader vehicle and other obstacles and a reinforcement learning (RL) algorithm to navigate the self-driving vehicle.

Two RL algorithms, namely Q-learning and Deep Q-learning have been investigated.
Simulation results show the convergence of the developed models and investigate their efficiency in following the leader.
It is shown that, with video frames only, promising results are achieved and that AVs can adopt a reasonable car-following behavior.

title: Alertness Estimation Using Connection Parameters of the Brain Network
abstract: Alertness mechanism of unmanned monitoring vehicles to environment is important.
Especially, the vigilance modeling of underground security robots has a particularly significance because the underground is a dangerous environment.
However, there is no a mature methodology for the alertness computation.
In this work, four parts of the alertness estimation are focused.
First, an autonomous robot alertness mechanism framework is proposed by using the deep reinforcement learning model of the human alertness mechanism.
Second, a fast K-T filtering algorithm is developed to eliminate the multiple noises of the electroencephalograph (EEG) signals by the blind source separation and the adjustable Q factor wavelet transform.

Third, the description problem of the directed interaction stability of the cortical EEG signals is solved by the ensemble empirical mode decomposition and the directional transfer function.
Fourth, the human alertness estimation is explored by using the support vector regression of the dynamically spatial-temporal brain network connection parameters.
Experiments show that, the mean square error and the determination coefficient of the explored alertness estimation are respectively 0.115 and 0.8337.
Compared with the scalp EEG alertness estimation, it has a better performance because the mean square error is decreased by 0.0684, and the determination coefficient is increased by 0.023.

title: Effective and Semantic Communication in Multi-Agent Reinforcement Learning
abstract: My project focuses on designing goal-oriented communication frameworks and systems using machine learning optimisation techniques.The communication problem can be divided into 3 levels:1.
Technical problem: How accurately can the symbols of communication be transmitted?2.
Semantic problem: How precisely do the transmitted symbols convey the desired meaning?3.
Effectiveness problem: How effectively does the received meaning affect conduct in the desired way?The leading prevailing paradigm is the technical problem perspective.
Consider streaming visual media or guiding a remote-controlled rover.
Current approaches to communications consider a layered strategy.
First, the message (for instance video frame or a rover command) is mapped to a bit pattern to remove redundancy.
Next, the compressed bit pattern is protected against channel distortion with an error-correcting code.
Finally, the protected bits are mapped to channel symbols for transmission.
After the transmission, the process is reversed at the receiver.
Each step of this process has been extensively researched in the last 80 years.
This method has a significant advantage - it allows for much simpler analysis at each stage.
The problem is broken into subproblems that are more manageable to tackle.This approach is optimal for specific sources and long enough block codes - it cannot be beaten asymptotically.
However, in general, this approach is not flawless.
Communication is rarely the goal in itself.
Instead, it is used to achieve some other end.
Thus, the success of communication should be measured in the context of the overall objective.
That is the main focus of my project - semantic and effective communication problems.
The modular system fails to exploit the interactions, dependencies, and correlations between the steps.
However, each of these stages is complex by itself.
Thus, doing away with the modular approach is not feasible in a straightforward analytic manner.
That is why the current advancements in statistical methods such as machine learning are especially promising.
The ability to learn inductively from examples allows for joining the modules of communication into a single system.
However, what constitutes the desired behaviour is not always apparent.
Thus, another framework is introduced.
Reinforcement learning is the set of algorithms that allows for learning complex behaviours through interactions with an environment.
By introducing realistic communication channels, we can extend those methods to allow for learning of the communication schemes themselves jointly with the desired conduct.
This framework can be extended to include multiple agents interacting and learning.
This study is relevant to remote control problems, drone swarm navigation, coordinated autonomous vehicle driving, distributed learning, or industrial internet of things where the number of independent actors need to coordinate to achieve a common goal.
Relevant EPSRC research areas: Artificial intelligence technologies, ICT networks and distributed systems, Digital signal processing, Statistics and applied probability.


title: Designing Integrated Strategies for Modularized Robotic Systems in Uncertain Environments
abstract: 

title: Hybrid Machine Learning/Simulation Approaches for Logistic Systems Optimization
abstract: 

title: Comfortable and energy-efficient speed control of autonomous vehicles on rough pavements using deep reinforcement learning
abstract: Rough pavements cause ride discomfort and energy inefficiency for road vehicles.
Existing methods to address these problems are time-consuming and not adaptive to changing driving conditions on rough pavements.
With the development of sensor and communication technologies, crowdsourced road and dynamic traffic information become available for enhancing driving performance, particularly addressing the discomfort and inefficiency issues by controlling driving speeds.

This study proposes a speed control framework on rough pavements, envisioning the operation of autonomous vehicles based on the crowdsourced data.
We suggest the concept of 'maximum comfortable speed' for representing the vertical ride comfort of oncoming roads.
A deep reinforcement learning (DRL) algorithm is designed to learn comfortable and energyefficient speed control strategies.
The DRL-based speed control model is trained using realworld rough pavement data in Shanghai, China.
The experimental results show that the vertical ride comfort, energy efficiency, and computation efficiency increase by 8.
22%, 24.
37%, and 94.
38%, respectively, compared to an optimization-based speed control model.

The results indicate that the proposed framework is effective for real-time speed controls of autonomous vehicles on rough pavements.

title: Integrated Energy Scheduling and Routing for a Network of Mobile Prosumers
abstract: 

title: Parallel, Angular and Perpendicular Parking for Autonomously Driven Vehicles
abstract: 

title: ARORA & NavSim: a simulator system for training autonomous agents with geospecific data
abstract: This study accompanies the initial public release of the software for ARORA, or A Realistic Open environment for Rapid Agent training, and marks a high point of several years of work for the mature and completely open ARORA simulator.

The purpose of ARORA is to support the training of an autonomous agent for tasks associated with a large-scale and geospecific outdoor urban environment, including the task of navigation as a car.
The study elaborates on the simulator's architecture, agent, and environment.
For the environment, ARORA provides an improvement on similar simulators through an unconstrained geospecific environment with detailed semantic annotation.
The agent is represented as a car available with four different options of physics fidelity.
The agent also has sensors available: a pose sensor, a camera sensor, and a set of three proximity sensors.
Future use cases from training extend to both civilians and militaries (including human training and wargaming), in terms of training autonomous agents in outdoor urban environments.
The study also presents a brief description of NavSim, a Python-based companion tool.
The purpose of NavSim is to connect to ARORA (or any other similar simulator) and train an agent using reinforcement-learning algorithms.
The study also provides challenges in development and subsequent work-arounds and solutions.
The goal of the ARORA & NavSim system is to provide communities with a high-fidelity, publicly available, free, and open-source system for training an autonomous agent as a car.

title: A reinforcement learning approach for transaction scheduling in a shuttle-based storage and retrieval system
abstract: With recent Industry 4.0 developments, companies tend to automate their industries.
Warehousing companies also take part in this trend.
A shuttle-based storage and retrieval system (SBS/RS) is an automated storage and retrieval system technology experiencing recent drastic market growth.
This technology is mostly utilized in large distribution centers processing mini-loads.
With the recent increase in e-commerce practices, fast delivery requirements with low volume orders have increased.
SBS/RS provides ultrahigh-speed load handling due to having an excess amount of shuttles in the system.
However, not only the physical design of an automated warehousing technology but also the design of operational system policies would help with fast handling targets.
In this work, in an effort to increase the performance of an SBS/RS, we apply a machine learning (ML) (i.
e.
, Q-learning) approach on a newly proposed tier-to-tier SBS/RS design, redesigned from a traditional tier-captive SBS/RS.

The novelty of this paper is twofold: First, we propose a novel SBS/RS design where shuttles can travel between tiers in the system; second, due to the complexity of operation of shuttles in that newly proposed design, we implement an ML-based algorithm for transaction selection in that system.

The ML-based solution is compared with traditional scheduling approaches: first-in-first-out and shortest process time (i.e., travel) scheduling rules.
The results indicate that in most cases, the Q-learning approach performs better than the two static scheduling approaches.

title: Managing mixed traffic at signalized intersections: An adaptive signal control and CAV coordination system based on deep reinforcement learning
abstract: Managing the mixed traffic involving connected and autonomous vehicles (CAVs) and human-driven vehicles (HVs) at a signalized intersection has become a concern of researchers.
However, the performances of most existing control methods are limited, especially when CAV penetration rate is low, since they fail to make a better trade-off between safety and operational efficiency for both CAVs and HVs.

To this end, this study proposes a deep reinforcement learning (DRL) powered control system for the mixed traffic at signalized intersections, which aims to optimize operational efficiency of both CAVs and HVs while assuring safety and reducing interference on HVs' driving habits.

The system adopts an adaptive traffic signal control strategy and an efficient CAV control policy with a passing rule proposed as a link in between.
The traffic signal control strategy allows traffic light to adaptively adjust its phase and duration based on real-time traffic information, while the CAV control policy permits the CAVs meeting certain safety constraints to form platoons and pass through the intersection in a coordinated manner regardless of traffic signals.

As an efficient DRL algorithm, Deep Q-Network (DQN) is adopted to adaptively control the signals and implement CAV coordination.
The proposed system is examined on Simulation of Urban Mobility (SUMO), given different CAV penetration rates and traffic conditions.
It is found that the proposed system not only outperforms the state-of-the-art control methods on reducing travel time and fuel consumption under low CAV penetration rate, but also enlarges its advantages with the increase of CAV penetration rate.

In certain traffic scenarios, the proposed system can even achieve a maximum reduction of travel time by 37.
33% and fuel consumption by 15.
95%, in comparison to the existing method with the best performance.

Besides, to some extent, the comparisons between the performances of CAVs and HVs demonstrate certain benefits of introducing CAVs into the mixed traffic.

title: EAGER: Real-Time: Formal Reinforcement Learning Methods for the Design of Safety-critical Autonomous Systems
abstract: This EArly-Concept Grant for Exploratory Research (EAGER) project takes a clean-slate first-principles approach to the design of safety-critical autonomous systems by integrating formal methods and reinforcement learning from data.

Several recent high-profile traffic incidents involving semi-autonomous vehicles have raised questions about whether current artificial intelligence (AI)-centered methods can ever lead us to Level 4 or 5 autonomy, i.
e.
, to the realization of fully-autonomous vehicles with performance equivalent to a human driver in all driving scenarios.

On the other hand, approaches rooted in formal methods for verification and synthesis can provide safety guarantees but have difficulty in efficiently reasoning about uncertainty and the correctness of data-driven models.

This project will combine these two, seemingly incompatible, paradigms for designing autonomous systems.
It will use model-free reinforcement learning algorithms to learn from semi-autonomous vehicle driving data.
It will adopt model-based methods for system design, verification, and synthesis to offer provably safe operation in highly uncertain scenarios.
An AutoDrive testbed will be set up where human driving data from scaled vehicular models will be leveraged to infer safe control policies using imitation and inverse reinforcement learning algorithms.

The research is relevant to the science of intelligent autonomous transportation systems with significant societal implications.
The experimental testbed will be used to provide hands-on research experience to undergraduate students and for K-12 outreach efforts.
<br/><br/>In particular, the project will develop a framework for optimal control synthesis for safety and performance specification expressed in signal temporal logic.

It will then incorporate vehicular and pedestrian kinematics in non-deterministic/probabilistic  transition models specified via probabilistic computation tree logic.
Finally, it will develop formal reinforcement learning methods for partially observed dynamic models subject to safety specifications and complex temporal goals by learning from traces of safe human drivers.

One key technical contribution of the project will be development of new formal reinforcement learning methods that may be useful in a broad array of applications wherein we must synthesize optimal controllers that satisfy certain safety specifications by learning from data.
<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


title: Modeling of Human Decision Making via Direct and Optimization-based Methods for Semi-Autonomous Systems
abstract: 

title: Generating merging strategies for connected autonomous vehicles based on spatiotemporal information extraction module and deep reinforcement learning
abstract: A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs.

Deep reinforcement learning (DRL) is a promising approach to address this challenge.
It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need compliance of HDVs.
However, the fusion of CAV sensing information is non-trivial.
Traditional DRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment.

Aimed at tackling these issues, this study proposes a DRL framework named spatiotemporal deep Q network (STDQN), by integrating a double deep Q network (DDQN) and a spatiotemporal information extraction module.

A long-short term memory neural network with an attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information.
In addition, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as the connectivity of multiple CAVs in a global range.
Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems.
Experimental results prove that as compared to baseline DRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort.
The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving.
(c) 2022 Elsevier B.V. All rights reserved.

title: CAREER: Online Learning-based Underwater Acoustic Communications and Networking
abstract: Underwater acoustic communication networks are the enabling technologies for unmanned, in situ, and real-time aquatic monitoring in a wide range of applications, such as scientific studies, pollution detection, offshore exploration, and tactical surveillance.

The lifespan of underwater systems varies from a few years to decades, while the spatiotemporal dynamics of underwater acoustic environments at multiple scales pose grand challenges to efficient and reliable acoustic data transmission.

The objective of this project is to develop a fundamental and systematic online-learning-based framework for underwater acoustic communications and networking, where the underwater acoustic system 1) models and predicts the long-term dynamics of the acoustic environment, and 2) proactively adapts its communication and networking strategy to the dynamics of the environment, thereby maximizing the long-term system performance in the aspects of energy efficiency, spectrum efficiency, and transmission reliability.

Through explicit learning about its environment, the proposed framework will allow harmonious co-existence with other acoustic systems, including marine animals, to achieve eco-friendly operation.
This project's research will be integrated with education through summer youth K-12 outreach, curriculum development, undergraduate and graduate student training that will be particularly tailored to females and underrepresented minorities, and collaboration with an underwater robotics team from the local Dollar Bay High School.

These activities are designed to motivate and better train rural, female, minority, and economically disadvantaged students to pursue STEM careers.
<br/><br/><br/>This project tackles fundamental challenges in online-learning-based underwater acoustic communications and networking by innovating across three interrelated domains.

First, novel signal processing and sparse learning techniques will be developed to model and predict the large-scale dynamics and the statistical distribution of small-scale fading of underwater acoustic environments, including the acoustic transmission loss, ambient soundscape, and statistical characterization of external (anthropogenic and marine animal) acoustic sources.

Second, an optimization framework will be developed, based on the acoustic environment prediction, for joint transmission power control, link scheduling, node-cooperative routing, and autonomous vehicle mobility control to achieve high network utility and harmonious coexistence with other acoustic systems.

Third, the acoustic environment exploration-exploitation tradeoff will be tackled in the Bayesian reinforcement learning framework, which will provide a principled approach to weighing the immediate reward of a communication and networking strategy and its associated long-term benefit of revealing the environment's dynamics.

Leveraging the geographic advantage of Michigan Tech and the state-of-the-art facilities of Michigan Tech's Great Lakes Research Center, extensive field experiments will be conducted for acoustic measurement collection and for offline and online algorithm evaluation in a software-defined networking architecture.

The methodologies and crosscutting techniques developed in this project can be applied to the design of intelligent radio-frequency communication networks.

title: Self-Optimizing Path Tracking Controller for Intelligent Vehicles Based on Reinforcement Learning
abstract: The path tracking control system is a crucial component for autonomous vehicles; it is challenging to realize accurate tracking control when approaching a wide range of uncertain situations and dynamic environments, particularly when such control must perform as well as, or better than, human drivers.

While many methods provide state-of-the-art tracking performance, they tend to emphasize constant PID control parameters, calibrated by human experience, to improve tracking accuracy.
A detailed analysis shows that PID controllers inefficiently reduce the lateral error under various conditions, such as complex trajectories and variable speed.
In addition, intelligent driving vehicles are highly non-linear objects, and high-fidelity models are unavailable in most autonomous systems.
As for the model-based controller (MPC or LQR), the complex modeling process may increase the computational burden.
With that in mind, a self-optimizing, path tracking controller structure, based on reinforcement learning, is proposed.
For the lateral control of the vehicle, a steering method based on the fusion of the reinforcement learning and traditional PID controllers is designed to adapt to various tracking scenarios.
According to the pre-defined path geometry and the real-time status of the vehicle, the interactive learning mechanism, based on an RL framework (actor-critic-a symmetric network structure), can realize the online optimization of PID control parameters in order to better deal with the tracking error under complex trajectories and dynamic changes of vehicle model parameters.

The adaptive performance of velocity changes was also considered in the tracking process.
The proposed controlling approach was tested in different path tracking scenarios, both the driving simulator platforms and on-site vehicle experiments have verified the effects of our proposed self-optimizing controller.

The results show that the approach can adaptively change the weights of PID to maintain a tracking error (simulation: within +/- 0.
071 m; realistic vehicle: within +/- 0.
272 m) and steering wheel vibration standard deviations (simulation: within +/- 0.
04 degrees; realistic vehicle: within +/- 80.
69 degrees); additionally, it can adapt to high-speed simulation scenarios (the maximum speed is above 100 km/h and the average speed through curves is 63-76 km/h).


title: Proactive longitudinal control to preclude disruptive lane changes of human-driven vehicles in mixed-flow traffic
abstract: Connected and autonomous vehicles (CAVs) can be leveraged to enable cooperative platooning control to alleviate traffic oscillations.
However, prior to a pure CAV environment, CAVs and human-driven vehicles (HDVs) will coexist on roads, creating a mixed-flow traffic environment.
Mixed-flow traffic introduces key challenges for CAV operations due to potential lane changes by HDVs in adjacent lanes, which can cause stop-and-go waves and traffic oscillations.
An understanding of the interactions between CAVs and HDVs in the lane-change process can be leveraged to use CAVs to proactively preclude disruptive lane changes by HDVs.
This study proposes a deep reinforcement learning-based proactive longitudinal control strategy (PLCS) for CAVs to counteract disruptive HDV lane-change behaviors that can induce disturbances, and to preserve the smoothness of traffic flow in the CAV platooning control process.

In it, a Transformer-based lane-change traffic condition predictor is constructed to predict whether an HDV will likely perform a disruptive lane change under the ambient traffic conditions.
If no disruptive lane change is predicted, an extended intelligent driver model is activated for the CAV to perform smooth car-following behavior under cooperative CAV platooning control.
If a disruptive lane change is predicted, a rainbow deep Q-network (RDQN)-based lane-change preclusion model is proposed through which the CAV can alter the lane-change traffic condition to preclude the HDV's lane change.

Results from numerical experiments suggest that a CAV controlled by the PLCS is effective in reducing disruptive lane-change maneuvers by an HDV in its vicinity, and can improve string stability performance in mixed-flow traffic.

Further, the effectiveness of the PLCS is illustrated under different lane-change scenarios, CAV control setups, and HDV driver types.

title: Physics-informed deep reinforcement learning-based integrated two-dimensional car-following control strategy for connected automated vehicles
abstract: Connected automated vehicles (CAVs) are broadly recognized as next-generation transformative transportation technologies having great potential to improve traffic safety, efficiency, and stability.
Efficiently controlling CAVs on two-dimensional curvilinear roadways to follow preceding vehicles is denoted as the two-dimensional car-following process, which is highly critical; this process is challenging to implement owing to the complexity and varied nature of driving environments.

This study proposes an innovative integrated two-dimensional control strategy for CAVs based on deep reinforcement learning (DRL), which efficiently regulates the two-dimensional car-following process of CAVs in terms of both stability-wise longitudinal control performance and accurate lateral path-tracking performance.

Within the control framework, each CAV can receive the surrounding information from downstream vehicles and roadway geometry based on vehicle-to-everything (V2X) communication.
To better utilize this information, we designed a physics-informed DRL state fusion approach and reward function, which efficiently embeds prior physics knowledge and borrows the merits of the equilibrium and consensus concepts from the control theory.

Given the physics-informed information, the DRL-based controller outputs the integrated control instructions for both longitudinal and lateral control.
For training, we constructed a roadway with a set of varying curvatures and em-bedded the ground-truth vehicle trajectory datasets to more effectively capture the realistic variations in the roadway geometry and driving environment.

To facilitate value function approximation and enhance the policy iteration process in training, the distributed proximal policy optimization (DPPO) algorithm was applied, owing to its balanced performance.

A series of simulated experiments were conducted to validate the controller's lateral control accuracy and stability-wise oscillation dampening performance in diverse traffic scenarios, including extreme ones.

(c) 2023 Elsevier B.V. All rights reserved.

title: Human Activity Recognition from Egocentric Videos and Robustness Analysis of Deep Neural Networks
abstract: 

title: CAREER: Systematic Approach for Extensively (SAfEly) Testing and Verifying the Security of Connected and Autonomous Vehicle
abstract: This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).
&lt;br/&gt;&lt;br/&gt;The potential benefits of connected autonomous vehicles (CAV) are numerous, and society is expecting that this technology will increase the quality of everyday life and follow through on its promises.

However, to be effective, they must be tested to demonstrate a standard level of safety and security.
The complex and interconnected nature of the transportation system makes the task of testing and verification exceedingly difficult, raising serious concerns regarding their safety and security.
It, thus, calls for new problem formulation and a novel systematic approach for the task of CAV testing and verification.
The existing testing solutions use ad-hoc methods, such as miles driven, to demonstrate some indication of safety, often assuming that the CAV's perception of the surrounding environment is comprehensive and ideal.

However, no fundamental structure has been developed to demonstrate the security of CAV products.
This CAREER proposal models the transportation system as a networked control system providing a novel resiliency metric enabling the testing resiliency of CAVs.
In addition, it utilizes the prior developed verification framework to formulate the testing and verification process as a centralized feedback control system enabling the development of a novel attack generator.

The expected outcomes of this project would pave the way towards safely testing CAVs, directly impacting the future of this technology and related standards, ultimately eliminating crash-related fatalities and saving lives.

The research findings can be further implemented for all networked control systems, such as high-assurance military systems and autonomous systems ranging from unmanned aerial vehicles to power systems.

The educational purpose of the project is to expand students', particularly underrepresented and women minorities, awareness of CAV security by designing fully integrated educational modules and demonstrations.

We plan to include the following activities to serve the need for rural and largely economically distressed regions: (i) develop after school online STEM curriculum adjusted for primary, High-school, and college students; (ii) provide workshops for educators and industrial partners as their professional development activities; (iii) involve underrepresented undergraduate and college students through research for undergraduate experience and internship program; (iv) develop an undergraduate and an advanced graduate courses.
&lt;br/&gt;&lt;br/&gt;This CAREER project addresses the problem of testing and verification for the security of CAVs.

The importance of the security of CAVs has been recognized in the existing literature and has motivated the development of several detection and compensation algorithms to ensure safety under faults, failures, and attacks.

However, not much effort is invested in the task of CAVs testing and verification.
This CAREER project illustrates that the current approaches are insufficient to safely verify the security of CAVs in a realistic environment, suffering from the lack of a metric that is dynamic-dependent to measure the system resiliency.

We describe a research plan where a transportation system is modeled as a networked control system where roads, pedestrians, vehicles, and traffic signs (due to their dynamic behavior) are modeled as agents, interacting with each other using sensors and communication networks.

The new perspective allows us to propose a novel resiliency metric to be used alongside the safety metric to develop reinforcement learning-based controllers for testing CAVs' security.
As there are infinite types of faults and attacks, the proposed controller formulates the effects of attacks rather than focusing on specific types, easing the process of fault and attack generation.
This project is expected to advance the area of testing and verification of CAVs by (i) Introducing a novel perspective using the concept of networked control systems enabling the development of a unique data stream generator utilizing reinforcement learning to generate attacks by modeling the testing process as a feedback control system where minimizing safety and security is the desired objective and (ii) Developing a unique experimental platform enriched with the power of mixed reality (MR) and vehicle-in-the-loop (ViL) to test the security of CAVs safely.
&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


title: Machine Learning-Based Decision Making in Autonomous Systems
abstract: 

title: Reinforcement Learning Applications in Unmanned Vehicle Control: A Comprehensive Overview
abstract: This paper briefly reviews the dynamics and the control architectures of unmanned vehicles; reinforcement learning (RL) in optimal control theory; and RL-based applications in unmanned vehicles.
Nonlinearities and uncertainties in the dynamics of unmanned vehicles (e.g.
aerial, underwater, and tailsitter vehicles) pose critical challenges to their control systems.
Solving Hamilton-Jacobi-Bellman (HJB) equations to find optimal controllers becomes difficult in the presence of nonlinearities, uncertainties, and actuator faults.
Therefore, RL-based approaches are widely used in unmanned vehicle systems to solve the HJB equations.
To this end, they learn the optimal solutions by using online data measured along the system trajectories.
This approach is very practical in partially or completely model-free optimal control design and optimal fault-tolerant control design for unmanned vehicle systems.

title: Path Planning Technology of Unmanned Vehicle Based on Improved Deep Reinforcement Learning
abstract: As the basic problem of unmanned vehicle navigation control, path planning has been widely studied.
Reinforcement learning (RL) has been found an effective way of path optimization for the highly nonlinear and unmodeled dynamics.
However, the RL based methods suffer from the "dimension disaster" under the high-dimension state spaces.
In this paper, the path planning of an unmanned vehicle with collision avoidance is considered, and an improved Deep Q-Network (DQN) algorithm is proposed to reduce the computation load in the high-dimension state space.

First, the states, actions and rewards are determined based on the task requirement, and a smoothing function is defined as an additional penalty term to modify the basic reward function.
Then, the two-dimension grid of the state space is mapped to a gray image, which is applied as the input of a neural network, i.e., the Q-Network.
Finally, simulation results show that the modified DQN algorithm is more stable and the fluctuation frequency is significantly reduced.

title: Safe Off-policy Reinforcement Learning Using Barrier Functions
abstract: This paper presents a safe off-policy reinforcement learning (RL) scheme to design optimal controllers for systems with uncertain dynamics.
The utility function for which its optimization achieves a desired behavior is augmented with a control barrier function (CBF) candidate providing a platform for merging safety planning and optimal control design.

A damping factor is included in the CBF providing a design tool to specify the relative importance of performance and safety.
As one of the main contributions of this paper, it is shown that by iterative approximation of the value function, the safety properties of CBF are certified which bridges the broad capability of barrier functions into the learning-based approaches.

Then, the safety of control system is proved accordingly.
Stability and optimality of the control system in a safe condition are verified.
Afterward, an off-policy RL algorithm is used to obtain the safe and optimal controller without requiring full knowledge about the system dynamics.
The efficiency of the proposed method is demonstrated on the lane changing as an automotive control problem.

title: Confrontation and Obstacle-Avoidance of Unmanned Vehicles Based on Progressive Reinforcement Learning
abstract: The core technique of unmanned vehicle systems is the autonomous maneuvering decision, which not only determines the applications of unmanned vehicles but also is the critical technique many countries are competing to develop.

Reinforcement Learning (RL) is the potential design method for autonomous maneuvering decision-making systems.
Nevertheless, in the face of complex decision-making tasks, it is still challenging to master the optimal policy due to the low learning efficiency caused by the complex environment, high dimensional state, and sparse reward.

Inspired by the human learning process from simple to complex, we propose a novel progressive deep RL algorithm for policy optimization in unmanned autonomous decision-making systems in this paper.
The proposed algorithm divides the training of the autonomous maneuvering decision into a sequence of curricula with learning tasks from simple to complex.
Finally, through the self-play stage, the iterative optimization of the policy is realized.
Furthermore, the confrontation environment with two unmanned vehicles with obstacles is analyzed and modeled.
Finally, the simulation leads to the one-to-one adversarial tasks demonstrate the effectiveness and applicability of the proposed design algorithm.

title: A Reinforcement Learning Framework for PQoS in a Teleoperated Driving Scenario
abstract: In recent years, autonomous networks have been designed with Predictive Quality of Service (PQoS) in mind, as a means for applications operating in the industrial and/or automotive sectors to predict unanticipated Quality of Service (QoS) changes and react accordingly.

In this context, Reinforcement Learning (RL) has come out as a promising approach to perform accurate predictions, and optimize the efficiency and adaptability of wireless networks.
Along these lines, in this paper we propose the design of a new entity, integrated at the RAN level that implements PQoS functionalities with the support of an RL framework.
Specifically, we focus on the design of the reward function of the learning agent, able to convert QoS estimates into appropriate countermeasures if QoS requirements are not satisfied.
We demonstrate via ns-3 simulations that our approach achieves better results in terms of QoS and Quality of Experience (QoE) performance of end users in a teleoperated driving scenario.

title: A Research on Intelligent Obstacle Avoidance of Unmanned Vehicle Based on DDPG Algorithm
abstract: An intelligent obstacle avoidance scheme for unmanned vehicle based on reinforcement learning is proposed in this paper.
In view of that the movement of unmanned vehicle must meet both interior and exterior constraints,including vehicle dynamics constraints and traffic rule constraints and its output must be continuous, which the traditional reinforcement learning cannot assure,an improved deep deterministic policy gradient algorithm is proposed to tackle continuous motion space issue and achieve the continuous output of steering wheel angle and acceleration.

Multi-source sensor data fusion is adopted to fulfill the state input of unmanned vehicle obstacle avoidance algorithm and both interior and exterior constraints are added to make output motion more reasonable and effective.

Finally a simulation is conducted on the open-source simulation platform TORCS and the effectiveness and robustness of the algorithm verified.

title: Generating Human Arm Kinematics Using Reinforcement Learning to Train Active Muscle Behavior in Automotive Research
abstract: Computational human body models (HBMs) are important tools for predicting human biomechanical responses under automotive crash environments.
In many scenarios, the prediction of the occupant response will be improved by incorporating active muscle control into the HBMs to generate biofidelic kinematics during different vehicle maneuvers.
In this study, we have proposed an approach to develop an active muscle controller based on reinforcement learning (RL).
The RL muscle activation control (RL-MAC) approach is a shift from using traditional closed-loop feedback controllers, which can mimic accurate active muscle behavior under a limited range of loading conditions for which the controller has been tuned.

Conversely, the RL-MAC uses an iterative training approach to generate active muscle forces for desired joint motion and is analogous to how a child develops gross motor skills.
In this study, the ability of a deep deterministic policy gradient (DDPG) RL controller to generate accurate human kinematics is demonstrated using a multibody model of the human arm.
The arm model was trained to perform goal-directed elbow rotation by activating the responsible muscles and investigated using two recruitment schemes: as independent muscles or as antagonistic muscle groups.

Simulations with the trained controller show that the arm can move to the target position in the presence or absence of externally applied loads.
The RL-MAC trained under constant external loads was able to maintain the desired elbow joint angle under a simplified automotive impact scenario, implying the robustness of the motor control approach.


title: Vehicle Following Hybrid Control Algorithm Based on DRL and PID in Intelligent Network Environment
abstract: Deep reinforcement learning (DRL) has not been widely used in the engineering field yet because RL needs to be learned through 'trial and error', which makes the application of this kind of algorithm in real physical environment more difficult, and it is impossible to carry out 'trial and error' learning on real vehicles.

By analyzing the motion state of the vehicle in the car following mode, the algorithm that combined traditional longitudinal motion control with DRL improves the safety of RL in the real physical environment and the poor adaptability of the traditional longitudinal motion control algorithm.

In this paper, the longitudinal motion of the unmanned vehicle is taken as the research object, and the PID algorithm is combined with the Deep Deterministic Policy Gradient (DDPG) algorithm to control the longitudinal motion of the unmanned vehicle.

The research results show that the longitudinal motion control hybrid algorithm performed better than the single PID algorithm or DDPG algorithm in the vehicle following control.
This strategy establishes the relationship between the vehicle longitudinal control and the state of both the ego vehicle and the front vehicle, while also considers the randomness of the front vehicle motion in the iterative learning process, which improves the safety, comfort and following performance.


title: Developing an unmanned vehicle local trajectory using a reinforcement learning algorithm
abstract: This article describes the algorithm development for constructing a local trajectory for an unmanned vehicle or for implementation in an ADAS system using the reinforcement learning method.
A special part is dedicated to reinforcement learning.
One of the methods that is best suitable for the task conditions will also be implemented.
This method will allow bypassing obstacles and reaching the specified short target points.

title: Reinforcement Learning based Integrated Sensing and Communication for Automotive MIMO Radar
abstract: Integrated sensing and communication (ISAC) is a promising technique in vehicular transportation thanks to its substantial gains in size, cost, power consumption, electromagnetic compatibility and spectrum congestion.

In this paper, we propose a reinforcement learning (RL) based ISAC system with a multi-input-multi-output (MIMO) automotive radar.
The target sensing and downlink communication are separately performed by dividing the transmit antennas into two non-overlapping but interweaving subarrays.
We first design a RL framework to adaptively allocate the proper number of transmit antennas for the two subarrays under any unknown environment.
The training is performed in the metrics of Cramer-Rao Bound (CRB) of direction of arrival (DOA) estimation for sensing and receive signal-to-noise (SNR) for communications, respectively.
We proceed to propose a co-design method to jointly optimize the configurations of the two subarrays to further enhance the sensing accuracy with a constrained communication quality.
The resultant problem is converted into the convex form via convex relaxation.
Simulations are provided to demonstrate the adaptability and effectiveness of the proposed RL based ISAC system under the unkown environment.

title: A Review of Reinforcement Learning-Based Powertrain Controllers: Effects of Agent Selection for Mixed-Continuity Control and Reward Formulation
abstract: One major cost of improving the automotive fuel economy while simultaneously reducing tailpipe emissions is increased powertrain complexity.
This complexity has consequently increased the resources (both time and money) needed to develop such powertrains.
Powertrain performance is heavily influenced by the quality of the controller/calibration.
Since traditional control development processes are becoming resource-intensive, better alternate methods are worth pursuing.
Recently, reinforcement learning (RL), a machine learning technique, has proven capable of creating optimal controllers for complex systems.
The model-free nature of RL has the potential to streamline the control development process, possibly reducing the time and money required.
This article reviews the impact of choices in two areas on the performance of RL-based powertrain controllers to provide a better awareness of their benefits and consequences.
First, we examine how RL algorithm action continuities and control-actuator continuities are matched, via native operation or conversion.
Secondly, we discuss the formulation of the reward function.
RL is able to optimize control policies defined by a wide spectrum of reward functions, including some functions that are difficult to implement with other techniques.
RL action and control-actuator continuity matching affects the ability of the RL-based controller to understand and operate the powertrain while the reward function defines optimal behavior.
Finally, opportunities for future RL-based powertrain control development are identified and discussed.

title: Reinforcement Learning based Integrated Sensing and Communication for Automotive MIMO Radar
abstract: Integrated sensing and communication (ISAC) is a promising technique in vehicular transportation thanks to its substantial gains in size, cost, power consumption, electromag-netic compatibility and spectrum congestion.

In this paper, we propose a reinforcement learning (RL) based ISAC system with a multi-input-multi-output (MIMO) automotive radar.
The target sensing and downlink communication are separately performed by dividing the transmit antennas into two non-overlapping but interweaving subarrays.
We first design a RL framework to adaptively allocate the proper number of transmit antennas for the two subarrays under any unknown environment.
The training is performed in the metrics of Cramer-Rao Bound (CRB) of direction of arrival (DOA) estimation for sensing and receive signal-to-noise (SNR) for communications, respectively.
We proceed to propose a co-design method to jointly optimize the configurations of the two subarrays to further enhance the sensing accuracy with a constrained communication quality.
The resultant problem is converted into the convex form via convex relaxation.
Simulations are provided to demonstrate the adaptability and effectiveness of the proposed RL based ISAC system under the unkown environment.

title: Optimal IPT Core Design for Wireless Electric Vehicles by Reinforcement Learning
abstract: In this article, optimal inductive power transfer (IPT) core structures for wireless electric vehicle (WEV), which can be derived by optimal reinforcement learning (RL) algorithms, are newly proposed.
Because the IPT cannot be theoretically analyzed to find a maximum value of mutual inductance for the optimal core structure design, intuitive and iterative process based on finite element method analysis are usually implemented.

This conventional method, however, is not preferred due to numerous possible combinations and computation times.
For this reason, RL algorithms are designed to optimize nonlinear system design, enabling the WEV IPT to be efficiently designed with high mutual inductance, even in the presence of severe misalignment conditions.

Contrary to the conventional RL algorithm for the IPT core design, the proposed RL algorithm can follow higher mutual inductance by shorter episodes; hence, 50% of computation time reduction and 2% of maximum mutual inductance were achieved.

A prototype of WEV IPT system designed by the proposed RL algorithm was fabricated, satisfying the standard J2954 of the society of automotive engineers for WPT3/Z3 case.
As a result, it is found that the proposed WEV IPT can be manufactured, considering the desired number of cores for reasonable cost and weight of the vehicle assembly.

title: A Generalised Method for Adaptive Longitudinal Control Using Reinforcement Learning
abstract: Adaptive cruise control (ACC) seeks intelligent and adaptive methods for longitudinal control of the cars.
Since more than a decade, high-end cars have been equipped with ACC typically through carefully designed model-based controllers.
Unlike the traditional ACC, we propose a reinforcement learning based approach - RL-ACC.
We present the RL-ACC and its experimental results from the automotive-grade car simulators.
Thus, we obtain a controller which requires minimal domain knowledge, is intuitive in its design, can accommodate uncertainties, can mimic human-like behaviour and may enable human-trust in the automated system.

All these aspects are crucial for a fully autonomous car and we believe reinforcement learning based ACC is a step towards that direction.

title: Cognitive Radar Using Reinforcement Learning in Automotive Applications [arXiv]
abstract: The concept of cognitive radar (CR) enables radar systems to achieve intelligent adaption to a changeable environment with feedback facility from receiver to transmitter.
However, the implementation of CR in a fast-changing environment usually requires a well-known environmental model.
In our work, we stress the learning ability of CR in an unknown environment using a combination of CR and reinforcement learning (RL), called RL-CR.
Less or no model of the environment is required.
We also apply the general RL-CR to a specific problem of automotive radar spectrum allocation to mitigate mutual interference.
Using RL-CR, each vehicle can autonomously choose a frequency subband according to its own observation of the environment.
Since radar's single observation is quite limited compared to the overall information of the environment, a long short-term memory (LSTM) network is utilized so that radar can decide the next transmitted subband by aggregating its observations over time.

Compared with centralized spectrum allocation approaches, our approach has the advantage of reducing communication between vehicles and the control center.
It also outperforms some other distributive frequency subband selecting policies in reducing interference under certain circumstances.

title: ADAS-RL: Adaptive Vector Scaling Reinforcement Learning For Human-in-the-Loop Lane Departure Warning
abstract: Multiple sensory modalities are fast becoming a key instrument in the future of the automotive industry.
Collision avoidance, lane departure warning, and self-parking are examples of Advanced Driver Assistance Systems (ADAS) that are becoming possible with the adoption of more sensors.
Moreover, thanks to the recent advances in mobile computing and wearable devices, the driver is now equipped with advanced sensory systems.
This rich sensory environment paves the way to integrate the human factor into the loop of computation of ADAS to provide a personalized experience.
In this paper, we introduce ADAS-RL, a Reinforcement Learning based algorithm that integrates the behavior and reactions of the driver with the vehicle context to continuously adapt and tune the warning interventions of Lane Departure Warning System (LDW).

We validated ADAS-RL against human drivers using CARLA simulator.
Our evaluation shows a significant enhancement in the driver experience compared to the standard LDW systems.
ADAS-RL shows the ability to track the changes in driving behavior and adapt the frequency of warnings allowing drivers to stay within a reasonable distance (around 1.
75m) from lane markings with a significant decrease in the false warnings.


title: Deep Reinforcement Learning Algorithm and Simulation Verification Analysis for Automatic Control of Unmanned Vehicles
abstract: This study conducted research mainly on the proven applicability of controlling the unmanned vehicle using a deep reinforcement learning algorithm and relative performance improvements.
In specific, this study chose the AirSim platform developed by Microsoft as the simulation environment and conducted simulations mainly in the indoor parking lot Unreal 4 environment.
In the simulations, the deep reinforcement learning method applied is Deep Q Networks for its effectiveness as well as simplicity.
To improve the performance of the trained network, object detection methodology YOLO v3 is applied as the detection algorithm for the unmanned vehicle, and the network is improved using the output of object detection as its input to accelerate the training process.

The implementation of the algorithms has efficiently proven the feasibility of using deep reinforcement learning agents for the unmanned vehicle in the project and the implementation of effective object detection.


title: Autonomous Navigation Exploration and Map Construction for Unmanned Vehicles in Unknown Environments
abstract: For the problem that the autonomous navigation exploration algorithm is easy to fall into the local area,this paper proposed an exploration algorithm combining sampling and deep reinforcement learning.
First,the Long-Short-Term Memory (LSTM) network was used locally to obtain the historical pose information of the unmanned vehicle to avoid repeated exploration of the explored area;secondly,the optimal action of the deep reinforcement learning strategy was used to output using deep reinforcement learning and the reward function was designed to encourage the unmanned vehicle to fully explore the unknown area;Finally,the horizontal movement factor of the unmanned vehicle was considered to generate a global exploration path conforming to its current attitude by solving the Asymmetric Travel Salesman Problem (ATSP).
In the 2 000 s mine tunnel simulation environment,compared with the Technologies for Autonomous Robot Exploration (TARE) algorithm,the proposed algorithm increased the exploration area by 346.
3 m~2 and reduced the total driving distance by 209.
4 m;in the real scene test,the exploration algorithm completed the exploration of the underground garage with an area of 3 444.
3 m~2 and returned to the starting point in 1 014 s and built the environment map.


title: A Reinforcement Learning Method for a Hybrid Flow-Shop Scheduling Problem
abstract: The scheduling problems in mass production, manufacturing, assembly, synthesis, and transportation, as well as internet services, can partly be attributed to a hybrid flow-shop scheduling problem (HFSP).

To solve the problem, a reinforcement learning (RL) method for HFSP is studied for the first time in this paper.
HFSP is described and attributed to the Markov Decision Processes (MDP), for which the special states, actions, and reward function are designed.
On this basis, the MDP framework is established.
The Boltzmann exploration policy is adopted to trade-off the exploration and exploitation during choosing action in RL.
Compared with the first-come-first-serve strategy that is frequently adopted when coding in most of the traditional intelligent algorithms, the rule in the RL method is first-come-first-choice, which is more conducive to achieving the global optimal solution.

For validation, the RL method is utilized for scheduling in a metal processing workshop of an automobile engine factory.
Then, the method is applied to the sortie scheduling of carrier aircraft in continuous dispatch.
The results demonstrate that the machining and support scheduling obtained by this RL method are reasonable in result quality, real-time performance and complexity, indicating that this RL method is practical for HFSP.


title: Towards Optimal Energy Management Strategy for Hybrid Electric Vehicle with Reinforcement Learning
abstract: In recent years, the development of Artificial Intelligence (AI) has shown tremendous potential in diverse areas.
Among them, reinforcement learning (RL) has proven to be an effective solution for learning intelligent control strategies.
As an inevitable trend for mitigating climate change, hybrid electric vehicles (HEVs) rely on efficient energy management strategies (EMS) to minimize energy consumption.
Many researchers have employed RL to learn optimal EMS for specific vehicle models.
However, most of these models tend to be complex and proprietary, making them unsuitable for broad applicability.
This paper presents a novel framework, in which we implement and integrate RL-based EMS with the open-source vehicle simulation tool called FASTSim.
The learned RL-based EMSs are evaluated on various vehicle models using different test drive cycles and prove to be effective in improving energy efficiency.

title: Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection
abstract: Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry.
Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262.
Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms.
In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults.
During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior.
The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values.
In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection.
The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.

title: Offline Reinforcement Learning for Autonomous Driving with Real World Driving Data
abstract: Since traditional reinforcement learning (RL) approaches need active online interaction with the environment, previous works are mainly investigated in the simulation environment rather than the real world environment, especially for safety-critical applications.

Offline RL has recently emerged as a promising data-driven learning paradigm to learn a policy from offline dataset directly.
It seems that offline RL is well suited for autonomous driving, as it is feasible to collect offline naturalized driving dataset.
However, it remains unclear how to deploy offline RL with real world driving dataset only including observation data, and whether current offline RL algorithms work well to learn a driving policy than imitation learning?
In this paper, we provide an offline RL benchmark for autonomous driving including the dataset, baselines, and a data driven simulator1.
First, we summarize and introduce the popular offline RL baseline methods.
Then, we construct an offline RL dataset for the car following task based on the real world driving dataset INTERACTION.
A data driven simulator is applied to obtain augmented data and test the driving policy.
Further, we deploy four popular offline algorithms and analyze their performances under different datasets including real world driving data and augmented data.
Finally, related conclusions and discussions are given to analyze the critical challenge for offline RL in autonomous driving.

title: ADAS-RL: Adaptive Vector Scaling Reinforcement Learning For Human-in-the-Loop Lane Departure Warning
abstract: Multiple sensory modalities are fast becoming a key instrument in the future of the automotive industry.
Collision avoidance, lane departure warning, and self-parking are examples of Advanced Driver Assistance Systems (ADAS) that are becoming possible with the adoption of more sensors.
Moreover, thanks to the recent advances in mobile computing and wearable devices, the driver is now equipped with advanced sensory systems.
This rich sensory environment paves the way to integrate the human factor into the loop of computation of ADAS to provide a personalized experience.
In this paper, we introduce ADAS-RL, a Reinforcement Learning based algorithm that integrates the behavior and reactions of the driver with the vehicle context to continuously adapt and tune the warning interventions of Lane Departure Warning System (LDW).

We validated ADAS-RL against human drivers using CARLA simulator.
Our evaluation shows a significant enhancement in the driver experience compared to the standard LDW systems.
ADAS-RL shows the ability to track the changes in driving behavior and adapt the frequency of warnings allowing drivers to stay within a reasonable distance (around 1.
75m) from lane markings with a significant decrease in the false warnings.


title: Generative Design by Reinforcement Learning: Maximizing Diversity of Topology Optimized Designs [arXiv]
abstract: Generative design is a design exploration process in which a large number of structurally optimal designs are generated in parallel by diversifying parameters of the topology optimization while fulfilling certain constraints.

Recently, data-driven generative design has gained much attention due to its integration with artificial intelligence (AI) technologies.
When generating new designs through a generative approach, one of the important evaluation factors is diversity.
In general, the problem definition of topology optimization is diversified by varying the force and boundary conditions, and the diversity of the generated designs is influenced by such parameter combinations.

This study proposes a reinforcement learning (RL) based generative design process with reward functions maximizing the diversity of the designs.
We formulate the generative design as a sequential problem of finding optimal parameter level values according to a given initial design.
Proximal Policy Optimization (PPO) was applied as the learning framework, which is demonstrated in the case study of an automotive wheel design problem.
This study also proposes the use of a deep neural network to instantly generate new designs without the topology optimization process, thus reducing the large computational burdens required by reinforcement learning.

We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner.
It is different from the previous approach using CPU which takes much more processing time and involving human intervention.

title: An End-to-End Deep Reinforcement Learning Model Based on Proximal Policy Optimization Algorithm for Autonomous Driving of Off-Road Vehicle
abstract: Most conventional unmanned vehicle control algorithms require human adjustment of parameters and design of precise rules, thus failing to adapt quickly to multiple situations when facing complex environments in the wild.

To address these problems, this paper adopts an end-to-end deep reinforcement learning model based on proximal policy optimization algorithm to control the steering, speed and braking of an unmanned vehicle, allowing it to autonomously learn motion control strategies from perceptionmap in un-known environments.

A novel environment simulator which contains variable passable areas and obstacles is also proposed to support agents to achieve target reward.
The proposed agent model has been proved to receive the highest reward over SAC and has the ability to overcome the complexity of the wild environment generated by the simulator.

title: Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee
abstract: It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique.
Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee.

By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained.
Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast.

Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.

title: An End-to-End Deep Reinforcement Learning Model Based on Proximal Policy Optimization Algorithm for Autonomous Driving of Off-Road Vehicle
abstract: Most conventional unmanned vehicle control algorithms require human adjustment of parameters and design of precise rules, thus failing to adapt quickly to multiple situations when facing complex environments in the wild.

To address these problems, this paper adopts an end-to-end deep reinforcement learning model based on proximal policy optimization algorithm to control the steering, speed and braking of an unmanned vehicle, allowing it to autonomously learn motion control strategies from perception map in un-known environments.

A novel environment simulator which contains variable passable areas and obstacles is also proposed to support agents to achieve target reward.
The proposed agent model has been proved to receive the highest reward over SAC and has the ability to overcome the complexity of the wild environment generated by the simulator.

title: Automatic Tracking Method of Unmanned Vehicle Trajectory Based on Reinforcement Learning
abstract: Up to now, unmanned driving is still a challenging research field in academia at home and abroad.
The vehicle trajectory tracking technology is a very critical and urgent link, because it provides important information for intelligent traffic monitoring.
The reinforcement learning method is an important method for learning in an unknown environment.
In the field of artificial intelligence machine learning, reinforcement learning research has made great progress in theory, algorithm and application, and has become a current hotspot in research.
Unmanned vehicle trajectory tracking is one of the key technologies in the field of unmanned driving research.
It uses built-in sensors to perceive the environment, uses trajectory planning algorithms to generate the required path in real time, and the decision system selects the best path.
Finally, the built-in path tracking controller implement it.
This article mainly adopts the experimental analysis method to discuss how to break through the problem of automatic trajectory tracking technology in the support of enhanced learning by unmanned vehicles, and compare and analyze the expected yaw rate and actual yaw rate and frequency of the target vehicle.

According to the experimental research results, the expected yaw rate and actual yaw rate of the unmanned vehicle trajectory automatic tracking test are relatively close, and the test system in this test has a certain tracking effect.


title: Explainable multi-agent deep reinforcement learning for real-time demand response towards sustainable manufacturing
abstract: The demand response (DR) plays a significant role in manufacturing system energy management and sustainable industrial development.
Current literature on DR management for manufacturing systems has mostly focused on day-ahead production scheduling, whose effectiveness is limited due to the lack of flexibility to control the production line in real time.

The development of reinforcement learning (RL) possesses huge potential for realtime production control to address the flexibility issue.
However, since production is the top priority for any manufacturing system, a trustful and explainable RL for manufacturing system energy management that can ensure the production requirements is necessary for this application.

This study proposes an explainable multiagent deep RL method, where the analytical manufacturing system model is applied to decompose the systemlevel energy management objective and production requirement to the agent level.

Based on the decomposed task, the agent can then form a safe action subset that is interpretable to achieve the original system-level production requirement while learning to reduce energy costs under DR.

The proposed RL method, which is referred to as decomposed multi-agent deep Q-network (DMADQN), is applied to control a section of an automotive assembly line using one year of DR electricity price data to validate its performance.

Results show that the proposed method ensures the achievement of the production requirement while providing better DR energy management performance in both RL training and testing phases.
In addition, the proposed approach can outperform the day-ahead scheduling approach and save up to an additional 30.7% of energy costs under dynamic DR.

title: Safe reinforcement learning: A control barrier function optimization approach
abstract: This article presents a learning-based barrier certified method to learn safe optimal controllers that guarantee operation of safety-critical systems within their safe regions while providing an optimal performance.

The cost function that encodes the designer's objectives is augmented with a control barrier function (CBF) to ensure safety and optimality.
A damping coefficient is incorporated into the CBF which specifies the trade-off between safety and optimality.
The proposed formulation provides a look-ahead and proactive safety planning and results in a smooth transition of states within the feasible set.
That is, instead of applying an optimal controller and intervening with it only if the safety constraints are violated, the safety is planned and optimized along with the performance to minimize the intervention with the optimal controller.

It is shown that addition of the CBF into the cost function does not affect the stability and optimality of the designed controller within the safe region.
This formulation enables us to find the optimal safe solution iteratively.
An off-policy reinforcement learning (RL) algorithm is then employed to find a safe optimal policy without requiring the complete knowledge about the system dynamics, while satisfies the safety constraints.

The efficacy of the proposed safe RL control design approach is demonstrated on the lane keeping as an automotive control problem.

title: Reinforcement Learning as a Path to Autonomous Intelligent Cyber-Defense Agents in Vehicle Platforms
abstract: Technological advancement of vehicle platforms exposes opportunities for new attack paths and vulnerabilities.
Static cyber defenses can help mitigate certain attacks, but those attacks must generally be known ahead of time, and the cyber defenses must be hand-crafted by experts.
This research explores reinforcement learning (RL) as a path to achieve autonomous, intelligent cyber defense of vehicle control networks-namely, the controller area network (CAN) bus.
We train an RL agent for the CAN bus using Toyota's Portable Automotive Security Testbed with Adaptability (PASTA).
We then apply the U.
S.
 Army Combat Capabilities Development Command (DEVCOM) Army Research Laboratory's methodology for quantitative measurement of cyber resilience to assess the agent's effect on the vehicle testbed in a contested cyberspace environment.

Despite all defenses having similar traditional performance measures, our RL agent averaged a 90% cyber resilience measurement during drive cycles executed on hardware versus 41% for a naive static timing defense and 98% for the bespoke timing-based defense.

Our results also show that an RL-based agent can detect and block injection attacks on a vehicle CAN bus in a laboratory environment with greater cyber resilience than prior learning approaches (1% for convolutional networks and 0% for recurrent networks).

With further research, we believe there is potential for using RL in the autonomous intelligent cyber defense agent concept.

title: Knowledge graph and behavior portrait of intelligent attack against path planning
abstract: The broad application of artificial intelligence (AI) shows more and more vulnerabilities.
Adversaries have more opportunities to attack AI systems.
For example, unmanned vehicles may be interfered with by adversaries in path planning, resulting in unmanned vehicles being unable to move according to the planned route, and even serious safety problems.

On the other side, the portrait technology can extract highly refined characteristics of different attack strategies, so that unmanned vehicles can defend themselves based on the characteristics of each attack.

Existing research lacks intelligent attack research on path planning in the field of unmanned vehicles, and lacks portraits of attack behaviors in this scenario.
This paper combines multiagent reinforcement learning technology, time-series segmentation clustering technology, and knowledge graph technology to study the portrait technology of adversary intelligent attack behavior in the field of unmanned vehicle path planning.

First, the simulation results of unmanned vehicle path planning are obtained, and the steps of adversary attack behavior are extracted by using Toeplitz inverse covariance-based clustering time-series segmentation cluster technology.

Second, the knowledge graph is used to save the attack strategy, so as to form the attack behavior portrait of unmanned vehicle path planning.
The test on the Neo4j platform shows that our method is universal, can effectively describe the attack steps for unmanned vehicle path planning, and provides the basis for attack detection to establish the defense system of unmanned vehicles.


title: Reinforcement Learning from Simulation to Real World Autonomous Driving using Digital Twin [arXiv]
abstract: Reinforcement learning (RL) is a promising solution for autonomous vehicles to deal with complex and uncertain traffic environments.
The RL training process is however expensive, unsafe, and time consuming.
Algorithms are often developed first in simulation and then transferred to the real world, leading to a common sim2real challenge that performance decreases when the domain changes.
In this paper, we propose a transfer learning process to minimize the gap by exploiting digital twin technology, relying on a systematic and simultaneous combination of virtual and real world data coming from vehicle dynamics and traffic scenarios.

The model and testing environment are evolved from model, hardware to vehicle in the loop and proving ground testing stages, similar to standard development cycle in automotive industry.
In particular, we also integrate other transfer learning techniques such as domain randomization and adaptation in each stage.
The simulation and real data are gradually incorporated to accelerate and make the transfer learning process more robust.
The proposed RL methodology is applied to develop a path following steering controller for an autonomous electric vehicle.
After learning and deploying the real-time RL control policy on the vehicle, we obtained satisfactory and safe control performance already from the first deployment, demonstrating the advantages of the proposed digital twin based learning process.


title: Multi-USV Dynamic Navigation and Target Capture: A Guided Multi-Agent Reinforcement Learning Approach
abstract: Autonomous unmanned systems have become an attractive vehicle for a myriad of military and civilian applications.
This can be partly attributed to their ability to bring payloads for utility, sensing, and other uses for various applications autonomously.
However, a key challenge in realizing autonomous unmanned systems is the ability to perform complex group missions, which require coordination and collaboration among multiple platforms.
This paper presents a cooperative navigating task approach that enables multiple unmanned surface vehicles (multi-USV) to autonomously capture a maneuvering target while avoiding both static and dynamic obstacles.

The approach adopts a hybrid multi-agent deep reinforcement learning framework that leverages heuristic mechanisms to guide the group mission learning of the vehicles.
Specifically, the proposed framework consists of two stages.
In the first stage, navigation subgoal sets are generated based on expert knowledge, and a goal selection heuristic model based on the immune network model is used to select navigation targets during training.

Next, the selected goals' executions are learned using actor-critic proximal policy optimization.
The simulation results with multi-USV target capture show that the proposed approach is capable of abstracting and guiding the unmanned vehicle group coordination learning and achieving a generally optimized mission execution.


title: Reliable Path Planning Algorithm Based on Improved Artificial Potential Field Method
abstract: In order to solve the "minimum trap" of Artificial Potential Field and the limitation of traditional path planning algorithm in dynamic obstacle environment, a path planning algorithm based on improved artificial potential field is proposed.

Firstly, a virtual potential field detection circle model (VPFDCM) with adjustable radius is proposed to detect the "minimum trap" formed by the repulsion field of obstacles in advance.
And the motion model of unmanned vehicle is established.
Combined with the improved reinforcement learning algorithm based on Long Short-Term Memory(LSTM), the radius of virtual potential field detection circle is adjusted to achieve effective avoidance of dynamic obstacles.

The reliable online collision free path planning of unmanned vehicle in semi closed dynamic obstacle environment is realized.
Finally, the reliability and robustness of the algorithm are verified by MATLAB simulation.
The simulation results show that the improved artificial potential field can effectively solve the problem of unmanned vehicle falling into the "minimum trap" and improve the reliability of unmanned vehicle movement.

Compared with the traditional artificial potential field method, the improved artificial potential field method can achieve more than 90% success rate in obstacle avoidance.

title: Deep reinforcement learning-based dual-mode congestion control for cellular V2X environments
abstract: The Society of Automotive Engineers (SAE) J2945/1 standard for Dedicated Short Range Communication (DSRC) environmentutilizes transmit (Tx) power control and rate control elements for the periodic BSM transmissions, which are intended to work in a complementary manner.

An equivalent standard for the cellular vehicle-to-everything (C-V2X) communication environment is J3161/1, but it eliminates Tx power control and uses only rate control.
However, the consequence is the degraded update delay of neighbouring vehicles' kinematics, potentially undermineing driving safety.
In this Letter, the authors propose to retain the dual-mode control in the C-V2X environment and find a policy through reinforcement learning (RL) to adjust the rate control function to maintain synergy.

Moreover, the authors can extract the RL-created policy from the neural network so that it can be explicitly specified in the standard, and downloaded and used more conveniently by vehicles.
Finally, the RL-generated policy achieves a better packet delivery frequency than J2945/1 or J3161/1.Current V2X standards typically specify human-designed heuristic control algorithms.
Their drawbacks are frequently discovered later by more thorough simulation experiments that were not done during standardization.
This Letter shows that AI-generated control can be automatically obtained while running simulation, and better performing than heuristic algorithms, with the case of congestion control.
Moreover, the policy can be extracted from the neural network and explicitly specified in the standard and used by vehicles instead of the neural network.image

title: Flexible control of Discrete Event Systems using environment simulation and Reinforcement Learning
abstract: Discrete Event Systems (DESs) are classically modeled as Finite State Machines (FSMs), and controlled in a maximally permissive, controllable, and nonblocking way using Supervisory Control Theory (SCT).

While SCT is powerful to orchestrate events of DESs, it fail to process events whose control is based on probabilistic assumptions.
In this research, we show that some events can be approached as usual in SCT, while others can be processed using Artificial Intelligence.
We present a tool to convert SCT controllers into Reinforcement Learning (RL) simulation environments, from where they become suitable for intelligent processing.
Then, we propose a RL-based approach that recognizes the context under which the selected set of stochastic events occur, and treats them accordingly, aiming to find suitable decision making as complement to deterministic outcomes of the SCT.

The result is an efficient combination of safe and flexible control, which tends to maximize performance for a class of DES that evolves probabilistically.
Two RL algorithms are tested, State-Action-Reward-State-Action (SARSA) and N-step SARSA, over a flexible automotive plant control.
Results suggest a performance improvement 9 times higher when using the proposed combination in comparison with non-intelligent decisions.
(C) 2021 Elsevier B.V. All rights reserved.

title: Extended Kalman Filter Based Resilient Formation Tracking Control of Multiple Unmanned Vehicles via Game-Theoretical Reinforcement Learning
abstract: In this paper, we discuss the resilient formation tracking control problem of multiple unmanned vehicles (MUV).
A dynamic leader-follower distributed control structure is utilized to optimize the performance of the formation tracking.
For the follower of the MUV, the leader is a cooperative unmanned vehicle, and the target of formation tracking is a non-cooperative unmanned vehicle with a nonlinear trajectory.
Therefore, an extended Kalman filter (EKF) observer is designed to estimate the state of the target.
Then the leader of the MUV is adjusted dynamically according to the state of the target.
In order to describe the interactions between the follower and dynamic leader, a Stackelberg game model is constructed to handle the hierarchical decision problems.
At the lower layer, each follower responds by observing the leader's strategy, and the potential game is used to prove a Nash equilibrium among all followers.
At the upper layer, the dynamic leader makes decisions depending on the response of all followers to reaching the Stackelberg equilibrium.
Moreover, the Stackelberg-Nash equilibrium of the designed game theoretical model is proven.
A novel reinforcement learning-based algorithm is designed to achieve the Stackelberg-Nash equilibrium of the game.
Finally, the effectiveness of the method is verified by a variety of formation tracking simulation experiments.

title: Path-Tracking Control Strategy of Unmanned Vehicle Based on DDPG Algorithm
abstract: This paper proposes a deep reinforcement learning (DRL)-based algorithm in the path-tracking controller of an unmanned vehicle to autonomously learn the path-tracking capability of the vehicle by interacting with the CARLA environment.

To solve the problem of the high estimation of the Q-value of the DDPG algorithm and slow training speed, the controller adopts the deep deterministic policy gradient algorithm of the double critic network (DCN-DDPG), obtains the trained model through offline learning, and sends control commands to the unmanned vehicle to make the vehicle drive according to the determined route.

This method aimed to address the problem of unmanned-vehicle path tracking.
This paper proposes a Markov decision process model, including the design of state, action-and-reward value functions, and trained the control strategy in the CARLA simulator Town04 urban scene.
The tracking task was completed under various working conditions, and its tracking effect was compared with the original DDPG algorithm, model predictive control (MPC), and pure pursuit.
It was verified that the designed control strategy has good environmental adaptability, speed adaptability, and tracking performance.

title: Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs
abstract: Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers.
Among many approaches, topology optimization based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches.
Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration.
This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs.
We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design.
Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem.
To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks.
With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics.
We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner.
It is different from the previous approach using CPU which takes much more processing time and involving human intervention.
(c) 2022 Elsevier Ltd. All rights reserved.

title: Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes [arXiv]
abstract: Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms.

While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated.

In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements.
In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node.
With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements.

Our models have the advantage that they can be trained for a small number of trucks and nodes, and then embedded into a large supply chain to yield solutions for larger numbers of trucks and nodes.
We test our approach on a real supply chain environment arising in the operations of Japanese automotive parts manufacturer Aisin Corporation, and find that our algorithm outperforms Aisin's previous best solution.


title: Bi-level Path Planning Method for Unmanned Vehicle Based on Deep Reinforcement Learning
abstract: With the wide application of intelligent unmanned vehicles,intelligent navigation,path planning and obstacle avoidance technology have become important research contents.
This paper proposes model-free deep reinforcement learning algorithms DDPG and SAC,which use environmental information to track to the target point,avoid static and dynamic obstacles,and can be generally suitable for different environments.
Through the combination of global planning and local obstacle avoidance,it solves the path planning problem with better globality and robustness,solves the obstacle avoidance problem with better dynamicity and generalization,and shortens the iteration time.
In the network training stage,PID,A* and other traditional algorithms are combined to improve the convergence speed and stability of the method.
Finally,a variety of experimental scenarios such as navigation and obstacle avoidance are designed in the robot operating system ROS and the simulation program gazebo.
Simulation results verify the reliability of the proposed approach,which takes the global and dynamic nature of the problem into account and optimizes the generated paths and time efficiency.


title: Object Detection with Deep Neural Networks for Reinforcement Learning in the Task of Autonomous Vehicles Path Planning at the Intersection
abstract: Among a number of problems in the behavior planning of an unmanned vehicle the central one is movement in difficult areas.
In particular, such areas are intersections at which direct interaction with other road agents takes place.
In our work, we offer a new approach to train of the intelligent agent that simulates the behavior of an unmanned vehicle, based on the integration of reinforcement learning and computer vision.
Using full visual information about the road intersection obtained from aerial photographs, it is studied automatic detection the relative positions of all road agents with various architectures of deep neural networks (YOLOv3, Faster R-CNN, RetinaNet, Cascade R-CNN, Mask R-CNN, Cascade Mask R-CNN).

The possibilities of estimation of the vehicle orientation angle based on a convolutional neural network are also investigated.
Obtained additional features are used in the modern effective reinforcement learning methods of Soft Actor Critic and Rainbow, which allows to accelerate the convergence of its learning process.
To demonstrate the operation of the developed system, an intersection simulator was developed, at which a number of model experiments were carried out.

title: Reinforcement Learning for Behavior Planning of Autonomous Vehicles in Urban Scenarios
abstract: 

title: Real-Time Optimal Energy Management of Electrified Powertrains with Reinforcement Learning
abstract: Reinforcement learning (RL) algorithm is employed in solving energy management problem for electrified powertrain in real-world driving scenarios and the application process is streamlined.
A near-global optimal control policy is articulated for the energy management system (EMS) using Q-learning algorithm which is real-time implementable.
The core of the EMS is an updating optimal control policy in the form of a changing look-up table comprising near-global optimal action value function (Q-values) corresponding to all feasible state-action combinations.

Using the updating control policy, the EMS can optimally decide power-split between electric machines (EMs) and internal combustion engine (ICE) in real-world driving situations.

title: Decentralized Automotive Radar Spectrum Allocation to Avoid Mutual Interference Using Reinforcement Learning
abstract: Nowadays, mutual interference among automotive radars has become a problem of wide concern.
In this article, a decentralized spectrum allocation approach is presented to avoid mutual interference among automotive radars.
Although decentralized spectrum allocation has been extensively studied in cognitive radio sensor networks, two challenges are observed for automotive sensors using radar.
First, the allocation approach should be dynamic as all radars are mounted on moving vehicles.
Second, each radar does not communicate with the others so it has quite limited information.
A machine learning technique, reinforcement learning, is utilized because it can learn a decision-making policy in an unknown dynamic environment.
As a single radar observation is incomplete, a long short-term memory recurrent network is used to aggregate radar observations through time so that each radar can learn to choose a frequency subband by combining both the present and past observations.

Simulation experiments are conducted to compare the proposed approach with other common spectrum allocation methods such as the random and myopic policy, indicating that our approach outperforms the others.


title: Research on intelligent combat decision making based on deep reinforcement learning
abstract: With the operational advantages of unmanned combat platforms in modern war gradually appearing, the research of unmanned combat platforms has become the focus of all circles.
In order to realize intelligent and autonomous unmanned operation in a real sense, a combat mission computer based on AI development board was proposed to be built as the control core of unmanned vehicles, simulate the operational mobility situation diagram of unmanned vehicles, and use the deep reinforcement learning network DQN to establish angle and distance decision-making network, so as to realize intelligent mobility decision-making of unmanned vehicles.

The experiment verified that the unmanned vehicle can maneuver to the target area autonomously, which proved that the deep reinforcement learning network can realize the feasibility of platform autonomous and intelligent decision-making, and provided a feasible technical approach and theoretical support for the construction of combat mission computer to realize intelligent, autonomous and unmanned combat in a real sense.


title: Distributed Reinforcement Learning for Autonomous Driving
abstract: 

title: UBER: An Unreal Engine Based Simulation Platform with Extensibility and Real-time Capability*
abstract: In this paper, we propose a real-time platform called UBER, which stands for Unreal Engine Based simulation platform with Extensibility and Real-time capability.
It provides a visualization way to the online tests of unmanned vehicle simulation with Unreal Engine 5 (UE5).
By building a TCP communication module to exchange message between two ends in UBER, we successfully solve the problem that no code script can be operated in blueprints of UE5.
Excellent physics components in UE5 makes simulation visualization more real and beautiful compared with other Reinforcement Learning environments, and by accessing the python scripts outside of blueprints, various of elements, such as scenario components and environmental settings, etc.
, can be easily modified.

Furthermore, a series of reinforcement learning and formation control algorithms are implemented on UBER to show its extensibility and real-time capability, proving that our TCP communication method brings forth a good solution to the problem that UE5 does not support python scripts within blueprint.


title: Electric Vehicle Steering Design and Automated Control Using CNN and Reinforcement Learning
abstract: Autonomous vehicles are one of the engrossing technological trends in the present automotive industry.
These vehicles enticed substantial attention in industry as well as in academia.
With the rising trend in research and development of autonomous vehicles, it is important to keep in mind the safety, control, and cost effectiveness of the system.
The cost and implementation of self-driving technologies hinder the development of similar systems in academia and research.
In this paper, we are mainly focused on developing a vision system, an automated steering system in an electric vehicle platform for academia and research.
The developed system has a provision to incorporate deep learningConvolutional neural network (CNN) and reinforcement learning (RL) for automated steering control.
The proposed automated steering model uses end-to-end learning and reinforcement learning for predicting the steering angles with at most 85% accuracy and control the steering.

title: Path planning method using dyna-q algorithm under complex urban environment
abstract: Path planning and obstacle avoidance problems are now the focus of robotics research.
This paper uses the Dyna-Q reinforcement learning algorithm to implement an obstacle avoidance and a path planning algorithm for unmanned ground vehicle(UGV) under urban environment.
Using the reinforcement learning algorithm, we calculate the waypoints of the unmanned vehicle and achieve obstacle avoidance tasks and path planning using a vector field.
Finally, we use a PID controller on unmanned aerial vehicle (UAV) to realize the air-ground collaboration task.
The algorithms and the agents' modeling in this paper are implemented in the lab's simulation platform.

title: Deep Reinforcement Learning-Driven Scheduling in Multijob Serial Lines: A Case Study in Automotive Parts Assembly
abstract: Multijob production (MJP) is a class of flexible manufacturing systems, which produces different products within the same production system.
MJP is widely used in product assembly, and efficient MJP scheduling is crucial for productivity.
Most of the existing MJP scheduling methods are inefficient for multijob serial lines with practical constraints.
We propose a deep reinforcement learning (DRL)-driven scheduling framework for multijob serial lines by properly considering the practical constraints of identical machines, finite buffers, machine breakdown, and delayed reward.

We analyze the starvation and the blockage time, and derive a DRL-driven scheduling strategy to reduce the blockage time and balance the loads.
We validate the proposed framework by using real-world factory data collected over six months from a tier-one vendor of a world top-three automobile company.
Our case study shows that the proposed scheduling framework improves the average throughput by 24.2% compared with the conventional approach.

title: Trajectory Planning of a CableBased Parallel Robot using Reinforcement Learning and Soft Actor-Critic
abstract: Industry 4.0 introduces the use of modular stations and better communication between agents to improve manufacturing efficiency and to lower the downtime between the customer and its final product.
Among novel mechanisms that have a high potential in this new industrial paradigm are cable-suspended parallel robot (CSPR): their payload-to-mass ratio is high compared to their serial robot counterpart and their setup is quick compared to other types of parallel robots such as Gantry system, popular in the automotive industry but difficult to set up and to adapt while the production line changes.

A CSPR can cover the workspace of a manufacturing hall and providing assistance to operators before they arrive at their workstation.
One challenge is to generate the desired trajectories, so that the CSPR could move to the desired area.
Reinforcement Learning (RL) is a branch of Artificial Intelligence where the agent interacts with an environment to maximize a reward function.
This paper proposes the use of a RL algorithm called Soft Actor-Critic (SAC) to train a two degrees-of-freedom (DOFs) CSPR to perform pick-and-place trajectory.
Even though the pick-and-place trajectory based on artificial intelligence has been an active research with serial robots, this technique has yet to be applied to parallel robots.

title: Reinforcement Learning Based Decentralized Automotive Radar Spectrum Allocation
abstract: 

title: Asynchronous Multithreading Reinforcement-Learning-Based Path Planning and Tracking for Unmanned Underwater Vehicle
abstract: The underwater unmanned vehicle (UUV) is widely used in various marine operations, in which path planning and trajectory tracking are the critical technologies to achieve autonomous motion planning.
Unlike previous research methods, this article proposes the asynchronous multithreading proximal policy optimization-based path planning (AMPPO-PP) and trajectory tracking (AMPPO-TT) algorithms and applies these two methods to different task scenarios of UUVs.

Taking advantage of the AMPPO, the expensive online computational procedure is converted to an offline training process.
The proposed algorithms enable the UUV to learn autonomous planning, tracking, and emergency obstacle avoiding.
Besides, the algorithm architecture of the AMPPO-PP and the AMPPO-TT is described in detail.
By refining the reward in each timestep and utilizing the reward-shaping trick, the reward sparsity is avoided.
The goal-distance heuristic reward function is used to make the UUV explore more directionally.
Various simulation environments are developed from simple to complex, along with multiple comparative experiments to verify the effectiveness of the proposed algorithms.

title: Decision Making for Overtaking of Unmanned Vehicle Based on Deep Q-learning
abstract: Overtaking decision under dynamic environment is one of the key research directions.
Traditional methods are difficult to solve such a complex optimization decision-making problem.
In recent years, reinforcement learning algorithm is developing continuously, which can be applied to solve the overtaking decision problem of unmanned vehicles.
Reinforcement learning generates new data through the continuous interaction between agent and environment, and feeds back to the agent for corresponding learning behavior, so as to improve its own behavior strategy.

In this paper, Deep Q-learning (DQN) is used to solve the overtaking decision-making problem on one-way through two lanes.
A one-way through two lane traffic environment is built in Python to train and test the algorithm.

title: Double-channel event-triggered adaptive optimal control of active suspension systems
abstract: An event-triggered adaptive fuzzy optimal control strategy is proposed for a quarter-car electromagnetic active suspension system, where the stiffness and the road input are unknown.
The event-triggered mechanism is utilized in both sensor-to-controller (SC) and controller-to-actuator (CA) channels, such that communication saving is achieved in double channels.
Two separate triggering conditions are constructed to guarantee optimal performance and stability.
Via the reinforcement learning (RL) method, the critic-actor architecture of fuzzy logic systems (FLSs) is constructed to approximate the solution of Hamilton-Jacobi-Bellman (HJB) equation, where there are two critics with one actor.

To overcome the "jumps of virtual control laws" (JVCL) problem arising in the backstepping-based ETC (see Deng et al.
in ISA Trans.
117:28-39 (2021)), undetermined continuous virtual control laws are constructed for analysis.
An event-triggered adaptive observer is fabricated to estimate the unknown road input.
It is proved that all the estimating and tracking errors are semi-globally uniformly ultimately bounded (SGUUB).
Simulation verifies the effectiveness of the proposed scheme.

title: Exploring Fault Parameter Space Using Reinforcement Learning-based Fault Injection
abstract: Assessing the safety of complex Cyber-Physical Systems (CPS) is a challenge in any industry.
Fault Injection (FI) is a proven technique for safety analysis and is recommended by the automotive safety standard ISO 26262.
Traditional FI methods require a considerable amount of effort and cost as FI is applied late in the development cycle and is driven by manual effort or random algorithms.
In this paper, we propose a Reinforcement Learning (RL) approach to explore the fault space and find critical faults.
During the learning process, the RL agent injects and parameterizes faults in the system to cause catastrophic behavior.
The fault space is explored based on a reward function that evaluates previous simulation results such that the RL technique tries to predict improved fault timing and values.
In this paper, we apply our technique on an Adaptive Cruise Controller with sensor fusion and compare the proposed method with Monte Carlo-based fault injection.
The proposed technique is more efficient in terms of fault coverage and time to find the first critical fault.

title: On the Applicability of Reinforced Learning in the Route Selection Task of an Unmanned Vehicle
abstract: This paper presents the process of describing and investigating methods for solving the optimal route search problem, choosing the means for developing a software module, including consideration of the required tools, description of the project structure, and features of the algorithm used.

An introduction to the field of optimal route finding is given through the basic terms of graph theory, which is a leading framework for the field of reinforcement learning.
The process of developing a learning environment is reviewed, starting with the selection of the environment and an explanation of its possible properties.
Then the research of selected multi-agent learning algorithms is described in order to make a final comparison according to the most important criteria.
Neural network architectures, hyperparameter learning tables and quality graphs of the learning models are plotted.

title: Detecting State of Charge False Reporting Attacks via Reinforcement Learning Approach
abstract: The increased push for green transportation has been apparent to address the alarming increase in atmospheric CO2 levels, especially in the last five years.
The success and popularity of Electric Vehicles (EVs) have led many carmakers to shift to developing clean cars in the next decade.
Moreover, many countries around the globe have set aggressive EV target adoption numbers, with some even aiming to ban gasoline cars by 2050.
Unlike their gasoline-based counterparts, EVs comprise many sensors, communication channels, and decision-making components vulnerable to cyberattacks.
Hence, the unprecedented demand for EVs requires developing robust defenses against these increasingly sophisticated attacks.
In particular, recently proposed cyberattacks demonstrate how malicious owners may mislead EV charging networks by sending false data to unlawfully receive higher charging priorities, congest charging schedules, and steal power.

This paper proposes a learning-based detection model that can identify deceptive electric vehicles.
The model is trained on an original dataset using real driving traces and a malicious dataset generated from a reinforcement learning agent.
The Reinforcement Learning (RL) agent is trained to create intelligent and stealthy attacks that can evade simple detection rules while also giving a malicious EV high charging priority.
We evaluate the effectiveness of the generated attacks compared to handcrafted attacks.
Moreover, our detection model trained with RL-generated attacks displays greater robustness to intelligent and stealthy attacks.

title: Reinforcement learning for sustainability enhancement of production lines
abstract: The importance of sustainability in industry is dramatically rising in recent years.
Controlling machine states to achieve the best trade-off between production rate and energy demand is an effective method for improving the energy efficiency of production systems.
This technique is referred to as energy-efficient control (EEC) and it triggers machines in a standby state with low power requests.
Reinforcement Learning (RL) algorithms can be used to successfully control production systems without the requirement of prior knowledge about system parameters.
Due to the difficulty in acquiring comprehensive information about system dynamics in real-world scenarios, this is considered an important factor.
The goal of this work is to create a novel RL-based model to apply EEC to multi-stage production lines with parallel machine workstations without relying on full knowledge of the system dynamics.
Numerical results confirm model benefits when applied to a real line from the automotive sector.
Further experiments confirm the effectiveness and generality of the approach.

title: When Crowdsourcing Meets Unmanned Vehicles: Toward Cost-Effective Collaborative Urban Sensing via Deep Reinforcement Learning
abstract: Mobile crowdsensing (MCS) and unmanned vehicle sensing (UVS) provide two complementary paradigms for large-scale urban sensing.
Generally, MCS has a lower cost but often confronts sensing imbalance and even blind areas due to the limitation of human mobility, whereas UVS is often capable of completing more demanding tasks at the expense of limited energy supply and hardware cost.

Thus, it is significant to investigate whether we could integrate the two paradigms for high-quality urban sensing in a cost-effective collaborative way.
However, it is nontrivial due to complex and long-term optimization objectives, uncontrolled dynamics, and a large number of heterogeneous agents.
To address the collaborative sensing problem, we propose an actor-critic-based heterogeneous collaborative reinforcement learning (HCRL) algorithm, which leverages several key ideas: local observation to handle expanded state space and extract the states of neighbor nodes, generalized model to avoid environment nonstationarity and ensure the scalability and stability of network, and proximal policy optimization to prevent the destructively large policy updates.

Extensive simulations based on a mobility model and a realistic trace data set are conducted to confirm that HCRL outperforms the state-of-the-art baselines.

title: Intelligent land vehicle model transfer trajectory planning method of deep reinforcement learning
abstract: Aiming at the problem of unmanned vehicles model automobiles tracking error and excessive dependence in the traditional motion planning, a method of unmanned vehicle path planning based on deep reinforcement learning model migration is proposed.

First, an abstract model of the real environment is extracted.
The model uses the deep deterministic policy gradient (DDPG) and the vehicle dynamics model to jointly train the enhanced learning model that approximates the optimal intelligent driving.
Secondly, the actual scenario problem is migrated through the model migration strategy.
In the virtual abstract model, the control and trajectory sequences are calculated according to the trained deep reinforcement learning model in the environment; then, the optimal trajectory sequence is selected according to the evaluation function in the real environment.

The experimental results show that the proposed method can process the continuous input state and generate a continuously controlled corner control sequence to reduce the lateral tracking error.
At the same time, the model can improve the generalization performance of the model and reduce the excessive dependence.

title: Reinforcement Learning-Based Co-Optimization of Adaptive Cruise Speed Control and Energy Management for Fuel Cell Vehicles
abstract: With the development of intelligent autodriving vehicles, the co-optimization of speed control and energy management under the insurance of safe and comfortable driving has become a vital issue.
Herein, the adaptive cruise control scenario is discussed.
A co-optimization method for speed control and energy management for fuel cell vehicles is suggested to delay the degradation of energy sources while preserving fuel cell efficiency.
A reward function based on a reinforcement learning (RL) algorithm is developed to optimize the safety coefficient, comfortability, car-following efficiency, and economy at the speed control level.
The RL agent learns to control vehicle speed while avoiding collisions and maximizing the cumulative rewards.
To handle the problem of energy management, an adaptive equivalent consumption minimization strategy, which takes into account the deterioration of energy sources, is implemented at the energy management level.

The results indicate that the suggested method reduces the demand power by 1.
7%, increases the lifetime of power sources, and reduces equivalent hydrogen consumption by 9.
4% compared to the model predictive control.
This study focuses on co-optimizing speed control and energy management for fuel cell vehicles in the adaptive cruise control scenario.

A reinforcement learning algorithm optimizes safety, comfort, car-following efficiency, and economy.
An equivalent consumption minimization strategy considers energy source degradation, resulting in reduced power demand, prolonged source lifespan, and decreased hydrogen consumption compared to model predictive control.
image (c) 2023 WILEY-VCH GmbH

title: Augmenting Drive-Thru Internet via Reinforcement Learning-Based Rate Adaptation
abstract: Drive-thru Internet has been considered as an effective Internet access method for Internet of Vehicles (IoV).
Through the opportunistic vehicle-to-roadside WiFi connection, it can provide high throughput performance with low communication cost for IoV applications, such as intelligent transportation system, automotive infotainment, etc.

However, its usability is highly affected by a fundamental issue called rate adaptation (RA), which is to adjust the modulation and coding rate to adapt to the dynamic wireless channel between the vehicle and the roadside access point (AP).

Conventional WiFi RA schemes are designed for indoor or quasistatic scenarios and do not account for the channel variations in drive-thru Internet.
In this article, we study the limitation of applying existing RA schemes in drive-thru Internet and propose a reinforcement learning (RL)-based RA scheme to capture the potential channel variation patterns and efficiently select the rate for every vehicle's egress frame.

Simulation results demonstrate that the proposed RA scheme outperforms the existing schemes in network throughput and that the efficiency of the learning model can be generalized under various conditions.

The proposed RA method can provide useful inspirations for designing robust and scalable link adaptation protocols in IoV.

title: Reinforcement learning based agents for improving layouts of automotive crash structures
abstract: The topology optimization of crash structures in automotive and aeronautical applications is challenging.
Purely mathematical methods struggle due to the complexity of determining the sensitivities of the relevant objective functions and restrictions according to the design variables.
For this reason, the Graph- and Heuristic-based Topology optimization (GHT) was developed, which controls the optimization process with rules derived from expert knowledge.
In order to extend the collected expert rules, the use of reinforcement learning (RL) agents for deriving a new optimization rule is proposed in this paper.
This heuristic is designed in such a way that it can be applied to many different models and load cases.
An environment is introduced in which agents interact with a randomized graph to improve cells of the graph by inserting edges.
The graph is derived from a structural frame model.
Cells represent localized parts of the graph and delineate the areas where agents can insert edges.
A newly developed shape preservation metric is presented to evaluate the performance of topology changes made by agents.
This metric evaluates how much a cell has deformed by comparing its shape in the deformed and undeformed state.
The training process of the agents is described and their performance is evaluated in the training environment.
It is shown how the agents and the environment can be integrated as a new heuristic into the GHT.
An optimization of the frame model and a vehicle rocker model with the enhanced GHT is carried out to assess its performance in practical optimizations.

title: Medium-term Capacity Management through Reinforcement Learning - Literature review and concept for an industrial pilot-application
abstract: Empty storage shelves and long customer lead times due to a sharp rise in market demand from industries on the one hand (e.g., pharmaceutical products).
On the other hand, increasing short-term working or unemployment due to a rapid decline in demand (e.g., automotive).
Current supply and demand gaps caused by the COVID-19 pandemic remind us that successful competition in volatile business environments requires rapid adjustments of production capacities.
Capacity management (CM) addresses these adjustments by adapting production capacity to market demand.
Operations managers of flexible manufacturing systems can adjust the capacity by using various levers (e.g., overtime, used machines, ...).
To guide these managers, decision support systems (DSS) exist for short-term CM (e.g., shop floor scheduling).
However, due to complexity and runtime problems, the decision-making process for medium-term CM is usually carried out with low technical support.
Increases in computing power and advances in algorithm performance over the past decades have enabled Machine Learning to solve ever more complex problems such as the aforementioned issues.
Reinforcement Learning (RL) in particular has shown good performance in solving short-term CM problems when compared to humans or other established heuristics.
In this work we review the current literature for CM using RL in flexible manufacturing systems.
We identify an existing lack of knowledge within the overlap of medium-term CM and RL.
However, good performance of RL in short-term CM indicates that an application in medium-term CM should be evaluated.
In addition, we propose a concept of a method for medium-term CM based on RL to support operations managers in the decision-making process.
The resulting DSS could have a significant impact on production performance, especially in terms of capacity adjustment speed.
All rights reserved Elsevier.

title: Towards Decentralized Predictive Quality of Service in Next-Generation Vehicular Networks [arXiv]
abstract: To ensure safety in teleoperated driving scenarios, communication between vehicles and remote drivers must satisfy strict latency and reliability requirements.
In this context, Predictive Quality of Service (PQoS) was investigated as a tool to predict unanticipated degradation of the Quality of Service (QoS), and allow the network to react accordingly.
In this work, we design a reinforcement learning (RL) agent to implement PQoS in vehicular networks.
To do so, based on data gathered at the Radio Access Network (RAN) and/or the end vehicles, as well as QoS predictions, our framework is able to identify the optimal level of compression to send automotive data under low latency and reliability constraints.

We consider different learning schemes, including centralized, fully-distributed, and federated learning.
We demonstrate via ns-3 simulations that, while centralized learning generally outperforms any other solution, decentralized learning, and especially federated learning, offers a good trade-off between convergence time and reliability, with positive implications in terms of privacy and complexity.


title: Charging Scheduling Strategies of Cooperated Car-hailing Operating Business for Electric Taxis
abstract: With the popularity of electric vehicles (EVs), the replacement on the traditional fuel taxis to the electric taxis (e-taxis) has been gradually emerged in a creasing number of cities.
Compared with the ordinary EVs, e-taxis require frequent charging due to their long daily mileage and high total power consumption.
Additionally, the disorderly charging behaviors of EVs have caused congestion in charging stations (CSs) and have seriously affected the normal business of e-taxi drivers.
This paper proposes a management architecture that combines the charging network and the car-hailing operating network of e-taxis.
With the goal of minimizing the charging cost of e-taxis, a set of scheduling strategy which combines car-hailing operating business and charging plan is designed based on reinforcement learning (RL).
The result shows that this strategy can effectively lower the charging cost of e-taxis by reducing the charging queue time in the CSs.

title: Reinforcement Learning Based Decision Making for Self Driving & Shared Control Between Human Driver and Machine
abstract: 

title: Inverse Reinforcement Learning in Automatic Driving Decision
abstract: With the urgent need of automatic driving on urban roads, autonomous unmanned system must complete the driving task considering safety, efficiency and comfort.
For the planning and decision-making module, reinforcement learning can learn human strategies in a human-like manner.
However, the reward function is difficult to be determined manually, and inverse reinforcement learning (IRL) can find a reasonable reward function that explains the human strategy.
In this paper, the machine learning method on unmanned system is studied, and the IRL based on maximum entropy is introduced to learn the reward function.
Experiments on the real-world nuScenes dataset is implemented by setting the features of reward function that conforms to urban environmental constraints.
Finally, a reasonable reward function is obtained, which demonstrates the weights of the features can describe the trajectory of unmanned vehicle under the urban road.

title: A DEEP REINFORCEMENT LEARNING APPROACH FOR INTEGRATED AUTOMOTIVE RADAR SENSING AND COMMUNICATION
abstract: We present a deep reinforcement learning approach to design an automotive radar system with integrated sensing and communication.
In the proposed system, sparse transmit arrays with quantized phase shifter are used to carry out transmit beamforming to enhance the performance of both radar sensing and communication.
Through interaction with environment, the automotive radar learns a reward that reflects the difference between mainlobe peak and the peak sidelobe level in radar sensing mode or communication user feedback in communication mode, and intelligently adjust its beamforming vector.

The Wolpertinger policy based actioncritic network is introduced for beamforming vector learning, which solves the dimension curse due to huge beamforming action space.

title: Cloud-Based Reinforcement Learning in Automotive Control Function Development
abstract: Automotive control functions are becoming increasingly complex and their development is becoming more and more elaborate, leading to a strong need for automated solutions within the development process.

Here, reinforcement learning offers a significant potential for function development to generate optimized control functions in an automated manner.
Despite its successful deployment in a variety of control tasks, there is still a lack of standard tooling solutions for function development based on reinforcement learning in the automotive industry.

To address this gap, we present a flexible framework that couples the conventional development process with an open-source reinforcement learning library.
It features modular, physical models for relevant vehicle components, a co-simulation with a microscopic traffic simulation to generate realistic scenarios, and enables distributed and parallelized training.

We demonstrate the effectiveness of our proposed method in a feasibility study to learn a control function for automated longitudinal control of an electric vehicle in an urban traffic scenario.
The evolved control strategy produces a smooth trajectory with energy savings of up to 14%.
The results highlight the great potential of reinforcement learning for automated control function development and prove the effectiveness of the proposed framework.

title: Comparison of Three Real-Time Implementable Energy Management Strategies for Multi-mode Electrified Powertrain
abstract: Three real-time implementable energy management system (EMS) strategies have been articulated for forward simulation vehicle model with an electrified powertrain.
Rule-based strategy and equivalent consumption minimization strategy (ECMS) have been profoundly used as a competent real-time implementable EMS strategy for electrified powertrain.
Rein-forcement learning (RL) is relatively new as a real-time EMS controller.
All these three controllers have been articulated for model-in-the-loop (MIL) simulation.
A comparison among state-of-the art RL-based controller, widely accredited ECMS, and rule-based control strategies is very crucial in order to analyze strengths and weaknesses of each of these strategies at the MIL and to make them apposite for the subsequent phases of utilitarian controller development.


title: Learning to navigate on the rough terrain: A multi-modal deep reinforcement learning approach
abstract: How to enable safe navigation of unmanned vehicles on complex and rough terrain is challenging and meaningful research.
In this paper, we propose an end-to-end reinforcement learning local navigation method with multi-modal data fusion, which effectively combines the intrinsic perception, such as Inertial Measurement Unit (IMU) measurements, and the extrinsic perception, such as Three-dimensional (3D) point clouds and images, of an unmanned vehicle.

A specific feature extraction network is constructed for each modal data, and the total network is effectively trained using a modal separation learning method.
The experimental results show that the proposed method can effectively address various obstacles such as rough roads, vegetation obstacles, and water pool disturbances to achieve autonomous and safe navigation of unmanned vehicles in off-road scenarios.


title: Simultaneous Learning and Planning in a Hierarchical Control System for a Cognitive Agent
abstract: The tasks of behavior planning and decision-making learning in a dynamic environment are usually divided and considered separately in control systems for intelligent agents.
A new unified hierarchical formulation of the problem of simultaneous learning and planning (SLAP) is proposed in the context of object-oriented reinforcement learning, and an architecture of a cognitive agent that solves this problem is described.

A new algorithm for learning actions in a partially observed external environment is proposed using a reward signal, an object-oriented subject description of the states of the external environment, and dynamically updated action plans.

The main properties and advantages of the proposed algorithm are considered, including the lack of a fixed cognitive cycle necessitating the separation of planning and learning subsystems in earlier algorithms and the ability to construct and update the model of interaction with the environment, thus increasing the learning efficiency.

A theoretical justification of some provisions of this approach is given, a model example is proposed, and the principle of operation of a SLAP agent when driving an unmanned vehicle is demonstrated.

title: Autonomous Overtaking Decision Making of Driverless Bus Based on Deep Q-learning Method
abstract: The autonomous overtaking maneuver is a valuable technology in unmanned vehicle field.
However, overtaking is always perplexed by its security and time cost.
Now, an autonomous overtaking decision making method based on deep Q-learning network is proposed in this paper, which employs a deep neural network(DNN) to learn Q function from action chosen to state transition.

Based on the trained DNN, appropriate action is adopted in different environments for higher reward state.
A series of experiments are performed to verify the effectiveness and robustness of our proposed approach for overtaking decision making based on deep Q-learning method.
The results support that our approach achieves better security and lower time cost compared with traditional reinforcement learning methods.

title: Residual Policy Learning for Powertrain Control
abstract: Eco-driving strategies have been shown to provide significant reductions in fuel consumption.
This paper outlines an active driver assistance approach that uses a residual policy learning (RPL) agent trained to provide residual actions to default power train controllers while balancing fuel consumption against other driver-accommodation objectives.

Using previous experiences, our RPL agent learns improved traction torque and gear shifting residual policies to adapt the operation of the powertrain to variations and uncertainties in the environment.

For comparison, we consider a traditional reinforcement learning (RL) agent trained from scratch.
Both agents employ the off-policy Maximum A Posteriori Policy Optimization algorithm with an actor-critic architecture.
By implementing on a simulated commercial vehicle in various car-following scenarios, we find that the RPL agent quickly learns significantly improved policies compared to a baseline source policy but in some measures not as good as those eventually possible with the RL agent trained from scratch.

Copyright (c) 2022 The Authors.
This is an open access article under the CC BY-NC-ND license

title: Lane-change Control for Unmanned Vehicle Based on REINFORCE Algorithm and Neural Network
abstract: For lane change and overtaking of unmanned vehicles,the paper studies the lane change control strategy of unmanned vehicles based on the REINFORCE algorithm and neural network.
The feedback,control input,and output limit requirement of the vehicle dynamics model are determined.
The REINFORCE algorithm is used to design the structure of the neural network controller and the training plan of the controller.
For too large data value and variance of the experience pool,a preprocessing method of the experience pool data is proposed to improve the controller training plan.
Besides analyzing sparse reward distribution in the reinforcement learning process,a reward shaping solution based on logarithmic function is proposed combined with the running condition of unmanned vehicles.
Compared with PID and LQR controllers,the experiment is carried out.
The results show that the proposed control strategy has smaller maximum error compared with PID,with a safer lane-change process.
The performance of the control strategy is similar to LQR,which proves its feasibility for the lane change control task of unmanned vehicles.
Also,the execution time of the control strategy in different platforms is recorded to prove its real-time performance and feasibility in lightweight platforms.


title: RDDRL: a recurrent deduction deep reinforcement learning model for multimodal vision-robot navigation
abstract: Existing deep reinforcement learning-based mobile robot navigation relies largely on single-modal visual perception to perform local-scale navigation.
However, multimodal visual fusion-based global navigation is still under technical exploration.
Visual navigation necessitates that agents drive safely in structured, changing, and even unpredictable environments; otherwise, inappropriate operations may result in mission failure and even irreversible damage to life and property.

We propose a recurrent deduction deep learning model (RDDRL) for multimodal vision-robot navigation to address these issues.
We incorporate a recurrent reasoning mechanism (RRM) into the reinforcement learning model, which allows the agent to store memory, predict the future, and aid in policy learning.
Specifically, the RRM first stores current observations and states by learning a parameterized environment model and then predicts future transitions.
The RRM then performs a self-assessment on the predicted behavior and perceives the consequences of the current policy, producing a more reliable decision-making process.
Furthermore, to obtain global-scale behavioral decision-making, information from scene recognition, semantic segmentation, and pose estimation are fused and used as partial observations of the RDDRL.
A large number of simulated experiments based on CARLA scenarios, as well as test results in real-world scenarios, show that RDDRL outperforms state-of-the-art RL methods in terms of driving stability and safety.

The results show that by training the agent, the collision rate in the global decision-making of the unmanned vehicle decreases from 0.2 % in the training state to 0.0 % in the test state.

title: Adaptive Decision-Making for Automated Vehicles Under Roundabout Scenarios Using Optimization Embedded Reinforcement Learning
abstract: The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions.
In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout.
The promotion is the modified actor of the Actor-Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly.
Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency.
When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making.
More notably, the modified actor matches human drivers' behaviors, macroscale behavior captures the human mind's jump, and medium-scale behaviors are preferentially adjusted through driving skills.
To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network.
In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time.

To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly.

The results show that the proposed method has high algorithm efficiency and better system performance.

title: An adaptive obstacle avoidance algorithm of collaborative unmanned vehicles in dynamic scenes with monocular cameras
abstract: Monocular camera is widely used in robots and unmanned vehicles because it is low cost and easy to calibrate.
However, the lacks of depth information hinders accurate estimation of the position and physical size of obstacles, which is specially important for a unmanned vehicle platform.
To solve this problem, we propose a collaborative structure to accurately acquire the position of static or dynamic obstacles based on partial observations from multiple monocular cameras.
After that, a reinforcement learning based obstacle avoidance algorithm is proposed for unmanned vehicles under unknown environments.
Specifically, we discuss the influence of obstacles' moving orientations on the performance of obstacles adaptive avoidance.
Simulation results verify the feasibility of the proposed approach.

title: Adaptive Optimal Control Via Reinforcement Learning : Theory and Its Application to Automotive Engine Systems
abstract: 

title: A Deep Reinforcement Learning Approach for Integrated Automotive Radar Sensing and Communication
abstract: We present a deep reinforcement learning approach to design an automotive radar system with integrated sensing and communication.
In the proposed system, sparse transmit arrays with quantized phase shifter are used to carry out transmit beamforming to enhance the performance of both radar sensing and communication.
Through interaction with environment, the automotive radar learns a reward that reflects the difference between mainlobe peak and the peak sidelobe level in radar sensing mode or communication user feedback in communication mode, and intelligently adjust its beamforming vector.

The Wolpertinger policy based action-critic network is introduced for beamforming vector learning, which solves the dimension curse due to huge beamforming action space.

title: Reward Machine Reinforcement Learning for Autonomous Highway Driving: An Unified Framework for Safety and Performance
abstract: Developing a safe and highly effective policy for autonomous vehicles (AVs) continues to pose a significant challenge in machine learning.
In this study, we propose a novel reinforcement learning approach with reward machine.
By tailoring the reward function to the specific needs of AVs in highway scenarios, we enable them to make more informed and efficient decisions.
Our focus is on designing a reward function to formalize traffic rules, which is crucial for achieving safe and effective AV behavior on highways.
To address this problem, we propose several innovative ideas that go beyond existing algorithmic techniques, specifically aimed at facilitating exploration and exploitation on different operations.
To our knowledge, this is the first reinforcement learning algorithm that can integrate the safe distance with autonomous highway driving, aiming at the Vienna Convention on road traffic.
Experimental results demonstrate the effectiveness of the proposed approach, which significantly improves AVs' safety and performance on highways.

title: On Achieving Acceptable Levels of Safety Risk in a Reinforcement Learning Environment
abstract: 

title: Reinforcement learning framework for the self-learning suppression of clutch judder in automotive drive trains
abstract: 

title: Reflex-Augmented Reinforcement Learning for Operating Strategies in Automotive Electrical Energy Management
abstract: This paper introduces reflex-augmented reinforcement learning (RARL) for operating strategies in automotive electrical energy management.
RARL makes it possible to overcome the limitations of rule-based decision systems (RBDS) and to face the increasing complexity in a vehicle's electrical energy system.
We suggest a deep Q-learning-based RARL approach for operating strategies determining the behavior of the electrical energy system.
This also provides a general approach to realize reinforcement learning in cybernetic management systems for safety-critical applications.
In a simulation-based study of more than 50 hours of driving with an extensive model of a vehicular electrical energy system, we show that RARL-based operating strategies fulfill the major requirements of a real vehicle.

Compared to an RBDS, RARL requires less effort to design an operating strategy of this level of performance.
Furthermore, we evaluate different variants of the biologically-inspired reflex of RARL enabling the application in safety-critical systems.
Finally, we do not only provide an approach to replace the RBDS, but we also suggest that RARL is a key to integrate further sources of information into decision-making to enhance electrical energy management.


title: Policy Space Exploration for Linear Quadratic Regulator (LQR) Using Augmented Random Search (ARS) Algorithm
abstract: Considering the recent developments in embedded systems and automotive industry, it is quite evident that in very near future many application-based electronic devices will adapt the automation in its daily based activities.

This automation will make the devices more powerful and will enhance its services.
Currently, automation is the result of the algorithms which are pre-coded into the devices, but its future is the result of algorithms which enable devices to learn from environment in which it needs to work.

It can be achieved utilizing the resources developed for a particular domain popularly known as reinforcement learning (RL).
Main objective of this paper is to enable an agent to explore a policy for achieving a control of dynamic system such that it will be capable to find an optimal solution to solve the environment.
It can be achieved using an algorithm known as augmented random search algorithm.
To improve the training speed, we will use concept of multiprocessing and environment-specific customizations along with ARS algorithm.

title: Reducing Overall Path Latency in Automotive Logical Execution Time Scheduling via Reinforcement Learning
abstract: The Logical Execution Time paradigm is a promising approach for achieving time-deterministic communication on multi-core CPUs.
Task scheduling under this paradigm is a variant of the Multi-Row Facility Layout Problem, which is known to be NP-hard.
In this paper, we propose using reinforcement learning to reduce the overall path latency among all scheduled runnables while adhering to other constraints, such as schedulability, load balance, and data contention control.

The neural networks, also known as agents, are trained on a real-world automotive powertrain project.
We compare two schedules generated by the agents to the current one and one produced by a genetic algorithm.
The agent trained with the Proximal Policy Optimization algorithm demonstrated the best performance.
Additionally, we investigate the generalization ability of the agents against software updates, and the results show that our agents are well-generalized.

title: Deep Reinforcement Learning Applied to Computation Offloading of Vehicular Applications: A Comparison
abstract: An observable trend in recent years is the increasing demand for more complex services designed to be used with portable or automotive embedded devices.
The problem is that these devices may lack the computational resources necessary to comply with service requirements.
To solve it, cloud and edge computing, and in particular, the recent multi-access edge computing (MEC) paradigm, have been proposed.
By offloading the processing of computational tasks from devices or vehicles to an external network, a larger amount of computational resources, placed in different locations, becomes accessible.
However, this in turn creates the issue of deciding where each task should be executed.
In this paper, we model the problem of computation offloading of vehicular applications to solve it using deep reinforcement learning (DRL) and evaluate the performance of different DRL algorithms and heuristics, showing the advantages of the former methods.

Moreover, the impact of two scheduling techniques in computing nodes and two reward strategies in the DRL methods are also analyzed and discussed.

title: Optimizing Trajectories in Simulations with Deep Reinforcement Learning for Industrial Robots in Automotive Manufacturing
abstract: This paper outlines the concept of optimizing trajectories for industrial robots by applying deep reinforcement learning in simulations.
An application of high technical relevance is considered in a production line of an autmotive manufacturer (AUDI AG), where industrial manipulators apply sealant on a car body to prevent water intrusion and hence corrosion.

A methodology is proposed that supports the human expert in the tedious task of programming the robot trajectories.
A deep reinforcement learning agent generates trajectories in virtual instances where the use case is simulated.
By making use of the automatically generated trajectories, the expert's task is reduced to minor changes instead of developing the trajectory from scratch.
This paper describes an appropriate way to model the agent in the context of Markov decision processes and gives an overview of the employed technologies.
The use case outlined in this paper is a proof of concept to demonstrate the applicability of reinforcement learning for industrial robotics.

title: Driver Assistance Eco-driving and Transmission Control with Deep Reinforcement Learning
abstract: With the growing need to reduce energy consumption and greenhouse gas emissions, Eco-driving strategies provide a significant opportunity for additional fuel savings on top of other technological solutions being pursued in the transportation sector.

In this paper, a model-free deep reinforcement learning (RL) control agent is proposed for active Eco-driving assistance that trades-off fuel consumption against other driver-accommodation objectives, and learns optimal traction torque and transmission shifting policies from experience.

The training scheme for the proposed RL agent uses an off-policy actor-critic architecture that iteratively does policy evaluation with a multi-step return and policy improvement with the maximum posteriori policy optimization algorithm for hybrid action spaces.

The proposed Eco-driving RL agent is implemented on a commercial vehicle in car following traffic.
It shows superior performance in minimizing fuel consumption compared to a baseline controller that has full knowledge of fuel-efficiency tables.

title: Critical Concrete Scenario Generation using Scenario-Based Falsification
abstract: Autonomous vehicles have the potential to lower the accident rate when compared to human driving.
Moreover, it has been the driving force of automated vehicles' rapid development over the last few years.
In the higher Society of Automotive Engineers (SAE) automation level, the vehicle's and passengers' safety responsibility is transferred from the driver to the automated system, so thoroughly validating such a system is essential.

Recently, academia and industry have embraced scenario-based evaluation as the complementary approach to road testing, reducing the overall testing effort required.
It is essential to determine the system's flaws before deploying it on public roads as there is no safety driver to guarantee the reliability of such a system.
This paper proposes a Reinforcement Learning (RL) based scenario-based falsification method to search for a high-risk scenario in a pedestrian crossing traffic situation.
We define a scenario as risky when a system under test (SUT) does not satisfy the requirement.
The reward function for our RL approach is based on Intel's Responsibility Sensitive Safety(RSS), Euclidean distance, and distance to a potential collision.
Code and videos are available online at https://github.com/dkarunakaran/scenario_based_falsification.

title: Minimizing energy consumption from connected signalized intersections by reinforcement learning
abstract: Explicit energy minimization objectives are often discouraged in signal optimization algorithms due to its negative impact on mobility performance.
One potential direction to solve this problem is to provide a balanced objective function to achieve desired mobility with minimized energy consumption.
This research developed a reinforcement learning (RL) based control with reward functions considering energy and mobility in a joint manner-a penalty function is introduced for number of stops.
Further, we proposed a clustering-based technique to make the state-space finite which is critical for a tractable implementation of the RL algorithm.
We implemented the algorithm in a calibrated NG-SIM network within a traffic micro-simulator-PTV VISSIM.
With sole focus on energy, we report 47% reduction in energy consumption when compared with existing signal control schemes, however causing a 65.6% increase in system travel time.
In contrast, the control strategy focusing on energy minimization with penalty for stops yields 6.7% reduction in energy consumption with 27% increase in system travel time.
The developed RL algorithm with a flexible penalty function in the reward will achieve desired energy goals for a network of signalized intersections without compromising on the mobility performance.
Disclaimer: This manuscript has been authored by UT-Battelle, LLC under Contract No.

DE-AC05-00OR22725 with the U.S. Department of Energy.
The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.

The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan(http://energy.gov/downloads/doe-public-access-plan

title: Unit Layout Design Supporting System of Cell Assembly Machine Using Two Robots by Reinforcement Learning
abstract: In this study, we explain the development of Design Supporting System for Cell Assembly Machine System (CAMS) which systemizes the decision of the unit layout that composes the assembly machine using two robots.

CAMS uses Profit Sharing which is one of the Reinforcement Learning methods, determining each units layout.
We apply CAMS to the assembly of the differential gear box in automotive parts to verify its validity.

title: AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities [arXiv]
abstract: The requirement of wireless data demands is increasingly high as the sixth-generation (6G) technology evolves.
Reconfigurable intelligent surface (RIS) is promisingly deemed to be one of 6G techniques for extending service coverage, reducing power consumption, and enhancing spectral efficiency.
In this article, we have provided some fundamentals of RIS deployment in theory and hardware perspectives as well as utilization of artificial intelligence (AI) and machine learning.
We conducted an intelligent deployment of RIS (i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs associated with an mmWave base station (BS) and a receiver.
The RISs are deployed on the AGV with configured incident/reflection angles.
While, both the mmWave BS and receiver are associated with an edge server monitoring downlink packets for obtaining system throughput.
We have designed a federated multi-agent reinforcement learning scheme associated with several AGV-RIS agents and sub-agents per AGV-RIS consisting of the deployment of position, height, orientation and elevation angles.

The experimental results presented the stationary measurement in different aspects and scenarios.
The i-Dris can reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with comparably low complexity as well as rapid deployment, which outperforms the other existing works.
At last, we highlight some opportunities and future issues in leveraging RIS-empowered wireless communication networks.

title: 2010 49th IEEE Conference on Decision and Control (CDC 2010)
abstract: The following topics are dealt with: tall transfer functions; singular spectra and econometric modelling; distributed parameter systems; adaptive control; sliding mode control; decentralized control; delays systems; control theory; electric smart grids; robotics; identification; aerospace; sensor networks; switched systems; filtering and estimation; optimisation; autonomous systems; stochastic control; robust control; network analysis and control; hybrid systems; stability; fault diagnosis; computational methods; biological and biomedical systems; model-based systems engineering; Markov processes; dynamic resource allocation and optimization in networks; energy efficient infrastructures; game theory; air traffic control systems theory; Kalman filtering; agents and autonomous systems; visual servo control; variational methods; energy systems; output feedback and observers; communication networks; plug and play control; image analysis; biology; networked control systems; randomization in systems and control; stochastic systems; decentralised network analysis; nonlinear systems; Petri nets; queueing systems; subspace methods; system identification and estimation; integrated vehicle dynamics and control; sparsity and compressive sensing in system identification; fluid flow systems; quantum information and control; adaptive dynamic programming; reinforcement learning; feedback control; cooperative control; mean field stochastic systems and control; H-infinity control; networked nonlinear dynamical systems; event-based control; Lyapunov methods; process control; LMI; medicine; optimal control; stochastic hybrid systems; switched systems; positivity constraints; electrical and power systems; networked control systems; automotive and aerospace systems; linear systems; constrained control; predictive control; model-controller reduction; geometric control on nonlinear manifolds; robot-assisted exploration; scalar potential fields; automotive control; mobile sensor networks; cooperative control; large-scale systems; formal methods in systems and control; secure control systems; modelling and control of bio mechanical systems; stochastic model predictive control; nonlinear predictive control; stochastic systems; algebraic methods; geometric methods; autonomous robots; nonlinear system identification; control of communication systems; robust stability; large-scale interconnections; information dynamics in social and economic networks; discrete event systems; analysis and design of alarm systems; systems with uncertainty; iterative learning control; distributed control; switched systems stability; joint spectral radius; dynamic networked multi-agent systems; sampled-data control; randomized algorithms; and behavioral systems.


title: STAP in Automotive MIMO Radar with Transmitter Scheduling
abstract: Automotive radars often employ multiple-input multiple-output (MIMO) array to attain high angular resolution with few antenna elements.
The diversity gain is generally achieved by time-division multiplexing (TDM) during the transmission of frequency-modulated continuous-wave (FMCW) signals.
However, TDM mode leads to longer pulse repetition intervals and, therefore, inherently and severely limits the maximum unambiguous Doppler velocity that a radar is able to detect.
In this paper, we address the Doppler ambiguity problem in TDM MIMO automotive radars through a space-time adaptive processing (STAP) approach.
A direct application of STAP may lead to a high antenna sidelobe level that hampers the detection performance.
We mitigate this through optimal transmitter scheduling.
We formulate the problem as combinatorial optimization and solve it via reinforcement learning.
Numerical and experimental results demonstrate the efficacy of our method when compared with conventional techniques.

title: Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach
abstract: For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing.
Yet it still struggles to find widespread implementation in industrial environments.
Traditional programming has so far proven to be insufficient in providing the required flexibility and dexterity to solve complex assembly tasks.
Research in robotic control using deep reinforcement learning (DRL) advances quickly, however, the transfer to real-world applications in industrial settings is lagging behind.
In this study, we apply DRL for robotic motion control to a multi-body contact automotive assembly task.
Our focus lies on optimizing the final performance on the real-world setup.
We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability of the controller's performance.
We train the agent exclusively in simulation and successfully perform the Sim-to-Real transfer.
Finally, we evaluate the controller's performance and robustness on an industrial setup and reflect its adherence to the high standards of automotive production.

title: Reducing Overall Path Latency in Automotive Logical Execution Time Scheduling via Reinforcement Learning
abstract: The Logical Execution Time paradigm is a promising approach for achieving time-deterministic communication on multi-core CPUs.
Task scheduling under this paradigm is a variant of the Multi-Row Facility Layout Problem, which is known to be NP-hard.
In this paper, we propose using reinforcement learning to reduce the overall path latency among all scheduled runnables while adhering to other constraints, such as schedulability, load balance, and data contention control.

The neural networks, also known as agents, are trained on a real-world automotive powertrain project.
We compare two schedules generated by the agents to the current one and one produced by a genetic algorithm.
The agent trained with the Proximal Policy Optimization algorithm demonstrated the best performance.
Additionally, we investigate the generalization ability of the agents against software updates, and the results show that our agents are well-generalized.

title: Online Learning-Based Waveform Selection for Improved Vehicle Recognition in Automotive Radar
abstract: This paper describes important considerations and challenges associated with online reinforcement-learning based waveform selection for target identification in frequency modulated continuous wave (FMCW) automotive radar systems.

We present a novel learning approach based on satisficing Thompson sampling, which quickly identifies a waveform expected to yield satisfactory classification performance.
We demonstrate through measurement-level simulations that effective waveform selection strategies can be quickly learned, even in cases where the radar must select from a large catalog of candidate waveforms.

The radar learns to adaptively select a bandwidth for appropriate resolution and a slow-time unimodular code for interference mitigation in the scene of interest by optimizing an expected classification metric.


title: Risk Averse Robust Adversarial Reinforcement Learning
abstract: Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks.
A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents.
A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations.
Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary.
A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk.
Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective.
In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary.
We test our approach on a self-driving vehicle controller.
We use an ensemble of policy networks to model risk as the variance of value functions.
We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary.
Supplementary materials are available at https://sites.google.com/view/rararl.

title: Deep Reinforcement Learning for Robotic Control in High-Dexterity Assembly Tasks - A Reward Curriculum Approach
abstract: For years, the fully-automated robotic assembly has been a highly sought-after technology in large-scale manufacturing.
Yet it still struggles to find widespread implementation in industrial environments.
Traditional programming has so far proven to be insufficient to provide the required flexibility and dexterity to solve complex assembly tasks.
Although research in robotic control using deep reinforcement learning (DRL) advances quickly, the transfer to real-world applications in industrial settings is lagging behind.
In this study, we apply DRL for robotic motion control at the use-case of a multi-body contact automotive assembly task and focus on optimizing the final performance on the real-world setup.
We propose a reward-curriculum learning approach in combination with domain randomization to obtain both force-sensitivity and generalizability.
We train an agent exclusively in simulation and successfully perform the Sim-to-Real transfer.
Finally, we evaluate the controller's performance and robustness on an industrial setup and reflect its adherence to the high automotive production standards.

title: Multiagent Manuvering with the Use of Reinforcement Learning
abstract: This paper presents an approach for defining, solving, and implementing dynamic cooperative maneuver problems in autonomous driving applications.
The formulation of these problems considers a set of cooperating cars as part of a multiagent system.
A reinforcement learning technique is applied to find a suboptimal policy.
The key role in the presented approach is a multiagent maneuvering environment that allows for the simulation of car-like agents within an obstacle-constrained space.
Each of the agents is tasked with reaching an individual goal, defined as a specific location in space.
The policy is determined during the reinforcement learning process to reach a predetermined goal position for each of the simulated cars.
In the experiments, three road scenarios-zipper, bottleneck, and crossroads-were used.
The trained policy has been successful in solving the cooperation problem in all scenarios and the positive effects of applying shared rewards between agents have been presented and studied.
The results obtained in this work provide a window of opportunity for various automotive applications.

title: Multilink and AUV-Assisted Energy-Efficient Underwater Emergency Communications
abstract: Recent development in wireless communications has provided many reliable solutions to emergency response issues, especially in scenarios with dysfunctional or congested base stations.
Prior studies on underwater emergency communications, however, remain understudied, which poses a need for combining the merits of different underwater communication links (UCLs) and the manipulability of unmanned vehicles.

To realize energy-efficient underwater emergency communications, we develop a novel underwater emergency communication network (UECN) assisted by multiple links, including underwater light, acoustic, and radio-frequency links, and autonomous underwater vehicles (AUVs) for collecting and transmitting underwater emergency data.

First, we determine the optimal emergency response mode for an underwater sensor node (USN) using greedy search and reinforcement learning (RL), so that isolated USNs (I-USNs) can be identified.
Second, according to the distribution of I-USNs, we dispatch AUVs to assist I-USNs in data transmission, i.
e.
, jointly optimizing the locations and controls of AUVs to minimize the time for data collection and underwater movement.

Finally, an adaptive clustering-based multiobjective evolutionary algorithm is proposed to jointly optimize the number of AUVs and the transmit power of I-USNs, subject to a given set of constraints on transmit power, signal-to-interference-plus-noise ratios (SINRs), outage probabilities, and energy, achieving the best tradeoff between the maximum emergency response time (ERT) and the total energy consumption (EC).

Simulation results indicate that our proposed approach outperforms benchmark schemes in terms of energy efficiency (EE), contributing to underwater emergency communications.

title: Towards Predictive Lifetime-Oriented Temperature Control of Power Electronics in E-vehicles via Reinforcement Learning
abstract: As the electric vehicle (EV) industry rapidly grows, the reliability of EVs is an ongoing challenge to the automotive industry.
Among them, due to the introduction of electric motors, the aging and degradation of power electronics in EVs have a direct influence on the overall system safety and may lead to total failure.
Therefore, extending the lifetime of power electronics has been a focus in the last decades, where reducing the temperature swings is the key to achieving the goal.
However, temperature optimization usually requires future information on power loads, which is not available in classic approaches.
Therefore, in this paper, we propose a baseline framework for lifetime-oriented temperature control with reinforcement learning (RL).
Focusing on long-term prediction, the framework integrates various physical modelings of EV modules (sensors, actuators, vehicle dynamics and temperature management) and utilizes real-time route information to train an agent for driving behavior prediction.

Based on further interaction with the EV model, future temperature development can be estimated in advance, thus enabling better swing optimization.
Experiments demonstrate the effectiveness of our approach and the lifetime of power electronics can be extended by up to 63% on a representative test route.
Compared to the classic approach, our predictive temperature control shows impressive energy efficiency by achieving up to 2.8x power loss reduction with better lifetime optimization.

title: Real-Time Optimal Energy Management of Electrified Powertrains with Reinforcement Learning
abstract: Reinforcement learning (RL)algorithm is employed in solving energy management problem for electrified powertrain in real-world driving scenarios and the application process is streamlined.
A near-global optimal control policy is articulated for the energy management system (EMS) using Q-learning algorithm which is real-time implementable.
The core of the EMS is an updating optimal control policy in the form of a changing look-up table comprising near-global optimal action value function (Q-values) corresponding to all feasible state-action combinations.

Using the updating control policy, the EMS can optimally decide power-split between electric machines (EMs) and internal combustion engine (ICE) in real-world driving situations.

title: Deep reinforcement learning for gearshift controllers in automatic transmissions
abstract: Control design for gearshifts in modern automotive automatic transmissions constitutes a challenging, time consuming task performed by highly trained experts.
This is due to the fact that a variety of non-linear and partially observable systems need to be actuated, such that a comfortable shifting behavior is achieved within an sufficiently low shifting time.

The presented approach leverages deep reinforcement learning (DRL) to control gear shifts, outperforming current state of the art controller performance.
This requires formulating the shifting task as a Markov decision process by designing suitable action and observation spaces as well as a meaningful reward function.
Due to the sample complexity of DRL methods, the control agents are trained in simulation and are subsequently transferred to a real transmission on a test bench.
To successfully transfer DRL agents from simulation to reality, methods such as domain randomization and domain adaption leveraging evolutionary optimization are applied.
To the best of the authors' knowledge, this work is the first to successfully apply DRL for the closed loop control of a real world automotive automatic transmission of realistic complexity.

title: Applicability Study of Model-Free Reinforcement Learning Towards an Automated Design Space Exploration Framework
abstract: Design space exploration is a crucial aspect of engineering and optimization, focused on identifying optimal design configurations for complex systems with a high degree of freedom in the actor set.
It involves systematic exploration while considering various constraints and requirements.
One of the key challenges in design space exploration is the need for a control strategy tailored to the particular design.
In this context, reinforcement learning has emerged as a promising solution approach for automatically inferring control strategies, thereby enabling efficient comparison of different designs.
However, learning the optimal policy is computationally intensive, as the agent determines the optimal policy through trial and error.
The focus of this study is on learning a single strategy for a given design and scenario, enabling the evaluation of numerous architectures within a limited time frame.
The study also highlights the importance of plant modeling considering different modeling approaches to effectively capture the system complexity on the example of vehicle dynamics.
In addition, a careful selection of an appropriate hyperparameter set for the reinforcement learning algorithm is emphasized to improve the overall performance and optimization process.

title: A Novel Reinforcement Learning-Optimization Approach for Integrating Wind Energy to Power System with Vehicle-To-Grid Technology
abstract: 

title: Numerical and thermo-energy analysis of cycling in automotive air-conditioning operating with hybrid nanolubricants and R1234yf
abstract: Studies on automotive air-conditioning (AAC) systems involving compressor on-off cycling are still limited.
This study focuses on improving the cycling of the AAC system using hybrid nanolubricants and hydrofluoroolefin-1234yf refrigerant.
A dynamic model for an AAC system with a thermostatic switch that controls the on-off compressor was developed.
The model was built in MATLAB Simulink and based on the state-space model using the fundamental conservation principles at the condenser, evaporator, and expansion valve.
The experimental data were used to calculate the AAC system pressure, compressor, heat transfer coefficient of the condenser-evaporator, and expansion valve setting.
The validation of the experimental data and the predicted data by the simulation suggested that the dynamic model could predict the AAC system's performance within +/- 5% deviation.
The AAC system operating with Al2O3-SiO2/PAG nanolubricants has a lower temperature cycling frequency than the AAC system with the original PAG lubricant, representing less energy consumed.
In addition, the AAC system with hybrid nanolubricants was performed with lower power consumption and significantly higher cooling capacity than the original system.
The present simulation confirmed the feasibility of hybrid nanolubricants for application in an AAC system with a thermostatic switch.

title: A FOREGROUND OBJECT BASED QUANTITATIVE ASSESSMENT OF DENSE STEREO APPROACHES FOR USE IN AUTOMOTIVE ENVIRONMENTS
abstract: There has been significant recent interest in stereo correspondence algorithms for use in the urban automotive environment [1, 2, 3].
In this paper we evaluate a range of dense stereo algorithms, using a unique evaluation criterion which provides quantitative analysis of accuracy against range, based on ground truth 3D annotated object information.

The results show that while some algorithms provide greater scene coverage, we see little differentiation in accuracy over short ranges, while the converse is shown over longer ranges.
Within our long range accuracy analysis we see a distinct separation of relative algorithm performance.
This study extends prior work on dense stereo evaluation of Block Matching (BM)[4], Semi-Global Block Matching (SGBM)[5], No Maximal Disparity (NoMD)[6], Cross[7], Adaptive Dynamic Programming (AdptDP)[8], Efficient Large Scale (ELAS)[9], Minimum Spanning Forest (MSF)[10] and Non-Local Aggregation (NLA)[11] using a novel quantitative metric relative to object range.


title: Autonomous Overtaking Decision Making System Based on Hierarchical Reinforcement Learning and Social Preferences
abstract: To describe the interaction between the host vehicle (HV) and the overtaken vehicle (OV) in overtaking scenarios, the psychological term 'social preference' was introduced to describe the longitudinal behavioral pattern of OV, and a data-driven classification method was adopted to extract the social preference and incorporate it into the design of reinforcement learning based autonomous overtaking decision-making system (RL-based AODMS).

By analyzing social preferences of overtaken vehicles based on the realistic overtaking data, this method was able to generate proper overtaking decisions in response to different preferences.
First, the state transition probability of an overtaken vehicle during the overtaking interaction was calculated from a large number of realistic overtaking data and divided into three types: altruistic, egoistic, and prosocial.

Then, a semi-model-based advanced Q-learning algorithm was proposed to integrate three preferences into decision model training.
Meanwhile, an online classifier of social preference was built to determine the real-time preference of the overtaken vehicle.
Combined with our previous study on lane-changing controllers, a hierarchical reinforcement learning based autonomous overtaking system (HRL-based AOS) was constructed.
Finally, the joint validation on autonomous overtaking was done by collected realistic data and simulation.
The results showed that the AODMS considering social preferences can predict the social preference of OV in real time and make reasonable decisions in complicated overtaking scenarios.
Meanwhile, compared to the traditional AOS without considering social preference, the complete AOS constructed in this study showed better comfort and stability.
To conclude, this study innovatively operationalizes data-driven social preference in overtaking decision making and improving the adaptability and rationality of decisions, which will contribute to the development of safe and reliable AOS.


title: DATA of Fuzzy_Q-learning_Interaction_Controller_Design_for_Collaborative_Robot
abstract: DATA of Fuzzy_Q-learning_Interaction_Controller_Design_for_Collaborative_Robot Copyright: CC BY 4.0

title: Actor-Critic Traction Control Based on Reinforcement Learning with Open-Loop Training
abstract: The use of actor-critic algorithms can improve the controllers currently implemented in automotive applications.
This method combines reinforcement learning (RL) and neural networks to achieve the possibility of controlling nonlinear systems with real-time capabilities.
Actor-critic algorithms were already applied with success in different controllers including autonomous driving, antilock braking system (ABS), and electronic stability control (ESC).
However, in the current researches, virtual environments are implemented for the training process instead of using real plants to obtain the datasets.
This limitation is given by trial and error methods implemented for the training process, which generates considerable risks in case the controller directly acts on the real plant.
In this way, the present research proposes and evaluates an open-loop training process, which permits the data acquisition without the control interaction and an open-loop training of the neural networks.

The performance of the trained controllers is evaluated by a design of experiments (DOE) to understand how it is affected by the generated dataset.
The results present a successful application of open-loop training architecture.
The controller can maintain the slip ratio under adequate levels during maneuvers on different floors, including grounds that are not applied during the training process.
The actor neural network is also able to identify the different floors and change the acceleration profile according to the characteristics of each ground.

title: Identify and prioritise the critical factors in implementing the reverse logistics practices: a case of Indian auto component manufacturer
abstract: In recent years, reverse logistics (RL) practices have been perceived a great recognition among researchers/practitioners.
In this paper, we intend to identify and prioritise the critical factors (CFs) in implementing the RL practices, from the industrial viewpoint.
There are 13 CFs crucial in accomplishing the RL practices were recognised on the basis of critical review of literature and experts opinion.
These finalised 13 CFs were then analysed to determine their priority by means of analytical hierarchy process (AHP) technique.
The AHP technique assists in determining the relative importance of the identified RL implementation critical factors.
The findings of the work may help managers to address the related issues in RL implementation.
Inputs needed to carry out this research work are taken from an Indian automotive components manufacturing company.
The results of the study may help researchers/practitioners to prioritise their efforts to implement RL practices in effective manner.
In the end, sensitivity analysis is carried out to examine the proposed RL implementation CFs stability.

title: A deep reinforcement learning (DRL) decision model for heating process parameters identification in automotive glass manufacturing
abstract: This research investigates the applicability of Deep Reinforcement Learning (DRL) to control the heating process parameters of tempered glass in industrial electric furnace.
In most cases, these heating process parameters, also called recipe, are given by a trial and error procedure according to the expert process experience.
In order to optimize the time and the cost associated to this recipe choice, we developed an offline decision system which consists of a deep reinforcement learning framework, using Deep Q-Network (DQN) algorithm, and a self-prediction artificial neural network model.

This decision system is used to define the main heating parameters (the glass transfer speed and the zone temperature) based on the desired outlet temperature of the glass, and it has the capacity to improve its performance without further human assistance.

The results show that our DQN algorithm converges to the optimal policy, and our decision system provides good recipe for the heating process with deviation not exceeding process limits.
To our knowledge, it is the first demonstrated usage of deep reinforcement learning for heating process of tempered glass specifically and tempering process in general.
This work also provides the basis for dealing with the problem of energy consumption during the tempering process in electric furnace.

title: High-Level Sensor Models for the Reinforcement Learning Driving Policy Training
abstract: Performance limitations of automotive sensors and the resulting perception errors are one of the most critical limitations in the design of Advanced Driver Assistance Systems and Autonomous Driving Systems.

Ability to efficiently recreate realistic error patterns in a traffic simulation setup not only helps to ensure that such systems operate correctly in presence of perception errors, but also fulfills a key role in the training of Machine-Learning-based algorithms often utilized in them.

This paper proposes a set of efficient sensor models for detecting road users and static road features.
Applicability of the models is presented on an example of Reinforcement-Learning-based driving policy training.
Experimental results demonstrate a significant increase in the policy's robustness to perception errors, alleviating issues caused by the differences between the virtual traffic environment used in the policy's training and the realistic conditions.


title: A Decision-making Method for Self-driving Based on Deep Reinforcement Learning
abstract: L5-level autonomous driving is the development trend of the future in the automotive industry, and the realization of autonomous driving through deep reinforcement learning algorithms are one of the research directions.

Soft Actor-Critic the algorithm adds the maximum entropy term to the original deep reinforcement learning the objective function, and it shows great advantages in continuous control problems.
Here, based on the open-source platform TORCS, this algorithm will be used to conduct automatic driving simulation experiments, design a reasonable reward function, add relevant constraints, use vehicle radar sensor information to make automatic driving decisions, and compare experiments with the Deep Deterministic Policy Gradient Algorithm.

SAC can effectively extend training time, improve stability, and improve generalization ability.

title: Cross-Type Transfer for Deep Reinforcement Learning Based Hybrid Electric Vehicle Energy Management
abstract: Developing energy management strategies (EMSs) for different types of hybrid electric vehicles (HEVs) is a time-consuming and laborious task for automotive engineers.
Experienced engineers can reduce the developing cycle by exploiting the commonalities between different types of HEV EMSs.
Aiming at improving the efficiency of HEV EMSs development automatically, this paper proposes a transfer learning based method to achieve the cross-type knowledge transfer between deep reinforcement learning (DRL) based EMSs.

Specifically, knowledge transfer among four significantly different types of HEVs is studied.
We first use massive driving cycles to train a DRL-based EMS for Prius.
Then the parameters of its deep neural networks, wherein the common knowledge of energy management is captured, are transferred into EMSs of a power-split bus, a series vehicle and a series-parallel bus.

Finally, the parameters of 3 different HEV EMSs are fine-tuned in a small dataset.
Simulation results indicate that, by incorporating transfer learning (TL) into DRL-based EMS for HEVs, an average 70% gap from the baseline in respect of convergence efficiency has been achieved.
Our study also shows that TL can transfer knowledge between two HEVs that have significantly different structures.
Overall, TL is conducive to boost the development process for HEV EMS.

title: New Features in Feko/WinProp 2019
abstract: This paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp).
These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.


title: Hybrid Electric Vehicle Energy Management With Computer Vision and Deep Reinforcement Learning
abstract: Modern automotive systems have been equipped with a highly increasing number of onboard computer vision hardware and software, which are considered to be beneficial for achieving eco-driving.
This article combines computer vision and deep reinforcement learning (DRL) to improve the fuel economy of hybrid electric vehicles.
The proposed method is capable of autonomously learning the optimal control policy from visual inputs.
The state-of-the-art convolutional neural networks-based object detection method is utilized to extract available visual information from onboard cameras.
The detected visual information is used as a state input for a continuous DRL model to output energy management strategies.
To evaluate the proposed method, we construct 100 km real city and highway driving cycles, in which visual information is incorporated.
The results show that the DRL-based system with visual information consumes 4.
3-8.
8% less fuel compared with the one without visual information, and the proposed method achieves 96.
5% fuel economy of the global optimum-dynamic programming.


title: New Features in Feko and WinProp 2019
abstract: paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp).
These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.


title: A multi-objective reinforcement learning approach for resequencing scheduling problems in automotive manufacturing systems
abstract: This study investigated a multi-objective resequencing scheduling problem in the automotive manufacturing systems due to operational requirements on the color-batching of the paint shop and sequential requirements on the sequence adherence of an assembly shop.

Resequencing cars as color-oriented batches reduced the costs of color changes and operational costs for paint shops.
Also, assembly shops required paint shops to complete cars with fewer delays so that high sequence adherence with its demand was assured.
Based on real-world applications, we investigated two contradictory objectives-color change costs and sequence tardiness-in a single-machine flowshop scheduling environment.
A multi-objective-deep-Q-network algorithm was developed to determine the Pareto frontier.
Reward shaping was designed to improve the convergence of the neural network.
The 2D-folded-normal distribution was designed to sample the preference, which made the exploration and exploitation of the neural network more comprehensive and improved the training efficiency.
Two experiments were conducted and showed that the proposed approach outperformed the meta-heuristic algorithm and the envelope Q-learning algorithm in solving time, performance, the convergence of the neural network, and the diversity of the Pareto frontier.

Therefore, the proposed approach can be used in automotive paint shops to improve scheduling efficiency and reduce operational costs.

title: Reinforcement learning for energy-efficient control of parallel and identical machines
abstract: Nowadays, the growing interest in industry for enhancing the sustainability of manufacturing processes is becoming a major trend.
Energy consumption can be lowered by controlling machine states with energy -efficient control policies that switch off/on the device.
Recent studies have shown that Reinforcement Learning algorithms can effectively control manufacturing systems without the requirement of prior knowledge about system parameters.
This is a significant factor since full information on system dynamics is difficult to obtain in real-world applications.
This work proposes a new Reinforcement Learning-based algorithm to apply energy-efficient control strategies to a single workstation consisting of identical parallel machines.
The model goal is to achieve the optimum trade-off between system productivity and energy demand without relying on full knowledge of the system dynamics.
Numerical experiments confirm ef-fectiveness, applicability, and generality of the proposed approach, even when applied to a real-world in-dustrial system from the automotive sector.& COPY; 2023 CIRP.

title: Reinforcement Learning in Eco-Driving for Connected and Automated Vehicles
abstract: 

title: A Method for Road Accident Prevention in Smart Cities based on Deep Reinforcement Learning
abstract: Autonomous vehicles play a key role in the smart cities vision: they bring benefits and innovation, but also safety threats, especially if they suffer from vulnerabilities that can be easily exploited.

In this paper, we propose a method that exploits Deep Reinforcement Learning to train autonomous vehicles with the purpose of preventing road accidents.
The experimental results demonstrated that a single self-driving vehicle can help to optimise traffic flows and mitigate the number of collisions that would occur if there were no self-driving vehicles in the road network.

Our results proved that the training progress is able to reduce the collision frequency from 1 collision every 32.
40 hours to 1 collision every 53.
55 hours, demonstrating the effectiveness of deep reinforcement learning in road accident prevention in smart cities.


title: Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning
abstract: Discovering the underlying structure of HMI software efficiently and sufficiently for the purpose of testing without any prior knowledge on the software logic remains a difficult problem.
The key challenge lies in the complexity of the HMI software and the high variance in the coverage of current methods.
In this paper, we introduce the PathFinder, an effective and automatic HMI software exploration framework.
PathFinder adopts a curiosity-based reinforcement learning framework to choose actions that lead to the discovery of more unknown states.
Additionally, PathFinder progressively builds a navigation model during the exploration to further improve state coverage.
We have conducted experiments on both simulations and real-world HMI software testing environment, which comprise a full tool chain of automobile dashboard instrument cluster.
The exploration coverage outperforms manual and fuzzing methods which are the current industrial standards.

title: Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks
abstract: In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications.
For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs).
However, the in- vehicle network environment can be starkly different from the Internet where TCP has been optimized.
The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments.
In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.

title: Edge intelligence based digital twins for internet of autonomous unmanned vehicles
abstract: It aims to explore the efficient and reliable wireless transmission and cooperative communication mechanism of Internet of Vehicles (IoV) based on edge intelligence technology.
It first proposes an intelligent network architecture for IoV services by combining network slicing and deep learning (DL) technology, and then began to study the key technologies needed to achieve the architecture.

It designs the cooperative control mechanism of unmanned vehicle network based on the full study of wireless resource allocation algorithm from the micro level.
Second, in order to improve the safety of vehicle driving, deep reinforcement learning is used to configure the wireless resources of IoV network to meet the needs of various IoV services.
The research results show that the accuracy rate of the improved AlexNet algorithm model can reach 99.
64%, the accuracy rate is more than 80%, the data transmission delay is less than 0.
02 ms, and the data transmission packet loss rate is less than 0.
05.

The algorithm model has practical application value for solving the data transmission related problems of vehicular internet communication, providing an important reference value for the intelligent development of unmanned vehicle internet.


title: Adaptive adaptive obstacle avoidance algorithm of collaborative unmanned vehicles in dynamic scenes with monocular cameras
abstract: The monocular camera is widely used in robots and unmanned vehicles system because it is low cost and easy to calibrate and identify.
However, the depth lack of the monocular camera hinders positioning and determining the real size of obstacles in the unmanned vehicle system.
To solve the problem, we propose a collaborative structure to accurately acquire the position of static or dynamic obstacles based on the partially observing information from multiple monocular cameras.

After that, a reinforcement learning based obstacle avoidance algorithm is proposed for unmanned vehicles under an unknown environment.
Specifically, we discuss the influence of obstacles' moving orientations on the performance of obstacles adaptive avoidance.
Simulation results verify the feasibility of the proposed algorithm.

title: Reinforcement Learning based Optimization of Bayesian Networks for Generating Feasible Vehicle Configuration Suggestions
abstract: A promising method in the automotive industry to anticipate future customer demands is the concept of planned orders.
Due to multi-variant products, changing customer demands, and dynamic environments the process of generating planned orders is challenging.
This paper introduces an approach using graphical models to generate planned order suggestions in a multi-variant order management process.
Bayesian networks are modelled by learning the structure from different data sources, which enable the possibility to directly sample configuration suggestions.
To find an optimized graph structure, a method using hierarchical correlation clustering and reinforcement learning is applied, taking into account technical and sales-operated feasibility constraints.

The method has high potential in practical usage and is evaluated by a realworld use case of the Dr. Ing.
h.c. F. Porsche AG.

title: Machine Learning Based Methods for Virtual Validation of Autonomous Driving
abstract: 

title: GenRL at the SBST 2022 Tool Competition
abstract: GenRL is a Deep Reinforcement Learning-based tool designed to generate test cases for Lane-Keeping Assist Systems.
In this paper, we briefly presents GenRL, and summarize the results of its participation in the Cyber-Physical Systems (CPS) tool competition at SBST 2022.

title: Risk Averse Robust Adversarial Reinforcement Learning [arXiv]
abstract: Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks.
A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents.
A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations.
Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary.
A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk.
Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective.
In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary.
We test our approach on a self-driving vehicle controller.
We use an ensemble of policy networks to model risk as the variance of value functions.
We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary.

title: Performance Metrics for Human-Robot Collaboration: An Automotive Manufacturing Case
abstract: A human-robot collaborative system in the form of a power and skill assist robotic system was developed where a human and a robot could collaborate to perform object manipulation for targeted assembly tasks in automotive manufacturing.

We assumed such assembly tasks as the representative assembly tasks in automotive manufacturing.
We reflected human's weight perception in the dynamics and control of the power and skill assist system following a psychophysical method using a reinforcement learning scheme.
We recruited 20 human subjects who separately performed assembly tasks with the system in human-robot collaboration (HRC).
We then observed the collaborative assembly tasks, conducted extensive literature reviews, reviewed our previous and ongoing related works and brainstormed with the subjects and other relevant researchers, and then proposed HRC performance assessment metrics and methods for collaborative automotive manufacturing.

The proposed metrics comprised of assessment criteria and methods related to both human-robot interaction (HRI) and manufacturing performance.
We then verified the proposed performance metrics in pilot studies in the laboratory environment using the same collaborative system and subjects.
The verification results proved the effectiveness of the assessment metrics and methods in terms of usability, practicability and reliability.
We then proposed to apply classification and regression type machine learning approaches under supervised and reinforcement learning setups to learn different classes and decision-making rules respectively regarding HRC performance.

The proposed performance metrics and methods can serve as the preliminary efforts towards developing comprehensive assessment metrics for HRC in general and for human-robot collaborative automotive manufacturing in particular.


title: An integrated framework for evaluating the barriers to successful implementation of reverse logistics in the automotive industry
abstract: Reverse logistics (RL) strategy can have a positive impact on productivity, and the diminishing resources, along with the strict environmental regulations, have strengthened the need for this strategy.

The purpose of this study is to develop an integrated framework for identifying: (1) the critical barriers to the successful implementation of RL in the automotive industry; (2) the importance and implementation priorities of these barriers; and (3) the causal relations among them.

The proposed framework is composed of the Delphi method to identify the most relevant barriers, the best-worst method (BWM) to determine their importance, and the weighted influence non-linear gauge system (WINGS) to analyze their causal relationships.

The proposed framework is applied to a case study in the automotive industry.
The results indicate the economic barriers are the most important, and the knowledge barriers are the least important barriers to the successful implementation of RL in the automotive industry.
(c) 2020 Elsevier Ltd. All rights reserved.

title: A Multi-agent Deep Reinforcement Learning-Based Collaborative Willingness Network for Automobile Maintenance Service
abstract: With the growth of maintenance market scale of automobile manufacturing enterprises, simple information technology is not enough to solve the problem of uneven resource allocation and low customer satisfaction in maintenance chain services.

To solve this problem, this paper abstracts the automotive maintenance collaborative service into a multi-agent collaborative model based on the decentralized partially observable Markov decision progress (Dec-POMDP).

Based on this model, a multi-agent deep reinforcement learning algorithm based on collaborative willingness network (CWN-MADRL) is presented.
The algorithm uses a value decomposition based MADRL framework, adds a collaborative willingness network based on the original action value network of the agent, and uses the attention mechanism to improve the impact of the collaboration between agents on the action decision-making, while saving computing resources.

The evaluation results show that, our CWN-MADRL algorithm can converge quickly, learn effective task recommendation strategies, and achieve better system performance compared with other benchmark algorithms.


title: A Comprehensive Review of Reverse Logistics in the Automotive Industry
abstract: Reverse logistics (RL) of automobiles has received wide attention in recent years with the innovation of resource utilization and the increase of environmental awareness.
Many research papers have been published in the RL discipline focusing on the automotive industry.
However, no review article is available on product-specific issues.
To bridge this gap, 91 papers published in the Web of Science (WOS) database between January 2013 and March 2023 were selected and analyzed using content analysis to classify the articles into survey, evaluation, decision making, framework, modeling and review.

The main findings of this paper are as follows: (1) Research on RL activities has mainly focused on recycling and remanufacturing, and insufficient research has been conducted on activities such as dismantling and waste management.

(2) In terms of research objects, End-of-Life Vehicles (ELVs) and automotive batteries have received a lot of attention, while less research has been done on automotive tires, engines, waste oil, etc.
, which need further attention.

(3) Integrated economic, environmental and social considerations are research opportunities for future evaluation and decision making.
(4) The establishment of multi-objective problems and the innovation of solution methods may be the future research direction.
(5) Green and sustainability themes are the main trends in the development of RL in the automotive industry in the future.

title: Reinforcement Learning as a Pre-Diagnostic Tool for TCP/IP Protocols on In-Car Networks
abstract: In-car networks such as Automotive Ethernet will enable new services in smart vehicles, but they are an untrodden territory for many network protocols that should support the applications.
For instance, TCP has been newly incorporated as an integral part of AUTOSAR, the standard software framework for electronic control units (ECUs).
However, the in-vehicle network environment can be starkly different from the Internet where TCP has been optimized.
The disparity between the two environments warrants re-evaluation of the Internet protocols such as TCP in the forecasted in-car network environments.
In this paper, we propose a novel approach where we employ reinforcement learning as a pre-diagnostic tool to predict potential problems and to present possible remedies.

title: Design and Research of Vehicle Platoon Formation Algorithm Based on Multi-Agent Reinforcement Learning
abstract: The field of artificial intelligence is advancing rapidly, with reinforcement learning making significant strides in solving various sequential decision problems in machine learning.
As research progresses, multi-agent reinforcement learning has emerged in the field of reinforcement learning and has been applied to numerous domains.
Vehicle formation is an important means of transportation for reducing vehicle energy consumption and improving air quality.
Controlling sparse vehicles on the road to form formations is a fascinating research topic.
In this paper, we propose a planning framework for formation control based on federated learning and multi-agent reinforcement learning to address this problem.
We model vehicle energy consumption to accurately assess energy usage during vehicle formation.
Additionally, we incorporate reinforcement learning algorithms into the vehicle formation process to enable multi-vehicle asynchronous decision-making and save formation time.
We also introduce federated learning into the training process to significantly reduce overall system communication.

title: Modular production control using deep reinforcement learning: proximal policy optimization
abstract: EU regulations on CO2 limits and the trend of individualization are pushing the automotive industry towards greater flexibility and robustness in production.
One approach to address these challenges is modular production, where workstations are decoupled by automated guided vehicles, requiring new control concepts.
Modular production control aims at throughput-optimal coordination of products, workstations, and vehicles.
For this np-hard problem, conventional control approaches lack in computing efficiency, do not find optimal solutions, or are not generalizable.
In contrast, Deep Reinforcement Learning offers powerful and generalizable algorithms, able to deal with varying environments and high complexity.
One of these algorithms is Proximal Policy Optimization, which is used in this article to address modular production control.
Experiments in several modular production control settings demonstrate stable, reliable, optimal, and generalizable learning behavior.
The agent successfully adapts its strategies with respect to the given problem configuration.
We explain how to get to this learning behavior, especially focusing on the agent's action, state, and reward design.

title: Model free Reinforcement Learning to determine pricing policy for car parking lots
abstract: Finding a parking space has not only become painful but also costs a lot in most of the metropolitan cities.
With the increase in number of vehicles and limited resources such as manpower and space, the need for effective management of parking lots has increased.
Improper management of parking lots can have negative consequences such as traffic congestion, wastage of time in search of parking spaces, air pollution and even loss of revenue for the parking lot managers.

Dynamic pricing is a powerful tool to control the behavior of drivers by diverting them towards the unoccupied and cheaper parking lots.
Though there are several existing dynamic pricing strategies, determining the right prices is quite challenging due to lack of knowledge of drivers' behavior and several uncertainties like harsh weather and special days.

In this paper Reinforcement Learning(RL) technique called Q-learning is used to calculate the dynamic prices for parking lots on hourly basis without the need of prior information about the system.
Crucial factors like distance of the parking lots from the city centers, weather and holidays are considered in the proposed algorithm to achieve better accuracy.
Price Elasticity of Demand (PED) is used in the proposed work to calculate the new state(occupancy) when an action(dynamic price charged by the parking lot owner) is taken place.
Hourly prices are estimated using the proposed algorithm and simulation results show that the calculated prices can efficiently manage parking occupancy during peak and off peak hours.
The simulation output also shows that the proposed algorithm can successfully increase the revenue of the parking lot owners.

title: Automated function development for emission control with deep reinforcement learning
abstract: The conventional automotive development process for embedded systems today is still time-and data -inefficient, and requires highly experienced software developers and calibration engineers.
Consequently, it is cost-intensive and at the same time prone to sub-optimal solutions.
Reinforcement Learning offers a promising approach to address these challenges.
The evolved agents have proven their ability to master complex control tasks in a close-to-optimal manner without any human intervention, but the training procedures are hardly compatible with current development processes.

As a result, Reinforcement Learning has rarely been used in powertrain development until now.
This work describes an integration of Reinforcement Learning in the embedded system development process to automatically train and deploy agents in transient driving cycles.
Using the example of exhaust gas re-circulation control for a Diesel engine, an agent is successfully trained in a fully virtualized environment, achieving emission reductions of up to 10% in comparison to a state-of-the-art controller.

Further investigations are carried out to quantify the impact of the driving cycle and ambient conditions on the agent's performance.
To demonstrate the transferability between different levels of virtualization, the experienced agent is then tested in closed-loop with a real hardware controller to operate the physical actuator.
By confirming the reproducibility of the learned strategy on real hardware, this article serves as proof-of-concept for a sustainable, Reinforcement Learning based path to automatically develop embedded controllers for complex control problems.


title: <i>EdgeMap</i>: CrowdSourcing High Definition Map in Automotive Edge Computing
abstract: High definition (HD) map needs to be updated frequently to capture road changes, which is constrained by limited specialized collection vehicles.
To maintain an up-to-date map, we explore crowdsourcing data from connected vehicles.
Updating the map collaboratively is, however, challenging under constrained transmission and computation resources in dynamic networks.
In this paper, we propose EdgeMap, a crowdsourcing HD map to minimize the usage of network resources while maintaining the latency requirements.
We design a DATE algorithm to adaptively offload vehicular data on a small time scale and reserve network resources on a large time scale, by leveraging the multi-agent deep reinforcement learning and Gaussian process regression.

We evaluate the performance of EdgeMap with extensive network simulations in a time-driven end-to-end simulator.
The results show that EdgeMap reduces more than 30% resource usage as compared to state-of-the-art solutions.

title: Intelligent Learning Algorithm and Intelligent Transportation-Based Energy Management Strategies for Hybrid Electric Vehicles: A Review
abstract: As one of the alternatives to conventional fuel vehicles, hybrid electric vehicles (HEV) offer lower fuel consumption and fewer exhaust emissions.
To improve the performance of the HEV, the energy management strategy (EMS) is one of the most critical technologies.
Classic EMS can be broadly classified into rule-based and optimization-based.
With the development of machine learning technology, the deep reinforcement learning (DRL) algorithm of intelligent learning algorithms has been applied to the EMS.
This paper mainly reviews the research progress of the EMS based on DRL from two aspects of the algorithm and training environment, and the EMS research involving combining the intelligent transportation system (ITS) is reviewed.

In addition, the experimental test progress situations of DRL-based EMS research are discussed.
Finally, the challenge of DRL-based EMSs is analyzed and some solutions are provided.
In particular, it also involves some discussion about automotive cyber security in the intelligent transportation environment.

title: Cloud resource allocation for cloud-based automotive applications
abstract: There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive tasks.
Efficient utilization of on-demand cloud resources holds a significant potential to improve future vehicle safety, comfort, and fuel economy.
In the meanwhile, issues like cyber security and resource allocation pose great challenges.
In this paper, we treat the resource allocation problem for cloud-based automotive systems.
Both private and public cloud paradigms are considered where a private cloud provides an internal, company-owned Internet service dedicated to its own vehicles while a public cloud serves all subscribed vehicles.

This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-based automotive systems.
Complications such as stochastic communication delays and task deadlines are explicitly considered.
In particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited to utilize the cloud resources for best Quality of Services.
On the other hand, a decentralized auction-based model is developed for public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a "selfish" agent.
Numerical examples are presented to illustrate the effectiveness of the developed techniques.

title: Modular Production Control with Multi-Agent Deep Q-Learning
abstract: The automotive industry is increasingly focusing on product customization.
The concept of Modular Production addresses this issue by providing more flexibility in production with Automated Guided Vehicles transporting products between modular workstations.
The added complexity of Modular Production Control calls for approaches that can handle the scheduling complexity while also minimizing production costs.
As a result, literature has focused on two promising approaches: Deep Reinforcement Learning and Multi-Agent Systems.
Both approaches have their advantages.
Especially in complex, large-scale production environments with random breakdowns, those two fields have been seldomly combined, though.
As a result, this article aims to fill that research gap by introducing a Deep Reinforcement Learning Multi-Agent System approach for Modular Production Control.
We introduce a reward design incentivizing agents to achieve maximal throughput.
In addition, we show that the method learns optimal behavior even in a large-scale production environment with random machine breakdowns.
Lastly, we compare the Multi-Agent System to a single-agent implementation of the Deep Reinforcement Learning approach and conclude that the Multi-Agent Deep Reinforcement Learning method learns and solves the Modular Production Control problem with the same solution quality as the single agent.

Hence, the approach allows to foster MAS benefits such as robustness without losses in the solution quality.

title: An End-to-End Curriculum Learning Approach for Autonomous Driving Scenarios
abstract: In this work, we combine Curriculum Learning with Deep Reinforcement Learning to learn without any prior domain knowledge, an end-to-end competitive driving policy for the CARLA autonomous driving simulator.

To our knowledge, we are the first to provide consistent results of our driving policy on all towns available in CARLA.
Our approach divides the reinforcement learning phase into multiple stages of increasing difficulty, such that our agent is guided towards learning an increasingly better driving policy.
The agent architecture comprises various neural networks that complements the main convolutional backbone, represented by a ShuffleNet V2.
Further contributions are given by (i) the proposal of a novel value decomposition scheme for learning the value function in a stable way and (ii) an ad-hoc function for normalizing the growth in size of the gradients.

We show both quantitative and qualitative results of the learned driving policy.

title: Deep Q-learning for Control: Technique and Implementation Considerations on a Physical System: Active Automotive Rear Spoiler Case
abstract: Deep Q-learning is the combination of artificial neural networks advantages (ANNs) with Q-learning.
ANNs have expanded the possibilities on a variety of algorithms by enhancing their capabilities and surpassing their limitations.
This is the case of reinforcement learning.
Nowadays, Deep Q-learning is used in a variety of applications in different fields, including the development of intelligent algorithms to control physical systems.
Deep Q-learning has demonstrated the possibility of achieving effective results by solving specific tasks that are highly complex to model through classical approaches.
An important drawback is that these models require an elaborated implementation process, and several design decisions must be taken in order to achieve reliable results.
Often, developers might find the design process mostly experimental rather than ruled-based.
Addressing this problem, the present work describes in detail the implementation process of Deep Q-learning to control a physical system, proposes considerations and analysis parameters for each of the main steps.

Demonstrated in the development of the "Active automotive rear spoiler", the results present a methodology that successfully guides towards a proper implementation of Deep Q-learning.
The knowledge of this paper should not be taken as a recipe, but rather as an evaluation reference to equip the reinforcement learning developers with tools for the development of projects.

title: Reinforcement Learning-Based Energy Management System Enhancement Using Digital Twin for Electric Vehicles
abstract: Compared to conventional engine-based powertrains, electrified powertrain exhibit increased energy efficiency and reduced emissions, making electrification a key goal for the automotive industry.
For a vehicle with hybrid energy storage system, its performance and lifespan are substantially affected by the energy management system.
Reinforcement learning-based methods are gaining popularity in vehicle energy management, but most of the literature in this area focus on pure simulation while hardware implementation is still limited.

This paper introduces the digital twin methodology to enhance the Q-learning-based energy management system for battery and ultracapacitor electric vehicles.
The digital twin model can exploit the bilateral interdependency between the virtual model and the actual system, which improves the control performance of the energy management system.
The physical model is established based on a hardware-in-the-loop simulation platform.
In addition, battery degradation is also considered for prolonging the battery lifespan to reduce the operating cost.
The validation results of the trained reinforcement learning agent illustrate that the digital twin-enhanced Q-learning energy management system improves the energy efficiency by 4.
36% and the battery degradation is reduced by 25.
28%.


title: A Deep Reinforcement Learning-Based Energy Management Optimization for Fuel Cell Hybrid Electric Vehicle Considering Recent Experience
abstract: This study emphasizes a recent experience sampling method in conjunction with Deep Deterministic Policy Gradient (DDPG) to enhance the training speed and improve the training outcomes.
Firstly, to ensure the safe operation of the battery and energy storage system under peak power, a power demand decoupling method based on frequency domain is proposed to achieve power stratification.
Subsequently, a multi-objective equivalent consumption minimization strategy model is established based on the data types of the experimental platform, and the improved DDPG algorithm is employed to solve it.

Finally, simulation results demonstrate that compared to conventional DDPG algorithms, the improved DDPG algorithm can enhance efficiency by an average of 2.02%.

title: Learning to Schedule Joint Radar-Communication With Deep Multi-Agent Reinforcement Learning
abstract: Radar detection and communication are two essential sub-tasks for the operation of next-generation autonomous vehicles (AVs).
The forthcoming proliferation of faster 5G networks utilizing mmWave has raised concerns on interference with automotive radar sensors, which has led to a body of research on Joint Radar-Communication (JRC).

This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV.
We first formulate the problem as a Markov Decision Process (MDP).
We then propose a more general multi-agent system, with an appropriate medium access control (MAC) protocol, which is formulated as a partially observed Markov game (POMG).
To solve the POMG, we propose a multi-agent extension of the Proximal Policy Optimization (PPO) algorithm, along with algorithmic features to enhance learning from raw observations.
Simulations are run with a range of environmental parameters to mimic variations in real-world operation.
The results show that the chosen deep reinforcement learning methods allow the agents to obtain strong performance with minimal a priori knowledge about the environment.

title: Learning in Mean-Field Games and Continuous-Time Stochastic Control Problems
abstract: 

title: Q-learning based Reinforcement Learning Approach for Lane Keeping
abstract: The paper presents the application of a Q-learning reinforcement learning method in the area of vehicle control.
The purpose of the research presented is to design an end-to-end behavior control of a kinematic vehicle model placed in a simulated race track environment, by using reinforcement learning approach.
The varied trajectory of the track to provide different situations for the agent.
The environment sensing model is based on high-level sensor information.
Track curvature, lateral position, and relative yaw angle can be reached from the current automotive sensors, such as camera or IMU based systems.
The objectives were reached through the definition of a rewarding system, with subrewards and penalties enforcing lane keeping.
After the description of the theoretical basis the environment model with the reward function is detailed.
Finally the experiments with the learning process are presented and the results of the evaluation are given from some aspects of control quality and safety.

title: MAConAuto: Framework for Mobile-Assisted Human-in-the-Loop Automotive System
abstract: Automotive is becoming more and more sensor-equipped.
Collision avoidance, lane departure warning, and self-parking are examples of applications becoming possible with the adoption of more sensors in the automotive industry.
Moreover, the driver is now equipped with sensory systems like wearables and mobile phones.
This rich sensory environment and the real-time streaming of contextual data from the vehicle make the human factor integral in the loop of computation.
By integrating the human's behavior and reaction into the advanced driver-assistance systems (ADAS), the vehicles become a more contextaware entity.
Hence, we propose MAConAuto, a framework that helps design human-in-the-loop automotive systems by providing a common platform to engage the rich sensory systems in wearables and mobile to have context-aware applications.

By personalizing the context adaptation in automotive applications, MAConAuto learns the behavior and reactions of the human to adapt to the personalized preference where interventions are continuously tuned using Reinforcement Learning.

Our general framework satisfies three main design properties, adaptability, generalizability, and conflict resolution.
We show how MAConAuto can be used as a framework to design two applications as human-centric applications, forward collision warning, and vehicle HVAC system with negligible time overhead to the average human response time.


title: Deep reinforcement learning for a color-batching resequencing problem
abstract: In automotive paint shops, changes of colors between consecutive production orders cause costs for cleaning the painting robots.
It is a significant task to re-sequence orders and group orders with identical color as a color batch to minimize the color changeover costs.
In this paper, a Color-batching Resequencing Problem (CRP) with mix bank buffer systems is considered.
We propose a Color-Histogram (CH) model to describe the CRP as a Markov decision process and a Deep Q-Network (DQN) algorithm to solve the CRP integrated with the virtual car resequencing technique.
The CH model significantly reduces the number of possible actions of the DQN agent, so that the DQN algorithm can be applied to the CRP at a practical scale.
A DQN agent is trained in a deep reinforcement learning environment to minimize the costs of color changeovers for the CRP.
Two experiments with different assumptions on the order attribute distributions and cost metrics were conducted and evaluated.
Experimental results show that the proposed approach outperformed conventional algorithms under both conditions.
The proposed agent can run in real time on a regular personal computer with a GPU.
Hence, the proposed approach can be readily applied in the production control of automotive paint shops to resolve order-resequencing problems.

title: An Energy Management Strategy of Power-Split Hybrid Electric Vehicles Using Reinforcement Learning
abstract: With the rapid development of science and technology, the automobile industry is also gradually expanding due to which energy security and ecological security are seriously threatened.
This paper was aimed at studying the energy organization strategy of power-split hybrid electric vehicles based on a reinforcement learning algorithm.
A power-split hybrid electric vehicle (HEV) combines the advantages of both series and parallel hybrid vehicle architectures by using a planetary gear set to split and combine the power generated by electric machines and a combustion engine.

This improves the fuel economy to some extent.
However, to increase the fuel economy to a greater extent.
This study primarily introduces the hybrid electric vehicle's structure and presents a reinforcement learning-based management of energy approach for hybrid electric vehicles.
It constructs the vehicle power model of HEV and Markov probability transfer model, then designs the energy control strategy based on reinforcement learning, and finally compares it with the energy control strategy based on PID (Proportional-IntegralDerivative).

Using MATLAB/Simulink, the cycle conditions of NEDC (New European Driving Cycle) and FTP-75 (Federal Test Procedure) are selected to carry out simulation experiments.
The energy management technique suggested in this study, which is based on reinforcement learning, may efficiently enhance the usage rate of automotive gasoline.
The replication results show that the fuel consumption per 100 km (kilometers) based on reinforcement learning management strategy is 4.
6% and 2.
7% lower than the PID management strategy under two working conditions.

The economy of fuel of the hybrid electric vehicle is also effectively improved.

title: IT and relationship learning in networks as drivers of green innovation and customer capital: evidence from the automobile sector
abstract: Purpose - Despite the positive effects of customer capital (CC), questions remain over how managers enable CC growth by applying their skills and capabilities through managerial actions and strategies, such as developing information technology (IT) capability, fostering relationship learning (RL) activities and developing green innovation performance (GIP) with clients.

These questions are especially pertinent in small and medium-sized enterprises and automotive industry companies that operate through supply chains, where knowledge about customers is likely to result from personal contact between customers and organisational members.

The purpose of this paper is to analyse the extent to which these managerial actions were more likely to lead to the successful creation of CC.
Design/methodology/approach - Using the partial least squares technique, this paper studies how these three managerial actions impact on CC.

To do so, data from 140 companies in the Spanish automotive components manufacturing sector have been used.Findings - The findings support the influence of RL on both GIP and CC.
RL is a key managerial action in exploiting customer information and knowledge advantages, enabling firms to structure and reconfigure resources to produce new ways to compete and to satisfy stakeholders.

In addition, results show that GIP is a determinant of CC because of its contribution to achieving sustainable competitive advantage, with GIP performing a mediating role in the relationship between RL and CC.

A second contribution shows that IT is not in itself able to yield a competitive advantage, thereby validating the existence of complementary or co-focused strategic assets such as RL and GIP, which enhance IT's influence on CC.
Research limitations/implications - The authors were unable to explore the subtleties of the processes over time.

Future research should include a longitudinal study.Practical implications - This study considers RL an essential factor in achieving both GIP and CC.
Consequently, managers should seek to build strong RL cultures.
In addition, this study shows that IT is not in itself able to yield a competitive advantage, thereby validating the existence of complementary or co-focused strategic assets such as RL and GIP.
Originality/value - No study has ever examined these three antecedent variables (IT, RL and GIP) together, with the aim to examine their effects on CC.


title: Reinforcement Learning based on Energy Management Strategy for HEVs
abstract: This paper presents a new architecture of real-time HEV's energy management problem under a V2V and V21 environment using policy-based deep reinforcement learning.
The ideal energy management controller that minimizes HEV energy costs needs to run engines most efficiently in the whole running considering battery SoC.
The controller needs to predict the future vehicle speed and plan the power distribution to achieve it because the thermal efficiency of engines is more efficient when its rotational speed is higher.
The future vehicle speed has relationship with connectivity information such as the behavior of the car in front, the traffic light signals, crowd of cars, and so on.
This paper assumes the connectivity environment in the future and applies proximal policy optimization (PPO) [5] that is known as policy-based deep reinforcement learning algorithm to achieve the optimal power distribution predicting the future behavior by using connectivity information.

In addition, this paper shows that locating the local controller in the reinforcement learning loop enables the AI controller to learn robustly.
The local controller corrects against an exploration that is obviously not optimal or doesn't satisfy the constraints.

title: Reinforcement Learning for Joint V2I Network Selection and Autonomous Driving Policies
abstract: Vehicle-to-Infrastructure (V2I) communication is becoming critical for the enhanced reliability of autonomous vehicles (AVs).
However, the uncertainties in the road-traffic and AVs' wireless connections can severely impair timely decision-making.
It is thus critical to simultaneously optimize the AVs' network selection and driving policies in order to minimize road collisions while maximizing the communication data rates.
In this paper, we develop a reinforcement learning (RL) framework to characterize efficient network selection and autonomous driving policies in a multi-band vehicular network (VNet) operating on conventional sub-6GHz spectrum and Terahertz (THz) frequencies.

The proposed framework is designed to (i) maximize the traffic flow and minimize collisions by controlling the vehicle's motion dynamics (i.
e.
, speed and acceleration) from autonomous driving perspective, and (ii) maximize the data rates and minimize handoffs by jointly controlling the vehicle's motion dynamics and network selection from telecommunication perspective.

We cast this problem as a Markov Decision Process (MDP) and develop a deep Q-learning based solution to optimize the actions such as acceleration, deceleration, lane-changes, and AV-base station assignments for a given AV's state.

The AV's state is defined based on the velocities and communication channel states of AVs.
Numerical results demonstrate interesting insights related to the inter-dependency of vehicle's motion dynamics, handoffs, and the communication data rate.
The proposed policies enable AVs to adopt safe driving behaviors with improved connectivity.

title: Physically Realizable Targeted Adversarial Attacks on Autonomous Driving
abstract: 

title: Hybrid DDPG Approach for Vehicle Motion Planning
abstract: The paper presents a motion planning solution which combines classic control techniques with machine learning.
For this task, a reinforcement learning environment has been created, where the quality of the fulfilment of the designed path by a classic control loop provides the reward function.
System dynamics is described by a nonlinear planar single track vehicle model with dynamic wheel mode model.
The goodness of the planned trajectory is evaluated by driving the vehicle along the track.
The paper shows that this encapsulated problem and environment provides a one-step reinforcement learning task with continuous actions that can be handled with Deep Deterministic Policy Gradient learning agent.

The solution of the problem provides a real-time neural network-based motion planner along with a tracking algorithm, and since the trained network provides a preliminary estimate on the expected reward of the current state-action pair, the system acts as a trajectory feasibility estimator as well.


title: DRL-assisted delay optimized task offloading in automotive-industry 5.0 based VECNs
abstract: The rapid growth of Automotive-Industry 5.0 and its emergence with beyond fifth-generation (B5G) communications, is making vehicular edge computing networks (VECNs) increasingly complex.
The latency constraints of modern automotive applications make it difficult to run complex applications on vehicle on-board units (OBUs).
While multi-access edge computing (MEC) can facilitate task offloading to execute these applications, it is still a challenge to access them promptly and optimally.
Traditional algorithms struggle to guarantee accuracy in such dynamic environment, but deep reinforcement learning (DRL) methods offer improved accuracy, robustness, and real-time decision-making capabilities.

In this paper, we propose a DRL-based mobility, contact, and load aware cooperative task offloading (DCTO) scheme.
DCTO is designed for both cellular and mmWave radio access technologies (RATs), and both binary and partial offloading mechanisms.
DCTO targets delay minimization by opportunistically switching RATs and offloading mechanisms.
We consider relative efficacy and neutrality factors as key performance indicators and use them to derive the DRL agent's reward function.
Extensive evaluations demonstrate that the DCTO scheme exhibits a substantial enhancement in task success rate, with an increase from 2.61% to 21.34%.
It also improves the efficacy factor from 1.38 to 3.52 and reduces the neutrality factor from 4.99 to 0.76.
Furthermore, the average task processing time is reduced by a range of 3.77% to 24.15%.
Additionally, the DCTO scheme outperforms the other evaluated schemes in terms of reward and TFPS ratio.
(c) 2023 The Author(s).
Published by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

title: Cloud Resource Allocation for Cloud-Based Automotive Applications [arXiv]
abstract: There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive tasks.
Efficient utilization of on-demand cloud resources holds a significant potential to improve future vehicle safety, comfort, and fuel economy.
In the meanwhile, issues like cyber security and resource allocation pose great challenges.
In this paper, we treat the resource allocation problem for cloud-based automotive systems.
Both private and public cloud paradigms are considered where a private cloud provides an internal, company-owned internet service dedicated to its own vehicles while a public cloud serves all subscribed vehicles.

This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-based automotive systems.
Complications such as stochastic communication delays and task deadlines are explicitly considered.
In particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited to utilize the cloud resources for best Quality of Services.
On the other hand, a decentralized auction-based model is developed for public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a "selfish" agent.
Numerical examples are presented to illustrate the effectiveness of the developed techniques.

title: System reconfiguration for reverse logistics: A case study
abstract: Manufacturers need to effectively address returned products in their system, and challenges occur in the remanufacturing processes due to the uncertainty related to the quantities and quality of the returned components.

This research focuses on identifying the challenges encountered in a remanufacturing Reverse Logistics (RL) system and are illustrated with an automotive case study.
Lean Six-sigma techniques are used to find these issues encountered in the RL process or system, and a linear programming approach is taken to reconfigure the system to improve the throughput.
Additional analyses need to be performed to explore the influence of different production strategies and system layouts.
Copyright (C) 2022 The Authors.

title: Reinforcement Learning-based Path Following Control for a Vehicle with Variable Delay in the Drivetrain
abstract: In this contribution we propose a reinforcement learning-based controller able to solve the path following problem for vehicles with significant delay in the drivetrain.
To efficiently train the controller, a control-oriented simulation model for a vehicle with combustion engine, automatic gear box and hydraulic brake system has been developed.
In addition, to enhance the reinforcement learning-based controller, we have incorporated preview information in the feedback state to better deal with the delays.
We present our approach of designing a reward function which enables the reinforcement learning-based controller to solve the problem.
The controller is trained using the Soft Actor-Critic algorithm by incorporating the developed simulation model.
Finally, the performance and robustness is evaluated in simulation.
Our controller is able to follow an unseen path and is robust against variations in the vehicle parameters, in our case an additional payload.

title: Modular Production Control with Multi-Agent Deep Q-Learning
abstract: The automotive industry is increasingly focusing on product customization.
The concept of Modular Production addresses this issue by providing more flexibility in production with Automated Guided Vehicles transporting products between modular workstations.
The added complexity of Modular Production Control calls for approaches that can handle the scheduling complexity while also minimizing production costs.
As a result, literature has focused on two promising approaches: Deep Reinforcement Learning and Multi-Agent Systems.
Both approaches have their advantages.
Especially in complex, large-scale production environments with random breakdowns, those two fields have been seldomly combined, though.
As a result, this article aims to fill that research gap by introducing a Deep Reinforcement Learning Multi-Agent System approach for Modular Production Control.
We introduce a reward design incentivizing agents to achieve maximal throughput.
In addition, we show that the method learns optimal behavior even in a large-scale production environment with random machine breakdowns.
Lastly, we compare the Multi-Agent System to a single-agent implementation of the Deep Reinforcement Learning approach and conclude that the Multi-Agent Deep Reinforcement Learning method learns and solves the Modular Production Control problem with the same solution quality as the single agent.

Hence, the approach allows to foster MAS benefits such as robustness without losses in the solution quality.

title: Trajectory Tracking Control of Intelligent Vehicle Based on DDPG Method of Reinforcement Learning
abstract: To address the problem of lateral control of an intelligent vehicle during trajectory tracking,a trajectory tracking control method for an intelligent vehicle based on the deep deterministic policy gradient (DDPG) method of reinforcement learning is proposed.
First,the tracking control of an intelligent vehicle was described as a reinforcement learning process based on the Markov decision process (MDP).
The main framework of reinforcement learning was the actor-critic composed of actor and critic neural networks.
The reinforcement learning environment included vehicle,tracking,and road models as well as a reward function.
Then,the learning agent of the proposed method was updated by DDPG,in which the replay buffer was used to solve the problem of sample correlate on,and the actor and critic neural networks were copied to solve the problem of update divergence.
Finally,the proposed method was tested under different scenarios and compared with the deep Q-learning (DQN) and model predictive control (MPC) methods.
The results show that the reinforcement learning method based on DDPG has the advantages of a short learning time,small lateral deviation,and small angular deviation,and it can meet the requirements of vehicle tracking at different speeds.
When DDPG and DQN are used as the two reinforcement learning methods,both methods can achieve the maximum cumulative reward of training under different scenarios.
In the two simulation scenarios,the total learning time of DDPG is 9.
53% and 44.
19% of DQN,respectively,and the learning time of a single round of training is only 20.
28%and 22.
09%of DQN.
When DDPG,DQN,and MPC are used for control,in the first scenario,the maximum lateral deviation based on DDPG is 87.
5%and 50%of DQN and MPC,respectively.
In the second scenario,the maximum lateral deviation based on the DDPG method is 75% and 21.
34% of DQN and MPC,respectively,and the simulation time is 20.
64%and 58.
60%of DQN and MPC,respectively.


title: Proving Ground Test of a DDPG-based Vehicle Trajectory Planner
abstract: The paper presents real-world test cases of an optimal trajectory design solution that combines modern control techniques with machine learning.
The first step of the current research is to train a reinforcement learning agent in a simulated environment, where the conditions and the applied vehicle are modeled.
System dynamics is described by a nonlinear single-track vehicle with dynamic wheel model.
The designed trajectory is evaluated by driving the vehicle using a control loop.
The reward of the method is based on the sum of different measures considering safety and passenger comfort.
The proposed method forms a special one-step reinforcement learning task handled by Deep Deterministic Policy Gradient (DDPG) learning agent.
As a result, the learning process provides a real-time neural-network-based motion planner and a tracking algorithm.
The evaluation of the algorithm under real conditions is made by using an experimental test vehicle.
The test setup contains a high precision GPS module, an automotive inertial sensor, an industrial PC, and communication interface devices.
The test cases were performed on the ZalaZone automotive proving ground.

title: Deep Reinforcement Learning-Based Spectrum Allocation Algorithm in Internet of Vehicles Discriminating Services
abstract: With the rapid development of global automotive industry intelligence and networking, the Internet of Vehicles (IoV) service, as a key communication technology, has been faced with an increasing spectrum of resources shortage.

In this paper, we consider a spectrum utilization problem, in which a number of co-existing cellular users (CUs) and prioritized device-to-device (D2D) users are equipped in a single antenna vehicle-mounted communication network.

To ensure a business-aware spectrum access mechanism with delay granted in a complex dynamic environment, we consider optimizing a metric that maintains a trade off between maximizing the total capacity of vehicle to vehicle (V2V) and vehicle to infrastructure (V2I) links and minimizing the interference of high priority links.

A low complexity priority-based spectrum allocation scheme based on the deep reinforcement learning method is developed to solve the proposed formulation.
We trained our algorithm using the deep Q-learning network (DQN) over a set of public bandwidths.
Simulation results show that the proposed scheme can allocate spectrum resources quickly and effectively in a high dynamic vehicle network environment.
Concerning improved channel transmission rate, the V2V link rate in this scheme is 2.
54 times that of the traditional random spectrum allocation scheme, and the V2I link rate is 13.
5% higher than that of the traditional random spectrum allocation scheme.

The average total interference received by priority links decreased by 14.2 dB compared to common links, realized service priority distinction and has good robustness to communication noise.

title: Coordinated control in Agent of automotive airbag roll device
abstract: Airbags tension as an important part of quality assurance which is produced in the airbag roll debice.
In this paper, an adaptive reinforcement learning method, which is based on Agent, is used to achieve real-time coordination of two servo motor.
The results show that the system has a good dynamic and static characteristics, and which can ensure the constant tension of the airbags and product quality.

title: Smart OTA Scheduling for Connected Vehicles using Prescriptive Analytics and Deep Reinforcement Learning
abstract: OTA (over the air) updates help automotive manufacturers to reduce vehicle warranty and recall costs.
Vehicle recall is expensive, and many automotive manufacturers have implemented OTA updates.
Updating parameters for connected vehicles can be challenging when dealing with thousands of vehicles across different regions.
For example, how does the manufacturer prioritise which vehicles need updating?
Environmental and geographical factors affect degradation rates and vehicles in hotter regions or congested cities may degrade faster.
For EVs, updating the BMS (battery management system) parameters requires careful analysis prior to the update being deployed, to maximise impact and reduce the likelihood of adverse behaviour being introduced.

The analysis overhead increases with the number of vehicles.
This is because it requires simulation and optimisation of the fleet BMS calibration in a digital twin environment.
A targeted approach is the best option to prioritise vehicles for software updates.
Smart OTA scheduling makes use of predictive analytics for battery health prediction together with prescriptive analytics in a smart decision engine.
The smart scheduling system uses a deep reinforcement learning (DRL) agent in a digital twin environment.
The DRL agent can learn and simulate different scenarios and identify the best update sequence depending on monthly temperature profile, traffic congestion, and many other factors to slow down the degradation of fleet health.

Whenever there is an update, the DRL agent assesses the situation and recommends the appropriate action to minimise vehicle failures and maintain fleet health.
In a large-scale simulation, this approach improved average fleet battery life by 8 to 13% and increased average vehicle range by 2%.

title: Reluctant Reinforcement Learning
abstract: 

title: Data Analytics and Machine Learning for Smart Decision Making in Automotive Sector
abstract: The objective of this thesis is to conduct scientific research on the use of data science and artificial intelligence techniques in the practices of automotive dealership companies to assist them in their decision-making processes and to use data-driven methods with modeling approaches for computing these enterprises.

By proposing algorithms capable of continuously extracting relevant information from a diverse and multi-structured automotive environment.
Due to the large amount of data available within these companies, we will develop algorithms to correctly assess the situation, suggest recommendations for decision-making, develop marketing strategies, and automate manual tasks that cost time, effort, and money.


title: A Long-term Energy Management Strategy for Fuel Cell Electric Vehicles Using Reinforcement Learning
abstract: The two power sources of a fuel cell electric vehicle (FCEV) are proton electrolyte membrane fuel cell (PEMFC) and Li-ion battery (LIB).
The health status of PEMFC and LIB decreases with the use of FCEV, so the energy management strategy (EMS) needs to give an optimal power distribution based on the health status of power sources throughout the lifetime.

However, rule-based control strategies cannot achieve this.
To prolong the service lifetime of two power sources by optimizing power distribution, this article proposes a long-term energy management strategy (LTEMS) for FCEV, which contains a reinforcement learning module and an improved thermostat controller.

By designing a reward function, the reinforcement learning module outputted various LIB state of charge (SOC) boundary which changes with power source attenuation.
Based on SOC boundary, the improved thermostat controller will control the fuel cell current under specific driving conditions.
Simulation was carried out based on different LIB state of health (SOH) and external temperature, and the simulation results were compared with the data collected from FCEV under rule-based (RB) strategies.

It can be found that the proposed LTEMS can effectively reduce fuel cell and LIB attenuation, and meet the FCEV power demand.

title: Safe Trajectory Planning Using Reinforcement Learning for Self Driving [arXiv]
abstract: Self-driving vehicles must be able to act intelligently in diverse and difficult environments, marked by high-dimensional state spaces, a myriad of optimization objectives and complex behaviors.
Traditionally, classical optimization and search techniques have been applied to the problem of self-driving; but they do not fully address operations in environments with high-dimensional states and complex behaviors.

Recently, imitation learning has been proposed for the task of self-driving; but it is labor-intensive to obtain enough training data.
Reinforcement learning has been proposed as a way to directly control the car, but this has safety and comfort concerns.
We propose using model-free reinforcement learning for the trajectory planning stage of self-driving and show that this approach allows us to operate the car in a more safe, general and comfortable manner, required for the task of self driving.


title: Electric bus charging facility planning with uncertainties: Model formulation and algorithm design
abstract: This paper investigates the electric bus charging facility planning (EB-CFP) problem for a bus transit company operating a heterogeneous electric bus (EB) fleet to provide public transportation services, taking into account uncertainties in both EB travel time and battery degradation.

The goal of the EB-CFP problem is to determine the number and type of EB chargers that should be deployed at bus terminals and depots to meet daily EB charging demand while minimizing total cost.
The problem is formulated as a two-stage stochastic programming model, with the first stage determining the EB charger deployment scheme and the second stage estimating the EB daily operational cost with respect to a predetermined EB trip timetable for a given EB charger deployment scheme.

To effectively address the second stage problem, a multi -agent EB transit simulation system that mimics the daily EB operation process is developed.
We then design two heuristic methods, the reinforcement learning (RL)-based method and the surrogate-based optimization (SBO), that use the developed multi-agent EB transit simulation system to solve the two-stage stochastic programming model for large-scale instances.

Lastly, we run a series of experiments on a fictitious EB transit network and a real-world EB transit network in Singapore to evaluate the performance of the models and algorithms.
In order to improve the performance of the EB transit system, some managerial insights are also provided to urban bus transit companies.

title: ICTs and Relational Learning in Networks as Drivers of Green Innovation and Customer Capital: Empirical Evidence From the Spanish Automotive Industry
abstract: For the purposes of our research, we use the concept of information technology (IT) infrastructure, defined as the shared IT capabilities that enable the flow of knowledge in an organization to be supported.

In this category we include a set of technological resources, both hardware and software applications, which support different utilization characteristics of knowledge and relational learning (RL) activities, such as: business intelligence, technologies for collaborating and distributing knowledge, knowledge generation and storage, and support hardware for these technologies.

An emerging stream of research on IT and RL seeks to guide the application of technologies that support RL.
IT is involved in the various knowledge management processes, which include knowledge creation.
A great variety of procedures, tools and activities may act as a support to the knowledge generation and creation process.
IT contributes to sustainable competitive advantage through its interaction with other resources.
Recent literature suggests that RL is a process that plays an important role in enhancing a firm's capabilities and competitive advantage and which may benefit from the judicious application of IT.
It has also been argued that for firms to be successful they must complement IT with RL.
This study aims to assess the role played by information technology (IT) in relational learning activities (RL).
We also examine how IT and RL influence both green innovations (GI) and the development of the customer capital (CC).
These relationships have been tested via an empirical analysis carried out with a sample of industrial companies belonging to the Spanish automotive industry.
Our findings allow us to confirm that IT acts as an enabler of the RL process and influences on the development of GI, which allow the achievement of a better customer capital (CC).

title: Authentication and Resource Allocation Strategies during Handoff for 5G IoVs Using Deep Learning
abstract: One of the most sought-after applications of cellular technology is transforming a vehicle into a device that can connect with the outside world, similar to smartphones.
This connectivity is changing the automotive world.
With the speedy growth and densification of vehicles in Internet of Vehicles (IoV) technology, the need for consistency in communication amongst vehicles becomes more significant.
This technology needs to be scalable, secure, and flexible when connecting products and services.
5G technology, with its incredible speed, is expected to power the future of vehicular networks.
Owing to high mobility and constant change in the topology, cooperative intelligent transport systems ensure real time connectivity between vehicles.
For ensuring a seamless connectivity amongst the entities in vehicular networks, a significant alternative to design is support of handoff.
This paper proposes a scheme for the best Road Side Unit (RSU) selection during handoff.
Authentication and security of the vehicles are ensured using the Deep Sparse Stacked Autoencoder Network (DS2AN) algorithm, developed using a deep learning model.
Once authenticated, resource allocation by RSU to the vehicle is accomplished through Deep-Q learning (DQL) techniques.
Compared with the existing handoff schemes, Reinforcement Learning based on the MDP (RL-MDP) has been found to have a 13% lesser decision delay for selecting the best RSU.
A higher level of security and minimum time requirement for authentication is achieved using DS2AN.
The proposed system simulation results demonstrate that it ensures reliable packet delivery, significantly improving system throughput, upholding tolerable delay levels during a change of RSUs.

title: Reinforcement Learning based Optimization of Bayesian Networks for Generating Feasible Vehicle Configuration Suggestions
abstract: A promising method in the automotive industry to anticipate future customer demands is the concept of planned orders.
Due to multi-variant products, changing customer demands, and dynamic environments the process of generating planned orders is challenging.
This paper introduces an approach using graphical models to generate planned order suggestions in a multi-variant order management process.
Bayesian networks are modelled by learning the structure from different data sources, which enable the possibility to directly sample configuration suggestions.
To find an optimized graph structure, a method using hierarchical correlation clustering and reinforcement learning is applied, taking into account technical and sales-operated feasibility constraints.

The method has high potential in practical usage and is evaluated by a real-world use case of the Dr. Ing.
h.c. F. Porsche AG.

title: Critical barriers in implementing reverse logistics in the Chinese manufacturing sectors
abstract: Reverse logistics (RL) is gaining momentum worldwide due to global awareness and as a consequence of resource depletion and environmental degradation.
Firms encounter RI implementation challenges from different stakeholders, both internally and externally.
On the one hand, various governmental agencies are coming out with different environmental regulations while on the other hand academics and researchers are contributing solutions and suggestions in different country contexts.

In a real sense however, the benefits of RL implementation is not yet fully realized in the emerging economies.
This paper proposes a theoretical RL implementation model and empirically identifies significant RL barriers with respect to management, financial, policy and infrastructure in the Chinese manufacturing industries such as automotive, electrical and electronic, plastics, steel/construction, textiles and paper and paper based products.

Key barriers from our study, with respect to these four categories, are: within management category a lack of reverse logistics experts and low commitment, within financial category a lack of initial capital and funds for return monitoring systems, within policy category a lack of enforceable laws and government supportive economic policies and, finally, within infrastructure category a the lack of systems for return monitoring.

Contingency effect of ownership was carried out to understand the similarities and differences in RL barriers among the multinational firms and domestic firms investigated.
(C) 2012 Elsevier B.V. All rights reserved.

title: Time Series Prediction Integrated RNN-DRL for Energy-Emission Management in HEVs
abstract: Learning-based strategies have gained widespread adoption in the field of energy management strategy (EMS) for hybrid electric vehicles (HEVs).
However, existing algorithms often struggle to maintain the effectiveness in different driving scenarios.
Additionally, achieving a fully observable Markov decision process in real-world situations is impractical due to the limitation in sensor sensitivity and noise.
Therefore, it is crucial to design a stable EMS for HEVs that can adapt to various driving scenarios.
This paper proposes a novel approach called the twin-delayed deep deterministic policy gradient with a long-short-term combined algorithm.
It aims to enhance the generalization performance and EMS capability of HEVs' learning-based EMS by integrating temporal predicted information using a recurrent neural network.
Comparing this algorithm with other established deep reinforcement learning algorithms, the results demonstrate significant improvements in EMS performance for HEVs, including state of charge maintenance, energy conservation, and emission reduction.

Notably, the algorithm achieves these improvements while ensuring robust generalization across diverse driving cycles and scenarios.

title: Automatic Generation of Critical Test Cases for the Development of Highly Automated Driving Functions
abstract: The development of highly automated driving functions is currently one of the key drivers for the automotive industry and research.
In addition to the technical constraints in the implementation of these functions, a major challenge is the verification of functional safety.
Conventional approaches aiming at statistical validation in the sense of real test drives are reaching their economic limits.
On the other hand, there are simulation methods that allow a lot of freedom in test case design, but whose representativeness and relevance must be proven separately.
In this paper an approach is presented that allows to generate critical concrete scenarios and test cases for automated driving functions by means of a reinforcement learning based optimization using here the example of an overtaking assistant.

For this purpose, a Q-Learning approach is used that automates the parameter generation for the test cases.
While pure combinatorics of the variable parameters leads to an unmanageable amount of test cases, the percentage of actually relevant critical test cases is very low.
In this work we show how the share of critical and thus relevant test cases can be increased significantly by using the presented method compared to a purely combinatorial parameter variation.

title: Policy Gradient based Reinforcement Learning Approach for Autonomous Highway Driving
abstract: The paper presents the application of the Policy Gradient reinforcement learning method in the area of vehicle control.
The purpose of the research presented is to design an end-to-end behavior control of a kinematic vehicle model placed in a simulated highway environment, by using reinforcement learning approach.
The environment model for the surrounding traffic uses microscopic simulation to provide different situations for the agent.
The environment sensing model is based on high-level sensor information, e.g.
the odometry, lane position and surrounding vehicle states that can be reached from the current automotive sensors, such as camera and radar based systems.
The objectives were reached through the definition of a rewarding system, with subrewards and penalties enforcing desired speed, lane keeping, keeping right and avoiding collision.
After the description of the theoretical basis the environment model with the reward function is detailed.
Finally the experiments with the learning process are presented and the results of the evaluation are given from some aspects of control quality and safety.

title: Bench calibration method for automotive electric motors based on deep reinforcement learning
abstract: The efficiency and control accuracy of Interior Permanent Magnet Synchronous Motor (IPMSM) are the main factors affecting performance.
Manual calibration has the disadvantage of high work intensity, long calibration period and high technical requirement, which leads to low calibration accuracy and motor efficiency.
Thus, a novel calibration method based on Deep Deterministic Policy Gradient (DDPG) and Long Short-Term Memory (LSTM) is proposed.
By constructing a deep reinforcement learning network, the self-optimization of the optimal working point under any working condition is realized, and the MAP for IPMSM in full speed-torque range is obtained.

The method can be used to quickly realize the optimal matching of d-q axis current with arbitrary stator current.
It focuses on solving the problem of motor overheating caused by long adjustment time of manually calibrated MAP when the motor is overloaded, to realize fast calibration in overload area.
Moreover, the method reduces the dependence on the motor parameters and increases the adaptability of the calibration MAP data to the operating conditions.
The simulation and bench test indicate that the method can meet the response requirements of motor torque, and results reveal that the motor efficiency is greatly improved.

title: KARNet: Kalman Filter Augmented Recurrent Neural Network for Learning World Models in Autonomous Driving Tasks [arXiv]
abstract: Autonomous driving has received a great deal of attention in the automotive industry and is often seen as the future of transportation.
The development of autonomous driving technology has been greatly accelerated by the growth of end-to-end machine learning techniques that have been successfully used for perception, planning, and control tasks.

An important aspect of autonomous driving planning is knowing how the environment evolves in the immediate future and taking appropriate actions.
An autonomous driving system should effectively use the information collected from the various sensors to form an abstract representation of the world to maintain situational awareness.
For this purpose, deep learning models can be used to learn compact latent representations from a stream of incoming data.
However, most deep learning models are trained end-to-end and do not incorporate any prior knowledge (e.g., from physics) of the vehicle in the architecture.
In this direction, many works have explored physics-infused neural network (PINN) architectures to infuse physics models during training.
Inspired by this observation, we present a Kalman filter augmented recurrent neural network architecture to learn the latent representation of the traffic flow using front camera images only.
We demonstrate the efficacy of the proposed model in both imitation and reinforcement learning settings using both simulated and real-world datasets.
The results show that incorporating an explicit model of the vehicle (states estimated using Kalman filtering) in the end-to-end learning significantly increases performance.

title: Improving the quality of service by continuous traffic monitoring using reinforcement learning model in VANET
abstract: The growing number of automobiles on the road has now become a significant source of traffic, accidents, and pollution.
Intelligent Transportation Systems (ITSs) could be the key to finding solutions that drastically reduce these issues.
The linked vehicular networks channel is a fast-expanding topic for workflow management system and development.
Traffic detection is a big issue on city streets.
To make informed decisions in order to prevent traffic jams, one of the solutions is the Vehicular Ad-Hoc Network (VANET).
We describe an approach for detecting traffic jams in both vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, which provides vehicle drivers with multiple alternatives for determining the location of congestion, its size, and how to prevent becoming stuck in a traffic jam.

The data are sent to the driver to assist him in making the appropriate selection.
Delivering Quality of Service (QoS) for automotive networks is a difficult challenge due to the characteristics such as terms of transmission or high mobility, congested and fragmented channels, hardware defects, and a large number of vehicular devices.

As a result, it is extremely desired to get and distribute resources efficiently.
This research uses Reinforcement Learning with Decision-Making Model (RLDMM) method to enhance channel allocation.
By using latency, Signal-to-Interference Ratio (SIR) and QoS, the available channel is initially determined.
This method is used to determine suitable channel for platoon members.
As a result, the proposed RLDMM achieves an SINR reduction of 29 Db, 69 kbps of throughput, 21.4% of collision probability with 4ms of latency.

title: Human-centric data-driven optimization and recommendation in EV-interfaced grid at city scale: poster abstract
abstract: The fast development of electric vehicles (EV) and EV chargers introduces many factors that affect the grid.
EV charging and charge scheduling also bring challenges to EV drivers and grid operators.
In this work, we propose a human-centric, data-driven, city-scale, multivariate optimization approach for the EV-interfaced grid.
This approach takes into account user historical driving and charging habits, user preferences, EV characteristics, city-scale mobility, EV charger availability and price, and grid capacity.
The user preferences include the trade-off between cost and time to charge, as well as incentives to participate in different energy-saving programs.
We leverage deep reinforcement learning (DRL) to make recommendations to EV drivers and optimize their welfare while enhancing grid performance.

title: Intelligent Orchestration of ADAS Pipelines on Next Generation Automotive Platforms [arXiv]
abstract: Advanced Driver-Assistance Systems (ADAS) is one of the primary drivers behind increasing levels of autonomy, driving comfort in this age of connected mobility.
However, the performance of such systems is a function of execution rate which demands on-board platform-level support.
With GPGPU platforms making their way into automobiles, there exists an opportunity to adaptively support high execution rates for ADAS tasks by exploiting architectural heterogeneity, keeping in mind thermal reliability and long-term platform aging.

We propose a future-proof, learning-based adaptive scheduling framework that leverages Reinforcement Learning to discover suitable scenario based task-mapping decisions for accommodating increased task-level throughput requirements.


title: From potential absorptive capacity to innovation outcomes in project teams: The conditional mediating role of the realized absorptive capacity in a relational learning context
abstract: Starting from the construct absorptive capacity, this study separately treats its two dimensions - potential absorptive capacity (PACAP) and realized absorptive capacity (RACAP) - and analyzes their influence on innovation outcomes (IO) in project teams.

We also examine potential absorptive capacity as an antecedent of realized absorptive capacity.
In addition, we propose that relational learning (RL) will play a moderator role reinforcing the PACAP and RACAP link.
Consequently, this paper builds and tests a conditional process model.
Data was collected from a sample of 110 project managers of firms belonging to the Spanish automotive components manufacturing sector.
Results from variance-based structural equation modeling and PROCESS tool show that RACAP fully mediates the influence of the PACAP on IO, and this indirect effect is positively conditioned by RL.
This paper provides evidence that when RL achieves a low value, this indirect influence is not different from zero.
(C) 2014 Elsevier Ltd. APM and IPMA.
All rights reserved.

title: A technology maturity assessment framework for Industry 5.0 machine vision systems based on systematic literature review in automotive manufacturing
abstract: When considering how an intelligent factory can 'see,' the answer lies in machine vision technology.
To assess the current technological advancements of machine vision systems and propose a technology maturity assessment framework, a nine-phase Systematic Literature Review (SLR) strategy was implemented.

As the automotive industry stands at the forefront of autonomous systems, we analysed 85 works across the entire automotive manufacturing life cycle.
The findings revealed that machine vision is utilised in each technological pillar of Industry 4.
0, encompassing autonomous robots, augmented reality, predictive maintenance, additive manufacturing, and more.

In analysing 22 vision-based applications in 47 automotive components, we clustered machine vision systems' architectural components and processing techniques, ranging from threshold-based methods to advanced reinforcement learning techniques suitable for the I5.
0 environment.

Leveraging the insights gathered, we propose the I5.0 technology maturity assessment framework for machine vision systems, evaluating nine functional components across five scaling technology levels.
This framework serves as a valuable tool to identify weaknesses and opportunities for improvement, guiding machine vision integration into an intelligent factory.

title: Mastering Nordschleife -- A comprehensive race simulation for AI strategy decision-making in motorsports [arXiv]
abstract: In the realm of circuit motorsports, race strategy plays a pivotal role in determining race outcomes.
This strategy focuses on the timing of pit stops, which are necessary due to fuel consumption and tire performance degradation.
The objective of race strategy is to balance the advantages of pit stops, such as tire replacement and refueling, with the time loss incurred in the pit lane.
Current race simulations, used to estimate the best possible race strategy, vary in granularity, modeling of probabilistic events, and require manual input for in-laps.
This paper addresses these limitations by developing a novel simulation model tailored to GT racing and leveraging artificial intelligence to automate strategic decisions.
By integrating the simulation with OpenAI's Gym framework, a reinforcement learning environment is created and an agent is trained.
The study evaluates various hyperparameter configurations, observation spaces, and reward functions, drawing upon historical timing data from the 2020 N\"urburgring Langstrecken Serie for empirical parameter validation.

The results demonstrate the potential of reinforcement learning for improving race strategy decision-making, as the trained agent makes sensible decisions regarding pit stop timing and refueling amounts.

Key parameters, such as learning rate, decay rate and the number of episodes, are identified as crucial factors, while the combination of fuel mass and current race position proves most effective for policy development.

The paper contributes to the broader application of reinforcement learning in race simulations and unlocks the potential for race strategy optimization beyond FIA Formula~1, specifically in the GT racing domain.


title: Online Scheduling Optimization of User Load Based on Deep Reinforcement Learning and Demand Response
abstract: In recent years, the daily electricity consumption of ordinary users has been continuously increasing, which may result in the wastage of power resources and an increase in the burden of power resource scheduling on the grid.

Responding to electricity demand and making reasonable use of power resources has become one of the solutions to this problem.
In this paper, we propose the use of Deep Reinforcement Learning (DRL) technology to optimize the scheduling of user load Demand Response (DR).
Firstly, we establish a comprehensive scheduling model of user load for both the power grid-load and user-load parts.
We then add controllable loads, such as commonly used household appliances, user electric vehicles (EVs), and distributed photovoltaic (PV), to construct a load interaction environment.
We also proposes that electric vehicles can charge and discharge flexibly based on real-time electricity prices to alleviate the burden of power grid scheduling and prioritize the consumption of distributed photovoltaics.

Finally, we solve the Markov decision process and provide experimental evidence for the feasibility and effectiveness of deep reinforcement learning methods in solving the problem of user load demand response.


title: Optimal Placement of Charging Stations in Road Networks: A Reinforcement Learning Approach with Attention Mechanism
abstract: With the aim of promoting energy conservation and emission reduction, electric vehicles (EVs) have gained significant attention as a strategic industry in many countries.
However, the insufficiency of accessible charging infrastructure remains a challenge, hindering the widespread adoption of EVs.
To address this issue, we propose a novel approach to optimize the placement of charging stations within a road network, known as the charging station location problem (CSLP).
Our method considers multiple factors, including fairness in charging station distribution, benefits associated with their placement, and drivers' discomfort.
Fairness is quantified by the balance in charging station coverage across the network, while driver comfort is measured by the total time spent during the charging process.
Then, the CSLP is formulated as a reinforcement learning problem, and we introduce a novel model called PPO-Attention.
This model incorporates an attention layer into the Proximal Policy Optimization (PPO) algorithm, enhancing the algorithm's capacity to identify and understand the intricate interdependencies between different nodes in the network.

We have conducted extensive tests on urban road networks in Europe, North America, and Asia.
The results demonstrate the superior performance of our approach compared to existing baseline algorithms.
On average, our method achieves a profit increase of 258.04% and reduces waiting time by 73.40%, travel time by 18.46%, and charging time by 40.10%.

title: Deep reinforcement learning-based energy management strategies for energy-efficient driving of hybrid electric buses
abstract: The fuel economy of hybrid electric vehicles is inextricably linked to the energy management strategy (EMS).
In this study, a practicality-oriented learning-based EMS for a power-split hybrid electric bus (HEB) is presented, which combines the generative adversarial imitation learning (GAIL) and deep reinforcement learning (DRL).

Considering the regular and fixed route of the HEB, optimal control samples that are not affected by the cost function can be obtained by the boundary-line dynamic programing (B-DP) method.
On this basis, the samples are dynamically fitted using the GAIL method to inverse derive the reward function that can explain the B-DP control behavior.
Concurrently, the proximal policy optimization DRL algorithm will regulate the energy distribution of the vehicle in real time and continuously optimize the energy management capability based on the reward feedback from GAIL.

Finally, the feasibility and effectiveness of the proposed EMS is verified by simulation.
The results show that the proposed strategy exhibits near-optimal control performance under both the China heavy-duty commercial vehicle cycle-bus and China city bus cycle.

title: Development and Evaluation of a Smart Charging Strategy for an Electric Vehicle Fleet Based on Reinforcement Learning
abstract: Governments are currently subsidizing growth in the electric car market and the associated infrastructure in order to accelerate the transition to more sustainable mobility.
To avoid the grid overload that results from simultaneously charging too many electric vehicles, there is a need for smart charging coordination systems.
In this paper, we propose a charging coordination system based on Reinforcement Learning using an artificial neural network as a function approximator.
Taking into account the baseload present in the power grid, a central agent creates forward-looking, coordinated charging schedules for an electric vehicle fleet of any size.
In contrast to optimization-based charging strategies, system dynamics such as future arrivals, departures, and energy consumption do not have to be known beforehand.
We implement and compare a range of parameter variants that differ in terms of the reward function and prioritized experience.
Subsequently, we use a case study to compare our Reinforcement Learning algorithm with several other charging strategies.
The Reinforcement Learning-based charging coordination system is shown to perform very well.
All electric vehicles have enough energy for their next trip on departure and charging is carried out almost exclusively during the load valleys at night.
Compared with an uncontrolled charging strategy, the Reinforcement Learning algorithm reduces the variance of the total load by 65%.
The performance of our Reinforcement Learning concept comes close to that of an optimization-based charging strategy.
However, an optimization algorithm needs to know certain information beforehand, such as the vehicle's departure time and its energy requirement on arriving at the charging station.
Our novel Reinforcement Learning-based charging coordination system therefore offers a flexible, easily adaptable, and scalable approach for an electric vehicle fleet under realistic operating conditions.


title: STAP in Automotive MIMO Radar with Transmitter Scheduling
abstract: Automotive radars often employ multiple-input multiple-output (MIMO) array to attain high angular resolution with few antenna elements.
The diversity gain is generally achieved by time-division multiplexing (TDM) during the transmission of frequency-modulated continuous-wave (FMCW) signals.
However, TDM mode leads to longer pulse repetition intervals and, therefore, inherently and severely limits the maximum unambiguous Doppler velocity that a radar is able to detect.
In this paper, we address the Doppler ambiguity problem in TDM MIMO automotive radars through a space-time adaptive processing (STAP) approach.
A direct application of STAP may lead to a high antenna sidelobe level that hampers the detection performance.
We mitigate this through optimal transmitter scheduling.
We formulate the problem as combinatorial optimization and solve it via reinforcement learning.
Numerical and experimental results demonstrate the efficacy of our method when compared with conventional techniques.

title: A Car-following Control Algorithm Based on Deep Reinforcement Learning
abstract: Longitudinal acceleration decisions in a car-following control mode are directly determined by the state of the preceding vehicle.
A driver's uncertainty makes car-following control difficult because of the complexity in state prediction of the target vehicle.
To address the problem in which the performance of adaptive cruise control may deteriorate without consideration of the uncertainty of the preceding vehicle,a car-following control strategy based on deep reinforcement learning was proposed.
To study the characteristics of human drivers,a drivingdata- acquisition platform was established,and substantial amounts of human-driving data were collected.
Based on the assumption that longitudinal control decisions are mainly affected by the preceding vehicle,a two-predecessor following structure was established.
The vehicles in the driving dataset were taken as target vehicles 1#and 2#of the car-following control.
Based on the real-world driving dataset,a stochastic process model was established to describe the characteristics of preceding vehicle 1# based on Gaussian process algorithm.
Then car-following control was established as a Markov decision process.
A car-following control method based on deep reinforcement learning was obtained through iterative learning with the stochastic process model using proximal policy optimization.
Finally,the algorithm was verified based on the driving dataset.
The results demonstrate that the mapping between longitudinal acceleration decisions and the states of the host and preceding vehicles can be obtained through iterative learning with consideration of the uncertainty of the target vehicle.


title: Digital Twin of a Driver-in-the-Loop Race Car Simulation With Contextual Reinforcement Learning
abstract: In order to facilitate rapid prototyping and testing in the advanced motorsport industry, we consider the problem of imitating and outperforming professional race car drivers based on demonstrations collected on a high-fidelity Driver-in-the-Loop (DiL) hardware simulator.

We formulate a contextual reinforcement learning problem to learn a human-like and stochastic policy with domain-informed choices for states, actions, and reward functions.
To leverage very limited training data and build human-like diverse behavior, we fit a probabilistic model to the expert demonstrations called the reference distribution, draw samples out of it, and use them as context for the reinforcement learning agent with context-specific states and rewards.

In contrast to the non-human-like stochasticity introduced by Gaussian noise, our method contributes to a more effective exploration, better performance and a policy with human-like variance in evaluation metrics.

Compared to previous work using a behavioral cloning agent, which is unable to complete competitive laps robustly, our agent outperforms the professional driver used to collect the demonstrations by around 0.
4 seconds per lap on average, which is the first time known to the authors that an autonomous agent has outperformed a top-class professional race driver in a state-of-the-art, high-fidelity simulation.

Being robust and sensitive to vehicle setup changes, our agent is able to predict plausible lap time and other performance metrics.
Furthermore, unlike traditional lap time calculation methods, our agent indicates not only the gain in performance but also the driveability when faced with modified car balance, facilitating the digital twin of the DiL simulation.


title: Improved NSGA-â…¡ based on reinforcement learning to solve energy-saving scheduling problem of flexible job shop
abstract: For the flexible job shop scheduling problem in the context of green manufacturing,a multi-objective integer programming model was established to minimize the completion time,machine load and energy consumption of the workshop,and an improved Non-dominated Sorting Genetic Algorithm-â…¡ (NSGA-â…¡)based on Q-learning was proposed to solve it.
Firstly,multiple heuristic algorithms were used to initialize the population to balance the machine load,and an elite pool was introduced to conduct a dual-strategy hybrid crossover for improving the population quality.
Secondly,the state space of reinforcement learning based on two metrics of the population was constructed and the hybrid crossover ratio was adjusted by Q-learning training to ensure the uniformity and diversity of the population distribution, as well as avoiding premature maturity of the algorithm.

Finally, the performance of the algorithm was analyzed and evaluated by solving Kacem and Brandimarte benchmark cases and a production example of manufacturing parts for automotive engine cooling system.
The effectiveness of the model and algorithm in solving the flexible job shop scheduling problem and the superiority in balancing machine load and energy consumption are verified.


title: Sentio: Driver-in-the-Loop Forward Collision Warning Using Multisample Reinforcement Learning
abstract: Thanks to the adoption of more sensors in the automotive industry, context-aware Advanced Driver Assistance Systems (ADAS) become possible.
On one side, a common thread in ADAS applications is to focus entirely on the context of the vehicle and its surrounding vehicles leaving the human (driver) context out of consideration.
On the other side, and due to the increasing sensing capabilities in mobile phones and wearable technologies, monitoring complex human context becomes feasible which paves the way to develop driver-in-the-loop context-aware ADAS that provide personalized driving experience.

In this paper, we propose Sentio(1); a Reinforcement Learning based algorithm to enhance the Forward Collision Warning (FCW) system leading to Driver-in-the-Loop FCW system.
Since the human driving preference is unknown a priori, varies between different drivers, and moreover, varies across time for the same driver, the proposed Sentio algorithm needs to take into account all these variabilities which are not handled by the standard reinforcement learning algorithms.

We verified the proposed algorithm against several human drivers.
Our evaluation, across distracted human drivers, shows a significant enhancement in driver experience-compared to standard FCW systems-reflected by an increase in the driver safety by 94.
28%, an improvement in the driving experience by 20.
97%, a decrease in the false negatives from 55.
90% down to 3.
26%, while adding less than 130 ms runtime execution overhead.


title: Leap: A Model-Based Reinforcement Learning Framework for Fast Object Detection
abstract: 

title: SDCoR: Software Defined Cognitive Routing for Internet of Vehicles
abstract: The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field.
Large amounts of sensor data require to be transferred in real-time.
Most of the routing protocols are specifically targeted to specific situations in IoV.
But communication environment of IoV usually changes in the space-time dimension.
Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy.

Sensing and learning constitute two key steps of the cognition procedure.
Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability.

To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV.
We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm.
Results show that our algorithm achieves better performance than several typical protocols in IoV.
We also show the feasibility and effectiveness of our proposed SDCIV.

title: Electric Vehicle Charging Management for Avoiding Transformer Congestion Using Policy-based Reinforcement Learning
abstract: With the widespread integration of electric vehicles (EV) in distribution networks (DN), the distribution network operator faces new challenges relating to facility overloads such as distribution transformers.

Due to similar charging behaviors of EV customers in residential areas, the peak loads caused by simultaneous charging processes will have a significant impact on the operation of DN.
In this paper, the focus is on a cooperative approach among EVs with different individual preferences regarding charging demand urgency with the aim to jointly alleviate transformer congestion.
An online algorithm using policy-based reinforcement learning is proposed, which takes the advantage of bidirectional charging infrastructures with continuous controllable charging rate for more flexible cooperation among EVs.

A particular aim is to overcome the unscalability of neural networks due to the determined network structures before training processes.
For this reason, a grouping method concerning individual charging behaviors is used for dimensionality reduction and scalable representation of the state and action space.
Simulation results show that the proposed charging management is able to identify the preference of each EV group and to generate an appropriate charging rate with the purpose of minimizing the damage of EV charging satisfactions while avoiding overloading of transformers.


title: MARL based resource allocation scheme leveraging vehicular cloudlet in automotive-industry 5.0
abstract: Automotive-Industry 5.
0 will use Beyond Fifth-Generation (B5G) communications to provide robust, abundant computation resources and energy-efficient data sharing among various Intelligent Transportation System (ITS) entities.

Based on the vehicle communication network, the Internet of Vehicles (IoV) is created, where vehicles' resources, including processing, storage, sensing, and communication units, can be leveraged to construct Vehicular Cloudlet (VC) to realize resource sharing.

As Connected and Autonomous Vehicles (CAV) onboard computing is becoming more potent, VC resources (comprising stationary and moving vehicles' idle resources) seems a promising solution to tackle the incessant computing requirements of vehicles.

Furthermore, such spare computing resources can significantly reduce task requests' delay and transmission costs.
In order to maximize the utility of task requests in the system under the maximum time constraint, this paper proposes a Secondary Resource Allocation (SRA) mechanism based on a dual time scale.
The request service process is regarded as M/ M/1 queuing model and considers each task request in the same time slot as an agent.
A Partially Observable Markov Decision Process (POMDP) is constructed and combined with the Multi-Agent Reinforcement Learning (MARL) algorithm known as QMix, which exploits the overall vehicle state and queue state to reach effective computing resource allocation decisions.

There are two main performance metrics: the system's total utility and task completion rate.
Simulation results reveal that the task completion rate is increased by 13%.
Furthermore, compared with the deep deterministic policy optimization method, our proposed algorithm can improve the overall utility value by 70% and the task completion rate by 6%.
& COPY; 2022 The Author(s).
Published by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

title: An energy management strategy of deep reinforcement learning based on multi-agent architecture under self-generating conditions
abstract: To improve the driving efficiency of hybrid power vehicle, an energy management strategy of deep reinforcement learning based on multi-agent architecture under self-generating vehicle driving conditions is proposed.

Firstly, the kinematics segments are self-generated based on the Wasserstein generative adversarial network.
The generator network G is used to generate kinematics segments.
The discriminator network D is used to judge the credibility of the generated kinematics segments with the Wasserstein distance.
The speed distribution characteristics of the training conditions and verification conditions established based on the self-generated segments are verified.
Afterward, a multi-agent algorithm based on twin delayed deep deterministic policy gradient algorithm for hybrid systems is proposed by introducing centralized training with decentralized execution framework.

The engine and a motor are used as two independent agents respectively.
Different reward functions are designed based on training objectives to establish a mutually beneficial relationship of cooperation-restraint between the two agents.
A driving mode constraint is designed in the environment to improve sample utilization.
Finally, the simulation results demonstrate that our method can achieve better performance compared with other existing works.

title: Exploring the Application of Machine Learning in Computational Fluid Dynamics
abstract: Computational fluid dynamics (CFD) is an important tool for understanding and predicting the behavior of fluids in various systems.
Machine learning techniques have the potential to significantly improve the efficiency and accuracy of CFD simulations, and have been applied to a wide range of tasks, including turbulence modeling, boundary layer prediction, flow separation prediction, and optimization of system design.

In this review, we provide an overview of the main machine learning methods that have been applied in CFD, including neural networks, support vector machines, evolutionary algorithms, and reinforcement learning.

We also discuss the benefits and limitations of these methods and their potential applications in different fields, such as aerospace engineering, automotive engineering, biomedical engineering, and environmental engineering.

Finally, we identify future research directions and provide recommendations for the effective use of machine learning techniques in CFD.

title: 2011 IEEE International Symposium on Computer-Aided Control System Design
abstract: The following topics are dealt with: formation shape control; multiagent system; energy systems control; embedded control; fault diagnostics; adaptive control; aerospace application; reinforcement learning; robust model-based control; fusion plasmas control; automotive application; control education; repetitive processe; iterative learning control; optimization; intelligent mechatronics; drill string vibration control; Kalman filtering; optimal control and renewable energy electric grid integration.


title: Object Shape Error Correction using Deep Reinforcement Learning for Multi-Station Assembly Systems
abstract: The paper proposes a novel approach, Object Shape Error Correction (OSEC), to determine corrective action in order to mitigate root cause(s) (RCs) of dimensional and geometric product shape errors.
It leverages Deep Deterministic Policy Gradient (DDPG) algorithm to learn optimal process parameters update policies based on high dimensional state estimates of multi-station assembly systems (MAS).
These policies can be interpreted in engineering terms as sequential corrective adjustments of process parameters that are necessary to mitigate RCs of product shape errors.
The approach has the capability to estimate adjustments of process parameters related to fixturing and joining while simultaneously accounting for (i) RC uncertainty estimation, (ii) Key Performance Indicator (KPI) improvement, (iii) MAS design architecture; and, (iv) MAS inherent stochasticity.

In addition, the OSEC methodology leverages a reward function parameterized by user interpretable functional coefficients for optimal tradeoff involving various corrections requirements.
Benchmarking using an industrial, automotive cross-member assembly system demonstrates a 40% increase in the effectiveness of corrective actions when compared to current approaches.

title: Self-Learning Takagi-Sugeno Fuzzy Control With Application to Semicar Active Suspension Model
abstract: In this article, we investigate the optimal control problem for semicar active suspension systems (SCASSs).
First, we model the SCASSs by Newtonian dynamics as well as considering the uncertainties and nonlinear dynamics of the actuator.
Second, in order to solve the complexity brought by uncertainties, we apply the Takagi-Sugeno (T-S) fuzzy approach to transform the SCASSs as multilinear systems, as well as solving the optimal control problem as a zero-sum problem to find the solution of Nash-equilibrium.

Third, we construct a novel self-learning method based on the reinforcement learning framework, and propose two algorithms to solve the fuzzy game algebraic Riccati equation.
Especially, in the second algorithm, without using any model information of the SCASSs, we only use the state and input information in control design by a self-learning manner removing the traditional dependence problem, which is more preferable for practical applications.

Finally, we give a simulation result of the SCASSs to demonstrate the effectiveness and practicability for the designed self-learning algorithms.

title: Learning to Schedule Joint Radar-Communication Requests for Optimal Information Freshness
abstract: Radar detection and communication are two of several sub-tasks essential for the operation of next-generation autonomous vehicles (AVs).
The former is required for sensing and perception, more frequently so under various unfavorable environmental conditions such as heavy precipitation; the latter is needed to transmit time-critical data.

Forthcoming proliferation of faster 5G networks utilizing mmWave is likely to lead to interference with automotive radar sensors, which has led to a body of research on the development of Joint Radar Communication (JRC) systems and solutions.

This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV.
We formulate the problem as a Markov Decision Process (MDP) where the JRC agent determines in a real-time manner when radar detection is necessary, and how to manage a multiclass data queue where each class represents different urgency levels of data packets.

Simulations are run with a range of environmental parameters to mimic variations in real-world operation.
The results show that deep reinforcement learning allows the agent to obtain good results with minimal a priori knowledge about the environment.

title: Investigation on Reverse Logistics of End of Life Cars in the Uk
abstract: 

title: Decision Making Method for Control Right Transition of Human- machine Shared Driving Based on Driver-vehicle Risk State
abstract: Frequent traffic accidents have proved that driving is a high-risk event and risky driving behaviors are one of the main causes.
Using an automatic driving system as an agent to assist or replace human drivers is considered an effective way to fundamentally solve the threats caused by human factors.
First, to maximize the overall safety of intelligent vehicles, a human-vehicle risk game model was established by utilizing entropy- technique for order preference by similarity to ideal solution (TOPSIS) and complete static game theory.

A strategy function to maximize the relative utility was proposed and embedded in the reinforcement learning reward function, then the reward and punishment mechanism guided by maximizing vehicle safety expectation were deduced.

Second, taking advantage of reinforcement learning which is good at solving sequence decision-making problems, a human-vehicle driving control transition method based on advantage actor critical (A2C) was proposed.

The output effect of the decision model was optimized by iterating the decision weights and reward functions, and the validity of the training process and result was verified by the model performance evaluation indices.

Finally, the influence of different transition times on vehicle safety was analyzed through simulation test.
A control right decision-making method that can limit risky behaviors and improve vehicle safety timely and effectively was proposed.
The results showed that this research innovatively takes the actor and critic modules mapped from the human and vehicle risk monitoring module to A2C as the framework, which fully utilizes the interaction between intelligent vehicle and human-vehicle risk state.

Moreover, it achieves the maximum return by obtaining rewards updated iteratively.
The decision-making method of human-machine driving control right guided by promoting the maximization of vehicle safety is realized.

title: Optimized Real-Time Control for Modular Multilevel Converters using Adaptive Neural Networks
abstract: This paper presents a novel control approach using a Neural Network and reinforcement learning for Modular Multilevel Series Parallel Converters.
The Modular Multilevel Series Parallel Converters topology offers great advantages in automotive applications such as higher efficiency compared to conventional converters and active balancing of the State of Charge.

Furthermore the internal resistance and the current ripple of the batteries can be reduced by parallel interconnections.
For ideal control, all of these parameters have to be weighted according to their significance, which changes during operation of the MMSPC.
Due to multiple degrees of freedom which are mostly non-linear, it is challenging to find the optimal switching strategy in conventional ways.
We present a novel approach which uses not only a Neural Network, but also reinforcement learning to optimize the control strategy in a simulative setup.
Compared to conventional optimization methods a faster computation time can be achieved component tolerances and ageing processes can be taken into account.

title: Energy management control strategy for plug-in fuel cell electric vehicle based on reinforcement learning algorithm
abstract: To cope with the increasingly stringent emission regulations,major automobile manufacturers have been focusing on the development of new energy vehicles.
Fuel-cell vehicles with advantages of zero emission,high efficiency,diversification of fuel sources,and renewable energy have been the focus of international automotive giants and Chinese automotive enterprises.

Establishing a reasonable energy management strategy,effectively controlling the vehicle working mode,and reasonably using battery energy for hybrid fuel-cell vehicles are core technologies in domestic and foreign automobile enterprises and research institutes.

To improve the equilibrium between fuel-cell hydrogen consumption and battery consumption and realize the optimal energy distribution between fuel-cell systems and batteries for plug-in fuel-cell electric vehicles (PFCEVs),considering vehicles as the environment and vehicle control as an agent,an energy management strategy for the PFCEV based on reinforcement learning algorithm was proposed in this paper.

This strategy considered the immediate return and future cumulative discounted returns of a fuel-cell vehicles real-time energy allocation.
The vehicle simulation model was built by Matlab /Simulink to carry out the simulation test for the proposed strategy.
Compared with the rule-based strategy,the battery can store a certain amount of electricity,and the integrated energy consumption of the vehicle was notably reduced under different mileages.
The energy consumption in 100 km was reduced by 8.84%,29.5%,and 38.6% under 100, 200,and 300 km mileages,respectively.
The hardware-in-loop-test was performed on the D2P development platform,and the final energy consumption of the vehicle was reduced by 20.8% under urban dynamometer driving schedule driving cycle.
The hardware-in loop-test results are consistent with the simulation findings,indicating the effectiveness and feasibility of the proposed energy management strategy.

title: Behavior Modeling and Motion Planning for Autonomous Driving Using Artificial Intelligence
abstract: 

title: Optimized Real-Time Control for Modular Multilevel Converters using Adaptive Neural Networks
abstract: This paper presents a novel control approach using a Neural Network and reinforcement learning for Modular Multilevel Series Parallel Converters.
The Modular Multilevel Series Parallel Converters topology offers great advantages in automotive applications such as higher efficiency compared to conventional converters and active balancing of the State of Charge.

Furthermore the internal resistance and the current ripple of the batteries can be reduced by parallel interconnections.
For ideal control, all of these parameters have to be weighted according to their significance, which changes during operation of the MMSPC.
Due to multiple degrees of freedom which are mostly non-linear, it is challenging to find the optimal switching strategy in conventional ways.
We present a novel approach which uses not only a Neural Network, but also reinforcement learning to optimize the control strategy in a simulative setup.
Compared to conventional optimization methods a faster computation time can be achieved component tolerances and ageing processes can be taken into account.

title: Pricing Strategy of Fast Charging Stations in Coupled Power-Transportation System Considering Responsive Traffic Demand
abstract: The price of fast charging stations (FCSs) can dramatically impact the behavior of electric vehicle (EV) users on choosing paths or charging stations, which will further influence the operation of both power distribution network (PDN) and transportation network (TN).

This paper proposes a unique framework to get an optimal pricing strategy for FCSs considering the responsive traffic demand of EV users.
The responsive traffic demand correlates closely with the current charging price and traffic condition.
The optimal pricing problem of FCSs considered coupled operation of PDN and TN is first formulated as a bi-level optimization model to maximize the profit of the distribution power system operator (DSO).

Then a deep reinforcement learning method is adopted to approximately solve the hard bi-level problem.
Numerical experiments demonstrate that the proposed strategy can redistribute charging loads and enhance the operation of PDN.

title: A Neural Approach Towards Real-time Management for Integrated Energy System Incorporating Carbon Trading and Electrical Vehicle Scheduling
abstract: This paper proposes a real-time integrated energy system (IES) management approach which aims at promoting overall energy efficiency, increasing renewable energy penetration, and smoothing load fluctuation.

The electric vehicles (EVs) charging scheduling is incorporated into the IES management, where the uncertain arrivals and departures of multiple EVs are considered as a stochastic but flexible load to the IES.

Furthermore, towards the carbon neutralization target, a carbon emissions trading mechanism is introduced into the IES management to incentivize the system to operate in an eco-friendlier manner.
To tackle the computational complexity induced by the stochastic and intermittent nature of the renewable energy sources and EVs load, the scheduling of the IES is realized in a neural network based real-time manner, driven by a deep reinforcement learning approach that guarantees safe training and operation.

The case study verifies the effectiveness of the proposed approach.

title: From networking orientation to green image: A sequential journey through relationship learning capability and green supply chain management practices. Evidence from the automotive industry
abstract: Drawing on the resource-based view of the firm (RBV) and resources and capabilities theory, this study develops a model that extends our understanding of the mechanisms through which strategic assets, capabilities, and green supply chain management practices (GSCMP) contribute to green image (GI).

The model comprises (i) two new antecedents of GSCMP: relationship learning capability (RL) and strategic networking orientation (NO), and (ii) the direct and mediated impacts of GSCMP and their antecedents on firms' GI.

To empirically study the proposed relationships, data were collected from 106 Spanish firms in the automotive industry and analyzed using partial least squares structural equation modelling (PLS-SEM).
The results indicate that NO, RL capability, and GSCMP positively affect GI through a sequential mediation relationship.
An important implication is the identification of a stream of research proposing that GSCMP act similarly to a lower-order capability and that it is the interaction with other ordinary capabilities that can contribute to improving the green image.


title: Proximal Policy Optimization for Energy Management of Electric Vehicles and PV Storage Units
abstract: Connected autonomous electric vehicles (CAEVs) are essential actors in the decarbonization process of the transport sector and a key aspect of home energy management systems (HEMSs) along with PV units, CAEVs and battery energy storage systems.

However, there are associated uncertainties which present new challenges to HEMSs, such as aleatory EV arrival and departure times, unknown EV battery states of charge at the connection time, and stochastic PV production due to weather and passing cloud conditions.

The proposed HEMS is based on proximal policy optimization (PPO), which is a deep reinforcement learning algorithm suitable for continuous complex environments.
The optimal solution for HEMS is a tradeoff between CAEV driver's range anxiety, batteries degradation, and energy consumption, which is solved by means of incentives/penalties in the reinforcement learning formulation.

The proposed PPO algorithm was compared to conventional methods such as business-as-usual (BAU) and value iteration (VI) solutions based on dynamic programming.
Simulation results indicate that the proposed PPO's performance showed a daily energy cost reduction of 54% and 27% compared to BAU and VI, respectively.
Finally, the developed PPO algorithm is suitable for real-time operations due to its fast execution and good convergence to the optimal solution.

title: Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows [arXiv]
abstract: The past decade has seen a rapid penetration of electric vehicles (EV) in the market, more and more logistics and transportation companies start to deploy EVs for service provision.
In order to model the operations of a commercial EV fleet, we utilize the EV routing problem with time windows (EVRPTW).
In this research, we propose an end-to-end deep reinforcement learning framework to solve the EVRPTW.
In particular, we develop an attention model incorporating the pointer network and a graph embedding technique to parameterize a stochastic policy for solving the EVRPTW.
The model is then trained using policy gradient with rollout baseline.
Our numerical studies show that the proposed model is able to efficiently solve EVRPTW instances of large sizes that are not solvable with any existing approaches.

title: Real-Time Dynamic Map With Crowdsourcing Vehicles in Edge Computing
abstract: Autonomous driving perceives surroundings with line-of-sight sensors that are compromised under environmental uncertainties.
To achieve real time global information in high definition map, we investigate to share perception information among connected and automated vehicles.
However, it is challenging to achieve real time perception sharing under varying network dynamics in automotive edge computing.
In this paper, we propose a novel real time dynamic map, named LiveMap to detect, match, and track objects on the road.
We design the data plane of LiveMap to efficiently process individual vehicle data with multiple sequential computation components, including detection, projection, extraction, matching and combination.

We design the control plane of LiveMap to achieve adaptive vehicular offloading with two new algorithms (central and distributed) to balance the latency and coverage performance based on deep reinforcement learning techniques.

We conduct extensive evaluation through both realistic experiments on a small-scale physical testbed and network simulations on an edge network simulator.
The results suggest that LiveMap significantly outperforms existing solutions in terms of latency, coverage, and accuracy.

title: DataSheet1_A Modified Long Short-Term Memory-Deep Deterministic Policy Gradient-Based Scheduling Method for Active Distribution Networks.docx
abstract: To improve the decision-making level of active distribution networks (ADNs), this paper proposes a novel framework for coordinated scheduling based on the long short-term memory network (LSTM) with deep reinforcement learning (DRL).

Considering the interaction characteristics of ADNs with distributed energy resources (DERs), the scheduling objective is constructed to reduce the operation cost and optimize the voltage distribution.

To tackle this problem, a LSTM module is employed to perform feature extraction on the ADN environment, which can realize the recognition and learning of massive temporal structure data.
The concerned ADN real-time scheduling model is duly formulated as a finite Markov decision process (FMDP).
Moreover, a modified deep deterministic policy gradient (DDPG) algorithm is proposed to solve the complex decision-making problem.
Numerous experimental results within a modified IEEE 33-bus system demonstrate the validity and superiority of the proposed method.
Copyright: CC BY 4.0

title: Electric Vehicle Charging and Discharging Algorithm Based on Reinforcement Learning with Data-Driven Approach in Dynamic Pricing Scheme
abstract: In the smart grid environment, the penetration of electric vehicle (EV) is increasing, and dynamic pricing and vehicle-to-grid technologies are being introduced.
Consequently, automatic charging and discharging scheduling responding to electricity prices that change over time is required to reduce the charging cost of EVs, while increasing the grid reliability by moving charging loads from on-peak to off-peak periods.

Hence, this study proposes a deep reinforcement learning-based, real-time EV charging and discharging algorithm.
The proposed method utilizes kernel density estimation, particularly the nonparametric density function estimation method, to model the usage pattern of a specific charger at a specific location.
Subsequently, the estimated density function is used to sample variables related to charger usage pattern so that the variables can be cast in the training process of a reinforcement learning agent.
This ensures that the agent optimally learns the characteristics of the target charger.
We analyzed the effectiveness of the proposed algorithm from two perspectives, i.e., charging cost and load shifting effect.
Simulation results show that the proposed method outperforms the benchmarks that simply model usage pattern through general assumptions in terms of charging cost and load shifting effect.
This means that when a reinforcement learning-based charging/discharging algorithm is deployed in a specific location, it is better to use data-driven approach to reflect the characteristics of the location, so that the charging cost reduction and the effect of load flattening are obtained.


title: A Data-Driven and Human-Centric EV Charging Recommendation System at City-Scale
abstract: Electric vehicles (EVs) have gained widespread popularity in recent years, and the scheduling and routing of EV charging have impacted the welfare of both EV drivers and the grid.
In this paper, we present a practical, data-driven, and human-centric EV charging recommendation system at the city-scale based on deep reinforcement learning (DRL).
The system co-optimizes the welfare of both the EV drivers and the grid.
We augmented and aggregated data from various sources, including public data, location-based data companies, and government authorities, with different formats and time granularities.
The data includes EV charger information, grid capacity, EV driving behavior information, and city-scale mobility.
We created a 30-day per-minute unified EV charger information dataset with charging prices and grid capacity, as well as an EV driving behavior dataset with location and State of Charge (SoC) information.

Our evaluation of the recommendation system shows that it is able to provide recommendations that reduce the average driver-to-charger distance and minimize the number of times chargers switch to a different driver.

The dataset we prepared for training the DRL agent, including augmented EV driving data and charging station information, will be open-sourced to benefit future research in the community.

title: Deep Reinforcement Learning framework for Autonomous Driving [arXiv]
abstract: Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes.
Despite its perceived utility, it has not yet been successfully applied in automotive applications.
Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning.
This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks.

As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework.
It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios.
It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware.
The framework was tested in an open source 3D car racing simulator called TORCS.
Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.
[IS&T Electronic Imaging, Autonomous Vehicles and Machines 2017, AVM-023, pg.
70-76 (2017) doi:10.2352/ISSN.2470-1173.2017.19.AVM-023].

title: Dynamic Unicast-Multicast Scheduling for Age-Optimal Information Dissemination in Vehicular Networks
abstract: This paper investigates the problem of minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles.

Each vehicle is interested in maintaining the freshness of its information status about one or more physical processes.
A framework is proposed to optimize the decisions to unicast, multicast, broadcast, or not transmit updates to vehicles as well as power allocations to minimize the AoI and the RSU's power consumption over a time horizon.

The formulated problem is a mixed-integer nonlinear programming problem (MINLP), thus a global optimal solution is difficult to achieve.
In this context, we first develop an ant colony optimization (ACO) solution which provides near-optimal performance and thus serves as an efficient benchmark.
Then, for real-time implementation, we develop a deep reinforcement learning (DRL) framework that captures the vehicles' demands and channel conditions in the state space and assigns processes to vehicles through dynamic unicast-multicast scheduling actions.

Complexity analysis of the proposed algorithms is presented.
Simulation results depict interesting trade-offs between AoI and power consumption as a function of the network parameters.

title: How to Implement Automotive Fault Diagnosis Using Artificial Intelligence Scheme
abstract: The necessity of vehicle fault detection and diagnosis (VFDD) is one of the main goals and demands of the Internet of Vehicles (IoV) in autonomous applications.
This paper integrates various machine learning algorithms, which are applied to the failure prediction and warning of various types of vehicles, such as the vehicle transmission system, abnormal engine operation, and tire condition prediction.

This paper first discusses the three main AI algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, and compares the advantages and disadvantages of each algorithm in the application of system prediction.

In the second part, we summarize which artificial intelligence algorithm architectures are suitable for each system failure condition.
According to the fault status of different vehicles, it is necessary to carry out the evaluation of the digital filtering process.
At the same time, it is necessary to preconstruct its model analysis and adjust the parameter attributes, types, and number of samples of various vehicle prediction models according to the analysis results, followed by optimization to obtain various vehicle models.

Finally, through a cross-comparison and sorting, the artificial intelligence failure prediction models can be obtained, which can correspond to the failure status of a certain car model and a certain system, thereby realizing a most appropriate AI model for a specific application.


title: Target State Optimization: Drivability Improvement for Vehicles with Dual Clutch Transmissions
abstract: Vehicles with dual clutch transmissions (DCT) are well known for their comfortable drivability since gear shifts can be performed jerklessly.
The ability of blending the torque during gear shifts from one clutch to the other, making the type of automated transmission a perfect alternative to torque converters, which also comes with a higher efficiency.

Nevertheless, DCT also have some drawbacks.
The actuation of two clutches requires an immense control effort, which is handled in the implementation of a wide range of software functions on the transmission control unit (TCU).
These usually contain control parameters, which makes the behavior adaptable to different vehicle and engine platforms.
The adaption of these parameters is called calibration, which is usually an iterative time-consuming process.
The calibration of the embedded software solutions in control units is a widely known problem in the automotive industry.
The calibration of any vehicle subsystem (e.g., engine, transmission, suspension, driver assistance systems for autonomous driving, etc.)
requires costly test trips in different ambient conditions.
To reduce the calibration effort and the accompanying use of professionals, several approaches to automize the calibration process are proposed.
Due to the fact that a solution is desired which can optimize different calibration problems, a generic metaheuristic approach is aimed.
Regardless, the scope of the current research is the optimization of the launch behavior for vehicles equipped with DCT since, particularly at low speeds, the transmission behavior must meet the intention of the driver (drivers tend to be more perceptive at low speeds).

To clarify the characteristics of the launch, several test subject studies are performed.
The influence factors, such as engine sound, maximal acceleration, acceleration build-up (mean jerk), and the reaction time, are taken into account.
Their influence on the evaluation of launch with relation to the criteria of sportiness, comfort, and jerkiness, are examined based on the evaluation of the test subject studies.
According to the results of the study, reference values for the optimization of the launch behavior are derived.
The research contains a study of existing approaches for optimizing driving behavior with metaheuristics (e.g., genetic algorithms, reinforcement learning, etc.).
Since the existing approaches have different drawbacks (in scope of the optimization problem) a new approach is proposed, which outperforms existing ones.
The approach itself is a hybrid solution of reinforcement learning (RL) and supervised learning (SL) and is applied in a software in the loop environment, and in a test vehicle.

title: Code accompanying the paper "Validating human driver models for interaction-aware automated vehicle controllers: A human approach"
abstract: This python package contains scripts needed to train IRL Driver models on HighD datasets.
This code is accompanying the paper "Validating human driver models for interaction-aware automated vehicle controllers: A human factors approach - Siebinga, Zgonnikov & Abbink 2021" and should be used in combination with TraViA, a program for traffic data visualization and annotation.

A preprint of this paper can be found on arxiv: https://arxiv.org/abs/2109.13077 Copyright: GPL 3.0+

title: Adaptive Dynamic Programming-based Control of Unknown Networked Control Systems
abstract: Abstract<br/>The overall objective of this study is to provide online robust adaptive dynamic programming (ADP) based optimal controllers with guaranteed performance, supported by a rigorous design and mathematical framework, and without utilizing policy and value iterations, for unknown linear and nonlinear networked control systems (NCS).

The approach taken here employs adaptive network learning as a fundamental block and utilizes past history of cost-to-go information, and updates the control input once a sampling interval in a forward-in-time manner without using a system model and offline learning phase for the NNs.
<br/><br/>Intellectual Merit<br/>The proposed research presents an opportunity to deal with a more powerful and unified paradigm of complex learning problems and envisions a brain-like controller.

The proposed effort will advance the state of the art in ADP for control and guarantees stability and performance in the presence of not only uncertain system dynamics and disturbances, but also network imperfections such as random delays, packet losses and quantization errors without using iterative approach.

<br/><br/>Broader Impact<br/>This effort would directly impact all real-time practical systems such as the efficient operation and energy security of the smart grid, near zero-emission automotive control systems, and next generation manufacturing system.

Such control schemes are required for global competitiveness of the US industry.
Technology transfer will occur through the NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems.
Within the research community, this work will inspire more theoretical results while providing training opportunities to next generation students, future scientists and engineers including from underrepresented groups.


title: Energy management for hybrid electric vehicles based on imitation reinforcement learning
abstract: An effective energy management strategy (EMS) in hybrid electric vehicles (HEVs) is indispensable to promote consumption efficiency due to time-varying load conditions.
Currently, learning based algorithms have been widely applied in energy controlling performance of HEVs.
However, the enormous computation intensity, massive data training and rigid requirement of prediction of future operation state hinder their substantial exploitation.
To mitigate these concerns, an imitation reinforcement learning-based algorithm with optimal guidance is proposed in this paper for energy control of hybrid vehicles to accelerate the solving process and meanwhile achieve preferable control performance.

Firstly, offline global optimization is firstly conducted considering various driving conditions to search power allocation trajectories.
Then, the battery depletion boundaries with respect to driving distance are introduced to generate a narrowed state space, in which the optimal trajectory is fused into the training process of reinforcement learning to guide the high-efficiency strategy production.

The simulation validations reveal that the proposed method provides preferable energy reduction for HEVs in arbitrary driving scenarios, and suggests an efficient solution instruction for similar problems in mechanical and electrical systems with constraints and optimal information.


title: e-Uber: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing
abstract: There is growing interest in deploying the sharing-economy-based business model to energy systems, with modalities like peer-to-peer (P2P) energy trading, Electric Vehicles (EV)-based Vehicle-to-Grid (V2G), Vehicle-to-Home (V2H), Vehicle-to-Vehicle (V2V), and Battery Swapping Technology (BST).

This paper exploits the increasing diffusion of EVs to realize a crowdsourcing platform called e-Uber that jointly enables ride-sharing and energy-sharing through V2G and BST.
We employ theoretical concepts of online spatial crowdsourcing, reinforcement learning, and reverse auction to devise this novel platform.
Experimental results using real data from New York City taxi trips and energy consumption show that e-Uber performs close to the optimum and outperforms a state-of-the-art approach.

title: DataSheet1_Large-scale power inspection: A deep reinforcement learning approach.PDF
abstract: Power inspection plays an important role in ensuring the normal operation of the power grid.
However, inspection of transmission lines in an unoccupied area is time-consuming and labor-intensive.
Recently, unmanned aerial vehicle (UAV) inspection has attracted remarkable attention in the space-ground collaborative smart grid, where UAVs are able to provide full converge of patrol points on transmission lines without the limitation of communication and manpower.

Nevertheless, how to schedule UAVs to traverse numerous, dispersed target nodes in a vast area with the least cost (e.g., time consumption and total distance) has rarely been studied.
In this paper, we focus on this challenging and practical issue which can be considered as a family of vehicle routing problems (VRPs) with regard to different constraints, and propose a Diverse Trajectory-driven Deep Reinforcement Learning (DT-DRL) approach with encoder-decoder scheme to tackle it.

First, we bring in a threshold unit in our encoder for better state representation.
Secondly, we realize that the already visited nodes have no impact on future decisions, and then devise a dynamic-aware context embedding which removes irrelevant nodes to trace the current graph.
Finally, we introduce multiply decoders with identical structure but unshared parameters, and design a Kullback-Leibler divergence based regular term to enforce decoders to output diverse trajectories, which expands the search space and enhances the routing performance.

Comprehensive experiments on five types of routing problems show that our approach consistently outperforms both DRL and heuristic methods by a clear margin.
Copyright: CC BY 4.0

title: Machine Learning-Based Fault Injection for Hazard Analysis and Risk Assessment
abstract: Current automotive standards such as ISO 26262 require Hazard Analysis and Risk Assessment (HARA) on possible hazards and consequences of safety-critical components.
This work attempts to ease this labour-intensive process by using machine learning-based fault injection to discover representative hazardous situations.
Using a Simulation-Aided Hazard Analysis and Risk Assessment (SAHARA) methodology, a visualisation and suggested hazard classification is then presented for the safety engineer.
We demonstrate this SAHARA methodology using machine learning-based fault injection on a safety-critical use case of an adaptive cruise control system, to show that our approach can discover, visualise, and classify hazardous situations in a (semi-)automated manner in around twenty minutes.


title: Object Shape Error Correction using Deep Reinforcement Learning for Multi-Station Assembly Systems
abstract: The paper proposes a novel approach, Object Shape Error Correction (OSEC), to determine corrective action in order to mitigate root cause(s) ( RCs) of dimensional and geometric product shape errors.
It leverages Deep Deterministic Policy Gradient (DDPG) algorithm to learn optimal process parameters update policies based on high dimensional state estimates of multi-station assembly systems (MAS).
These policies can be interpreted in engineering terms as sequential corrective adjustments of process parameters that are necessary to mitigate RCs of product shape errors.
The approach has the capability to estimate adjustments of process parameters related to fixturing and joining while simultaneously accounting for (i) RC uncertainty estimation, (ii) Key Performance Indicator (KPI) improvement, (iii) MAS design architecture; and, (iv) MAS inherent stochasticity.

In addition, the OSEC methodology leverages a reward function parameterized by user interpretable functional coefficients for optimal tradeoff involving various corrections requirements.
Benchmarking using an industrial, automotive cross-member assembly system demonstrates a 40% increase in the effectiveness of corrective actions when compared to current approaches.

title: New Features in Feko / WinProp 2019
abstract: This paper describes some of the latest features in the commercial electromagnetic software Feko (including WinProp).
These include the modeling of non-ideal cable shield connections, the parallel direct adaptive cross approximation (ACA) solver, edge and wedge diffraction for the ray launching geometrical optics (RL-GO) solver, and several new features related to automotive radar.


title: A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems
abstract: Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g.
state transition probability).
Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications.
However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging.

In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems.
We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-demand ratio and the charging utilization rate across the city under model uncertainty.

Experiments show that the ROCOMA can learn an effective and robust rebalancing policy.
It outperforms non-robust MARL methods in the presence of model uncertainties.
It increases the system fairness by 19.6% and decreases the rebalancing costs by 75.8%.

title: A reinforcement learning backstepping-based control design for a full vehicle active Macpherson suspension system
abstract: This paper presents a reinforcement learning backstepping-based control scheme for the design of a full vehicle active suspension system such that both the superior ride comfort and stabilization are achieved.

It is well known that the traditional backstepping control scheme is suitable to solve higher order nonlinear system based on the strict-feedback form; however, the disadvantage of the procedure requires computing the derivatives of virtual control signals, which is a very complex task.

Therefore, a reinforcement learning scheme using the deep deterministic policy gradient (DDPG) control strategy is developed to replace the process of finding virtual control force in backstepping method.

It not only avoids the complexity of analytically calculating the derivatives of the virtual control signals, but also retains the systems robustness when there exist random disturbances from road irregularities.

To verify the performance of the proposed active suspension control scheme, the random road unevenness profiles according to ISO 8608 is considered as vertical and lateral disturbance input excitation of a full-vehicle suspension system.

Compared with conventional passive suspension system and backstepping control scheme, the proposed approach can be demonstrated to not only effectively improve ride comfort under random road excitation, but also improve the transient response and robustness.


title: Acceleration control strategy for Battery Electric Vehicle based on Deep Reinforcement Learning in V2V driving
abstract: The transportation sector is seeing the flourishing of one of the most interesting technologies, autonomous driving (AD).
In particular, Cooperative Adaptive Cruise Control (CACC) systems ensure higher levels both of safety and comfort, enhancing at the same time the reduction of energy consumption.
In this framework a real-time velocity planner for a Battery Electric Vehicle, based on a Deep Reinforcement Learning algorithm called Deep Deterministic Policy Gradient (DDPG), has been developed, aiming at maximizing energy savings, and improving comfort, thanks to the exchange of information on distance, speed and acceleration through the exploitation of vehicle-to-vehicle technology (V2V).

The aforementioned DDPG algorithm relies on a multi-objective reward function that is adaptive to different driving cycles.
The simulation results show how the agent can obtain good results on standard cycles, such as WLTP, UDDS and AUDC, and on real-world driving cycles.
Moreover, it displays great adaptability to driving cycles different from the training one.

title: A Progressive Review: Emerging Technologies for ADAS Driven Solutions
abstract: Over the last decade, the Advanced Driver Assistance System (ADAS) concept has evolved significantly.
ADAS involves several technologies such as automotive electronics, vehicle-to-vehicle (V2V) vehicle-to-infrastructure (V2I) communication, RADAR, LIDAR, computer vision, and machine learning.
Of these, computer vision and machine learning based solutions have mainly been effective that have allowed real-time vehicle control, driver aided systems, etc.
However, most of the existing works deal with the deployment of ADAS and autonomous driving functionality in countries with well-disciplined lane traffic.
Nevertheless, these solutions and frameworks do not work in countries and cities with less-disciplined/chaotic traffic.
This paper identifies the research gaps, reviews the state-of-the-art looking at the different functionalities of ADAS and its levels of autonomy.
Importantly, it provides a detailed description of vision intelligence and computational intelligence for ADAS.
The eye-gaze and head pose estimation in vision intelligence is detailed.
Notably, the learning algorithms such as supervised, unsupervised, reinforcement learning and deep learning solutions for ADAS are considered and discussed.
Significantly, this would enable developing a real-time recommendation system for system-assisted/autonomous vehicular environments with less-disciplined road traffic.

title: Decentralized Multiagent Reinforcement Learning-based Cooperative Perception with Dual-functional Radar-Communication V2V Links
abstract: The ever-increasing deployment of connected automated vehicles (CAVs) equipped with multiple radar transceivers increases the demand for both efficient environment sensing and reliable data sharing for safe driving.

However, two challenges are observed for cooperative perception used in automotive scenarios, i.
e.
, performance tradeoff between sensing and communication, as well as effective resource scheduling in the dynamic environment.

In this paper, a fully decentralized multiagent reinforcement framework is developed to optimize cooperative perception based on integrated sensing and communication (ISAC) technology.
We first formulate the beamforming and power allocation problem as a Markov decision process (MDP).
Then we design a beacon signal to implement data broadcast and reformulate the problem as a more general multiagent partially observed Markov decision process (POMDP).
In this case, the dual-functional radar- communication (DFRC) waveform can implement sensing and vehicle-to-vehicle (V2V) data exchange simultaneously without central nodes, mitigating interference and avoiding an extra communication resource demand.

Numerical results show that our method is effective for real-time perception.
It fairly provides identical quality of service (QoS) to all CAVs and achieves a low age of information (AoI) with minimal prior knowledge about the environment.

title: Intelligent Vehicle-following Control Strategy Based on Self- learning and Supervised-learning Hybrid-driven Framework
abstract: With the continuous progress of artificial intelligence, an increasing number of data-driven methods have been adopted for car-following control for intelligent vehicles.
A car-following control strategy based on self-learning and a supervised-learning hybrid-driven framework was proposed for human-like high-performance car-following control.
First, the car-following data were collected through a real vehicle-testing platform.
The car-following control problem was modeled as a Markov decision process, and a self-learning car-following control strategy was established by using the deep deterministic policy gradient, which is a deep reinforcement-learning method, long short-term memory was adopted to model the state transition of the Markov decision process and predict future states according to historical data, and a car-following reference model with human-driver characteristics was established based on Gaussian mixture regression combined with a hidden Markov model and introduced into the reinforcement-learning architecture.

A self-learning and supervised-learning hybrid-driven architecture was proposed.
The introduction of the human-driver car-following reference model can guide the reinforcement learning to learn the characteristics of a human driver correctly, the personification of the strategy is improved, and the Marko-decision-process state-transition model enables the strategy to consider the state randomness of the car-following process when acceleration decisions are made.

Finally, we used real car-following data to verify the strategy, compared the car-following control performance of the proposed strategy and that of the supervised learning strategy, and a series of quantitative evaluation indexes were provided.

The results demonstrate that the proposed strategy outperforms human drivers in tracking front-vehicle speed and maintaining the ideal following distance, effectively reduces the acceleration shock in the process of car following, achieves satisfactory personification, and ensures safety, tracking, and comfort in car-following.


title: A Reinforcement Learning-Based Data-Driven Voltage Regulator for Wireless Chargers of Electric Vehicles
abstract: As a critical component in the concept of smart cities, autonomous vehicles are under extensive investigation.
The need for intelligent fueling without human assistance stimulates the development of wireless charging.
There are, however, several issues to be addressed to enhance the efficiency and reliability of charging.
The vehicle battery exhibits varying resistance during the charging process.
Additionally, the alignment and gap may change with external positioning, which affects the coupling coefficient of the transmitter and receiver coils.
Thus, a data-driven control scheme is desired to tackle these environmental uncertainties.
This paper adopts the control scheme of embedding a Buck converter at the receiver side.
Different from state-of-the-art literature, a reinforcement learning-based data-driven control approach is employed to regulate the charging voltage.
Stable charging voltage is attained regardless of the knowledge of the coupling coefficient, load variations, or component values.
System simulations in Simulink have proved the effectiveness of the proposed control method.

title: An AI system for automated profiling of driving patterns
abstract: In this project PhD project, in collaboration with a dynamic start-up, Evezy, we will develop an AI system for the automated profiling of driving patterns using historical measurements generated by a large fleet of vehicles and the detection of risky behaviour in real-time.

This aligns in particular with EPSRC research aims for artificial intelligence technologies, complexity science, and infrastructure and urban systems.
Vehicle fleet management businesses rely on safe drivers to support their operations.

Safe drivers are preferable as they have a direct economic and financial impact on the fleet operator's business.
Aggressive driving increases fuel consumption, cost of vehicle maintenance, risk of accidents, cost of insurance, and risk of poor customer experience (e.g.
for taxi related operations).
This work aims to assess sensor information for risk identification, in a manner that is not only accurate but has an explanatory process or assessment criteria.
While something like a deep neural networks may be necessary to process and classify characteristics from a raw image data-set, the implications of the identified characteristics can be described by a myriad of plausible Reinforcement Learning techniques.

Potential impacts or applications include utilising images captured by sensors on automotive vehicles to identify, classify and react to possible threats to driver and pedestrian safety.
The system will implement a metric score reflecting the degree of risk that a driver exhibits at any given time based on a combination of historical as well as current driving data.

The AI system will leverage a large volume of historical data collected by Evezy customers characterising how the vehicles are being operated and in what driving conditions, at a temporal resolution of approximately 2 seconds.

Measurements generated from the on-board telemetry system and other sensors (such as GPS coordinates, acceleration/breaking, front camera video feeds, battery levels, mileage, vehicle's status indicating whether the engine is operating, doors are locked, and others) will be overlaid with publicly available data (such as street maps and places of interest, historical traffic conditions and accidents, speed limits, etc.
)
and mobile usage data (Evezy customers have to use the Evezy app to unlock and start the car, as well as to swap vehicle).
Using this unique historical database, we will implement a robust risk scoring system whereby higher scores indicate riskier driving behaviour, and a tracking system for adaptively assigning risk scores and raising alerts in real-time.
Machine learning methodologies will be used to processes the data to identify key characteristics.

However, a standard black box algorithm approach may not have sufficient capacity to perform this task, the lack of descriptive power can create an impasse to being implemented for real-world purposes where risk to human safety is concerned.

Therefore, while something like a deep neural network may be necessary to process and classify characteristics from a raw image data-set, the implications of the identified characteristics can be described by several possible flavours of Reinforcement Learning (RL); Deep RL, Model-Based RL, or even Hierarchical RL which creates an encoding of a kind of memory into the algorithm so as that sequences of events can be identified as potential problems not just the event at one particular instant of time.

It may also be necessary to compare several different methods and optimise the trade off between computational efficiency and descriptive power.

title: FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT
abstract: Thanks to the rapid growth in wearable technologies, monitoring complex human context becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously.

Nevertheless, a central challenge in designing such personalized IoT applications arises from human variability.
Such variability stems from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change the behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability).

To that end, we propose FaiR-IoT, a general reinforcement learning-based framework for adaptive and fairness-aware human-in-the-loop IoT applications.
In FaiR-IoT, three levels of reinforcement learning agents interact to continuously learn human preferences and maximize the system's performance and fairness while taking into account the intra-, inter-, and multi-human variability.

We validate the proposed framework on two applications, namely (i) Human-in-the-Loop Automotive Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House.
Results obtained on these two applications validate the generality of FaiR-IoT and its ability to provide a personalized experience while enhancing the system's performance by 40%-60% compared to non-personalized systems and enhancing the fairness of the multi-human systems by 1.
5 orders of magnitude.


title: Hierarchical Evasive Path Planning Using Reinforcement Learning and Model Predictive Control
abstract: Motion planning plays an essential role in designing self-driving functions for connected and autonomous vehicles.
The methods need to provide a feasible trajectory for the vehicle to follow, fulfilling different requirements, such as safety, efficiency, and passenger comfort.
In this area, algorithms must also meet strict real-time expectations, since, especially in an emergency, the decision time is limited, which raises a trade-off for the feasibility requirements.
This article proposes a hierarchical path planning solution for evasive maneuvering, where a Twin Delayed DDPG reinforcement learning agent generates the parameters of a geometric path consisting of chlotoids and straight sections, and an underlying model predictive control loop fulfills the trajectory following tasks.

The method is applied to the automotive double lane-change test, a common emergency situation, comparing its results with human drivers' performance using a dynamic simulation environment.
Besides the test's standardized parameters, a broader range of topological layouts is chosen, both for the training and performance evaluation.
The results show that the proposed method highly outperforms human drivers, especially in challenging situations, while meeting the computational requirements, as the pre-trained neural network and path generation algorithm can provide a solution in an instant, based on the experience gained during the training process.


title: DeepTread: Exploring Transfer Learning in Tyre Quality Classification
abstract: In recent years, the automotive industry has witnessed a significant shift towards leveraging advanced technologies for quality control and assessment, with a particular focus on tire quality.
This research study presents "DeepTread," an innovative approach that explores the untapped potential of transfer learning in the domain of tire quality classification.
Transfer learning, a powerful paradigm within deep learning, allows the adaptation of pre-trained neural networks for the purpose of tire quality evaluation.
By leveraging the knowledge gained from various related domains, DeepTread aims to improve the accuracy and efficiency of tire quality classification, thereby contributing to safer and more reliable automotive solutions.

The methodology's effectiveness is validated through extensive experiments, demonstrating promising results and encouraging future developments in the field of tire quality assessment.

title: Soft Actor-Critic DRL for Live Transcoding and Streaming in Vehicular Fog-Computing-Enabled IoV
abstract: With the rapid development of automotive industry and telecommunication technologies, live streaming services in the Internet of Vehicles (IoV) play an even more crucial role in vehicular infotainment systems.

However, it is a big challenge to provide a high quality, low latency, and low bitrate variance live streaming service for vehicles due to the dynamic properties of wireless resources and channels of IoV.

To solve this challenge, we propose a novel live video transcoding and streaming scheme that maximizes the video bitrate and decreases time-delays and bitrate variations in vehicular fog-computing (VFC)-enabled IoV, by jointly optimizing vehicle scheduling, bitrate selection, and computational/spectrum resource allocation.

This joint optimization problem is modeled as a Markov decision process (MDP), considering time-varying characteristics of the available resources and wireless channels of IoV.
A soft actor-critic deep reinforcement learning (DRL) algorithm that is based on the maximum entropy framework, is subsequently utilized to solve the above MDP.
Extensive simulation results based on the data set of the real world show that compared to other baseline algorithms, the proposed scheme can effectively improve video quality while decreasing latency and bitrate variations, and access excellent performance in terms of learning speed and stability.


title: Human-Robot Collaboration in Automotive Assembly
abstract: 

title: Research on Cooperative Path Planning Model of Multiple Unmanned Vehicles in Real Environment
abstract: The cluster combat application of unmanned ground vehicles(UVS)is a hot research issue of the intersection of artificial intelligence and battle command.
Aiming at the cooperative path planning multiple unmanned vehicles not meeting the dynamic threat condition requirement, by combining the global path planning algorithm A-STAR with the local path planning algorithm RL, from the perspective of perception to behavioral decision making, the cooperative path planning model of multiple unmanned vehicles is studied.

The cooperative combat situation threat algorithm, state and action space, reward function and sphere of influence function are designed, the sub-models of formation configuration strategy generation and dynamic optimization of strike path are carried out, and the cooperative path planning control model of multiple unmanned vehicles based on autonomous learning is constructed and solved.

An application example shows that the proposed path planning model can effectively cope with the requirements of multi-unmanned vehicle collaborative path planning task in complex urban environment,and has important theoretical research and practical application value.


title: Circular economy-based reverse logistics: dynamic interplay between sustainable resource commitment and financial performance
abstract: PurposeThe study aims to propose a circular economy-based reverse logistics (CERL) that emphasises the mediation effect of reverse logistics (RL) on sustainable resource commitment and financial performance.
Design/methodology/approachThe structural equation modelling (SEM) approach has been applied to analyse the data acquired through the survey method that included 113 vendors of automotive supplies of the 1st and 2nd levels.
FindingsThe results confirm that CERL acts as an essential intervening entity between resources and financial performance.

The findings of the study have provided research and development (R & D) opportunities for the industries to find alternative revenue streams and generate profit from resource investment whilst upholding environmental standards through reverse logistic practices.
Practical implicationsReverse logistic practices are the key components of a circular business model and a sustainable supply chain.

The manufacturing companies need to explore critical enablers that can contribute to business productivity and financial growth.
Originality/valueThe study has validated a CERL model that portrays the circular economy's resilient relationship with RL practices.


title: A twin delayed deep deterministic policy gradient-based energy management strategy for a battery-ultracapacitor electric vehicle considering driving condition recognition with learning vector quantization neural network
abstract: Deep reinforcement learning algorithms have been widely applied in the energy management of hybrid energy storage systems.
However, these deep reinforcement learning algorithms, such as DQN and DDPG, have the problem of discontinuous action space and consistently overestimated Q values.
To address this issue, a novel energy management strategy based on a twin delayed deep deterministic policy gradient (TD3) algorithm is proposed for the battery-ultracapacitor electric vehicles in this study.

In addition, the driving condition recognition method is integrated into the energy management strategy framework to reduce the training time of the TD3 agent.
The detailed implementation steps are as follows.
At first, dynamic experiments were performed to establish high-precision models of the battery and ultracapacitor.
Secondly, learning vector quantization neural networks are applied to classify driving conditions, namely, urban, suburban and highway conditions.
Furthermore, three parallel TD3 agents are trained for urban, suburban and highway conditions, respectively.
Finally, the proposed strategy is evaluated under standard driving cycles.
The simulation results indicate that compared with the TD3-based strategy, the proposed strategy improves the economy by 1 % and reduces the training time by 34 %, and the economic gap with the dynamic programming-based energy management strategy is narrowed down to 3 %.


title: On the stability analysis of optimal state feedbacks as represented by deep neural models [arXiv]
abstract: Research has shown how the optimal feedback control of several non linear systems of interest in aerospace applications can be represented by deep neural architectures and trained using techniques including imitation learning, reinforcement learning and evolutionary algorithms.

Such deep architectures are here also referred to as Guidance and Control Networks, or G&CNETs.
It is difficult to provide theoretical proofs on the control stability of such neural control architectures in general, and G&CNETs in particular, to perturbations, time delays or model uncertainties or to compute stability margins and trace them back to the network training process or to its architecture.

In most cases the analysis of the trained network is performed via Monte Carlo experiments and practitioners renounce to any formal guarantee.
This lack of validation naturally leads to scepticism especially in cases where safety and validation are of paramount importance such as is the case, for example, in the automotive or space industry.
In an attempt to narrow the gap between deep learning research and control theory, we propose a new methodology based on differential algebra and automated differentiation to obtain formal guarantees on the behaviour of neural based control systems.


title: Iterative Learning Procedure With Reinforcement for High-Accuracy Force Tracking in Robotized Tasks
abstract: The paper focuses on industrial interaction robotics tasks, investigating a control approach involving multiples learning levels for training the manipulator to execute a repetitive (partially) changeable task, accurately controlling the interaction.

Based on compliance control, the proposed approach consists of two main control levels: 1) iterative friction learning compensation controller with reinforcement and 2) iterative force-tracking learning controller with reinforcement.

The learning algorithms rely on the iterative learning and reinforcement learning procedures to automatize the controllers parameters tuning.
The proposed procedure has been applied to an automotive industrial assembly task.
A standard industrial UR 10 Universal Robot has been used, equipped by a compliant pneumatic gripper and a force/torque sensor at the robot end-effector.

title: Adversarial Trajectories Generation for Automotive Applications
abstract: The development of Advanced Driver Assistance Systems (ADAS) with a high level of autonomy requires immense testing efforts to ensure the safety and robustness of developed algorithms in critical situations.

Unfortunately, exploration of difficult situations through test drives in natural traffic is ineffective due to the rarity of such events.
While scenario-based testing in a virtual environment is often proposed as an effective method that helps to evaluate system performance in difficult situations, the manual definition of virtual test scenarios poses a significant challenge itself.

Performance drops in tested systems, especially ones containing machine learning components, may be related to situations that are not necessarily considered challenging for a human driver and thus are difficult to predict in a test design.

In this paper, we propose a method that allows to generate a variety of virtual test scenarios for ADAS through an adversarial trajectories generation.
The method generates scenarios by finding trajectories of the road users in the proximity of the vehicle controlled by the tested algorithm that result in safety-critical events, such as collisions.
We demonstrate the effectiveness of the presented method on an example of a critical scenario generation for a vehicle control policy based on Reinforcement Learning methods.

title: EdgeMap: CrowdSourcing High Definition Map in Automotive Edge Computing
abstract: High definition (HD) map needs to be updated frequently to capture road changes, which is constrained by limited specialized collection vehicles.
To maintain an up-to-date map, we explore crowdsourcing data from connected vehicles.
Updating the map collaboratively is, however, challenging under constrained transmission and computation resources in dynamic networks.
In this paper, we propose EdgeMap, a crowdsourcing HD map to minimize the usage of network resources while maintaining the latency requirements.
We design a DATE algorithm to adaptively offload vehicular data on a small time scale and reserve network resources on a large time scale, by leveraging the multi-agent deep reinforcement learning and Gaussian process regression.

We evaluate the performance of EdgeMap with extensive network simulations in a time-driven end-to-end simulator.
The results show that EdgeMap reduces more than 30% resource usage as compared to state-of-the-art solutions.

title: Improving the Targets' Trajectories Estimated by an Automotive RADAR Sensor Using Polynomial Fitting
abstract: A way to reduce the uncertainty at the output of a Kalman filter embedded into a tracker connected to an automotive RADAR sensor consists of the adaptive selection of parameters during the tracking process.

Different informed strategies for automatically tuning the tracker's parameters and to jointly learn the parameters and state/output sequence using: expectation maximization; optimization approaches, including the simplex algorithm; coordinate descent; genetic algorithms; nonlinear programming using finite differencing to estimate the gradient; Bayesian optimization and reinforcement learning; automatically tuning hyper-parameters in the least squares, were already proposed.

We develop here a different semi-blind post-processing approach, which is faster and more robust.
Starting from the conjecture that the trajectory is polynomial in Cartesian coordinates, our method supposes to fit the data obtained at the output of the tracker to a polynomial.
We highlight, by simulations, the improvement of the estimated trajectory's accuracy using the polynomial fitting for single and multiple targets.
We propose a new polynomial fitting method based on wavelets in two steps: denoising and polynomial part extraction, which compares favorably with the classical polynomial fitting method.
The effect of the proposed post-processing methods is visible, the accuracy of targets' trajectories estimations being hardly increased.

title: LiveMap: Real-Time Dynamic Map in Automotive Edge Computing
abstract: Autonomous driving needs various line-of-sight sensors to perceive surroundings that could be impaired under diverse environment uncertainties such as visual occlusion and extreme weather.
To improve driving safety, we explore to wirelessly share perception information among connected vehicles within automotive edge computing networks.
Sharing massive perception data in real time, however, is challenging under dynamic networking conditions and varying computation work-loads.
In this paper, we propose LiveMap, a real-time dynamic map, that detects, matches, and tracks objects on the road with crowdsourcing data from connected vehicles in sub-second.
We develop the data plane of LiveMap that efficiently processes individual vehicle data with object detection, projection, feature extraction, object matching, and effectively integrates objects from multiple vehicles with object combination.

We design the control plane of LiveMap that allows adaptive offloading of vehicle computations, and develop an intelligent vehicle scheduling and offloading algorithm to reduce the offloading latency of vehicles based on deep reinforcement learning (DRL) techniques.

We implement LiveMap on a small-scale testbed and develop a large-scale network simulator.
We evaluate the performance of LiveMap with both experiments and simulations, and the results show LiveMap reduces 34.1% average latency than the baseline solution.

title: Optimal Priority Rule-Enhanced Deep Reinforcement Learning for Charging Scheduling in an Electric Vehicle Battery Swapping Station
abstract: For a battery swapping station (BSS) with solar generation, N charging bays, and an inventory of M batteries, we study the charging scheduling problem under random EV arrivals, renewable generation, and electricity prices.

To minimize the expected weighted sum of charging cost (sum of electricity and battery degradation costs) and EV owners' waiting cost, we formulate the problem as a Markov decision process with unknown state transition probability.

Under a mild heavy-traffic assumption, we rigorously establish the optimality of the Less Demand First (LDF) priority rule under arbitrary system dynamics: batteries with less demand shall be charged first.

The optimality result enables us to integrate the LDF rule into a state-of-the-art deep reinforcement learning (DRL) method, proximal policy optimization (PPO), reducing the dimensionality of its output from O(M+N) to O(1), without loss of optimality in the heavy-traffic scenario.

Numerical results (on real-world data) demonstrate that the proposed LDF enhanced PPO approach significantly outperforms classical DRL methods and FCFS (first come, first served) priority rule based DRL counterparts.


title: 2011 IEEE International Conference on Control Applications (CCA), part of the IEEE Multi-Conference on Systems & Control (MSC)
abstract: The following topics are dealt with: multi-agent control systems; control engineering computing; energy systems control; industrial control; adaptive control; aerospace applications; reinforcement learning; robust model-based control; tokamak control; automotive applications; control education; iterative learning control; drill string vibration control; Kalman filtering; optimal control; and renewable energy electric grid integration.


title: Evaluation of collection methods in reverse logistics by using fuzzy mathematics
abstract: Purpose - The efficiency and effectiveness of reverse logistics (RL) is dependent on collection methods as the collection activities are critical in determining the economic viability of the entire recovery chain.

The purpose of this paper is to evaluate the various collection methods used in RL under uncertain environment.
Design/methodology/approach - An integrated fuzzy multi-criteria decision model has been developed for the evaluation of various collection methods.
The evaluation has been done based on the criteria of initial investment, value added recovery, return volume, operating cost, degree of supply chain control, and level of customer satisfaction.
The three alternatives used in the study are collection by the manufacturer directly from the customer, collection by the retailer, and collection by the third party.
The fuzzy analytical hierarchy process has been used to compute the criteria weights and fuzzy technique for order preference by similarity to ideal solution has been used to rank the alternative collection methods.

Fuzzy mathematics has been used to take care of uncertainties in the RL.
Findings - Selection and evaluation of alternative collection methods is affected by multiple criteria like initial investment, value added recovery, return volume, operating cost, degree of supply chain control, and level of customer satisfaction.

The utility of the proposed evaluation methodology has been validated by solving a case example from automotive company.
Originality/value - The proposed methodology will provide a useful tool to the decision maker for the evaluation and selection of the alternative collection methods in RL.
This will help companies in strategic decision making to prioritize and develop collection facilities accordingly.

title: Kryptonite : Worst-Case Program Interference Estimation on Multi-Core Embedded Systems
abstract: Due to the low costs and energy needed, cyber-physical systems are adopting multi-core processors for their embedded computing requirements.
In order to guarantee safety when the application has real-time constraints, a critical requirement is to estimate the worst-case interference from other executing programs.
However, the complexity of multi-core hardware inhibits precisely determining the Worst-Case Program Interference.
Existing solutions are either prone to overestimate the interference or are not scalable to different hardware sizes and designs.
In this paper we present Kryptonite, an automated framework to synthesize Worst-Case Program Interference (WCPI) environments for multi-core systems.

Fundamental to Kryptonite is a set of tiny hardware-specific code gadgets that are crafted to maximize interference locally.
The gadgets are arranged using a greedy approach and then molded using a Reinforcement Learning algorithm to create the WCPI environment.
We demonstrate Kryptonite on the automotive grade Infineon AURIX TC399 processor with a wide range of programs that includes a commercial real-time automotive application.
We show that, while being easily scalable and tunable, Kryptonite creates WCPI environments increasing the runtime by up to 58% for benchmark applications and 26% for the automotive application.

title: A Smart Road Side Unit in a Microeolic Box to Provide Edge Computing for Vehicular Applications
abstract: Deployment of technologies for Intelligent Transportation Systems (ITS) involves the installation of Road Side Units (RSU) on the roadway and On-Board Units (OBU) inside vehicles.
In this direction, 5G technologies will make a great impulse providing low latency communication and computation at the edge of the network.
First, this paper defines VMEC-in-a-Box, a smart RSU combined with a MEC station, aimed at providing edge computing for vehicular applications by enabling job offloading from vehicles.
VMEC-in-a-Box is equipped with a microeolic power generator to be autonomous and self-consistent even in presence of low levels of wind.
The behavior of VMEC-in-a-Box is controlled by artificial intelligence to vary its computing capacity dynamically to pursue the best tradeoff between performance and power consumption, and to cooperate by offloading jobs to each other (horizontal offload) to improve performance and reliability of the system.

Hence, the paper defines a Markov model to support decisions to optimize by means of Reinforcement Learning the system behavior according to two reward functions defined at the MEC and at the Vehicular Domains.

To the best of our knowledge, this is the first work proposing an integrated framework to maximize reliability and performance at both Vehicular and MEC Domains.

title: Comparison of Modern Controls and Reinforcement Learning for Robust Control of Autonomously Backing Up Tractor-Trailers to Loading Docks
abstract: 

title: When cyber-physical systems meet AI: a benchmark, an evaluation, and a way forward
abstract: Cyber-Physical Systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc.
In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS.
Despite the popularity of AI-enabled CPS, few benchmarks are publicly available.
There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains.
To bridge this gap, we present a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods.
Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify current challenges and future opportunities.
Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains.

Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS to achieve optimal performance and reliability.
Our benchmark, code, detailed evaluation results, and experiment scripts are available on https://sites.google.com/view/ai-cps-benchmark.

title: Modeling and Dynamic Analysis of Three-Degree-of-Freedom Spherical Actuator under Deep Reinforcement Learning Control
abstract: Multi-degree-of-freedom (multi-DOF) spherical actuators have been developed for the fields of robotics and industrial machinery.
The input current for each coil is calculated using a torque generating equation based on a torque map.
If there is a difference between the analyzed and measured torque maps, this modeling error will reduce positioning accuracy.
Permanent magnet type actuators can generate unexpected cogging torque due to various manufacturing errors.
This manufacturing variation causes a torque map modeling error and significantly deteriorates the performance of conventional control systems using proportional-integral-differential (PID) controllers.

Therefore, we propose a method to improve the positioning accuracy by introducing a compensator using reinforcement learning for the torque map modeling error pre-calculated by a 3-D finite element method (3-D FEM).

We designed a simulation that assumes manufacturing variations in cogging torque and applied this current compensator to test its performance.

title: Vision-Based Eco-Oriented Driving Strategies for Freeway Scenarios Using Deep Reinforcement Learning
abstract: 

title: Deep Reinforcement Learning-based Integrated Control of Hybrid Electric Vehicles Driven by High Definition Map in Cloud Control System
abstract: In the context of the development of intelligence, connectivity, and new energy, the automotive industry combines computer, information communication, artificial intelligence(AI) to achieve integrated development.

Based on the new generation of information and communication technology--cloud control system(CCS) of intelligent and connected vehicles(ICVs), the cloud-level automatic driving of new energy vehicles is realized driven by connected data, which provides innovative planning and control ideas for vehicle driving and power systems.

Firstly, based on the resource platform of CCS, the latitude, longitude, altitude, and weather of the target road are obtained, and a high definition(HD) path model including slope, curvature, and steering angle is established.

Secondly, a deep reinforcement learning(DRL)-based integrated control method for hybrid electric vehicle(HEV) drive by the HD model is proposed.
By adopting two DRL algorithms, the speed and steering of the vehicle and the engine and transmission in the powertrain are controlled, and the synchronous learning of four control strategies is realized.

Finally, processor-in-the-loop(PIL) tests are performed by using the high-performance edge computing device NVIDIA Jetson AGX Xavier.
The results show that under a variable space including 14 states and 4 actions, the DRL-based integrated control strategy realizes the precise control of the speed and steering of the vehicle layer under the high-speed driving cycle of 172 km, and achieves a fuel consumption of 5.
53L/100km.

Meanwhile, it only consumes 104.14s in the PIL test, which verifies the optimization and real-time performance of the learning-based multi-objective integrated control strategy.

title: <i>LiveMap</i>: Real-Time Dynamic Map in Automotive Edge Computing
abstract: Autonomous driving needs various line-of-sight sensors to perceive surroundings that could be impaired under diverse environment uncertainties such as visual occlusion and extreme weather.
To improve driving safety, we explore to wirelessly share perception information among connected vehicles within automotive edge computing networks.
Sharing massive perception data in real time, however, is challenging under dynamic networking conditions and varying computation workloads.
In this paper, we propose LiveMap, a real-time dynamic map, that detects, matches, and tracks objects on the road with crowdsourcing data from connected vehicles in sub-second.
We develop the data plane of LiveMap that efficiently processes individual vehicle data with object detection, projection, feature extraction, object matching, and effectively integrates objects from multiple vehicles with object combination.

We design the control plane of LiveMap that allows adaptive offloading of vehicle computations, and develop an intelligent vehicle scheduling and offloading algorithm to reduce the offloading latency of vehicles based on deep reinforcement learning (DRL) techniques.

We implement LiveMap on a small-scale testbed and develop a large-scale network simulator.
We evaluate the performance of LiveMap with both experiments and simulations, and the results show LiveMap reduces 34.1% average latency than the baseline solution.

title: Multi-Agent Deep Reinforcement Learning for Charging Coordination of Electric Vehicles
abstract: Virtual power plant (VPP) is a power system control center that mediates among the energy market, power grid, distributed energy resources, power storage units, controllable power loads, and electric vehicles (EV s), As energy consumers and wireless communication users, an EV is one of the key components of VPP systems that pose stringent requirements on both charging system management and communication infrastructure.

On one hand, the optimal power flow (OPF) needs to minimize power system loss and voltage variance.
On the other hand, communication quality of service (QoS) needs to be accounted for, in terms of data rate and latency in VPP systems.
These lead long-term spatial EV charging coordination (i.e., charging station (CS) selection) a non-convex optimization problem, which is yet an open issue.
In this regard, by embedding SG millimeter-wave (mm W) access point (AP) into CS, an AP-enabled CS architecture is first devised in this paper to support VPP control information flow and EV communication applications, such as autonomous driving.

Based on the AP-enabled CS, an AP-enabled-CS-assisted multi-agent deep reinforcement learning algorithm (ABC-MADRL) is proposed to provide an intelligent long-term EV charging coordination solution.
Experimental results illustrate that ABC-MADRL dramatically reduces the power loss and voltage variance by 24.3%, compared to the uncoordinated charging.

title: Multi-level Reinforcement Learning for flow control
abstract: Flow control is the process of targeted manipulation of fluid flow fields to accomplish a prescribed objective (e.g.
reduce drag).
Flow control uses information from the flow (provided by sensors) to adapt to incoming perturbations and adjust to changing flow conditions.
General flow control is a largely unsolved mathematical problem appearing in many industries, including automotive, aerospace and environmental subsurface flow problems.
The missing ingredient for turning flow control into a practical tool is the development of general flow control algorithms that can handle the following: (a) uncertainties in the system perturbations (e.
g.

the speed and direction of the perturbation), (b) uncertainties in the flow model parameters, (c) sparsity of the observations (i.e.
partial and noisy observations) (d) modelling errors due to discretization and parameter upscaling.
In this proposal, Reinforcement Learning (RL) algorithms will be utilized to learn general flow control polices using reliable simulated flow environments.

From an application point of view, the developed mathematical techniques address flow control in two applications: (a) increasing energy efficiency in transportation trucks by flow control of incompressible Navier-Stokes flow past an obstacle and (b) safe and efficient storage of anthropogenic carbon dioxide (CO2) in deep geological formations using flow control in a Darcy-type subsurface flow.

For the first application, road freight transportation accounts for approximately 5% of the UK's carbon footprint and flow control to reduce the aerodynamic drag could significantly improve the fuel efficiency, for example a 15% reduction in drag is equivalent to about 5% in fuel savings.

For the CO2 storage application, the produced CO2 by human activities, for example from a power stations or an energy-intensive industries, could be injected into deep saline aquifers as a possible mitigation strategy to reduce anthropogenic emissions of carbon dioxide into the atmosphere.

The control of injection strategies in the subsurface storage sites, given the inherent uncertainties in the subsurface properties, would minimize the risk of leakage while maximising the storage capacity.


title: Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study [arXiv]
abstract: In today's era, autonomous vehicles demand a safety level on par with aircraft.
Taking a cue from the aerospace industry, which relies on redundancy to achieve high reliability, the automotive sector can also leverage this concept by building redundancy in V2X (Vehicle-to-Everything) technologies.

Given the current lack of reliable V2X technologies, this idea is particularly promising.
By deploying multiple RATs (Radio Access Technologies) in parallel, the ongoing debate over the standard technology for future vehicles can be put to rest.
However, coordinating multiple communication technologies is a complex task due to dynamic, time-varying channels and varying traffic conditions.
This paper addresses the vertical handover problem in V2X using Deep Reinforcement Learning (DRL) algorithms.
The goal is to assist vehicles in selecting the most appropriate V2X technology (DSRC/V-VLC) in a serpentine environment.
The results show that the benchmarked algorithms outperform the current state-of-the-art approaches in terms of redundancy and usage rate of V-VLC headlights.
This result is a significant reduction in communication costs while maintaining a high level of reliability.
These results provide strong evidence for integrating advanced DRL decision mechanisms into the architecture as a promising approach to solving the vertical handover problem in V2X.

title: Intelligent Electric Vehicle Charging Scheduling in Transportation-Energy Nexus with Distributional Reinforcement Learning
abstract: Dear Editor, This letter is concerned with electric vehicle (EV) charging scheduling problem in transportation-energy nexus using an intelligent decision-making strategy with probabilistic self-adaptability features.

In order to accommodate the coupling effects of stochastic EV driving behavior on transport network and distribution network, a risk-captured distributional reinforcement learning solution is presented by using explicit probabilistic information for action and reward function in Markov decision process (MDP) model, where the Bellman equation is extended to a more generalized version.

Scheduling EV charging in a transportation-energy nexus, according to both transport and distribution network conditions, is an important topic in recent studies to improve the driving and charging energy efficiency, especially considering the high penetration rate of EV nowadays and even more extremely higher one in the future [1].

In order to accommodate the coupling effects of stochastic EV driving behavior and battery state-of-charge (SoC) on transport and distribution network, various methods have been developed for designing the smart charging scheduling strategy with consideration of electricity price, renewable energy adoption, road conditions and many others.


title: A Deep Reinforcement Learning Approach for Robotic Bicycle Stabilization
abstract: 

title: Solving task scheduling problems in cloud manufacturing via attention mechanism and deep reinforcement learning
abstract: Cloud manufacturing (CMfg) offers a cloud platform for both consumers and providers, and allocating consumer tasks to service providers requires many-to-many scheduling.
Deep reinforcement learning (DRL) has been gradually employed to solve CMfg scheduling problems and has achieved satisfactory performance in dynamic and uncertain cloud environments.
Nonetheless, we need better scheduling algorithms and more accessible modeling methods to enable practical implementation.
In this study, an end-to-end scheduling algorithm is proposed to address task scheduling problems in CMfg.
The proposal extracts intercorrelations in enterprise-enterprise and enterprise-task with the multi-head attention mechanism and is trained by DRL.
Our proposal has extremely low time-response compared to heuristic algorithms and can generate a scheduling solution in seconds.
In contrast to other DRL algorithms, our proposal can achieve better scheduling performances and uses a more accessible modeling method: the objective function alone is sufficient to train the model stably, without the need for a step-based reward function.

Applying multi-head attention and DRL to scheduling problems is an exploratory attempt to achieve positive results.
Experimental results on a case of automobile structure part processing in CMfg indicated that our proposal showed competitive scheduling performances and running time compared to eight DRL algorithms, two heuristic algorithms, and two priority dispatching rules.

Besides, the results proved that our proposal's generalizability and scalability were better than the other eight DRL algorithms.

title: Battery longevity-conscious energy management predictive control strategy optimized by using deep reinforcement learning algorithm for a fuel cell hybrid electric vehicle
abstract: Energy management strategies play an essential role in improving fuel economy and extending battery lifetime for fuel cell hybrid electric vehicles.
However, the traditional energy management strategy ignores the lifetime of the battery for good fuel economy.
To overcome this drawback, a battery longevity-conscious energy manage-ment predictive control strategy is proposed based on the deep reinforcement learning algorithm predictive equivalent consumption minimization strategy (DRL-PECMS) in this study.

To begin with, the back-propagation neural network is devised for predicting demand power, and the predictive equivalent consumption minimum strategy (PECMS) is proposed to improve the hydrogen consumption.

Then, in order to improve the battery durability performance, the deep reinforcement learning algorithm is utilized to optimize the battery power and improve battery lifetime.
Finally, numerical verification and hard-ware in the loop experiments are conducted to validate hydrogen consumption and battery durability performance of the proposed strategy.
The validation results show that, compared with CD/CS and SQP(Sequential Quadratic Programming), the PECMS combined can achieve better fuel economy with the fuel consumption reduction by 55.
6 % and 5.
27 %, which effectively improves the fuel economy.

The DRL-PECMS can reduce the effective Ah-throughput by 3.1 % compared with the PECMS.
The numerous validations and comparisons demonstrate that the proposed strategy effectively accom-plishes the trade-off optimization between energy consumption and battery durability performance.

title: Optimal operation of energy storage system in photovoltaic-storage charging station based on intelligent reinforcement learning
abstract: Optimizing the energy storage charging and discharging strategy is conducive to improving the economy of the integrated operation of photovoltaic-storage charging.
The existing model-driven stochastic optimization methods cannot fully consider the complex operating characteristics of the energy storage system and the uncertainty of photovoltaic power generation and electric vehicle charging load characteristics.

Therefore, an optimal operation method for the entire life cycle of the energy storage system of the photovoltaic-storage charging station based on intelligent reinforcement learning is proposed.
Firstly, the energy storage operation efficiency model and the capacity attenuation model are finely modeled.
Then, the energy storage optimization operation strategy based on reinforcement learning was established with the goal of maximizing the revenue of photovoltaic charging stations, taking into account the uncertainty of electric vehicle charging demand, photovoltaic output, and electricity prices to satisfy the charging requirements and photovoltaic consumption of electric vehicles.

A dual delay depth deterministic strategy gradient algorithm is used to solve the problem because of the continuity of decision-making actions for energy storage charging and discharging.
The model is trained by the actual historical data, and the energy storage charging and discharging strategy is optimized in real time based on the current period status.
Finally, the proposed method and model are tested, and the proposed method is compared with the traditional model-driven method.
The results verify the effectiveness of the proposed method and model, and the revenue of optical storage charging stations throughout their energy storage life cycle is improved.

title: Research on Fast CFD Simulation of Automobile Flow Field Based on Artificial Intelligence
abstract: Fast automotive aerodynamic evaluation can extremely reduce the design cycles of automotive.
This paper establishes a rapid simulation model of the automobile flow field based on the artificial intelligence surrogate model.
The end-to-end network is used to map the geometric model, incoming flow conditions, and result.
The decoder is used to splice high-dimensional and low-dimensional features to achieve feature sharing.
The average MAE error of the optimal model is 5.249%.
The average calculation time of a single example is 1.2968s, which is about 0.62% of CFD solver.
The simulation result demonstrate that the deep learning method can not only accelerate the calculation, but also can improve the design efficiency of automobile aerodynamic profile with high accuracy.


title: 2011 IEEE International Symposium on Intelligent Control (ISIC) part of the IEEE Multi-Conference on Systems & Control (MSC)
abstract: The following topics are dealt with: multiagent system; energy system control; fault diagnostics; industrial systems; embedded control; adaptive control; aerospace application; reinforcement learning; approximation based control; nonzero sum games; robust model based control; fusion plasma control; Tokamaks; automotive applications; control education; repetitive process; iterative learning control; intelligent mechatronics; drill string vibrations; optimization; Kalman filtering; Kalman estimation; optimal control; and renewable energy electric grid integration.


title: Real-Time metadata-driven routing optimization for electric vehicle energy consumption minimization using deep reinforcement learning and Markov chain model
abstract: A real-time, data-driven electric vehicle (EVs) routing optimization to achieve energy consumption minimization is proposed in this work.
The proposed framework utilizes the concept of Double Deep Q-learning Network (DDQN) in learning the maximum travel policy of the EV as an agent.
The policy model is trained to estimate the agent's optimal action per the obtained reward signals and Q-values, representing the feasible routing options.
The agent's energy requirement on the road is assessed following Markov Chain Model (MCM), with Markov's unit step represented as the average energy consumption that takes into consideration the different driving patterns, agent's surrounding environment, road conditions, and applicable restrictions.

The framework offers a better exploration strategy, continuous learning ability, and the adoption of individual routing preferences.
A real-time simulation in the python environment that considered real-life driving data from Google's API platform is performed.
Results obtained for two geographically different drives show that the proposed energy consumption minimization framework reduced the energy utilization of the EVs to reach its intended destination by 5.
89% and 11.
82%, compared with Google's proposed routes originally.

Both drives started at 4.30 PM on April 25th, 2019, in Los Angeles, California, and Miami, Florida, to reach EV's charging stations that are located six miles away from both of the starting locations.

title: Finite Element Analysis of Reinforcement Rocker Part Made from JAC780Y Steel Sheets
abstract: The manufacturing industries for automotive parts aim to develop technologies for reducing vehicle weight in order to decrease fuel consumption.
However, passive safety function for drivers and passengers must not be impaired or should be even improved.
Therefore, advanced high strength steel sheet plays more and more important role in designing automotive components.
Nowadays, prediction of formability for sheet metal stamping has high capability more than the past.
The major challenge is springback prediction.
Moreover, it assists in the tooling design to correctly compensate for springback.
Especially in automotive production, springback effects have been generally exhibited distinct after forming process of the high strength steel sheets.
The springback effect occurred in the deformed state of metal parts must be taken into account by designing any sheet metal panels.
Then, the purpose of the present research is to investigate the springback phenomenon of an automotive part named Reinforcement Rocker RL made from an advanced high strength steel grade JAC780Y, after stamping.

In addition, the tools design has been carried out.
Finite Element (FE) program known as DYNAFORM (based on LS-DYNA solver), has been applied to analyze and improve the springback effect on such forming part.
An anisotropic material model according to type 36 (MAT_036 3-PARAMETER_BARAT) was applied.
The results obtained from simulations were compared with required parts in each section.
Then, the die surface from compensation in 2nd step forming was modified to use.
Finally, the simulation part was verified with the real stamping part.
It was found that the finite element simulation showed high capability for prediction and compensation of springback in high strength steel sheets forming.

title: Automated Production Ramp-up Through Self-Learning Systems
abstract: The ramp-up of production systems is characterised by situations that arise for the first time.
Due to the unpredictability of system behaviour in such situations, instabilities occur that lead to reduced production effectiveness.
In order to deal with the resulting uncertainty, this paper presents an approach for self-directed systems capable of "learning", that is, they adapt their behaviour depending on the signals and changes of the circumfluent world.

The advantages of such systems are significant, as they can react to changing products, production equipment and process constraints, and are able to function in exceptional situations.
The presented concept makes use of reinforcement learning, one of the most general approaches to learning control.
Simulations of three different ramp-up processes are used, where, as a demonstration, robots have to assemble windscreens on a moving truck.
(C) 2016 Published by Elsevier B.V.

title: End-to-End Deep Reinforcement Learning for Lane Keeping Assist [arXiv]
abstract: Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications.

There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment.
Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning.
This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks.

As this is a relatively new area of research for autonomous driving, we will formulate two main categories of algorithms: 1) Discrete actions category, and 2) Continuous actions category.
For the discrete actions category, we will deal with Deep Q-Network Algorithm (DQN) while for the continuous actions category, we will deal with Deep Deterministic Actor Critic Algorithm (DDAC).
In addition to that, We will also discover the performance of these two categories on an open source car simulator for Racing called (TORCS) which stands for The Open Racing car Simulator.
Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction with other vehicles.
Finally, we explain the effect of some restricted conditions, put on the car during the learning phase, on the convergence time for finishing its learning phase.

title: Using Reinforcement Learning and Bayesian Optimization on Problems in Vehicle Dynamics and Random Vibration Environmental Testing
abstract: 

title: Safe Adaptive Deep Reinforcement Learning for Autonomous Driving in Urban Environments. Additional Filter? How and Where?
abstract: Autonomous driving (AD) provides a reliable solution for safe driving by replacing human drivers responsible for the majority of accidents.
The emergence of Machine Learning, specifically Deep Reinforcement Learning (DRL), and its ability to solve complex games proved its potential to address AD challenges.
However, model-free methods still suffer from safety-related issues that can be resolved using safe-DRL approaches.
The addition of model-based safety filters to the learning-based algorithms provides safety bounds on their performance and constraint satisfaction.
In this paper, we investigate the addition of a safety filter based on Model Predictive Control and show an increase in mean testing episode reward by 110% from -75 mean episode reward during testing for 50 episodes for Deep Deterministic Policy Gradient (DDPG) to 7.
758.

We study the impacts of safety filters (7.
758 mean reward), heuristic rules, bounded additive noises (0.
49% performance increase comparing to noise-free case), and exploration (3.
425 mean reward) on the learning algorithm.

We compare the effects of filters in the context of simulated exploration and bounded exploration and prove that bounded exploration results in 9.
86% increase in mean reward and 12.
95% decrease in std comparing to the other method.

Additionally, inspired by Deep Internal Learning and biological mechanisms like brain plasticity, we investigate the idea of using each sample for training only once instead of utilizing stochastic batches which increases the mean testing accumulated reward by 1.
87% and leads to the best performance (7.
942 mean reward and 0.
048 std).

Finally, the results demonstrate better automotive results for our proposed method than DDPG.
Our proposed method, DDPG with safety filter in bounded exploration and adaptive learning under noisy input conditions, has a success rate of 100% under different traffic densities for the simulation environment used in this paper and our assumptions.

The proposed method's automotive results are shown for a braking scenario to avoid collision with other road users.

title: Statistical Methods for Building Robust Spoken Dialogue Systems in an Automobile
abstract: We investigate the potential of statistical techniques for spoken dialogue systems in an automotive environment.
Specifically, we focus on partially observable Markov decision processes (POMDPs), which have recently been proposed as a statistical framework for building dialogue managers (DMs).
These statistical DMs have explicit models of uncertainty, which allow alternative recognition hypotheses to be exploited, and dialogue management policies that can be optimised automatically using reinforcement learning.

This paper presents a voice-based in-car system for providing information about local amenities (e.g.
restaurants).
A user trial is described which compares performance of a trained statistical dialogue manager with a conventional handcrafted system.
The results demonstrate the differing behaviours of the two systems and the performance advantage obtained when using the statistical approach.

title: Transcoding for Live Streaming-based on Vehicular Fog Computing: An Actor-Critic DRL Approach
abstract: As one of the most frequently launched applications in vehicular networks, live-streaming service is gaining momentum from the rapid development of automotive industry and telecommunication technologies.

However, due to the dynamic characteristics of wireless links and topology of vehicular networks, providing high quality, low latency, and low variance video streaming for vehicles is a big challenge.
This study aims to maximize video quality while decreasing time-delays and bitrate switches in vehicular fog computing (VFC) systems, by jointly optimizing vehicle scheduling, bitrate selection, computational and spectrum resource allocation.

Considering the dynamic characteristics of vehicular networks and available computational/spectrum resource, the above problem is modeled as a Markov decision process (MDP).
Since the action space of the MDP is multi-dimensional continuous variables mixed with discrete variables, the study utilizes an actor-critic deep reinforcement learning (DRL) algorithm to resolve the MDP problem.

Extensive simulation results based on the dataset of the real world show the effectiveness of the proposed algorithm.

title: Automatic identification of mechanical parts for robotic disassembly using the PointNet deep neural network
abstract: Identification is the first step towards the manipulation of parts for robotic disassembly and remanufacturing.
PointNet is a recently developed deep neural network capable of identifying objects from 3D scenes (point clouds) irrespective of their position and orientation.
PointNet was used to recognise 12 instances of components of turbochargers for automotive engines.
These instances included different mechanical parts, as well as different models of the same part.
Point clouds of partial views of the parts were created from CAD models using a purpose-developed depth-camera simulator, reproducing various levels of sensor imprecision.
Experimental evidence indicated PointNet can be consistently trained to recognise with accuracy the objects.
In the presence of sensor imprecision, the accuracy in the recall phase can be increased adding stochastic error to the training examples.
Training 12 independent classifiers, one for each part, did not yield significant improvements in accuracy compared to using one classifier for all the parts.
[Submitted 13 September 2019; Accepted 27 March 2020]

title: Proposition of a Framework for Classifying Returnable Transport Items Closed-Loop/Reverse Logistics Issues
abstract: In this paper, we propose a conceptual framework for classifying operations management-related issues for returnable transport items (RTI).
RTI closed-loop supply chain (CLSC) issues are classified into three categories.
Our proposal is based on a global literature review in the field of CLSC and reverse logistics (RL), with a specific focus on transport items reuse.
We also derive our work from observations made in the automotive industry.
The motivation for dealing with this topic is that more and more companies are adopting RTIs in their supply chain, and despite the growing attention that CLSC and RL research have gained during the last years, the focus is still more oriented toward remanufacturing issues.

The objectives of this paper are twofold.
Firstly, to provide practitioners with a clear vision on different issues that arise when implementing a RTI system from an operations management point of view, and secondly, to give suggestions to researchers regarding existing gaps that still need to be filled in this field.

(C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.

title: Time Efficient Offloading Optimization in Automotive Multi-Access Edge Computing Networks Using Mean-Field Games
abstract: Emerging connected vehicular services, such as intelligent driving and high-definition (HD) map, are gaining increasing interest with the fast development of multi-access edge computing (MEC).
For most time-sensitive and computation-intensive vehicular services, the data offloading process significantly influences the capacity and performance of MEC, especially when the number of connected vehicles is enormous.

In this work, we consider data offloading optimization for a large-scale automotive MEC network.
The problem is challenging due to the large number of connected vehicles and the complicated interaction between vehicles and edge servers.
To tackle the scalability problem, we reformulate the original offloading optimization problem into a Mean-Field-Game (MFG) problem by abstracting the interaction among the connected vehicles as a distribution over their state spaces of task sizes, known as the mean-field term.

To solve the problem efficiently, we propose a G-prox Primal-Dual-Hybrid-Gradient (PDHG) algorithm that transforms the MFG problem into a saddle-point problem.
Based on our developed MFG model and G-prox PDHG algorithm, we propose the first data offloading scheme whose computation time is independent of the number of connected vehicles in automotive MEC systems.

Extensive evaluation results corroborate the superior performance of our proposed scheme compared with the state-of-the-art methods.

title: A Simulation Environment of Solar-Wind Powered Electric Vehicle Car Park for Reinforcement Learning and Optimization
abstract: In accordance with the United Kingdom's goal to reach net zero by 2050, electric vehicles (EVs) play a crucial role in transportation.
However, if the electricity used to charge EVs is derived from fossil fuels, this does not necessarily imply a reduction of overall emissions nationally or globally.
To achieve optimal EV charging, a deeper comprehension of the unpredictability of on-site renewable energy sources (ORES) energy output is required.
In this paper, the predicted renewable energy generated is used as the actual value for the reinforcement learning algorithm simulation environment.
Such a model can represent the relationship between the power generation and the wind speed as well as solar irradiation, which are characterized by significant uncertainties.
The uncertainty analysis shows that the wind speed at Newcastle upon Tyne can be modelled as a Weibull distribution with parameters A = 19.98 and B = 1.91.
As for energy demand, this paper integrates information from an Oslo (Norway) car parking garage-based set of EV charging stations with EVs' demand statistics.
The charging habits of EV users range from 800 min to 1,000 min of parking time, and from 5 kWh to 20 kWh in terms of charging energy.
The maximum connection frequency for EV charging is 20 min.
In addition, this paper develops methods for stochastic EV charging and parking space occupancy employing actual data.
Based on the aforesaid renewable energy generation and the EV charging status, it is possible to develop a decision algorithm to optimal renewable energy efficiency.

title: Efficient statistical validation with edge cases to evaluate Highly Automated Vehicles
abstract: The widescale deployment of Autonomous Vehicles (AV) seems to be imminent despite many safety challenges that are yet to be resolved.
It is well known that there are no universally agreed Verification and Validation (VV) methodologies to guarantee absolute safety, which is crucial for the acceptance of this technology.
Existing standards focus on deterministic processes where the validation requires only a set of test cases that cover the requirements.
Modern autonomous vehicles will undoubtedly include machine learning and probabilistic techniques that require a much more comprehensive testing regime due to the non-deterministic nature of the operating design domain.

A rigourous statistical validation process is an essential component required to address this challenge.
Most research in this area focuses on evaluating system performance in large scale real-world data gathering exercises (number of miles travelled), or randomised test scenarios in simulation.
This paper presents a new approach to compute the statistical characteristics of a systems behaviour by biasing automatically generated test cases towards the worst case scenarios, identifying potential unsafe edge cases.

We use reinforcement learning (RL) to learn the behaviours of simulated actors that cause unsafe behaviour measured by the well established RSS safety metric.
We demonstrate that by using the method we can more efficiently validate a system using a smaller number of test cases by focusing the simulation towards the worst case scenario, generating edge cases that correspond to unsafe situations.


title: Cloud-based health-conscious energy management of hybrid battery systems in electric vehicles with deep reinforcement learning
abstract: In order to fulfill the energy and power demand of battery electric vehicles, a hybrid battery system with a high-energy and a high-power battery pack can be implemented as the energy source.
This paper explores a cloud-based multi-objective energy management strategy for the hybrid architecture with a deep deterministic policy gradient, which increases the electrical and thermal safety, and meanwhile minimizes the system's energy loss and aging cost.

In order to simulate the electro-thermal dynamics and aging behaviors of the batteries, models are built for both high-energy and high-power cells based on the characterization and aging tests.
A cloud-based training approach is proposed for energy management with real-world vehicle data collected from various road conditions.
Results show the improvement of electrical and thermal safety, as well as the reduction of energy loss and aging cost of the whole system with the proposed strategy based on the collected real-world driving data.

Furthermore, processor-in-the-loop tests verify that the proposed strategy can achieve a much higher convergence rate and a better performance in terms of the minimization of both energy loss and aging cost compared with state-of-the-art learning-based strategies.


title: Resource Allocation for V2X Assisted Automotive Radar System based on Reinforcement Learning
abstract: Due to the complexity and variability of road traffic scenarios and the existence of blind spots in car radar detection, radar performance of single vehicle is limited.
To address this issue, we propose a joint communication and radar sensing (JCR) system for Intelligent Connected Vehicles (ICVs), where communication is used to assist in reducing the miss detection probability.

Based on this, we model the time resource allocation problem as a Markov Decision Process (MDP), and design the Q-learning and the Double Deep Q-learning Network (DDQN) algorithms to optimize the allocation of time resources for radar and communication functions dynamically.

The simulation results show that compared with the Round-robin algorithm, the Q-learning and the DDQN algorithms can increase the communication data throughput by more than 6.
6% and reduce the miss detection probability by more than 29.
4%.

The miss detection probability of the system using the assisted mode is 10.7%similar to 17.2% lower than that of the system without it.

title: Deep Reinforcement Learning for Fog Computing-based Vehicular System with Multi-operator Support
abstract: This paper studies the potential performance improvement that can be achieved by enabling multi-operator wireless connectivity for cloud/fog computing-connected vehicular systems.
Mobile network operator (MNO) selection and switching problem is formulated by jointly considering switching cost, quality-of-service (QoS) variations between MNOs, and the different prices that can be charged by different MNOs as well as cloud and fog servers.

A double deep Q network (DQN) based switching policy is proposed and proved to be able to minimize the long-term average cost of each vehicle with guaranteed latency and reliability performance.
The performance of the proposed approach is evaluated using the dataset collected in a commercially available city-wide LTE network.
Simulation results show that our proposed policy can significantly reduce the cost paid by each fog/cloud-connected vehicle with guaranteed latency services.

title: Trajectory Planning for Autonomous Vehicles combining Nonlinear Optimal Control and Supervised Learning
abstract: This paper considers computationally efficient planning of reference trajectories for autonomous on-road vehicles in a cooperative setting.
The basic element of the approach is the notion of so-called maneuvers, which allow to cast the nonlinear and non-convex planning task into a highly structured optimal control problem.
This can be solved quite efficiently, but not fast enough for online operation when considering nonlinear vehicle models.
Therefore, the approach proposed in this paper aims at approximating solutions using a supervised learning approach: First, training data are generated by solving optimal control problems and are then used to train a neural network.

As is demonstrated for a cooperative overtaking maneuver, this approach shows good performance, while (contrasting approaches like reinforcement learning) requiring only low training effort.
Copyright (C) 2020 The Authors.

title: Use of AI methods for clutch development in automotive drivetrains
abstract: Current development methods for modeling and optimization for vehicle clutches reach their limits in the field of rising expectations of ride comfort and energy efficiency.
This article examines the use of AI methods for clutch development and provides an overview based on various application examples in current Mercedes-Benz AG research projects.
By means of supervised learning and deep neural networks, a friction coefficient model and a temperature model of a clutch with high accuracy are developed.
Reinforcement learning with deep neural networks is used to synthesize controllers for various gear changes.
Vehicle measurement data is analyzed using cluster algorithms to derive action recommendations for the application of the engine restart of a hybrid drivetrain.
The methods shown increase the automation potential in development and may reduce the effort required to adopt complex development processes for new transmission variants and generations.

title: Compact integrated slot array antennas for the 79 GHz automotive band
abstract: This paper presents two compact slot array antennas for the 79 GHz automotive band integrated into a 28 mu m thick benzocyclobutene (BCB) substrate attached to a 325 mu m thick 5-20 Omega cm bulk resistivity silicon wafer.

The two antennas are a transmit (TX) 1 x 8 slot array antenna with a size of 1 x 23 mm and a receive (RX) 8 x 8 slot array antenna with a size of 15 x 23 mm.
Promising performance has been measured with the TX and RX subarray antennas (gain >4 dBi) with good matching to 50 Omega (RL > 10 dB) in the frequency range 70-90 GHz.
By using a metal cavity, mounted on the back of the antenna, potential parallel plate or cavity modes could be reduced and the gain could also be increased by 2 dB.
The measured and the simulated results are consistent.

title: Energy and Emission Management of Hybrid Electric Vehicles using Reinforcement Learning
abstract: The electrification of drivetrains of conventional vehicles plays a decisive role in reducing fuel consumption.
At the same time decreasing pollutant emission limits must be met also under real driving conditions.
This trade-off between fuel consumption and pollutant emissions needs to be optimized, which results in powertrains with increasing complexity.
A holistic energy and emission management is needed to control such systems in a way that the fuel consumption is minimized while emission limits are respected.
Mathematical optimization methods are difficult to apply in real-time applications due to high computational and calibration demands.

Self-learning algorithms, on the other hand, seem to be a suitable solution for such optimization problems.
In this paper a control strategy for a hybrid electrical vehicle is presented, consisting of a decision-making agent, trained on different test drives with Reinforcement Learning.

For these, the Proximal Policy Optimization method was applied.
The strategy controls the torque-split between the combustion engine and electric motor, the power of an electrically heated catalyst and internal engine measures.
The method is demonstrated in a simulation framework based on a Diesel P0-HEV with a SCR exhaust gas aftertreatment system.
In comparison to a reference strategy a fuel reduction of 3.1 % averaged over the test data set was achieved.
(C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.

title: Learning and Meta-Learning for Coordination of Autonomous Unmanned Vehicles A Preliminary Analysis
abstract: We study models of coordination, negotiation and collaboration in multi-agent systems (MAS).
More specifically, we investigate scalable models and protocols for various distributed consensus coordination problems in large-scale MAS.
Examples of such problems include conflict avoidance, leader election and coalition formation.
We are particularly interested in application domains where robotic or unmanned vehicle agents interact with each other in real-time, as they try to jointly complete various tasks in complex dynamic environments, and where decisions often need to be made "on the fly".

Such MAS applications, we argue, necessitate a multi-tiered approach to learning how to coordinate effectively.
One such collaborative MAS application domain is ensembles of autonomous micro unmanned aerial vehicles (micro-UAVs).
A large ensemble of micro-UAVs on a complex, multi-stage mission comprised of many diverse tasks with varying time and other resource requirements provides an excellent framework for studying multi-tiered learning how to better coordinate.

A variety of tasks and their resource demands, complexity and unpredictability of the overall environment, types of coordination problems that the UAVs may encounter in the course of their mission, multiple time scales at which the overall system can use learning and adaptation in order to perform better in the future, and multiple logical and organizational levels at which large ensembles of micro-UAVs can be analyzed and optimized, all suggest the need for a multi-tiered approach to learning.

We outline our theoretical and conceptual framework that integrates reinforcement learning and meta-learning, and discuss potential benefits that our framework could provide for enabling autonomous micro-UAVs (and other types of autonomous vehicles) to coordinate more effectively.


title: Vehicle Speed Aware Computing Task Offloading and Resource Allocation Based on Multi-Agent Reinforcement Learning in a Vehicular Edge Computing Network [arXiv]
abstract: For in-vehicle application, the vehicles with different speeds have different delay requirements.
However, vehicle speeds have not been extensively explored, which may cause mismatching between vehicle speed and its allocated computation and wireless resource.
In this paper, we propose a vehicle speed aware task offloading and resource allocation strategy, to decrease the energy cost of executing tasks without exceeding the delay constraint.
First, we establish the vehicle speed aware delay constraint model based on different speeds and task types.
Then, the delay and energy cost of task execution in VEC server and local terminal are calculated.
Next, we formulate a joint optimization of task offloading and resource allocation to minimize vehicles' energy cost subject to delay constraints.
MADDPG method is employed to obtain offloading and resource allocation strategy.
Simulation results show that our algorithm can achieve superior performance on energy cost and task completion delay.

title: Driving-Behavior-Aware Optimal Energy Management Strategy for Multi-Source Fuel Cell Hybrid Electric Vehicles Based on Adaptive Soft Deep-Reinforcement Learning
abstract: The majority of existing energy management strategies (EMSs), merely considering external driving conditions, often allocate demand power in an irrational way, resulting in a waste of energy and a short service life of power sources.

Therefore, it is necessary to integrate driving behavior in EMS to reduce the fuel consumption and improve the lifespan of power sources.
In this paper, a driving-behavior-aware adaptive deep-reinforcement-learning (DRL) based EMS is proposed for a three-power-source fuel cell hybrid electric vehicle (FCHEV).
To fully utilize each power source, a hierarchical power splitting method is adopted by an adaptive fuzzy filter.
Then, a high-performance driving behavior recognizer is employed, and Pontryagin's minimum principle (PMP) method is used to compute the optimal equivalent factor (EF) of each driving behavior.
To realize a trade-off between global learning and real-time implementation, an improved multi-learning-space DRL-based algorithm, applying driving-behavior-aware adaptive equivalent consumption minimization strategy (A-ECMS) and soft learning mechanism, is proposed and verified by a series of simulations.

Simulation results show that, compared with the benchmark method ECMS, the proposed P-DQL method can reduce the hydrogen consumption by 49.
9% on average, and the total cost to use by 31.
4% , showing a promising ability to increase fuel economy and reduce hydrogen consumption and the total cost to use of FCHEV.


title: Magnetic Field during Wireless Charging in an Electric Vehicle According to Standard SAE J2954
abstract: The Society of Automotive Engineers (SAE) Recommended Practice (RP) J2954 (November 2017) was recently published to standardize the wireless power transfer (WPT) technology to recharge the battery of an electric vehicle (EV).

The SAE J2954 RP establishes criteria for interoperability, electromagnetic compatibility (EMC), electromagnetic field (EMF) safety, etc.
The aim of this study was to predict the magnetic field behavior inside and outside an EV during wireless charging using the design criteria of SAE RP J2954.
Analyzing the worst case configurations of WPT coils and EV bodyshell by a sophisticated software tool based on the finite element method (FEM) that takes into account the field reflection and refraction of the metal EV bodyshell, it is possible to numerically assess the magnetic field levels in the environment.

The investigation was performed considering the worst case configurationa small city car with a Class 2 WPT system of 7.7 kVA with WPT coils with maximum admissible ground clearance and offset.
The results showed that the reference level (RL) of the International Commission on Non-Ionizing Radiation Protection (ICNIRP) guidelines in terms of magnetic flux density was exceeded under and beside the EV.

To mitigate the magnetic field, the currents flowing through the WPT coils were varied using the inductor-capacitor-capacitor (LCC) compensation instead of the traditional series-series (SS) compensation.

The corresponding calculated field was compliant with the 2010 ICNIRP RL and presented a limited exceedance of the 1998 ICNIRP RL.
Finally, the influence of the body width on the magnetic field behavior adopting maximum offset was investigated, demonstrating that the magnetic field emission in the environment increased as the ground clearance increased and as the body width decreased.


title: Artificial Intelligence Supported Cellular-Connected Extended Reality
abstract: 

title: A generalized energy management framework for hybrid construction vehicles via model-based reinforcement learning
abstract: Hybrid construction vehicles (HCVs) have more specific tasks and highly repetitive patterns than on-road ve-hicles.
Consequently, they are more suitable for model-based energy management.
However, distinctions be-tween work cycles result in adverse scenarios for generalizing model-based energy management.
In this study, we solve this problem by proposing a generalized strategy using a model-based reinforcement learning framework.
The generalized design highlights three aspects: 1) long-term stability, 2) self-learning ability, and 3) state transition model reuse.
A reward function with a trend term is proposed to avoid the cumulative errors between operation cycles and improve the long-term stability of learning.
In addition, Gaussian process regression is leveraged to approximate the value function, thereby reducing the computational load and improving the learning efficiency.
To further enhance the reusability of the environmental model, a modelling method based on the Gaussian mixture model is put forward.
Finally, a generalized HCV energy management framework that includes offline and online learning is designed, where a pre-learning model and an approximation function are adopted for reuse and dynamic learning.

Simulation results demonstrate the superiority of the proposed framework to conventional model-based methods in terms of stability, generality, and adaptability, accompanied by a reduction of 5.
9% in fuel consumption.


title: An Intelligent Predictive Maintenance Approach Based on End-of-Line Test Logfiles in the Automotive Industry
abstract: Through technological advents from Industry 4.
0 and the Internet of Things, as well as new Big Data solutions, predictive maintenance begins to play a strategic role in the increasing operational performance of any industrial facility.

Equipment failures can be very costly and have catastrophic consequences.
In its basic concept, Predictive maintenance allows minimizing equipment faults or service disruptions, presenting promising cost savings.
This paper presents a data-driven approach, based on multiple-instance learning, to predict malfunctions in End-of-Line Testing Systems through the extraction of operational logs, which, while not designed to predict failures, contains valid information regarding their operational mode over time.

For the case study performed, a real-life dataset was used containing thousands of log messages, collected in a real automotive industry environment.
The insights gained from mining this type of data will be shared in this paper, highlighting the main challenges and benefits, as well as good recommendations, and best practices for the appropriate usage of machine learning techniques and analytics tools that can be implemented in similar industrial environments.


title: Autonomous Decision-Making in Interdependent Computing Systems Based on Artificial Intelligence
abstract: 

title: Semantic exploitation of implicit patent information
abstract: In recent years patents have become increasingly important for businesses to protect their intellectual capital and as a valuable source of information.
Patent information is, however, not employed to its full potential and the interpretation of structured and unstructured patent information in large volumes remains a challenge.
We address this by proposing an integrated interdisciplinary approach that uses natural language processing and machine learning techniques to formalize multilingual patent information in an ontology.
The ontology further contains patent and domain specific knowledge, which allows for aligning patents with technological fields of interest and other business-related artifacts.
Our empirical evaluation shows that for categorizing patents according to well-known technological fields of interest, the approach achieves high accuracy with selected feature sets compared to related work focussing on monolingual patents.

We further show that combining OWL RL reasoning with SPARQL querying over the patent knowledge base allows for answering complex business queries and illustrate this with real-world use cases from the automotive domain.


title: A planning model for the optimisation of the end-of-life vehicles recovery network
abstract: European Directive 2000/53 on end-of-life vehicles (ELVs) introduced new constraints, regulations and recommendations on the vehicle design, waste disposal and take-back policy.
Automotive original equipment manufacturers (OEMs) are even more involved in the expected vehicle recovery network modifications.
As a consequence, one of the most critical and challenging issues for reverse logistics (RL) management is the development of effective tools to support strategic, tactical and operational decisions to yield the maximum economic benefit in compliance with the reference regulations.

In this paper, a mixed integer linear programming (MILP) model for ELV closed-loop network design is proposed.
The network cost minimisation objective function, the integration of forward and reverse logistics and the inclusion of remanufacturing activities for vehicle module reuse represent the model key features.

The proposed model is, further, applied on a realistic Italian case-study and the results of a sensitivity analysis are presented to identify the parameters most affecting the model outcomes.

title: Multi-player Bandits for Distributed Cognitive Radar
abstract: With new applications for radar networks such as automotive control and indoor localization, the need for spectrum sharing and general interoperability is expected to rise.
This paper describes the application of multi-player bandit algorithms for waveform selection to a distributed cognitive radar network that must coexist with a communications system.
Specifically, we make the assumption that radar nodes in the network have no dedicated communication channel.
As we will discuss later, nodes can communicate indirectly by taking actions which intentionally interfere with other nodes and observing the resulting collisions.
The radar nodes attempt to optimize their own spectrum utilization while avoiding collisions, not only with each other, but with the communications system.
The communications system is assumed to statically occupy some subset of the bands available to the radar network.
First, we examine models that assume each node experiences equivalent channel conditions, and later examine a model that relaxes this assumption.

title: Toward Intelligent Connected E-Mobility: Energy-Aware Cooperative Driving With Deep Multiagent Reinforcement Learning
abstract: In recent years, electrified mobility (e-mobility), especially connected and autonomous electric vehicles (CAEVs), has been gaining momentum along with the rapid development of emerging technologies such as artificial intelligence (AI) and Internet of Things.

The social benefits of CAEVs are manifested in the form of safer transportation, lower energy consumption, and reduced congestion and emissions.
Nevertheless, it is highly difficult to design driving policies that ensure road safety, travel efficiency, and energy conservation for all CAEVs in traffic flows, particularly in a mixed-autonomy scenario where both CAEVs and human-driven vehicles (HDVs) are on the road and interact with each other.

Here we present a novel deep multiagent reinforcement learning (DMARL)-enabled energy-aware cooperative driving solution, facilitating CAEVs to learn vehicular platoon management policies for guaranteeing overall traffic flow performance.

Specifically, with the aid of information communication technology (ICT), CAEVs can share their vehicle state and learned knowledge, such as their state of charge (SoC), speed, and driving policies.
Additionally, a cooperative multiagent actor-critic (CMAAC) technique is developed to optimize vehicular platoon management policies that map perceptual information directly to the group decision-making behaviors for the CAEV platoon.

The proposed approach is evaluated in highway on-ramp merging scenarios with two different mixed-autonomy traffic flows.
The results demonstrate the benefits of our scheme.
Finally, we discuss the challenges and potential research directions for the proposed energy-aware cooperative driving solution.

title: Design and Improvement of SD3-Based Energy Management Strategy for a Hybrid Electric Urban Bus
abstract: With the rapid development of machine learning, deep reinforcement learning (DRL) algorithms have recently been widely used for energy management in hybrid electric urban buses (HEUBs).
However, the current DRL-based strategies suffer from insufficient constraint capability, slow learning speed, and unstable convergence.
In this study, a state-of-the-art continuous control DRL algorithm, softmax deep double deterministic policy gradients (SD3), is used to develop the energy management system of a power-split HEUB.
In particular, an action masking (AM) technique that does not alter the SD3 ' s underlying principles is proposed to prevent the SD3-based strategy from outputting invalid actions that violate the system's physical constraints.

Additionally, the transfer learning (TL) method of the SD3-based strategy is explored to avoid repetitive training of neural networks in different driving cycles.
The results demonstrate that the learning performance and learning stability of SD3 are unaffected by AM and that SD3 with AM achieves control performance that is highly comparable to dynamic planning for both the CHTC-B and WVUCITY driving cycles.

Aside from that, TL contributes to the rapid development of SD3.
TL can speed up SD3's convergence by at least 67.61% without significantly affecting fuel economy.

title: An Online Reinforcement Learning Approach for Dynamic Pricing of Electric Vehicle Charging Stations
abstract: The global market share of electric vehicles (EVs) is on the rise, resulting in a rapid increase in their charging demand in both spatial and temporal domains.
A remedy to shift the extra charging loads at peak hours to off-peak hours, caused by charging EVs at public charging stations, is an online pricing strategy.
This paper presents a novel combinatorial online pricing strategy that has been established upon a reward-based model to prevent network instability and power outages.
In the proposed solution, the utility provides incentives to the charging stations for their contributions in the EVs charging load shifting.
Then, a constraint optimization problem is developed to minimize the total charging demand of the EVs during peak hours.
To control the EVs charging demands in supporting utility's stability and increasing the total revenue of the charging stations, treated as a multi-agent framework, an online reinforcement learning model is developed which is based on the combination of an adaptive heuristic critic and recursive least square algorithm.

The effective performance of the proposed model is validated through extensive simulation studies such as qualitative, numerical, and robustness performance assessment tests.
The simulation results indicate significant improvement in the robustness and effectiveness of the proposed solution in terms of utility's power saving and charging stations' profit.

title: Enhancing continuous control of mobile robots for end-to-end visual active tracking
abstract: In the last decades, visual target tracking has been one of the primary research interests of the Robotics research community.
The recent advances in Deep Learning technologies have made the exploitation of visual tracking approaches effective and possible in a wide variety of applications, ranging from automotive to surveillance and human assistance.

However, the majority of the existing works focus exclusively on passive visual tracking, i.
e.
, tracking elements in sequences of images by assuming that no actions can be taken to adapt the camera position to the motion of the tracked entity.

On the contrary, in this work, we address visual active tracking, in which the tracker has to actively search for and track a specified target.
Current State-of-the-Art approaches use Deep Reinforcement Learning (DRL) techniques to address the problem in an end-to-end manner.
However, two main problems arise: (i) most of the contributions focus only on discrete action spaces, and the ones that consider continuous control do not achieve the same level of performance; and (ii) if not properly tuned, DRL models can be challenging to train, resulting in considerably slow learning progress and poor final performance.

To address these challenges, we propose a novel DRL-based visual active tracking system that provides continuous action policies.
To accelerate training and improve the overall performance, we introduce additional objective functions and a Heuristic Trajectory Generator (HTG) to facilitate learning.
Through extensive experimentation, we show that our method can reach and surpass other State-of-the-Art approaches performances, and demonstrate that, even if trained exclusively in simulation, it can successfully perform visual active tracking even in real scenarios.

(C) 2021 Elsevier B.V. All rights reserved.

title: Dual Balancing of SoC/SoT in Smart Batteries Using Reinforcement Learning in Uppaal Stratego
abstract: Battery packs in electric vehicles are managed by battery management systems that influence the state of charge among the cells in the pack, where such systems have received much attention in research.

More recently, balancing the temperature among the cells has become a research topic.
In our work, we consider a dual-balancing problem where we aim to balance both the parameters of the state of charge and temperature.
We consider a Smart Battery Pack, where individual cells can be bypassed, meaning that no current is going to or from the cell, which allows the cell to cool off while the cell does not charge or discharge.

Moreover, a smart battery pack can estimate each cell's characteristics, which, in turn, can be used to define a model of cell and battery pack behavior.
We conduct experiments using the model of a battery pack where each cell differs in its configuration as an effect of aging.
For such a pack with heterogeneous cells, we use Q- Learning in U ppaal Stratego to synthesize a controller that maximizes the time spent in a balanced state, meaning that all cells' states are within a specific range of each other.

We show significant improvements in two aspects compared with two threshold-based controllers that balance either state of charge or temperature.
The synthesized controllers are only unbalanced with the state of charge between 1-4% of the time and for temperature between 15-20% of the time.
The threshold-based controllers are either unbalanced for the state of charge for as much as 37 % of the time or for temperature for as much as 44 % of the time.
Finally, the maximum variations of state of charge and temperature among the cells are decreased.

title: MDE and Learning for flexible Planning and optimized Execution of Multi-Robot Choreographies
abstract: Multi-Robot systems in automotive are safety-critical systems that consist of collaborating-aware robots and components that interact with external components, the environment, or humans at run-time.
This implies a significant complexity for the system engineer to design, model, validate the system, and optimize the cycle time, including considering unexpected events at run-time.
This paper addresses this challenge by describing a model-driven engineering approach that formally designs the system under the consideration of uncertainties and at run-time optimizes the system actions using learning-based approaches.

We implemented this approach in an industrial-inspired case study of a spot-welding multi-robot cell.
Based on the system requirements, we generate valid system strategies that consider unexpected events such as robot interruptions and failures.
Considering movement and interruption time models, we implemented a reinforcement learning method to optimize system actions at run-time.
We show that via simulations and learning, our approach can be used to synthesize time-efficient schedules for robot task assignments that improve the overall cycle time.

title: INCENTIVES AND PERFORMANCE IN LARGE-SCALE LEAN SOFTWARE DEVELOPMENT <i>An Agent</i>-<i>based Simulation Approach</i>
abstract: The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade.
However, a simple transfer of good practices from the automotive industry combined with experiences from agile development on a team level is not possible due to fundamental differences stemming from the particular domain specifics - i.
e.

different types of products and components (material versus immaterial goods), knowledge work versus production systems as well as established business models.
Especially team empowerment and the absence of a a hierarchical control on all levels impacts goal orientation and business optimization.
In such settings, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity of central importance.
Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes.

We (ii) compare different incentive schemes ranging from individual to team-based compensation.
Based on our results, we are (iii) able to provide recommendations on how to design such incentives, what their effect is, and how to chose an adequate development structure to foster overall software product development flow by means of more economic decisions and thus resulting in a shorter time to market.

For calibrating our simulation, we rely on practical experience from a very large software company piloting and implementing lean and agile for about three years.

title: An Improved Energy Management Strategy for Hybrid Electric Vehicles Integrating Multistates of Vehicle-Traffic Information
abstract: This study aims to answer the key question for hybrid electric vehicles (HEVs) on how to manage the power flow in HEVs with recent intelligent and electrified upgrades in automotive industries.
The new energy management strategy (EMS) needs to fuse both the physic and cyber systems, reflecting the dynamic vehicle system in the physical layer, as well as taking full advantage of the outside information in the cyber layer.

Given that, this article proposes the cyber-physical system (CPS)-based EMS using deep reinforcement learning (DRL) in two different types of vehicles [hybrid electric bus (HEB) and Prius].
Under the proposed framework, exploratory training is carried out for the EMS which is mediated by DRL algorithms, expert prior knowledge and multistate of traffic information.
Then, the prior valid knowledge trained by HEB is applied to Prius based on deep transfer learning, accelerating the new EMS convergence and ensuring the same initial parameters of the two vehicles' deep neural networks.

Moreover, the cyber information is decoupled from the vehicle itself, for the first time being visualized for comparative analysis.
The results show a significant improvement by considering traffic states (TS) and using dynamic programming (DP) as a benchmark with 6.
94% fuel economy improvement for deep deterministic policy gradients (DDPG) test results and 8.
12% for deep Q-learning (DQL), respectively.

The decoupling analysis distinguished the effect of various TS for the HEB and Prius due to their different characteristics in vehicle service, driving style, and vehicle structures, which further verifies the effectiveness of the proposed EMS.


title: Eco-Driving Cruise Control for 4WIMD-EVs Based on Receding Horizon Reinforcement Learning
abstract: Aiming to improve the distance per charge of four in-wheel independent motor-drive electric vehicles in intelligent transportation systems, a hierarchical energy management strategy that weighs their computational efficiency and optimization performance is proposed.

According to the information of an intelligent transportation system, a method combining reinforcement learning with receding horizon optimization is proposed at the upper level, which solves the cruising velocity for eco-driving in a long predictive horizon based on the online construction of a velocity planning problem.

At the lower level, a multi-objective optimal torque allocation method that considers energy saving and safety is proposed, where an analytical solution based on the state feedback control was obtained with the vehicle following the optimal speed of the upper level and tracking the centerline of the target path.

The energy management strategy proposed in this study effectively reduces the complexity of the intelligent energy-saving control system of the vehicle and achieves a fast solution to the whole vehicle energy optimization problem, integrating macro-traffic information while considering both power and safety.

Finally, an intelligent, connected hardware-in-the-loop (HIL) simulation platform is built to verify the method formulated in this study.
The simulation results demonstrate that the proposed method reduces energy consumption by 12.98% compared with the conventional constant-speed cruising strategy.
In addition, the computational time is significantly reduced.

title: Lane Change Intention Awareness for Assisted and Automated Driving on Highways
abstract: Today the automotive industry faces a robust trend toward assisted and automated driving.
The technology to accomplish this ambition has evolved rapidly over the last few years, and yet there are still a lot of algorithmical challenges left to make an automation of the driving task a safe and comfortable experience.

One of the main remaining challenges is the comprehension of the current traffic situation and the anticipation of all traffic participants' future driving behavior, which is needed for the technical system to obtain situation awareness: an indispensable foundation for successful decision-making.

In this paper, a prediction framework is presented that is able to infer a driver's maneuver intention.
This is achieved via a hybrid Bayesian network whose hidden layers represent a driver's lane contentedness.
A pre-training of the network's parameters with simulated data provides for human interpretable parameters even after running the expectation maximization algorithm based on data gathered on German highways.

Moreover, the future driving path of any traffic participant is predicted by solving an optimal control problem, whereby the parameters of the optimal control formulation are found via inverse reinforcement learning.


title: Energy Management Strategy Based on Deep Reinforcement Learning and Speed Prediction for Power-Split Hybrid Electric Vehicle with Multidimensional Continuous Control
abstract: An efficient energy management strategy (EMS) is significant to improve the economy of hybrid electric vehicles (HEVs).
Herein, a power-split HEV model is built and validated against test results, and then the EMS is proposed for this model based on vehicle speed prediction and deep reinforcement learning (DRL) algorithms.

The rule-based local controller and global optimal empirical knowledge are introduced to enhance the convergence speed.
It is shown in the results that the twin delayed deep deterministic policy gradient algorithm (TD3) achieves more satisfactory performance on converge speed and energy efficiency.
The networks of the DRL algorithm with continuous control update more robustly during iterations, in contrast to the discrete ones.
Although the power-split HEV with lower control dimension can reduce the learning burden for DRL EMS; however, the multidimensional control space shows greater optimization potential.
As a result, the equivalent fuel consumption of TD3-based EMS with multidimensional continuous control differences from the global optimal algorithm only by 4.92%.
Herein, it is demonstrated in the results that long short-term memory recurrent neural network (LSTM RNN) performs better for vehicle speed prediction than classical RNN and BP neural network, and the predictive vehicle speed feature helps improve fuel economy by 0.
55%.


title: High-Level Decision-Making Non-player Vehicles
abstract: Availability of realistic driver models, also able to represent various driving styles, is key to add traffic in serious games on automotive driving.
We propose a new architecture for behavioural planning of vehicles, that decide their motion taking high-level decisions, such as "keep lane", "overtake" and "go to rightmost lane".
This is similar to a driver's high-level reasoning and takes into account the availability of ever more sophisticated Advanced Driving Assistance Systems (ADAS) in current vehicles.
Compared to a low-level decision making system, our model performs better both in terms of safety and average speed.
As a significant advantage, the hierarchical approach allows to reduce the number of training steps, which is critical for ML models, by more than one order of magnitude.
The developed agent seems to show a more realistic behaviour.
We also showed feasibility of training models able to differentiate their performance in a way similar to the driving styles.
We believe that such agents could be profitably employed in state of the art SGs for driving, improving the realism of single NPVs and overall traffic.

title: Analysis of Switching Characteristics of Wide SOA and High Reliability 100 V N-LDMOS Transistor with Dual RESURF and Grounded Field Plate Structure
abstract: We proposed a wide SOA and high reliability 0.35 mum CMOS compatible 100 V dual RESURF LDMOS transistor with low switching loss and low specific on-resistance for automotive applications.
This paper describes detailed switching characteristics by changing load resistance RL and gate resistance RG for actual use which were not investigated.
TCAD simulations verified that the total energy loss (total switching loss + conduction loss) of the proposed device is sufficiently smaller (about 30 % down at the maximum) than that of the conventional device in most of the actual use range except for the following region: low duty cycle D < 0.
1 and high switching frequency f > 1.
1 MHz at a low RG of 1.
31 Omegamm2and a high RLof 65.
5 Omegamm2under a device layout area of 1 mm2.

Also, a unique switching characteristic of the proposed device, or a convex-shape gate plateau, not observed before, is analyzed.

title: Quantum Neural Networks for a Supply Chain Logistics Application
abstract: Problem instances of a size suitable for practical applications are not likely to be addressed during the noisy intermediate-scale quantum (NISQ) period with (almost) pure quantum algorithms.
Hybrid classical-quantum algorithms have potential, however, to achieve good performance on much larger problem instances.
One such hybrid algorithm on a problem of substantial importance: vehicle routing for supply chain logistics with multiple trucks and complex demand structure is investigated.
Reinforcement learning with neural networks with embedded quantum circuits is used.
In such neural networks, projecting high-dimensional feature vectors down to smaller vectors is necessary to accommodate restrictions on the number of qubits of NISQ hardware.
However, a multi-head attention mechanism is used where, even in classical machine learning, such projections are natural and desirable.
Data from the truck routing logistics of a company in the automotive sector is considered, and the methodology is applied by decomposing into small teams of trucks and results are found comparable to human truck assignment.


title: A Behavior Optimization Method for Unmanned Combat Aerial Vehicles Using Matrix Factorization
abstract: One of the fundamental technologies for unmanned combat aerial vehicles and combat simulators is behavior optimization, which finds a behavior that maximizes the probability of winning a battle.
With the advent of military science, combat logs became available, allowing machine learning algorithms to be used for the behavior optimization.
Due to implicit attributes such as the experience of an operator that are not explicitly presented in log data, existing methods for behavior optimization have limitations in performance improvement.
Furthermore, specific behaviors occur with low frequency, resulting in a dataset with imbalanced and empty values.
Therefore, we apply a matrix factorization (MF) method, which is one of latent factor models and known for sophisticated imputation of empty values, to the behavior optimization problem of unmanned combat aerial vehicles.

A situation-behavior matrix, whose elements are ratings indicating the optimality of behaviors in situations, is defined to implement the MF based method.
Experiments for performance comparison were conducted on combat logs, in which the proposed method yielded satisfactory results.

title: Naturalistic data-driven and emission reduction-conscious energy management for hybrid electric vehicle based on improved soft actor-critic algorithm
abstract: Energy management strategies (EMSs) are critical to saving fuel and reducing emissions for hybrid electric vehicles (HEVs).
Given that, this article proposes a naturalistic data-driven and emission reduction-conscious EMS based on deep reinforcement learning (DRL) for a power-split HEV.
In this article, for the purpose of evaluating the practical fuel economy of an HEV driving in a certain city region, a specific driving cycle is constructed by using a naturalistic data-driven method.

Furthermore, to realize the multi-objective optimization in terms of fuel conservation and emission reduction as well as the state of charge (SOC) sustaining, an intelligent EMS based on the improved soft actor-critic (SAC) algorithm with a novel experience replay method is innovatively proposed.

Finally, the effectiveness and optimality of the proposed EMS are verified.
Simulation results indicate that the constructed driving cycle can effectively reflect the real traffic scenarios of the test region.
Moreover, the proposed EMS achieves 95.
25% fuel economy performance of the global optimum, improving the fuel economy by 5.
29% and reducing the emissions by 10.
42% compared with the emission reduction-neglecting EMS based on standard SAC.

This article contributes to energy conservation and emission reduction for the transportation industry through advanced DRL methods.

title: Tutorial: Neural Network and Autonomous Cyber-Physical Systems Formal Verification for Trustworthy AI and Safe Autonomy
abstract: This interactive tutorial describes state-of-the-art methods for formally verifying neural networks and their usage within safety-critical cyber-physical systems (CPS).
The inclusion of deep learning models in safety-critical applications requires to formally analyze the behavior of the system, including reasoning about the individual components (e.
g.
, controller robustness), and their interactions and effects in the system as a whole.

This tutorial begins with a lecture on this emerging research area, followed by demos of these methods implemented in software tools, specifically the Neural Network Verification (NNV) tool.
Examples include systems from aerospace, automotive, and beyond.

title: A Fast Q-learning Energy Management Strategy for Battery/Supercapacitor Electric Vehicles Considering Energy Saving and Battery Aging
abstract: Electrified powertrain system brings advantages of improved energy efficiency and reduced fossil fuel consumption, which leads to the electrification of powertrain systems as a key target of automotive industry.

Advanced battery technology has been exploited in electric vehicle application and has made considerable progress.
However, the degradation of batteries in the vehicle operation could cause adverse effects on the performance and lifespan of electric vehicles.
Research on battery/supercapacitor hybrid energy storage systems of electric vehicles that considers energy saving and battery degradation is also lacking.
This paper presents a fast Q-learning based energy management strategy to maximize energy saving and minimize battery degradation.
Besides, battery only electric vehicle is also studied, and acts as a baseline vehicle.
An electrified powertrain system model considering the battery degradation effect is established to form the environment for the Q-learning strategy.
Under the training and validation driving cycles, the comparison indicates that the fast Q-learning strategy improves the energy efficiency by 3.83%, and the battery degradation is relieved by 26.36%.

title: Selection of a sustainable third-party reverse logistics provider based on the robustness analysis of an outranking graph kernel conducted with ELECTRE I and SMAA
abstract: Pressure from legislation and customers has motivated companies to consider reverse logistics (RL) in their operations.
Since it is a complex procedure that requires an adequate system, the recent trend consists in outsourcing RL to third-party reverse logistics providers (3PRLPs).
This paper provides the background of sustainable triple bottom line theory with focus on economic, environmental, and social aspects under 3PRL concerns.
The relevant sustainability criteria are used in a case study conducted in cooperation with an Indian automotive remanufacturing company.
To select the most preferred service provider, we use a hybrid method combining a variant of ELECTRE I accounting for the effect of reinforced preference, the revised Simos procedure, and Stochastic Multi-criteria Acceptability Analysis.

The incorporated approach exploits all parameters of an outranking model compatible with the incomplete preference information of the Decision Maker.
In particular, it derives the newly defined kernel acceptability and membership indices that can be interpreted as a support given to the selection of either a particular subset of alternatives or a single option.

The proposed ELECTRE-based method enriches the spectrum of multiple criteria decision analysis approaches that can be used to effectively approach the problem of the 3PRLP selection.
As indicated by the extensive review presented in the paper, this application field was so far dominated by Analytic Hierarchy Process and TOPSIS, whose weaknesses can be overcome by applying the outranking methods.

(C) 2018 Elsevier Ltd. All rights reserved.

title: Cross-Layer Approaches for 5G Wireless Communication
abstract: 

title: Generating Edge Cases for Testing Autonomous Vehicles Using Real-World Data
abstract: In the past decade, automotive companies have invested significantly in autonomous vehicles (AV), but achieving widespread deployment remains a challenge in part due to the complexities of safety evaluation.

Traditional distance-based testing has been shown to be expensive and time-consuming.
To address this, experts have proposed scenario-based testing (SBT), which simulates detailed real-world driving scenarios to assess vehicle responses efficiently.
This paper introduces a method that builds a parametric representation of a driving scenario using collected driving data.
By adopting a data-driven approach, we are then able to generate realistic, concrete scenarios that correspond to high-risk situations.
A reinforcement learning technique is used to identify the combination of parameter values that result in the failure of a system under test (SUT).
The proposed method generates novel, simulated high-risk scenarios, thereby offering a meaningful and focused assessment of AV systems.

title: 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)
abstract: The following topics are dealt with: multicore processors; GPU; magnetic recording drives; visual object recognition accelerator; sequential circuits; interconnects; cyclic logic encryptions; analog SAT solver; graph coloring; safety model checking; on-chip bit-flip detectors; EM side-channel attack resilience; deep neural network; monolithic 3D IC; TSV; NoC; offshore oil spill monitoring; deep reinforcement learning; nonlinear circuit simulation; controller area network; model predictive control; automotive cyber-physical systems; computer-aided design; CAD; robust design flow database; RDF; memristor aided logic; optical circuits; DSA-MP lithography; random access memory; multi-level cell STT-MRAM main memory; asymmetric cryptography; SSD flash-translation layers; adaptive distributed mobile learning system; IoT; mobile telemedicine; FPGA; split manufacturing; logic synthesis; self-powered wearable systems; memory access control; arithmetic circuits; high level synthesis; approximate lookup table-based accelerator; ApproxLUT; multilayer time-based spiking neuromorphic architecture; active power grids; thermal analysis; reconfigurable in-memory computing architecture; medical resource availability; intelligent transportation systems; system-on-chips; routing approach; MEDA biochips; VLSI; nonvonNeumann processor; Big Data Analysis; Industry 4.
0; FinFET technology; and IC yield improvement.


title: Sensorless Automatic Stop Control of Electric Vehicle in Semi-dynamic Wireless Power Transfer System with Two Transmitter Coils
abstract: Semi-dynamic wireless power transfer (SDWPT) system is one of the solutions to short driving distance in electric vehicle (EV).
This system provides electric power to EVs on the point of stopping.
A scheme of sensorless positioning system in SDWPT system via magnetic resonance coupling is proposed in this paper.
High transmission efficiency and short vehicular gap position are selected as the final stop position which lead to energy conservation and traffic jam problem reduction.
These proposed methods also show more practicality of positioning system for SDWPT system by using command value prediction (CVP) and reinforcement learning algorithm (RLA).
Performance of these two proposed method is evaluated by simulations and experiments.
The results pointed out that these two proposed methods are able to stop the vehicle at the high transmission efficiency position without any visual sensor.

title: When Cyber-Physical Systems Meet AI: A Benchmark, an Evaluation, and a Way Forward
abstract: Cyber-Physical Systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc.
In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS.
Despite the popularity of AI-enabled CPS, few benchmarks are publicly available.
There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains.
To bridge this gap, we present a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods.
Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify current challenges and future opportunities.
Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains.

Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS to achieve optimal performance and reliability.
Our benchmark, code, detailed evaluation results, and experiment scripts are available on https://sites.google.com/view/ai-cps-benchmark.

title: A cooperative hierarchical deep reinforcement learning based multi-agent method for distributed job shop scheduling problem with random job arrivals
abstract: Distributed manufacturing can reduce the production cost through the cooperation among factories, and it has been an important trend in the industrial field.
For the enterprises with daily delivered production tasks, the random job arrivals are regular.
Thus, the Distributed Job-shop Scheduling Problem (DJSP) with random job arrivals is studied, and it is a typical case from the equipment manufacturing industry.
The DJSP involves two coupled decision-making processes, job assigning and job sequencing, and the distributed and uncertain pro-duction environment requires the scheduling method to be more responsive and adaptive.

Thus, a Deep Rein-forcement Learning (DRL) based multi-agent method is explored, and it is composed of the assigning agent and the sequencing agent.
Two Markov Decision Processes (MDPs) are formulated for the two agents respectively.
In the MDP for the assigning agent, fourteen factory-and-job related features are extracted as the state features, seven composite assigning rules are designed as the candidate actions, and the reward depends on the total processing time of different factories.

In the MDP of the sequencing agent, five machine-and-job related features are set as the state features, six sequencing rules make up the action space, and the change of the factory makespan is the reward.

Besides, to enhance the learning ability of the agents, a Deep Q-Network (DQN) framework with variable threshold probability in the training stage is designed, which can balance the exploi-tation and exploration in the model training.

The proposed multi-agent method's effectiveness is proved by the independent utility test and the comparison test that are based on 1350 production instances, and its practical value in the actual production is implied by the case study from an automotive engine manufacturing company.


title: Client-Based Intelligence for Resource Efficient Vehicular Big Data Transfer in Future 6G Networks
abstract: Vehicular big data is anticipated to become the "new oil" of the automotive industry which fuels the development of novel crowdsensing-enabled services.
However, the tremendous amount of transmitted vehicular sensor data represents a massive challenge for the cellular network.
A promising method for achieving relief which allows to utilize the existing network resources in a more efficient way is the utilization of intelligence on the end-edge-cloud devices.
Through machine learning-based identification and exploitation of highly resource efficient data transmission opportunities, the client devices are able to participate in overall network resource optimization process.

In this work, we present a novel client-based opportunistic data transmission method for delay-tolerant applications which is based on a hybrid machine learning approach: Supervised learning is applied to forecast the currently achievable data rate which serves as the metric for the reinforcement learning-based data transfer scheduling process.

In addition, unsupervised learning is applied to uncover geospatially-dependent uncertainties within the prediction model.
In a comprehensive real world evaluation in the public cellular networks of three German Mobile Network Operator (MNO), we show that the average data rate can be improved by up to 223% while simultaneously reducing the amount of occupied network resources by up to 89%.

As a side-effect of preferring more robust network conditions for the data transfer, the transmission-related power consumption is reduced by up to 73%.
The price to pay is an increased Age of Information (AoI) of the sensor data.

title: DRL-driven zero-RIS assisted energy-efficient task offloading in vehicular edge computing networks
abstract: The increasing complexity of modern automotive applications presents difficulties when running them on the on-board units (OBUs) of vehicles.
While 5G/6G vehicular edge computing networks (VECNs) offer potential solutions through computation task offloading, ensuring prompt, energy-efficient access to these networks remains a significant challenge.

To overcome these challenges, reconfigurable intelligent surfaces (RIS) can play an important role in 6G vehicular networks.
With RIS, networks can provide better connectivity, increased data rate and energy efficient access, and communication channel security.
In this paper, we utilize zero-energy RIS (ze-RIS) to aid vehicular computation offloading while maximizing the energy and time savings while meeting the task and environmental constraints.
A joint power and offloading mechanism controlling DRL-driven RIS-assisted energy efficient task offloading (DREEO) scheme is proposed.
DREEO utilizes a hybrid approach that combines binary and partial offloading mechanisms, complemented by an intelligent communication link switching mechanism.
This strategy helps in saving both energy and time effectively.
An efficiency factor, serving as both a performance indicator and a reward function, is introduced for the DRL agent, considering both saved energy and time.
Through extensive evaluations, DREEO scheme shown an increase in task success rate from 2.13% to 7.36% and has improved the efficiency factor from 21.97 to 51.27.
Furthermore, compared to other evaluated schemes, the DREEO scheme consistently outperforms them in terms of reward and the TFPS ratio, the DRL properties.

title: Research on Cooperative Path Planning Model of Multiple Unmanned Vehicles in Real Environment
abstract: The cluster combat application of unmanned ground vehicles(UVS) is a hot research issue of the intersection of artificial intelligence and battle command.
Aiming at the cooperative path planning multiple unmanned vehicles not meeting the dynamic threat condition requirement, by combining the global path planning algorithm A-STAR with the local path planning algorithm RL, from the perspective of perception to behavioral decision making, the cooperative path planning model of multiple unmanned vehicles is studied.

The cooperative combat situation threat algorithm, state and action space, reward function and sphere of influence function are designed, the sub-models of formation configuration strategy generation and dynamic optimization of strike path are carried out, and the cooperative path planning control model of multiple unmanned vehicles based on autonomous learning is constructed and solved.

An application example shows that the proposed path planning model can effectively cope with the requirements of multi-unmanned vehicle collaborative path planning task in complex urban environment, and has important theoretical research and practical application value.

Â© 2023, The editorial Board of Journal of System Simulation.
All right reserved.

title: Reverse supply chain for end- of- life vehicles treatment: An in- depth content review
abstract: Reverse supply chain (RSC) implementation is integral to waste management.
Owing to economic, environmental and social benefits, RSC has been adopted to many industries and automotive is no exception.
Multiple components and materials found in End-of-life vehicles (ELVs) have potential recycling value but also cause environmental damages if improperly treated.
The number of ELVs is accelerating worldwide, especially in developing countries.
Research on RSCs for ELVs treatment has been increasing.
However, the literature still lacks a comprehensive understanding of this topic.
The authors thus conducted an in-depth content review of publications on RSC of ELVs.
The methodology applied PRISMA 2020 guideline combined with the model of Mayring (2001).
Extensive search strings were utilized to retrieve articles from SCOPUS and Web of Science.
After a rigid selection process and an intensive content assessment on 10,140 publications as raw materials, a total of 151 peer-reviewed papers was selected and carefully analyzed.
The content categories were developed deductively and inductively.
Major categories included research methodology overarching others: research themes, RSC/ELV types, stages in RSC, country specific and stakeholders.
Modeling is the most commonly used among the papers, especially in RSC network design and planning.
Mixed-integer linear programming prevails over other modeling approaches while AHP is predominant decision-making tool.
Studies on legislations, network performance evaluation, and forecasting remain negligible.
Categorization and significant achievements of articles are introduced.
Future research is suggested upon identified research gaps.
The review benefits academicians and practitioners in better comprehension of the field and promising research directions.

title: SIEGE: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems
abstract: Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems.

As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance.

However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties.

Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system.
Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications.
Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics.
Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand.
Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs.
In this paper, we propose SIEGE, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs.

We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications.
We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems.

Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system.

To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly (https://sites.google.com/view/ai-cps-siege/ home).

title: Energy management strategy for fuel cell vehicles via soft actor-critic-based deep reinforcement learning considering powertrain thermal and durability characteristics
abstract: Temperature can significantly affect the water equilibrium, electrochemical kinetics and mass transmission in a proton exchange membrane fuel cell (PEMFC) stack, meanwhile it also impacts the lifespan and safety of a lithium-ion battery (LIB).

Yet, energy management strategy (EMS) is rarely to synchronously study the durability performances of the LIB and PEMFC stack with their thermal effects in fuel cell vehicles (FCVs) under real-world driving scenarios.

Thus, this study proposes a deep reinforcement learning (DRL)-based EMS to minimize tran-sient costs of the LIB and PEMFC stack, which include their state-of-health (SOH) descents and overtemperature penalties.

Meanwhile, the transient costs are incorporated into the overall cost, which comprises the hydrogen consumption rate of the PEMFC stack, and penalty of maintaining the LIB state-of-charge (SOC).
Moreover, the soft actor-critic (SAC) is applied to the DRL-based EMS due to its advantage of stability across different random environments and no meticulous hyperparameter calibration.
Specifically, the proposed EMS intelligently allo-cates the direct current (DC) bus power of FCVs in real time to maximize a multi-objective reward in accordance with FCV states, in which the reward is the negative overall cost.

Then, long-term real-world driving scenarios in Chongqing city, China, are used for off-line training and real-time control to advance the adaptability of the proposed EMS.
The results show that in comparison with the deep Q-network (DQN)-based EMS considering the powertrain temperature and durability, and the SAC-based EMS neglecting the powertrain temperature and durability, the proposed strategy can actualize overall SOH increments of the powertrain up to 14.
01 % and 3.
45 %, respectively, and restrict the maximum temperatures of the PEMFC stack and LIB.

In addition, the general-ization of the proposed EMS is verified, in which the trained model of the proposed EMS is tested in other FCV and driving cycles, and it can acquire similar effectiveness.
Thus, the proposed strategy can enforce the lifespan durability and thermal stability of the powertrain system.

title: Modeling Lithium-Ion Batteries Using Machine Learning Algorithms for Mild-Hybrid Vehicle Applications
abstract: The prediction of voltage levels in an automotive 48V mild hybrid power supply system is safety-relevant while also enabling greater efficiency.
The high power-to-energy ratio in these power supply systems makes exact voltage prediction challenging, so that a method is established to model the behavior of the lithium-ion batteries by means of a recurrent neural network.

The raw data are consequently pre-processed with over- and undersampling, normalization and sequentialization algorithms.
The resulting database is used to train the constructed recurrent neural network models, while hyperparameter tuning is carried out with the optimization framework optuna.
This training methodology is performed with two battery types.
Validation shows a maximum error of 2.34 V for the LTO battery and a maximum error of 3.39 V for the LFP battery.
The results demonstrate performance of the proposed methodology in an appropriate error range for utilization as a tool to generate a battery model based on available data.

title: Table1_Coordinated control of multiple converters in model-free AC/DC distribution networks based on reinforcement learning.XLSX
abstract: Taking into account the challenges of obtaining accurate physical parameters and uncertainties arising from the integration of a large number of sources and loads, this paper proposes a real-time voltage control method for AC/DC distribution networks.

The method utilizes model-free generation and coordinated control of multiple converters, and employs a combination of agent modeling and multi-agent soft actor critic (MASAC) techniques for modeling and solving the problem.

Firstly, a complex nonlinear mapping relationship between bus power and voltage is established by training an power-voltage model, to address the issue of obtaining physical parameters in AC/DC distribution networks.

Next, a Markov decision process is established for the voltage control problem, with multiple intelligent agents distributed to control the active and reactive power at each converter, in response to the uncertainties of photovoltaic (PV) and load variations.

Using the MASAC method, a centralized training strategy and decentralized execution policy are implemented to achieve distributed control of the converters, with each converter making optimal decisions based on its local observation state.

Finally, the proposed method is verified by numerical simulations, demonstrating its sound effectiveness and generalization ability.
Copyright: CC BY 4.0

title: Q-learning and Neural Networks in Automated Driving Systems
abstract: With the development of artificial intelligence, machine learning, and other disciplines, automation in various fields is becoming popular.
Therefore, autonomous driving systems in the automotive field represent a major trend in the future of intelligent transportation systems.
This paper focuses on the application of Q-learning and deep neural networks in autonomous driving systems.
The simulation in Python is used to simulate the condition of the car driving alone on the road.
Firstly, the normal car driving situation was analyzed and the actions and states in Q-learning were set.
The action set-up ensures the distance sensors' number and direction.
Then, this paper draws the neural network and simulates it to get the data.
Secondly, it analyzes the resulting reward scores and iteration times and identifies uncertainties through line graphs.
In addition, it observes the simulation video and derives the reasons why uncertainty appears.
In the simulation, the Best Score, Average generation time and Genetic distance were obtained from the simulation against the number of Gens.
The data for genetic distance was found to increase with the number of times it was generated, indicating that the Q-learning component was successfully run in the simulation.
And it was found that the uncertainty was due to the birth point of the vehicle.
According to the uncertainty, further development of the simulation could include more road environments and different obstacles.

title: Failure safety analysis of artificial intelligence systems for smart/autonomous vehicles
abstract: Up to now failures in artificial intelligence systems, specifically machine learning algorithms which are their software components, are considered as systematic failures.
The goal of this paper is to introduce a new concept of quantitative failure analysis for machine learning algorithms which can be used in smart/autonomous vehicles to guarantee sufficiently low risk of residual errors in this application.

Firstly, a coincidence in evaluating impacts of unpredictable behaviours of machine learning algorithms and hardware components is introduced in order to statistically estimate failure rate based on a given number of data points.

Next, a metric utilising this randomic failure rate is proposed to assess functional safety of smart and/or autonomous vehicles and evaluate their safeness according to ISO 26262:2018, and ISO/PAS 21448.


title: Predicting Vehicle Trajectories at Intersections Using Advanced Machine Learning Techniques
abstract: 

title: The application of machine learning-based energy management strategy in a multi-mode plug-in hybrid electric vehicle, part II: Deep deterministic policy gradient algorithm design for electric mode
abstract: Machine learning (ML)-based methods have attracted great attention in the multi-objective optimization prob-lems, which is the key challenge in the energy management strategy (EMS) of the multi-power hybrid system.

Our recently published research in this journal verified the effectiveness and feasibility of a Deep Deterministic Policy Gradient (DDPG)-based EMS in the charge-sustaining (CS) stage of a multi-mode plug-in hybrid vehicle (PHEV).

However, the application of ML-based-EMS in the charge-depletion (CD) stage and the regenerative braking mode of PHEV are still missing.
This study proposes a discrete-continuous hybrid actions-based hier-archical EMS to optimally distribute the dual-motor driving force in battery electric driving and regenerative braking.
In the upper layer of EMS, DDPG is trained to learn the torque distribution principles of dual-motor operation to achieve better energy efficiency without losing dynamic performance.
Meanwhile, the total recoverable braking torque is also determined by the upper layer EMS considering the braking demand, me-chanical and electrical braking system conditions, vehicle safety, and the provisions of law.

In the lower level of EMS, the driving mode is determined under the guidance of energy consumption optimization.
The verified results show that the proposed EMS outperforms other deep reinforcement learning (DRL)-based hierarchical and non-hierarchical EMSs.

title: Data-Driven Framework for Energy Management in Extended Range Electric Vehicles Used in Package Delivery Applications
abstract: 

title: Sustainable DDPG-based Path Tracking For Connected Autonomous Electric Vehicles in extra-urban scenarios
abstract: This paper addresses the path-tracking control problem for Connected Autonomous Electric Vehicles (CAEVs) moving in a smart Cooperative Connected Automated Mobility (CCAM) environment, where a smart infrastructure suggests the reference behaviour to achieve.

To solve this problem, a novel energy-efficient Deep Deterministic Policy Gradient-based (DDPG) Algorithm, able to minimize its energy consumption while guaranteeing the optimal tracking of the suggested path, is proposed.

Specifically, in order to improve the power autonomy and the battery state of charge (SOC), a Comprehensive Power-based Electric Vehicle Consumption Model (CPEM) is exploited for the training of the DDPG agent.

The training process confirms the capability of DDPG agent into learning the safe eco-driving policy, while a case of study proves the advantages and the performance of the overall closed-loop of the proposed control strategy.


title: Reinforcement Learning Based Control Design for a Floating Piston Pneumatic Gearbox Actuator
abstract: Electro-pneumatic actuators play an essential role in various areas of the industry, including heavy-duty vehicles.
This article deals with the control problem of an Automatic Manual Transmission, where the actuator of the system is a double-acting floating-piston cylinder, with dedicated inner-position.
During the control design of electro-pneumatic cylinders, one must implement a set-valued control on a nonlinear system, when, as in the present case, non-proportional valves provide the airflow.
As both the system model itself and the qualitative control goals can be formulated as a Partially Observable Markov Decision Process, Machine learning frameworks are a conspicuous choice for handling such control problems.

To this end, six different solutions are compared in the article, of which a new agent named PG-MCTS, using a modified version of the "Upper Confidence bound for Trees" algorithm, is also presented.
The performance and strategic choice comparison of the six methods are carried out in a simulation environment.
Validation tests performed on an actual transmission system and implemented on an automotive control unit to prove the applicability of the concept.
In this case, a Policy Gradient agent, selected by implementation and computation capacity restrictions.
The results show that the presented methods are suitable for the control of floating-piston cylinders and can be extended to other fluid mechanical actuators, or even different set-valued nonlinear control problems.


title: A Q-learning Approach for SoftECU Design in Hybrid Electric Vehicles
abstract: A software electronic control unit (SoftECU) implements simplified versions of the algorithms coded in a corresponding real ECU.
In this paper a Q-learning approach for the design of SoftECUs is proposed.
The training phase of the SoftECU is based on the mismatch between the commands provided by the real ECU and those coming from the SoftECU model.
The technique is applied to the case of an energy management ECU for hybrid electric vehicles.
The effectiveness of the SoftECU is validated by considering the new European driving cycle and the worldwide harmonized light vehicles test procedure.

title: Prolonging Robot Lifespan Using Fatigue Balancing with Reinforcement Learning
abstract: 

title: High-dynamic high-power E-Motor Emulator for power electronic testing
abstract: In the past decade, inverter testing has become a very important and complex issue during inverter developments.
State-of-the-art inverters, key components of electrical power trains, offer multiple functions, high reliability, fault tolerance and they must ensure a high level of functional safety.
Especially automotive applications prove to be demanding due to their exceptionally high dynamic requirements, challenging mission profiles, harsh environment and special test cases such as the curb stone edge case with high currents, high dynamic and no speed.

Conventional test methods such as motor test beds or active / passive RL-loads are not suitable to verify inverter functionalities in a sufficient manner.
This paper presents a new and powerful inverter testing technology: The E-Motor Emulator - testing power electronics without an e-machine.

title: Semantic Exploitation of Implicit Patent Information
abstract: In recent years patents have become increasingly important for businesses to protect their intellectual capital and as a valuable source of information.
Patent information is, however, not employed to its full potential and the interpretation of structured and unstructured patent information in large volumes remains a challenge.
We address this by proposing an integrated interdisciplinary approach that uses natural language processing and machine learning techniques to formalize multilingual patent information in an ontology.
The ontology further contains patent and domain specific knowledge, which allows for aligning patents with technological fields of interest and other business-related artifacts.
Our empirical evaluation shows that for categorizing patents according to well-known technological fields of interest, the approach achieves high accuracy with selected feature sets compared to related work focussing on monolingual patents.

We further show that combining OWL RL reasoning with SPARQL querying over the patent knowledge base allows for answering complex business queries and illustrate this with real-world use cases from the automotive domain.


title: Decomposition-based real-time control of multi-stage transfer lines with residence time constraints
abstract: It is commonly observed in the food industry, battery production, automotive paint shop, and semiconductor manufacturing that an intermediate product's residence time in the buffer within a production line is controlled by a time window to guarantee product quality.

There is typically a minimum time limit reflected by a part's travel time or process requirement.
Meanwhile, these intermediate parts are prevented from staying in the buffer for too long by an upper time limit, exceeding which a part will be scrapped or need additional treatment.
To increase production throughput and reduce scrap, one needs to control machines' working mode according to real-time system information in the stochastic production environment, which is a difficult problem to solve, due to the system's complexity.

In this article, we propose a novel decomposition-based control approach by decomposing a production system into small-scale subsystems based on domain knowledge and their structural relationship.
An iterative aggregation procedure is then used to generate a production control policy with convergence guarantee.
Numerical studies suggest that the decomposition-based control approach outperforms general-purpose reinforcement learning method by delivering significant system performance improvement and substantial reduction on computation overhead.


title: Artificial intelligence planners for multi-head path planning of SwarmItFIX agents
abstract: Sheet metal manufacturing is finding wide applications in automotive and aerospace industries.
Handling of giant sheet materials in manufacturing industries is one of the key problems.
Utilization of robots, viz SwarmItFIX, will address this problem and automate the fixturing process, which greatly reduces lead time and thus the production cost.
Implementation of intelligence into the robots will further improve efficiency in handling and reduce manufacturing inaccuracies.
In this work, two different novel planners are proposed which do path planning for the heads of the SwarmItFIX agents.
The environment of the problem is modeled as a Markov Decision Problem.
The first planner uses the Value Iteration and Policy Iteration (PI) algorithms individually and the second planner performs the Monte Carlo control reinforcement learning.
Finally, when the simulation is done and parameters of the proposed three algorithms along with existing Constraint Satisfaction Problem algorithm are compared with each other.
It is observed that the proposed PI algorithm returns the plan much faster than the other algorithms.
In the near future, the efficient planning model will be tested and implemented into the SwarmItFIX setup at the PMAR laboratory, University of Genoa, Italy.

title: Energy consumption and battery aging minimization using a Q-learning strategy for a battery/ultracapacitor electric vehicle
abstract: Propulsion system electrification revolution has been undergoing in the automotive industry.
The electrified propulsion system improves energy efficiency and reduces the dependence on fossil fuel.
However, the batteries of electric vehicles experience degradation process during vehicle operation.
Research considering both battery degradation and energy consumption in battery/ultracapacitor electric vehicles is still lacking.
This study proposes a Q-learning-based strategy to minimize battery degradation and energy consumption.
Besides Q-learning, two rule-based energy management methods are also proposed and optimized using Particle Swarm Optimization algorithm.
A vehicle propulsion system model is first presented, where the severity factor battery degradation model is considered and exper-imentally validated with the help of Genetic Algorithm.
In the results analysis, Q-learning is first explained with the optimal policy map after learning.
Then, the result from a vehicle without ultra-capacitor is used as the baseline, which is compared with the results from the vehicle with ultracapacitor using Q-learning, and two rule-based methods as the energy management strategies.

At the learning and validation driving cycles, the results indicate that the Q-learning strategy slows down the battery degradation by 13-20% and increases the vehicle range by 1.
5-2% compared with the baseline vehicle without ultracapacitor.

(c) 2021 Elsevier Ltd. All rights reserved.

title: Assuring Safety and Trustworthiness of Deep Neural Networks by Making Them More Interpretable
abstract: Deep reinforcement learning (DRL) is one of the key techniques leading to the current wave of artificial intelligence.
However, the concerns on its safety and trustworthiness have been raised, particularly when it is applied to safety critical applications such as autonomous driving cars and un-manned aerial vehicles (UAVs).

Additional to ensure its functional correctness, techniques are urgently needed to explain the black-box model and its decision making.
An interpretable model can significantly improve human operators' trust on the DRL model.
In this project, we aim to develop a set of interpretability techniques for DRL models.
We will start by developing novel adversarial attacks for DRL models to study their robustness, and then investigate how to utilise the obtained adversarial examples for interpretability.
The success of this project may significantly improve the safety and trustworthiness of the DRL models, and enable its wide applications to the automotive and aerospace industries in the UK.
EPSRC research area: artificial intelligence techniques

title: Sensor Reliability in Cyber-Physical Systems Using Internet-of-Things Data: A Review and Case Study
abstract: Nowadays, reliability of sensors is one of the most important challenges for widespread application of Internet-of-things data in key emerging fields such as the automotive and manufacturing sectors.
This paper presents a brief review of the main research and innovation actions at the European level, as well as some on-going research related to sensor reliability in cyber-physical systems (CPS).
The research reported in this paper is also focused on the design of a procedure for evaluating the reliability of Internet-of-Things sensors in a cyber-physical system.
The results of a case study of sensor reliability assessment in an autonomous driving scenario for the automotive sector are also shown.
A co-simulation framework is designed in order to enable real-time interaction between virtual and real sensors.
The case study consists of an IoT LiDAR-based collaborative map in order to assess the CPS-based co-simulation framework.
Specifically, the sensor chosen is the Ibeo Lux 4-layer LiDAR sensor with IoT added capabilities.
The modeling library for predicting error with machine learning methods is implemented at a local level, and a self-learning-procedure for decision-making based on Q-learning runs at a global level.
The study supporting the experimental evaluation of the co-simulation framework is presented using simulated and real data.
The results demonstrate the effectiveness of the proposed method for increasing sensor reliability in cyber-physical systems using Internet-of-Things data.

title: Event Triggered Unknown Networked Control System Design by using Adaptive Dynamic Programming
abstract: This project will develop new universal control designs for Network Control Systems (NCS) such as the new distributed control systems needed for the smart grid and modern automotive systems.
Unlike many traditional NCS systems, these will be designed from the start so as to maximize performance over time, fully accounting for communication delays in networks and unexpected events, as well as nonlinearities, continuous random disturbances and other issues which this PI has addressed in the past.

He intends to develop rigorous mathematical proofs that these new designs will always lead to stable operation.
He will also maintain a pipeline to immediate practical applications, through his existing Industry-University Center on Intelligent Maintenance.
The designs are intended to be general, learning-based and massively parallel, in a way which may also help us understand how mammal brains can perform tasks beyond the scope of more traditional types of design.
<br/><br/>These designs will be an extension of work previously funded by NSF, described in the Handbook of RLADP, edited by Lewis and Liu.

ADP, like linear programming, is a class of challenges, not just one specific method, though there are relations between the successful methods.
ADP includes all the methods aimed at the general problem of multistage optimization in the face of nonlinearity and stochastic disturbance.
In order to cope with general nonlinear tasks, this project will include neural networks which have been proved to be more effective as approximators of general nonlinear functions than more traditional systems.

The chief novelty here is the ability to combine these capabilities with the presence of communication delays and random discrete events, which are an important practical issue in most NCS.

title: A dynamic programming-optimized two-layer adaptive energy management strategy for electric vehicles considering driving pattern recognition
abstract: To achieve optimal real-time power allocation in electric vehicles, a two-layer adaptive dynamic programming (DP) optimization energy management strategy (EMS) has been proposed.
The upper layer uses learning vector quantization (LVQ) to produce real-time driving pattern recognition (DPR) results.
The method determines 10 characteristic parameters for training the recognition network and the length of the sampling window is 120 s.
 The typical driving cycles are divided into different levels of blocks to identify the real-time DPR level.

The lower layer adopts the optimization strategy of DP extraction to adjust the power distribution between the battery pack and the supercapacitor pack according to the recognition results.
DP is used to define a cost function to minimize the energy loss of the hybrid energy storage system (HESS) and optimize the battery usage range in the system.
The near-optimal real-time EMS is extracted by analyzing the DP control behavior of the battery under the layered state of charge (SOC).
The simulation results indicate that the proposed new rule control based on DP optimization (NRB) EMS improves the system efficiency by 10 % compared with the original rule-based (RB) EMS under different temperatures and DPR levels.

In addition, the system efficiency gap is controlled at 3 % compared with DP.

title: How to certify machine learning based safety-critical systems? A systematic literature review
abstract: Context Machine Learning (ML) has been at the heart of many innovations over the past years.
However, including it in so-called "safety-critical" systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches.

Objective This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question "How to Certify Machine Learning Based Safety-critical Systems?".

Method We conduct a Systematic Literature Review (SLR) of research papers published between 2015 and 2020, covering topics related to the certification of ML systems.
In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification.

We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted.
Results The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of ML models.
It also emphasized the need to further develop connections between academia and industries to deepen the domain study.
Finally, it also illustrated the necessity to build connections between the above mentioned main pillars that are for now mainly studied separately.
Conclusion We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions.

title: Energy Reduction of Robot Stations with Uncertainties
abstract: 

title: Novel Method and Procedure for Evaluating Compliance of Sources With Strong Gradient Magnetic Fields Such as Wireless Power Transfer Systems
abstract: To date, the development of valid and globally universally accepted recognized methods for the accurate exposure assessment of wireless power transfer (WPT) technologies is lagging behind the rapid emergence of high power systems in the energy and automotive sectors.

WPT systems based on inductive and magnetic resonance technologies generate strong but rapidly decaying magnetic fields, which often exceed reference levels (RL) in the immediate vicinity of the WPT coils by up to a factor of >100.

Compliance testing of WPT systems with limits derived for homogeneous exposure can lead to estimations of exposure that exceed by up to 40 dB assessments of the fields induced in the human body, i.
e.
, the basic restrictions (BR) defined by the international safety guidelines.

Testing compliance with the BR is impractical for regulatory purposes due to the high costs of resources of determining the maximum exposure conditions.
This paper presents a novel compliance testing method that mitigates the overestimation of the exposure while maintaining the simplicity of the testing procedure.
This is achieved by using coupling transformation functions to correlate not only the amplitude and frequency but also the gradient of the incident field with the BR.
These novel conservative coupling functions have been determined by means of a large scale numerical study in the frequency range 3 kHz-10 MHz supported by a physics-based approximation.
The here proposed compliance testing method is still conservative in comparison to the compliance toward BR of localized sources.
However, in comparison to today's practice of applying RL directly, the overestimation is strongly reduced for high gradient fields (G(n) > 50 T/m/T), e.
g.
, by more than 3000 times for field gradients of about 200 T/m/T.

We have validated the method by numerical analysis of human exposure to actual WPT sources.
The adoption of the new method will help to accelerate the introduction of high-power wireless charging devices in the global market.

title: Integrated Corridor Management for Connected Vehicles and Park and Ride Structures
abstract: 

title: Auto Parts Defect Detection Based on Few-shot Learning
abstract: In the process of auto parts products from manufacturing to quality inspection, it is inevitable that there will be defects on the surface due to aging of machine tools or human factors.
How to better apply computer vision technology to defect detection of auto parts is still an issue discussed by the industry and academia.
For the condition that sufficient labeled samples is usually not available in the industrial field, this paper uses an improved few-shot metric learning model to identify auto parts samples.
According to the traditional few-shot learning models lack of the considerations of channel level and internal relations of support set, this paper adds ECA module and Transformer module to few-shot learning.

Add more attention to the key information of support set and query set samples at channel level while increasing the attention to the intrinsic feature of support set, to improve the classification effect.


title: Potential of Machine Learning Methods for Robust Performance and Efficient Engine Control Development
abstract: Increasingly strict legislation for greenhouse gas and real-world pollutant emissions makes it necessary to develop fuel-efficient and robust control solutions for future automotive engines.
Today's engine control development relies on traditional map-based and model-based control approaches.
Due to growing system complexity and real-world requirements, these expert-intensive and time-consuming approaches are facing a turning point, which will lead to unacceptable development time and costs in the near future.

Artificial Intelligence (AI) is a disruptive technology, which has interesting features that can tackle these challenges.
AI-based methods have received growing interest due to the increasing availability of data and the success of AI applications for complex problems.
This paper presents an overview of the state-of-the-art in Machine Learning (ML)-based methods that are applied for engine control development with focus on the time-consuming calibration process.
The overview here shows that the vast majority of studies concentrates on regression modelling to model complex processes, to reduce the number of model parameters and to develop real-time, ECU implementable models.

The identified promising directions for future ML-based engine control research include the application of reinforcement learning methods to on-line optimize engine performance and guarantee robust performance and unsupervised learning methods for data quality monitoring.

Copyright (C) 2021 The Authors.

title: Real time hardware and software systems for micro air vehicle flight control testing
abstract: 

title: Parametric analysis of single boost converter for energy harvester
abstract: This paper presents a conventional DC-DC boost converter for low and wide voltage supply range, suitable for energy harvesting purpose.
The output voltage can be increased by controlling the transistor switching frequency, duty cycle, inductance, load capacitor, rise, and fall time.
Both computer simulation and experiment results are performed in details.
Experiment results have shown an error less than 6 % with the simulation.
A linear trend of output voltage in the range of 4 V to 49 V is successfully converted from 100 mV to 1.5 V input voltage using low switching frequency of 2 kHz.
The circuit parameter for this voltage range are L = 100 muH, D = 50 %, tr = tf = 2.9 mus considering CL= 10 muF, and RL= 10 kOmega.
This circuit is suitable for medium voltage range application such as in automotive, aircraft, industry, and wireless measurement system.

title: Peering into a crystal ball: Forecasting behavior and industry foresight
abstract: Research Summary What makes some managers and entrepreneurs better at forecasting the industry context than others?
We argue that, regardless of experience or expertise, a learning-based forecasting behavior in which individuals attend to and incorporate new relevant information from the environment into an updated belief that aligns with the Bayesian belief updating process is likely to generate superior industry foresight.

However, the effectiveness of such a cognitively demanding process diminishes under high levels of uncertainty.
We find support for these arguments using an experimental design of forecasting tournaments in the managerially relevant context of the global automotive industry from 2016 to 2019.
The study provides a novel account of individual-level forecasting behavior and its effectiveness in an evolving industry and suggests important implications for managers and entrepreneurs.
Managerial Summary How a focal industry will evolve is a key forecasting problem faced by managers and entrepreneurs as they seek to identify opportunities and make strategic decisions.
However, developing superior industry foresight in the face of significant change, and limited and often contradictory information, can be especially challenging.
We study how individuals forecast the ongoing transformation of the global automotive industry with respect to electrification and autonomy, using a novel research design of forecasting tournaments.
A forecasting process in which individuals update their beliefs by neither ignoring prior information nor overacting to new information helps to generate superior industry foresight.
There was a significant penalty to forecasting accuracy when individuals did not update their beliefs at all, or when they updated, but overreacted to new information.

title: AI-based plant control
abstract: Expectations for machine learning and artificial intelligence (AI) are growing globally and related investment has been increasing in a diverse range of businesses.
Machine learning is used for autonomous car driving and robot control, and the use of its applications is rapidly increasing in factory automation (FA).
On the other hand, practical process control techniques based on machine learning and AI have not yet been developed for process automation (PA) although data analysis using process data is becoming more common.

PID control still remains the main technique and advanced control techniques by experts are used when complicated control is required.
Not to simulate but to apply machine learning technology to actual equipment, we performed AI-based control of a three-tank-level control system, which is a popular educational kit for process control.

This paper introduces the details of the experiment and the machine learning technology used to control this system.

title: Modern Autonomous Driving
abstract: 

title: Design and development of automobile assembly model using federated artificial intelligence with smart contract
abstract: With smart sensors and embedded drivers, today's automotive industry has taken a giant leap in emerging technologies like Machine learning, Artificial intelligence, and the Internet of things and started to build data-driven decision-making strategies to compete in global smart manufacturing.

This paper proposes a novel design framework that uses Federated learning-Artificial intelligence (FAI) for decision-making and Smart Contract (SC) policies for process execution and control in a completely automated smart automobile manufacturing industry.

The proposed design introduces a novel element called Trust Threshold Limit (TTL) that helps moderate the excess usage of embedded equipment, tools, energy, and cost functions, limiting wastages in the manufacturing processes.

This research highlights the use cases of AI in decentralised Blockchain with smart contracts, the company's trading policies, and its advantages for effectively handling market risk assessments during socio-economic crisis.

The developed model supported by real-time cases incorporated cost functions, delivery time and energy evaluations.
Results spotlight the use of FAI in decision accuracy for the developed smart contract-based Automobile Assembly Model (AAM), thereby qualitatively limiting the threshold level of cost, energy and other control functions in procurement assembly and manufacturing.

Customisation and graphical user interface with cloud integration are some challenges of this model.

title: Digital Twin Enabled Task Offloading for IoVs: A Learning-Based Approach
abstract: This article explores the optimal offloading strategy in the Internet of Vehicles (IoVs), which is challenged by three issues.
First, the resources of edge servers are shared by multiple vehicles, leading to random changes over time.
Second, as a vehicle would drive across consecutive edge servers, the offloading strategy needs to consider the overall edge resources along the trip.
Third, at each vehicle, the computing tasks arrive continuously when driving.
This dictates the offloading strategy to consider not only the current status but also the futuristic computing tasks.
To tackle these issues, we propose a digital twin (DT) network framework.
A DT network maintains DTs in the cyber-space to synchronize the real-world activities of vehicles.
Therefore, task offloading decisions can be benefited by combining both the global information aggregated from neighbor twins and historical information uploaded by vehicles.
With comprehensive information, the optimal offloading strategy can be determined.
We characterize the offloading problem as a Markov Decision Process (MDP) and develop an A3C-based decision-making algorithm, which can learn optimal offloading actions that minimize the long-term system costs.

Extensive experiments demonstrate the performance of our proposal in terms of fast convergence and low system costs when compared with other approaches.

title: An improved multi-objective optimization algorithm with mixed variables for automobile engine hood lightweight design
abstract: Engine hood is one of the important parts of the vehicles, which has influences on the lightweight, structural safety, pedestrian protection, and aesthetics.
The optimization design of engine hood is a high-dimensional, multi-objective, and mixed-variable optimization problem.
In order to reduce the physical test investment in the development and improve the efficiency of optimization, this article proposes a data-driven method for optimal hood design.
A newly proposed single-objective optimization algorithm is improved by several strategies for multi-objective constrained problem with mixed variables.
Then the hood is optimized through the specially designed machine learning model.
Finally, both the hood's weight and pedestrian injury are reduced while maintaining structural stiffness and frequency in the desired range.
The comparative study and final hood optimization results prove the effectiveness of the proposed method.

title: Autonomous Merging onto the Highway Using LSTM Neural Network
abstract: Merging onto highways is a complex task for autonomous vehicles and can cause catastrophic crashes, especially when considering the high speeds of other vehicles in the highway.
While methods like path tracking and reinforcement learning are commonly used to execute this task, they may not accurately imitate the behavior of an expert driver and simultaneously maintain accuracy, safety and passenger comfort.

In this paper, we present an autonomous driver model that uses an LSTM neural network trained on data gathered from two expert drivers executing the maneuver in a driving simulator.
We extracted five features from the dataset, and used them to train the neural network model.
The model approximates the proper steering wheel angle at each time step and feeds it to a PD controller which sets the physical steering wheel angle by managing the input voltage of a DC motor mounted on the driving simulator's steering rod.

The neural network model achieved an R2 score of 0.82057 and successfully guided the vehicle to merge onto the highway and reach the second lane without collisions in multiple trials in the simulator.
The results of this research shows the potential of using expert driver data and neural networks to improve the accuracy and safety of merging onto highways in autonomous vehicles.
As autonomous vehicles become more prevalent in the automotive industry, it is essential to develop reliable and safe highway-merging strategies.
Our model provides a promising solution to this challenge and can be further developed and integrated into autonomous vehicles to enhance their capabilities and ensure safe and efficient merging onto highways.


title: Safe and Scalable Planning Under Uncertainty for Autonomous Driving
abstract: 

title: Distributed Beamforming with Unmanned Vehicles and Edge Network Resource Orchestration for Wireless IoT : A Systems Perspective
abstract: 

title: Outdoor Operations of Multiple Quadrotors in Windy Environment
abstract: 

title: Parametric sweep analysis of medium voltage range boost converter for energy harvester application
abstract: This paper presents a parametric sweep analysis discussion on proposed DC-DC boost converter circuit for low and wide voltage supply range.
Analysis is initially done using computer simulation and then tested with experimental work.
Results are combined and discussed in details.
In this work, effect of parameter such as input voltage, switching frequency and inductance is presented in details.
A linear conversion has been observed in this work.
Low DC input voltage of 100 mV to 1.5 V is used and successfully converts to up to 50 V in linear inclination, considering CL = 10 muF, and RL = 10 kOmega.
The circuit parameter for this voltage range are L = 100 muH, D = 50 %, and 2 kHz frequency operation.
This circuit can be used for energy harvesting purpose and medium voltage application such as aircraft, wireless measurement system and automotive.

title: Prediction of Electric Buses Energy Consumption from Trip Parameters Using Deep Learning
abstract: The energy demand of electric buses (EBs) is a very important parameter that should be considered by transport companies when introducing electric buses into the urban bus fleet.
This article proposes a novel deep-learning-based model for predicting energy consumption of an electric bus traveling in an urban area.
The model addresses two important issues: accuracy and cost of prediction.
The aim of the research was to develop the deep-learning-based prediction model, which requires only the data readily available to bus fleet operators, such as location of the bus stops (coordinates, altitude), route traveled, schedule, travel time between stops, and to find the most suitable type and configuration of neural network to evaluate the model.

The developed prediction model was assessed with different types of deep neural networks using real data collected for several bus lines in a medium-sized city in Poland.
Conducted research has shown that the deep learning network with autoencoders (DLNA) neural network allows for the most accurate energy consumption estimation of 93%.
The proposed model can be used by public transport companies to plan driving schedules and energy management when introducing electric buses.

title: Client-based Intelligence for Resource Efficient Vehicular Big Data Transfer in Future 6G Network [arXiv]
abstract: Vehicular big data is anticipated to become the "new oil" of the automotive industry which fuels the development of novel crowdsensing-enabled services.
However, the tremendous amount of transmitted vehicular sensor data represents a massive challenge for the cellular network.
A promising method for achieving relief which allows to utilize the existing network resources in a more efficient way is the utilization of intelligence on the end-edge-cloud devices.
Through machine learning-based identification and exploitation of highly resource efficient data transmission opportunities, the client devices are able to participate in overall network resource optimization process.

In this work, we present a novel client-based opportunistic data transmission method for delay-tolerant applications which is based on a hybrid machine learning approach: Supervised learning is applied to forecast the currently achievable data rate which serves as the metric for the reinforcement learning-based data transfer scheduling process.

In addition, unsupervised learning is applied to uncover geospatially-dependent uncertainties within the prediction model.
In a comprehensive real world evaluation in the public cellular networks of three German Mobile Network Operators (MNOs), we show that the average data rate can be improved by up to 223 % while simultaneously reducing the amount of occupied network resources by up to 89 %.

As a side-effect of preferring more robust network conditions for the data transfer, the transmission-related power consumption is reduced by up to 73 %.
The price to pay is an increased Age of Information (AoI) of the sensor data.

title: On modelling strategy of the weld line for tailor-welded structures under quasi-static and dynamic scenarios
abstract: Tailor-welded blanks (TWBs) have been widely applied in the automotive industry owing to better balance between light weight and crashworthiness.
The obvious feature of the TWBs is the existence of weld lines.
This paper numerically compares the modelling strategies of the weld line under quasi-static and dynamic events.
A novel method with crossover scheme is proposed.
For the quasi-static tests, those existing modelling methods such as rigid links (RL), onefold beam (OB)/twofold beam (TB), shell (SH)/solid model (SN) and new crossover model are compared.
Note that the SH/SN model is considered to be accurate and as the baseline model.
Through the deformation analysis and equivalent plastic strains under the static scenario, it indicates that the novel scheme is superior to other modelling methods.
Then, the method is further applied to dynamic impacting.
Those results provide important information for numerical modelling especially when TWBs are fabricated through different materials with a relatively higher thickness ratio.

title: Advances in Game-Theoretic, Set-Theoretic and Optimal Control to Enhance Mobility
abstract: 

title: SDN-Enabled Efficient Resource Utilization in a Secure, Trustworthy and Privacy Preserving IOV-Fog Environment
abstract: 

title: Stochastic Optimal Control based Reinforcement Learning for Electric Vehicle Charging and Residential Demand Response: Efficiency and Scalability
abstract: 

title: Improved Spatial Invariance for Vehicle Platoon Application using New Pooling Method in Convolution Neural Network
abstract: imbalanced dataset is a prominent concern for automotive deep learning researchers.
The proposed work provides a new mixed pooling strategy with enhanced performance for imbalanced vehicle dataset based on Convolution Neural Network (CNN).
Pooling is crucial for improving spatial invariance, processing time, and overfitting in CNN architecture.
Max and average pooling are often utilized in contemporary research articles.
Both techniques of pooling have their own advantages and disadvantages.
In this study, the advantages of both pooling algorithms are evaluated for the classification of three vehicles: car, bus, and truck for imbalanced datasets.
For each epoch, the performance of max pooling, average pooling, and the new mixed pooling method was assessed using ROC, F1-score, and error rate.
Comparing the performance of the max-pooling method to that of the average pooling method, it has been found that the max-pooling method is superior.
The performance of the proposed mixed pooling approach is superior to that of the maximum pooling and average pooling methods.
In terms of Receiver Operating Characteristics (ROC), the proposed mixed pooling technique is approximately 2 per cent better than the maximum pooling method and 8 per cent better than the mixed pooling method.

Using a new pooling technique, the classification performance with an imbalanced dataset is improved, and also a novel mixed pooling method is proposed for the classification of vehicles.

title: Boost converter for low voltage energy harvesting applications: Basic component selection
abstract: Regulating energy from tiny sources such from ambient vibration to supply a continuous energy to power an electronics device is a beneficial and challenging work to do.
Having very small energy usually less than 1 V, needs a boost circuit to power up the device.
This paper shows that the passive components of a conventional boost converter play an important role to boost very small voltage.
This paper also presents that switching frequency effects to boost voltage.
Transient analyses have been performed using PSpice simulation tool to check the circuit response to various parameter changes.
The proposed circuit with components L=160 muH, RL=10 kOmega and switching frequency fS=1 kHz with duty cycle D=0.5 results optimum performance of the circuit.
This voltage converter is suitable for vibration energy harvesting having low frequency applications in kHz range.
The application of this converter is expected to be in automotive, healthcare and industrial field.

title: Data-Driven Permanent Magnet Temperature Estimation in Synchronous Motors With Supervised Machine Learning: A Benchmark
abstract: Monitoring the magnet temperature in permanent magnet synchronous motors (PMSMs) for automotive applications is a challenging task for several decades now, as signal injection or sensor-based methods still prove unfeasible in a commercial context.

Overheating results in severe motor deterioration and is thus of high concern for the machine's control strategy and its design.
Lack of precise temperature estimations leads to lesser device utilization and higher material cost.
In this work, several machine learning (ML) models are empirically evaluated on their estimation accuracy for the task of predicting latent high-dynamic magnet temperature profiles, specifically, ordinary least squares, support vector regression, k-nearest neighbors, randomized trees, and neural networks.

Having test bench data available, it is shown that ML approaches relying merely on collected data meet the estimation performance of classical thermal models built on thermodynamic theory.
Through benchmarking, this work reveals the potential of simpler ML models in terms of regression accuracy, model size, and their data demand in comparison to parameter-heavy deep neural networks, which were investigated in the literature before.

Especially linear regression and simple feed-forward neural networks with optimized hyperparameters mark strong predictive quality at low to moderate model sizes.

title: Design and Development of Adaptive EV Charging Management for Urban Traffic Environments
abstract: 

title: Maximizing Airtime Efficiency for Reliable Broadcast Streams in WMNs with Multi-Armed Bandits
abstract: Wireless broadcast routing is a complex problem, shown in the literature to be NP-complete.
Current protocols implement either heuristics to find solutions that are not guaranteed to be optimal or classic flooding.
However, many future use cases, like automotive applications, industrial robotics, and multimedia broadcast, will require efficient yet reliable methods.
In this work, we use contextual multi-armed bandits together with opportunistic routing (OR) and network coding (NC) to find approximately optimal solutions to the problem of broadcast routing in a distributed fashion.

Each router independently learns its own transmission credit, i.
e.
, the number of packets to forward for each innovative packet received, so that the airtime cost, subject to latency constraints, is minimized.

Results show that the proposed solutions, particularly the deep learning based one, vastly improve the overall reliability, while performing close to MORE multicast in terms of airtime and to B.
A.
T.
M.
A.
N.

in latency, both being the best candidates in the respective discipline among the tested ones.

title: Finding faults: A scoping study of fault diagnostics for Industrial Cyber-Physical Systems [arXiv]
abstract: Context: As Industrial Cyber-Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them.

Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains.
By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps.
Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature.
Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use.
These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches.
Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches.
We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory.
Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space.

While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems.
(Abridged).

[10.1016/j.jss.2020.110638].

title: Enabling Technologies and Applications for Networked Airborne Computing
abstract: 

title: Intelligent Flow Control of Connected Driverless Vehicles in Smart City Intersections
abstract: 

title: Multiple Target Tracking Using Random Finite Sets
abstract: 

title: An Exploration of Traffic Signal Control Using Multi-Agent Market-Based Mechanisms
abstract: 

title: Fast, Detailed, Accurate Simulation of a Thermal Car-Cabin Using Machine-Learning
abstract: Car-cabin thermal systems, including heated seats, air-conditioning, and radiant panels, use a large proportion of the energy budget of electric vehicles and thus reduce their effective range.
Optimising these systems and their controllers might be possible with computationally efficient simulation.
Unfortunately, state-of-the-art simulators are either too slow or provide little resolution of the cabin's thermal environment.
In this work, we propose a novel approach to developing a fast simulation by machine learning (ML) from measurements within the car cabin over a number of trials within a climatic wind tunnel.
A range of ML approaches are tried and compared.
The best-performing ML approach is compared to more traditional 1D simulation in terms of accuracy and speed.
The resulting simulation, based on Multivariate Linear Regression, is fast (5 microseconds per simulation second), and yields good accuracy (NRMSE 1.
8%), which exceeds the performance of the traditional 1D simulator.

Furthermore, the simulation is able to differentially simulate the thermal environment of the footwell versus the head and the driver position versus the front passenger seat, but unlike a traditional 1D model cannot support changes to the physical structure.

This fast method for obtaining computationally efficient simulators of car cabins will accelerate adoption of techniques such as Deep Reinforcement Learning for climate control.

title: Secure Data Communication in Vehicular Networks for Disaster Rescue
abstract: Ground recovery automobiles operating in the regions impacted by the catastrophe should coordinate their efforts and communicate a large amount of data with one another.
This data should include rescue orders, details on the state of the roadways, and data on prior rescue missions.
This will guarantee that you are able to drive safely and effectively respond.
Unmanned aerial vehicles (UAVs), which can be used to conduct instant recovery services in places that are destructed and assist in sharing data for ground Vehicular networks, can be utilised when connectivity investments are damaged as a consequence of climate change.

UAVs can help share data for field Iot environments and can conduct recovery services in areas that have been damaged.
However, in a tragedy scenario involving UAV-assisted IoV, there are possible safety threats on intelligence sharing among machines and UAVs.
These threats are the outcome of an unreliable distributed environment, unreliable improper behavior tracing, and minimal information.
These issues could compromise the integrity of shared data.
The interconnection of the relevant vehicles and unmanned aerial vehicles (UAVs) makes these dangers possible.
In this article, we create a lightweight vehicular blockchain-enabled secure (LVBS) data exchange architecture for the Automotive Internet of Things that is helped by unmanned aerial vehicles (UAVs) to assist in hurricane relief.

The concerns which have been voiced will be addressed more effectively as a result of this.
To begin, we will discuss the new UAV and smart contracts collaboration high - altitude system architectures that we have developed for usage in places that have been affected by natural disasters.
A resource consensus method is developed for the lighter vehicular ecosystem in the second step of our project.
This not only makes it possible for us to track and record data exchanges in a manner that is both safe and immutable, but it also helps us arrive at a majority more quickly while ensuring that its degree of safety is not compromised.

In the end, comprehensive models are tested, which demonstrate that LVBS has the ability to effectively increase the safety of the consensual phase and stimulate the data exchange of a great quality.
This is shown by the fact that LVBS has been shown to have this capacity.

title: ERI: Improving the Learning Efficiency of Adaptive Optimal Control Systems in Information-Limited Environments
abstract: This Engineering Research Initiation (ERI) grant will fund research that enables efficient, on-the-fly learning of optimal control strategies for complex engineering systems operating in uncertain environments, with application to connected and autonomous vehicles, thereby promoting the progress of science and advancing the national prosperity and welfare.

Many emerging control systems in the artificial intelligence, automotive, robotic, and energy fields require that optimal actions be identified and executed across a network of components in the absence of detailed system knowledge and based on limited input data.

Learning-based approaches have been developed to meet this requirement, but are challenged by very slow rates of learning, restrictive requirements on the control policy used to initiate the learning process, and complications due to sensor limitations and sparse data sharing between individual components.

This project will overcome these challenges by building a new learning-efficient control framework that integrates advantages of existing methods and demonstrates new solutions for handling missing data streams and optimizing the communication structure between system components.

When applied to networks of autonomous vehicles in complex traffic scenarios, the framework may enable improvements in roadway safety and reduction in road fatalities.
The broader impacts of this project include outreach efforts to the public intended to show how artificial intelligence and automatic control can be safely leveraged, as well as training and preparation of undergraduate and graduate students to pursue further education and advanced STEM careers.
&lt;br/&gt;&lt;br/&gt;This research aims to make fundamental contributions to the development of a learning-efficient, adaptive optimal control framework for nonlinear dynamical systems with completely unknown system models and under conditions of partial observability, and to enable the application of this framework to networked control systems with nontrivial communication topologies.

It will achieve this outcome by developing a new hybrid iterative form of reinforcement learning that achieves a quadratic rate of convergence even if a system model and an initial admissible control policy are unavailable.

Inspired by ideas from hierarchical reinforcement learning, a two-layer learning-efficient method will be created to enable simultaneous learning of robust distributed control strategies for individual network agents and an optimal network communication topology, including in the presence of communication delays between agents.

Micro-traffic simulations and physical experiments will be used to test the theoretical framework in the context of collision avoidance in several formation control scenarios.
&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


title: Building High Performance Wireless Rechargeable Sensor Networks and Vehicular Networks by Algorithm Designs
abstract: 

title: Autonomous Driving Dynamic-programming Algorithm Based on Improved Artificial Potential Field
abstract: Decision-making algorithms for autonomous driving can be divided into end-to-end and sequential planning algorithms.
Sequential algorithms are adopted by most OEMs because of their interpretability and robustness.
Planning module is the core of the sequential algorithm.
It receives information from a perception module and high-definition map and outputs the driving trajectories or actions.
Artificial potential field (APF) method, which is widely used in planning algorithms for autonomous driving, is becoming increasingly popular owing to its excellent planning efficiency and information extraction capability.

However, APF does not consider destination factor, and single-point destination gravitational field causes a large force, resulting in incorrect directions for long-distance cases, and cannot cope with complex traffic environments.

In response to these problems, this study proposed "Driving Intention & Risk Field" (IRF) to model traffic factors including destination, vehicles, and road boundaries and consider their characteristics separately and then in combination.

A global gravitational field considering the global route was created, and a global planned path was discretized into equidistant path points.
The path points within the range of the interest area were dynamically selected to construct a global gravitational field.
To verify the performance of the IRF, an IRF-SAC decision-planning algorithm platform was built, and highway, urban crossroad, and roundabout scenes were set in a CARLA simulation environment.
The research results show that compared with NF-SAC and FSM, the IRF-SAC algorithm significantly improves safety, comfort, and vehicle-passing efficiency.
In the highway scenario, IRF-SAC achieves high accuracy and robustness in path tracking, and the maximum displacement errors are reduced by 44.
8% and 70.
2% compared with the FSM and NF-SAC algorithms, respectively.

In the crossroad scenario, the average risk coefficients are reduced by 12.
0% and 20.
6%, and root mean squares of the longitudinal acceleration are reduced by 13.
2% and 44.
9%, compared with the NF-SAC and FSM algorithms, respectively.

Moreover, the driving time is reduced by 39.2% compared with the FSM algorithm.
In the roundabout scenario, the average risk coefficient is reduced by 31.
7% and 52.
9%, and the root mean square of the longitudinal acceleration is reduced by 27.
0% and 19.
0%, compared with the NF-SAC and FSM algorithms, respectively.


title: A Compact Integrated DM-CM Filter with PCB Embedded DC Current Sensor for High Altitude High Current Applications
abstract: This paper presents a novel multi-turn solution for integrated DM-CM filter used in DC-fed motor drive system.
The proposed solution is targeted for high-current and high-altitude applications by stacking multiple PCB(s) and interconnects.
A ladder network is proposed to realize the multi-turn solution, while edge plating is proposed for filter grounding.
A Printed Circuit Board (PCB) based Active Magneto Resistive sensor (AMR) is embedded within the PCB dielectric to measure DC side current.
PCB based feedthrough connection is implemented to improve DC filter attenuation characteristics at high frequency.
Challenges pertaining to partial discharge (PD) free operation, high current and grounding of the filter are discussed.
The filter and enclosure are tested in a high-altitude chamber for partial discharge.
The integrated AMR sensor is tested using Silicon Carbide (SiC) enabled three level inverter under open loop RL load tests.
The proposed structure is highly competitive compared to the state-of-the-art technologies and is recommended for low weight requirements.

title: Prognostics Health Monitoring (PHM) for Prior Damage Assessment in Electronics Equipment Under Thermo-Mechanical Loads
abstract: Methodologies for prognostication and health monitoring (HM) can significantly impact electronic reliability for applications in which even minimal risk of failure may be unbearable.
Presently, HM approaches such as the built-in self-test are based on reactive failure diagnostics and unable to determine residual-life (RL) or estimate residual-reliability.
Prognostics health-monitoring (PHM) approach presented in this paper is different from state-of-art diagnostics and resides in the prefailure-space of the electronic system, in which no macro-indicators such as cracks or delamination exist.

Applications for the presented PHM framework include, consumer and defense applications such as automotive safety systems including front and rear impact protection systems, chassis-control systems, x-by-wire systems, and defense applications such as avionics systems, naval electronic warfare systems.

The presented PHM methodologies enable the estimation of prior damage in deployed electronics by the interrogation of the system state for systems in which the prior stress-history may be unknown or unavailable.

The primary focus is on thermo-mechanical stresses.
The presented methodologies will trigger repair or replacement, significantly prior to failure.
The approach involves the use of condition monitoring devices which can be interrogated for damage proxies at finite time-intervals.
The system's residual life is computed based on residual-life computation algorithms.
Previously, we have developed several leading indicators of failure.
In this paper, a mathematical approach has been presented to calculate the prior damage in electronics subjected to cyclic and isothermal thermo-mechanical loads.
Electronic components operating in a harsh environment may be subjected to both temperature variations in addition to thermal aging during use-life.
Data have been collected for leading indicators of failure for 95.5Sn4Ag0.5Cu first-level interconnects under both single and sequential applications of cyclic and isothermal thermo-mechanical loads.
Methodology for the determination of prior damage history has been presented using non-linear least-squares method based on interrogation techniques.
The methodology presented used the Levenberg-Marquardt Algorithm.
The test vehicle includes various area-array packaging architectures soldered on immersion Ag finish, subjected to thermal cycling in the range of -40 degrees C to 125 degrees C and isothermal aging at 125 degrees C.


title: Adaptive dynamic programming for uncertain nonlinear systems through coupling of nonlinear analysis and data-based learning
abstract: Optimal control methods provide a means to associate a user-defined cost with control actions or decisions.
These methods have made pervasive impacts in a wide class of application domains.
The shift towards autonomy by the automotive industry in examples such as replacing mechanical systems with computer controlled electronic control systems, has resulted in efficiencies in fuel injection, braking, throttling, etc.

For electric vehicles, fuel economy, drivability, and emission control are functions of the engine design and the control strategies.
For robotic systems, the desire to achieve optimal behavior is essential for efficient task execution.
Likewise, aerospace systems have always been a mainstream application domain for optimal control methods because there has always been (and always will be) a tight coupling between performance and energy/fuel costs.

As the cost of energy and the awareness of the environmental impacts of producing energy have risen, optimal control may now play a timely role in a broader spectrum of application domains.
The Energy Efficiency and Renewable Energy office of the U.S. Department of Energy indicates that as much as 10-20 percent of American energy use could be saved by optimizing industrial systems.
It is self-evident that optimal control solutions can have significant impacts in a wide range of industries, but the development of optimal control solutions for real engineering systems is limited by numerous technical barriers.

The most fundamental and open-ended problems arise from barriers associated with developing optimal solutions in the presence of uncertainty.
The driving question in this project is how to arbitrate between gaining knowledge about a system while simultaneously making the optimal control decision for engineering systems that are uncertain and complex.
<br/><br/>The technical aims of this project are motivated by the hypothesis and observations from preliminary efforts that nonlinear analysis methods can be exploited to design real-time approximate optimal solutions, while concurrent background processing methods can be used to update the optimal control approximation for improved performance.

Intellectual merits in this project are realized through the development of classes of closed-loop controllers with associated stability analysis, and advanced function approximation methods, that ensure sufficient exploration of the system response while learning the approximate optimal control solution.

Outcomes of this research would allow for optimal control implementation in a broader class of application domains where the system exhibits nonlinear behaviors and uncertainty.
The broad impact of this framework is a merger of methods that bridge the gap between the computational intelligence community and control systems community to enable data-based learning methods to optimize control performance.


title: Autonomous Control of an All-Terrain Vehicle Using Embedded Systems and Artificial Intelligence Techniques
abstract: 

title: Robustness of AI-based prognostic and systems health management
abstract: Prognostic and systems Health Management (PHM) is an integral part of a system.
It is used for solving reliability problems that often manifest due to complexities in design, manufacturing, operating environment and system maintenance.
For safety-critical applications, using a model-based development process for complex systems might not always be ideal but it is equally important to establish the robustness of the solution.
The information revolution has allowed data-driven methods to diffuse within this field to construct the requisite process (or system models) to cope with the so-called big data phenomenon.
This is supported by large datasets that help machine-learning models achieve impressive accuracy.
AI technologies are now being integrated into many PHM related applications including aerospace, automotive, medical robots and even autonomous weapon systems.
However, with such rapid growth in complexity and connectivity, a systems' behaviour is influenced in unforeseen ways by cyberattacks, human errors, working with incorrect or incomplete models and even adversarial phenomena.

Many of these models depend on the training data and how well the data represents the test data.
These issues require fine-tuning and even retraining the models when there is even a small change in operating conditions or equipment.
Yet, there is still ambiguity associated with their implementation, even if the learning algorithms classify accordingly.
Uncertainties can lie in any part of the AI-based PHM model, including in the requirements, assumptions, or even in the data used for training and validation.
These factors lead to sub-optimal solutions with an open interpretation as to why the requirements have not been met.
This warrants the need for achieving a level of robustness in the implemented PHM, which is a challenging task in a machine learning solution.
This article aims to present a framework for testing the robustness of AI-based PHM.
It reviews some key milestones achieved in the AI research community to deal with three particular issues relevant for AI-based PHM in safety-critical applications: robustness to model errors, robustness to unknown phenomena and empirical evaluation of robustness during deployment.

To deal with model errors, many techniques from probabilistic inference and robust optimisation are often used to provide some robustness guarantee metric.
In the case of unknown phenomena, techniques include anomaly detection methods, using causal models, the construction of ensembles and reinforcement learning.
It elicits from the authors' work on fault diagnostics and robust optimisation via machine learning techniques to offer guidelines to the PHM research community.
Finally, challenges and future directions are also examined; on how to better cope with any uncertainties as they appear during the operating life of an asset.

title: EAGER: Real-Time: Reinforcement, Meta, and Episodic Learning for Control under Uncertainty
abstract: Machine learning and artificial intelligence are among the most important general purpose technologies for the coming decades, with potential to transform all aspects of society from health, to manufacturing, to business, to education, and to security.

In the last decade, there have been very important and impressive advances in machine learning driven by the use of deep neural networks, innovative training algorithms, computational resources including specialized hardware (graphics processors, tensor processing units), and large datasets.

Some of these developments have connections to emerging understanding from neuroscience on how the human brain learns to makes decisions in real-time.
However, there are major challenges in the use of these techniques in real-time control and decision making for engineering systems where stability, reliability, and safety are paramount concerns.
This project aims to connect major advances in machine learning and neuroscience to control systems and thereby advance myriad application domains.
Modern engineered systems are increasingly complicated.
They comprise large heterogeneous distributed networks of (IoT) connected devices, systems, and human/social agents, e.g., transportation, energy, water, manufacturing, health and agriculture.
A major challenge is performance, stability and reliability of these systems under large uncertainties.
The goal is to expand our understanding and integration of learning and control to derive principles and algorithms for the development of learning-based control systems for a variety of engineering applications.

<br/><br/>While there are significant historical connections between reinforcement learning and stochastic dynamic control, the potential for leveraging ongoing and future advances in machine learning for control remains significantly under- explored.

The field of control systems has deep and solid theoretical and mathematical foundations with comprehensive and well-established frameworks for linear, nonlinear, robust, adaptive, stochastic, distributed, and model-predictive control systems.

Equally importantly, control systems have applications in multiple domains, such as aerospace, automotive, manufacturing, energy, transportation, agriculture, water, and many other engineered and socio-technical systems.

Despite this rich spectrum of theoretical foundations and important applications, the domain of applicability of traditional control techniques is limited to situations where good mathematical models of the underlying systems are available, and where the environmental uncertainty is not too large.

This exploratory research project is aimed at overcoming these limitations via novel problem formulations in systems and control inspired by new insights coming from recent developments in machine learning.

A key focus will be on novel control architectures inspired by neuroscience and reinforcement learning.
Besides architectural innovations, the project will explore questions of stability, performance, and uncertainty by integrating ideas from rapid (one-shot) learning, meta-learning, and episodic control into control algorithms.

The ideas from this project will be at the core of a new graduate level course in learning for control which will be taught at the University of California, Irvine.
The resulting course materials will be made available to the research community and will benefit interested graduate students across the nation.
In addition, short courses will be offered at major professional conferences, e.
 g.
, American Control Conference, IEEE Conference on Decision and Control.
<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


title: Next generation intelligent driver-vehicle-infrastructure cooperative system for energy efficient driving in connected vehicle environment
abstract: 

title: Optimal sizing and learning-based energy management strategy of NCR/LTO hybrid battery system for electric taxis
abstract: This paper proposes an offline sizing method and an online energy management strategy for the electric vehicle with semi-active hybrid battery system (HBS).
The semi-active HBS is composed by Nickel Cobalt Rechargeable (NCR) and lithium titanate (LTO) batteries with a bi-directional DC/DC converter.
First, the vehicle dynamics and the HBS are modelled.
Second, a hierarchical optimal sizing method is proposed to minimize the distance-based cost (DBC) of electric taxi in a variety of driving cycles.
The lower layer optimizes the energy management strategy (EMS) with dynamic programming (DP), while the upper layer optimizes the sizes of HBS for minimum DBC.
Based on the sizing results, the DBC decreases firstly and then increases with the increasing LTO size.
In addition, the results of DP indicate the SOC of the LTO batteries works between 50% and 80% for optimal NCR lifespan.
Third, by using the rule extracted from DP, a learning-based EMS, i.e., deep deterministic policy gradient (DDPG), is proposed with excellent real-time control potential.
Finally, the simulation results show that the proposed DDPG EMS achieves the improved performance than fuzzy logic control EMS and closed result with what can be achieved through DP, yet the computation time is much less.

(c) 2022 Elsevier Ltd. All rights reserved.

title: A data-driven approach for collaborative optimization of large-scale electric vehicles considering energy consumption uncertainty
abstract: With the explosive growth of electric vehicles (EVs), it is an urgent task to incorporate low-carbon EVs with advanced optimization strategies to achieve orderly charging-discharging and economical operations for EVs.

Nevertheless, current charging-discharging optimization strategies for EVs may be impractical since their energy consumption uncertainty and the driving trip cost are generally not considered.
Therefore, a collaborative optimization model for large-scale EV charging-discharging with energy consumption uncertainty in this paper is proposed to simultaneously maximize passenger revenue and reduce the costs of the driving, charging-discharging, and battery depletion.

Subsequently, a data-driven approach is developed to tackle the model.
In this approach, an uncertainty predictor based on wavelet transform, deep deterministic policy gradient, and quantile regression is first applied to estimate the energy consumption uncertainty.
Then, an adaptive learning rate firefly algorithm is presented to identify the most satisfactory solution for the optimization model.
Finally, taking the actual data of 300 EVs in a city in China as case studies, the simulation results reveal that the proposed method is effective and has high application significance.

title: A Self-Learning Intersection Control System for Connected and Automated Vehicles
abstract: 

title: Technologies of virtual scenario construction for intelligent driving testing
abstract: With the continuous improvement of vehicle intelligence,the interaction of vehicles with the surrounding environment through perception is increasing.
The environment that needs to be dealt with,including many factors such as roads,surrounding traffic and weather conditions,is becoming increasing complex.
Limited by the development cycle and cost,especially safety factors and the consideration of complex and diverse working conditions,traditional open road or closed field tests are difficult to meet the requirements of intelligent driving testing.

Therefore,simulation test based on digital virtual technology has become a new important means for intelligent driving testing and verification.
The simulation test mainly adopts a combination of accurate physical modeling,efficient numerical simulation,and high-fidelity image rendering to realistically construct human-vehicle environment models,including vehicles,roads,weather and lighting,and traffic, and various types of vehicles.

The construction of virtual scenarios is a key technology simulation and is particularly important for improving the pressure and acceleration of intelligent driving testing.
The virtual scenarios can meet the needs of a large number of diverse test samples to reflect the complex and changeable application environment of intelligent driving.
They can also provide a large number of labeled datasets for machine learning that can contain rich data with boundary feature scenario content and lay a solid data foundation for deep learning perception and reinforcement learning planning algorithms.

Therefore,the simulation scenario construction technology for intelligent driving test has been investigated worldwide in the current automotive intelligence.
As an emerging technology,it still faces many challenges,and its methods need to be studied in depth.
This paper systematically expounds the progress and current situation of domestic and foreign studies in simulation scenario construction technology,including automatic scenario construction methods and traffic simulation modeling methods,and focuses on some issues worthy of in-depth study.

In the research of scenario construction methods, the key elements and characteristics of the limited scenarios can reflect the infinite richness and complex driving environment.
A deep understanding of the network structure and mutual coupling of the scenario is essential for the research of virtual scenario construction.
Establishing a description method of the scenario limit and boundary characteristics to form the scenario automated generation method can maximize the potential of accelerated testing of intelligent driving.

Researchers have promoted the rapid development of scenario generation technology from different perspectives.
However,they often use parameter traversal search ideas to determine the system state space.
The development and testing is time consuming and labor intensive due to the unlimited expansion of scenario search.
The construction of a scenario with dangerous characteristics requires in-depth exploration of the safety boundary of the ego vehicle driving.
Thus,the constructed corner scenario can provide effective information corresponding to real driving for realizing the enhanced generation of the corner characteristics of the scenario.
This condition responds to the accelerated testing of intelligent driving systems above level four.
In terms of traffic modeling methods,a deep understanding of the driving behavior and interaction characteristics of vehicles is the basis and primary task.
Determining the influence law of vehicle driving motion in data information and establishing the traffic model with random dangerous characteristics are the key to realize intelligent driving testing.

title: Software Mechanisms for Pervasive and Autonomous Computing
abstract: 

title: Harnessing Big Data for the Sharing Economy in Smart Cities
abstract: 

title: Price-Guided Cooperation of Combined Offshore Wind and Hydrogen Plant, Hydrogen Pipeline Network, Power Network, and Transportation Network
abstract: Nowadays, the combined offshore wind farm and hydrogen production plant (COWHP) has been installed all around the world and will play an important role in reducing the carbon dioxide emissions.
However, the cooperation of the COWHP, the hydrogen pipeline transportation, the power network transmission, and the hydrogen fuel cell electric vehicles (HFCEVs) scheduling is still an essential problem.

This problem falls into the category of mixed-integer nonlinear optimization, which involves a significant number of decision variables and constraints.
Existing methods have often struggled to effectively solve this problem due to its inherent complexity and non-linear nature.
In this paper, a price-guided method is proposed to cooperate with this complex system.
First, the COWHP model, the hydrogen pipeline network scheduling model, the power network flow model, the HFCEVs refuelling microgrid model, and the HFCEV traffic flow model in real-world transportation networks are presented.

Second, the price-based cooperation model is proposed.
Third, price strategies are deployed to cooperate with the complex system.
The decision making trial and evaluation laboratory-the technique for order preference by similarity to ideal solution (Dematel-TOPSIS) method is deployed to evaluate the real-time performance of different strategies.

The simulation results reveal that in the small vehicle flow transportation network cases, deep deterministic policy gradient (DDPG) has the best real-time evaluation performance; whereas for the large vehicle flow transportation network cases, long short term memory (LSTM) has the best real-time performance.

From the posterior evaluation view, LSTM has the best performance for all cases to reduce total operation cost, and also reduce the total waiting time.

title: Toolpath Generation and Optimization for Metal Additive Manufacturing
abstract: 

title: Impact of heat treatment and oxidation of Carbon-carbon composites on microstructure and physical properties
abstract: 

title: Deep Learning and GPU-Accelerated Algorithms for Computer-Aided Engineering
abstract: 

title: Building Transformative Framework for Isolation and Mitigation of Quality Defects in Multi-Station Assembly Systems Using Deep Learning
abstract: 

title: Investigations into the Design Rules for the Control of Wire Arc Additive Manufacturing
abstract: This award advances the understanding of defect formation in wire arc additive manufacturing, leading to improved processing and greater process control.
This research addresses surface waviness and nonuniform wall thickness challenges in wire arc additive manufacturing, making it acceptable for many U.
S.
 industries, including aerospace, defense, and automotive, and thereby benefiting the nations economy and well-being.

Wire arc additive manufacturing uses a welding arc as the energy source in making three-dimensional objects with high throughput.
This process can be easily integrated with existing computer-numerical-control routers, or robot arms.
Thus, the knowledge and methodology developed through this project can be directly transferred to small- and medium-sized enterprises interested in making or repairing metallic parts.
Additionally, this project develops the professional skills of K-12, undergraduate, and graduate students, including women and underrepresented minorities, by training them through a unique set of integrated education and multidisciplinary research opportunities in the areas of wire arc additive manufacturing and data analytics.

Accordingly, students are familiarized with emerging technology-intensive manufacturing and data science disciplines, thereby preparing a future workforce equipped with these new skills and knowledge.
&lt;br/&gt;&lt;br/&gt;The two overarching research goals of this project are to (1) gain fundamental knowledge about surface waviness and effective wall thickness formation mechanisms in wire arc additive manufacturing and (2) investigate the design rules for the monitoring and control of the manufacturing process.

An integrated experimental-characterization and theoretical-modeling framework are considered to achieve these goals.
The surface quality of wire arc additively manufactured structures is controlled by fine-tuning and balancing the surface tension, arc force, droplet impact, gravity, buoyancy, and friction for different manufacturing conditions.

The project framework consists of four components: (1) measurement and analysis of real-time process signatures (e.
g.
, voltage and current) and visualization data of the arc, droplet, and weld pool features; (2) data measurements and analysis of defect (e.
g.
, balling effect and voids) generation and propagation as a function of process parameters, wall thickness and inclination, and multi-bead/multilayer deposition; (3) characterization of the surface tension-based computational fluid dynamics model and its verification and validation through experiments; and (4) establishment of design rules using data-driven, low-fidelity surrogate models.

Machine learning algorithms (e.
g.
, convolutional neural network, AnoGAN, transfer learning, and reinforcement learning) are developed in detail for defect detections and classifications as well as process control.

This integrated framework provides unique, transformative, and efficient opportunities to synergistically understand the arc, droplet, and weld pool characteristics and defect formation in wire arc additive manufacturing.
&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


title: Advances in modeling emerging magnetoresistive random access memories: from finite element methods to machine learning approaches
abstract: Emerging spin transfer torque magnetoresistive random access memories (STT MRAM) are nonvolatile and offer high speed and endurance.
They are promising for stand-alone and embedded applications in the automotive industry, microcontrollers, Internet of Things, frame buffer memory, and slow SRAM.
The MRAM cell usually includes a CoFeB fixed reference layer and a free magnetic layer (FL), separated by a tunnel barrier.
To design ultra-scaled MRAM cells it is necessary to accurately model the torques acting on the magnetization in composite magnetic layers with one or several nonmagnetic inclusions between the ferromagnetic parts.

The magnetization dynamics is governed by the Landau-Lifshitz- Gilbert (LLG) equation supplemented with the corresponding torques.
The torques depend on nonequilibrium spin accumulation generated by an electric current.
The electric current and the spin accumulation also depend on the magnetization.
Therefore, the LLG and the spin-charge transport equations are coupled and must be solved simultaneously.
We apply the finite element method (FEM) to numerically solve this coupled system of partial differential equations.
To develop an open source solver, we use well-developed C++ FEM libraries.
The computationally most expensive part is the demagnetizing field calculation.
It is performed by a hybrid finite element-boundary element method.
This confines the simulation domain for the field evaluation to the ferromagnets only.
Advanced compression algorithms for large, dense matrices are used to optimize the performance of the demagnetizing field calculation in complex structures.
To evaluate the torques acting on the magnetization, a coupled spin and charge transport approach is implemented.
For the computation of the torques acting in a magnetic tunnel junction (MTJ), a magnetization-dependent resistivity of the tunnel barrier is introduced.
A fully three-dimensional solution of the equations is performed to accurately model the torques acting on the magnetization.
The use of a unique set of equations for the whole memory cell including the FL, fixed layer, contacts, and nonmagnetic spacers is one of the advantages of our approach.
To incorporate the temperature increase due to the electric current, we solve the heat transport equation coupled to the electron, spin, and magnetization dynamics, and we demonstrate that the FL temperature is highly inhomogeneous due to a non-uniform magnetization of the FL during switching.

Spin-orbit torque (SOT) MRAM is fast-switching and thus well suitable for caches.
By means of micromagnetic simulations, we demonstrate the purely electrical switching of a perpendicular FL by the SOTs due to two orthogonal short current pulses.
To further optimize the pulse sequence, a machine learning approach based on reinforcement learning is employed.
Importantly, a neural network trained on a fixed material parameter set achieves switching for a wide range of material parameter variations.

title: Dislocation Slip and Deformation Twinning in Face Centered Cubic Low Stacking Fault Energy High Entropy Alloys
abstract: 

title: Drift Counteraction Control: Theory and Applications
abstract: Drift Counteraction Control: Theory and Applications<br/>In many existing and emerging engineering systems there is an inherent tendency of process variables to drift.
In some systems this drift is caused by large, persistent disturbances due to interactions with the external environment.
In other systems, similar drift is caused by finite resources (fuel, energy, component life etc.)
being continuously depleted.
The operating objectives for such systems with drift are reflected in a set of constraints that must be satisfied for as long as possible by countering the drift, rather than in terms of usual set-point command tracking requirements.

This research project advances theory and methods for designing control algorithms that perform drift counteraction thereby establishing a foundation for addressing practical drift counteraction applications, and for incorporating drift counteraction technology into industrial products.

These advances will represent contributions to several areas of control theory including stochastic control, set-theoretic control, game-theoretic control, constrained control, and nonlinear control.
The developments in theory will proceed in close synergy with the demonstration of their benefits in several engineering applications to automotive and aerospace systems and dynamic motion simulators.
The research results will be integrated into university courses for graduate students, short courses for industrial and academic audiences, and into the computational software package implementing drift counteraction control design techniques.

<br/>The drift counteraction control is based primarily on constraints and does not use conventional set-points, differently from most of the traditional control theory.
The development of effective techniques for drift counteraction will be based on treating external disturbances causing drift and constraint violation first in the stochastic and then in the deterministic setting.

In the stochastic setting, advances in Stochastic Drift Counteraction Optimal Control (SDCOC) that maximizes the expected cost before the imposed constraints are violated will be pursued.
Stability and boundness conditions for closed-loop trajectories of systems operating under SDCOC control laws when disturbances settle to constant values or vary within a subset of the full range will be derived.

Sub-optimality and convergence properties of receding horizon and reinforcement learning variants of SDCOC will be studied.
Advances in SDCOC computational procedures will be made, firstly, by exploiting decomposition of the system dynamics into slow and fast subsystems and, secondly, based on Markov Chain models that use Fuzzy Encoding (MCFE).

By combining SDCOC and MCFE, evidence based control framework for drift counteraction in uncertain systems will be defined and complemented with estimation algorithms.
In the deterministic setting, set-theoretic, game-theoretic and disturbance estimation/cancellation based approaches to drift counteraction problems will be pursued.
The advances will focus on formulation and derivation of algorithms, the improvements in their off-board and on-board computations, and characterization of closed-loop properties (constraint violation time, ability to prevent constraint violation over an infinite time interval, trajectory bounds, etc.
).

Synergistically with the theoretical advances, several practical applications of drift counteraction control will be considered.
These applications will include increasing comfort and fuel efficiency of adaptive cruise control systems for conventional and hybrid passenger vehicles, extending range and life of electric and hybrid electric vehicles and their batteries, improving the ability to replicate motions in small scale dynamic motion simulators, and improving the capability and efficiency of spacecraft attitude control using momentum exchange devices.

For each application, its requirements will be reflected in a drift counteraction problem formulation, appropriate models will be established, drift counteraction control algorithms will be designed, and performance benefits will be quantified.

To ensure practical relevance of the results and facilitate their experimental validation in the vehicle adaptive cruise control case, interactions and collaborations with industry partners will be leveraged.


title: Smart Connected Buildings Design Automation: Foundations and Trends
abstract: Buildings are the result of a complex integration of multi-physics subsystems.
Besides the obvious civil engineering infrastructure, thermal, electrical, mechanical, control, communication and computing subsystems must co-exist and be operated so that the overall operation is smooth and efficient.

This is particularly important for commercial buildings but is also very relevant for residential buildings especially apartment buildings.
Unfortunately, the design and deployment of these subsystems is rarely synchronized: lighting, security, heating, ventilation and air conditioning systems are often designed independently.
However, simply putting together a collection of sub-systems, albeit optimized, has led to the inefficient buildings of today.
Worldwide, buildings consume 42% of all electrical power - more than any other asset - and it can be proven that much of this can be reduced if a holistic approach to design, deployment, and operation is taken.

Government agencies, academic institutions, building contractors and owners have realized the significant impact of buildings on the global environment, the electrical grid, and the mission of their organizations.

However, the economic impact for all constituencies is still difficult to assess.
Government regulations can play a fundamental role, as it has been the case for the transportation industry where regulations on emission and fuel consumption have been the single most important factor of innovation in automotive design.

We are convinced that by leveraging technology and utilizing a system-level approach to buildings, they will provide comfort, safety and functionality while minimizing energy cost, supporting a robust electric grid and mitigating environmental impact.

Realizing this vision requires adding intelligence from the beginning of the design phase, to deployment, from commissioning to operation, all the way to the end of the buildings life cycle.
In this issue, we attempt to provide an as-complete-as-possible overview of the activities in the field of smart connected building design automation that attempts to make the vision a reality.
The overarching range of such activities includes developing simulation tools for modeling and the design of buildings, and consequently control algorithms proposed to make buildings smarter and more efficient.

Furthermore, we will review real-world and large-scale implementation of such control strategies on physical buildings.
We then present a formal co-design methodology to design buildings, taking the view that buildings are prime examples of cyber-physical systems where the virtual and physical worlds meet as more traditional products such as thermostats are able to connect online and perform complicated computational tasks to control building temperature effectively.

We complete the presentation describing the growing role of buildings in the operation of the smart grid where buildings are not only consumers of energy, but are themselves also providers of services and energy to the grid.

The audiences for this monograph are industry professionals and researchers who work in the area of smart buildings, smart cities, and smart grid, with emphasis on energy efficiency, simulation tools, optimal control, and cyber-physical systems for the emerging power markets.


title: Development of a Method for Evaluating the Technical Condition of a Car's Hybrid Powertrain
abstract: The article presents the results of a study performed and substantiated based on the principles of a new method of diagnostics of technical conditions of a hybrid powertrain regardless of the structural diagram and design features of a hybrid vehicle.

The presented new technology of the diagnostics of hybrid powertrains allows an objective complex assessment of their technical condition by diagnostic parameters in contrast to existing diagnostic methods.

In the proposed method, a mechanism for the general standardization of diagnostic parameters has been developed as well as for determining the numerical values of the parameters of the powertrain.
The control subset was used to control the learning error.
As a result of debugging the system, the scatter of experimental and calculated points has decreased, which confirms the quality of debugging the tested fuzzy model.
As a result of training the artificial neural network, the standard deviation of the error in the control sample was 0.
012 center dot P-k.
 A symmetry method of diagnostics of the technical state of a hybrid propulsion system was developed based on the concept of a neural network together with a neuro-fuzzy control with an adaptive criteria based on the method of training a neural network with reinforcement.

The components of the vector functional include the criteria for control accuracy, the use of traction battery energy, and the degree of toxicity of exhaust gases.
It is proposed to use the principle of symmetry of the guaranteed result and the linear inversion of the vector criterion into a supercriterion to determine the technical state of a hybrid powertrain on a set of Pareto-optimal controls under unequal conditions of optimality.


